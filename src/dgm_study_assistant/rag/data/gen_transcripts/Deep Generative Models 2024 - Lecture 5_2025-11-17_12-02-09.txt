[00:00:00 - 00:00:04] ‫אז אנחנו נתחיל, שלום לכולם,
[00:00:06 - 00:00:08] ‫אני חושב שזה לא חוסר בעיה.
[00:00:12 - 00:00:17] ‫אנחנו נמשיך וזה כמו שקוראים לך דבר,
[00:00:18 - 00:00:20] ‫סיימנו את הדגימה,
[00:00:21 - 00:00:22] ‫אנחנו נהיה את הדגימה,
[00:00:23 - 00:00:25] ‫וזה זה על הלוח, ואז זה חלק שליש לשיעור
[00:00:26 - 00:00:30] ‫לשקפים ונתחיל לדבר על אבריה של אברינפריים.
[00:00:32 - 00:00:34] ‫אני אעלה את השקפים בהפקה,
[00:00:34 - 00:00:39] ‫צריך לשנות בצד את הספר ‫בשביל להערות את השקפים.
[00:00:40 - 00:00:46] ‫זה בעצם יהיה השיעור האחרון ‫שהוא כולו תיאורטי במובן של בלי ממש
[00:00:47 - 00:00:51] ‫לדבר על מודלים ‫שמתוססים על רשתות נוירוניות שיצטרכו לממש.
[00:00:51 - 00:00:55] ‫ואחר משבוע הבא אנחנו נתחיל לדבר ‫איך אפשר להשתמש כבר בעקרונות
[00:00:55 - 00:01:01] ‫שבאים לנו עכשיו כדי גם לעשות ‫לאמן מודלים שהם יותר מורכבים,
[00:01:02 - 00:01:05] ‫לדעת המורכב כמו תמונות.
[00:01:09 - 00:01:10] ‫ראיתם שפרסמתי תרגיל,
[00:01:11 - 00:01:12] ‫תרגיל שלישי, אז הוא יהיה ל-EM,
[00:01:14 - 00:01:15] ‫דיברנו
[00:01:16 - 00:01:19] על שני שיעורים בעיקר ‫וציימנו בשבוע שעבר ל-NCMC,
[00:01:21 - 00:01:22] ‫שנגמר דיבר בשבוע שעבר, ונסיים.
[00:01:23 - 00:01:29] ‫אוקיי, אז בואו נדבר רגע על דגימה. ‫ואנחנו רוצים לדגום מאיזושהי התפלגות
[00:01:32 - 00:01:34] ‫ולפתור כל מיני בעיות.
[00:01:35 - 00:01:38] ‫בשביל הדגימה עצמה היא בעצמה בעיה עכשיו.
[00:01:39 - 00:01:42] ‫זה בעצם מה שאנחנו מדברים עכשיו.
[00:01:43 - 00:01:43] ‫מתי הדגימה היא קלה?
[00:01:44 - 00:01:46] ‫אז ראיתם פה בתרגיל הראשון, אני חושב,
[00:01:47 - 00:01:51] ‫הן הוא דוגמה של דגימה מגאוסיאן רב-מימדי, ‫עם איזושהי התפלגות,
[00:01:52 - 00:01:53] ‫בין אישי קוווריאנס.
[00:01:54 - 00:01:55] ‫אז אם יש לנו
[00:01:56 - 00:01:59] משך x שווה התפלגות מורמאלית על x,
[00:02:01 - 00:02:03] ‫מאיזשהו וקטור מיור,
[00:02:04 - 00:02:06] ‫ואז יש קוווריאן סיגמא.
[00:02:06 - 00:02:08] ‫אז ראינו בתרגיל,
[00:02:09 - 00:02:12] ראיתם בתרגיל איך אפשר לדגום ‫באופן ישיר מהתפלגות הזאת.
[00:02:13 - 00:02:14] ‫אפשר לך עוד?
[00:02:16 - 00:02:17] ‫נגיד שיש לנו,
[00:02:17 - 00:02:21] ‫אנחנו יכולים לדגום ‫מהתפלגות גאוסיאנית חד-מימדית.
[00:02:21 - 00:02:29] ‫איך אנחנו משתמשים בזה ‫כדי לדגום ‫התפלגות גאוסיאנית פלילית כזאת, רב-מימדית?
[00:02:35 - 00:02:36] ‫יש לנו את ההמרה שכאילו,
[00:02:37 - 00:02:38] ‫מכל גאוסיאניה,
[00:02:39 - 00:02:41] ‫כחוץ לדאוסיאניה, לדאוסיאניה אחרת. ‫-איך את ההמרה הזאת?
[00:02:42 - 00:02:48] ‫יש לנו את המטריצה A, ‫שאנחנו מוכנים אותה ב-X שאנחנו נמצא, ‫ואנחנו נוסיף את המיון שם.
[00:02:49 - 00:02:50] ‫-כן, מה המטריצה A?
[00:02:51 - 00:02:52] ‫הסיגמה במינוס אחר.
[00:02:53 - 00:02:58] ‫לא, לא בדיוק. ‫-אז נרצה, אם אנחנו נתחיל ל-X,
[00:03:00 - 00:03:01] ‫X שווה ל...
[00:03:06 - 00:03:10] ‫זה כמו שאמרת, A כפול Z ועוד מי הוא,
[00:03:10 - 00:03:11] מי זה מי הוא זה?
[00:03:12 - 00:03:12] ‫Z
[00:03:14 - 00:03:14] התפלג
[00:03:15 - 00:03:16] ‫אוניברסיאנית
[00:03:17 - 00:03:19] ‫ואם תוכל את LFs
[00:03:20 - 00:03:21] ‫קוברנטס I,
[00:03:22 - 00:03:26] ‫זה אומר שגם אם זה ממד מאוד גבוה, ‫זה קל לי לעשות, כי זה פשוט,
[00:03:26 - 00:03:29] ‫כל הממדים הם בלתי תלויים, ‫אבל אני יכול לדגום אותם אחד-אחד.
[00:03:30 - 00:03:34] ‫אם יש לי פונקציה שרק לדעת ‫לתייצר לי גאוסיאניה אחת, ‫אני יכול לייצר את זה בפלוט,
[00:03:35 - 00:03:37] ‫ואז אם אני מכפיל אותו ב-A, ‫נוסיף לו את מי הוא,
[00:03:38 - 00:03:39] ‫כאשר A,
[00:03:40 - 00:03:43] ‫קוראים לזה ג'ולסקי.
[00:03:47 - 00:03:51] ‫קירוף כל הסיפורים האלה, ‫ביקרופורם.
[00:03:52 - 00:03:53] ‫אם
[00:03:57 - 00:03:57] ‫אם
[00:03:58 - 00:04:00] ‫אם אפשר לומר ש-A כפול A טרנספוז,
[00:04:01 - 00:04:01] ‫מתחבר כלילי.
[00:04:05 - 00:04:05] ‫אז
[00:04:06 - 00:04:07] בכל זה נקיים, אז הורצפי שה-X
[00:04:08 - 00:04:09] ‫מתפלג
[00:04:10 - 00:04:11] ‫כפי ש...
[00:04:17 - 00:04:30] ‫כן, תהליך ברור, תהליך מאוד פשוט ‫אז אנחנו מבקשים מנאמפאי ‫או משהו שזה לא יהיה, ‫תתן לנו את Z ‫במימד של X. ‫מכפילים אותו ב-A,
[00:04:31 - 00:04:33] ‫אז אנחנו צריכים לקחת את הסיגמה שלנו,
[00:04:34 - 00:04:37] ‫ולבקש מנאמפאי לתת לנו את ‫התירוך ג'ולסקי,
[00:04:38 - 00:04:44] ‫תכף זה נקרא Z. ‫מותן לנו את A, ‫אנחנו מכפילים את A ו-Z ‫ומוספים את מי הם.
[00:04:44 - 00:04:52] ‫זאת התופלת שלנו? ‫-כן, אנחנו מדברים X. ‫אם אנחנו דוגמים הרבה Zים ‫מההתפלגות המקורית, ‫יהיו לנו הרבה Xים,
[00:04:53 - 00:04:58] ‫וה-Xים האלה הם מתפלגים לפי ההתפלגות, ‫בסדר?
[00:04:59 - 00:05:02] ‫אז אחרתם את זה בעצם מתפלגין ארוך פעם.
[00:05:02 - 00:05:04] ‫אוקיי, זו דוגמה במצב שקרה לנו ‫להתגורם ממנו,
[00:05:05 - 00:05:08] ‫אבל הרבה פעמים אנחנו, יש לנו התפלגויות ‫יותר מורכבות,
[00:05:09 - 00:05:11] ‫ומה שהקדמנו, שבמשך עבר,
[00:05:12 - 00:05:20] ‫שקוראים לזה MC MC. ‫איך אנחנו דוגמים ‫התפלגות יותר מורכבת?
[00:05:24 - 00:05:26] ‫אנחנו בעצם בונים שרשרת מרקוב.
[00:05:26 - 00:05:29] ‫אנחנו מתארים לזה שרשרת מרקוב, ‫זה איזשהו תהליך
[00:05:30 - 00:05:34] ‫שבו כל צעד תלוי רק בצעד הקודם.
[00:05:35 - 00:05:37] ‫אז כאן יש לי איזושהי תפס,
[00:05:38 - 00:05:40] ‫לא תצטרך לייצר את גילס אחד,
[00:05:40 - 00:05:43] ‫אנחנו נותנים את זה בצעריים, ‫לא חה, הלאה.
[00:05:45 - 00:05:51] ‫יש לי משהו כלל הקודם, ‫אז יש לי איזשהו כלל מעבר ‫מהימה הקודמת לדגימה הנוכחית.
[00:05:52 - 00:05:53] ‫למשל, יש לי,
[00:05:54 - 00:05:55] ‫הכלל הזה יכול להיות הסתברותי.
[00:05:56 - 00:06:07] ‫יש לי איזשהו כלל של t של xt ‫בהינתן xt מינוס 1. ‫אז אנחנו בונים שרשרת כזאת, ‫ולמה אנחנו עושים את הדבר הזה? ‫כי אנחנו יודעים ששרשרת פרקוב
[00:06:07 - 00:06:08] ‫מתכנסת
[00:06:09 - 00:06:18] ‫לאיזושהי התפלגות סטציונרית, ‫ואם אנחנו יכולים לבנות את השרשרת ‫ככה שההתפלגות הסטציונרית ‫תהיה ההתפלגות שמעניינת אותנו,
[00:06:19 - 00:06:24] ‫אז אנחנו יכולים פשוט להריץ את השרשרת הזאת ‫ולקבל הרבה דגימות ‫מההתפלגות שמעניינת אותנו.
[00:06:24 - 00:06:29] ‫בהרבה מקרים אנחנו לא יודעים לדגום ‫באופן ישיר מהתפלגות, ‫אבל אנחנו יודעים לבנות שרשרת כזאת,
[00:06:30 - 00:06:33] ‫ככה שההתפלגות הסטציונרית ‫תהיא ההתפלגות שמעניינת אותה.
[00:06:34 - 00:06:37] ‫בסדר, זה היה כבר רעיון של אינסטינוסי.
[00:06:45 - 00:06:49] ‫ראינו קצת דוגמאות של נתן איזה מעניין, ‫אז ראינו למשל,
[00:06:51 - 00:06:53] ‫יש הרבה מקרים שאנחנו, ‫ההתבלגות שלנו מוגדרת
[00:06:55 - 00:06:56] ‫באמצע אחר,
[00:06:58 - 00:07:02] ‫על ידי איזשהו אקספוננט של איזושהי פונקציה, ‫שקבעה איזושהי פונקציה חצית מורכבת.
[00:07:04 - 00:07:08] ‫וכל הדבר הזה צריך שספור זה ‫בוא להיות מנורמל כדי שתהיה חייבת.
[00:07:09 - 00:07:11] ‫הרבה פעמים אנחנו יודעים ‫איך זאת נראית.
[00:07:12 - 00:07:21] ‫בנינו אותה לפי איזה שהם מגדלים, ‫למשל ששני מימדים של איקסים קרובים, ‫צריכים לקבל ערכים שווים, ‫אחרת הם מקבלים איזשהו
[00:07:22 - 00:07:29] ‫איזשהו פונקציה שמשלמת מחיר ‫על כל מיני דברים שקורים באיקסים.
[00:07:30 - 00:07:33] ‫אז זה אנחנו יודעים אולי לגנות, ‫אבל אנחנו לא יודעים לגנות את ההתבלגות,
[00:07:33 - 00:07:36] ‫כי אנחנו לא יודעים איך לנרמל את הדבר הזה ככה, ‫שבסופו של דבר זו תהיה התבלגות תקנית.
[00:07:40 - 00:07:44] ‫במקרים כאלה, הרבה פעמים, ‫אנחנו פעמים לפרק את F של X
[00:07:45 - 00:07:49] ‫לאוסף של כללים כאלה בין ה-X'ים, ‫בין הממדים של ה-X'ים השונים,
[00:07:50 - 00:07:53] ‫ואז קל לנו לבנות MCMC.
[00:07:55 - 00:07:56] ‫זה איזשהו תהליך כזה,
[00:07:56 - 00:08:00] ‫אני מזכיר, על ידי Game Center. ‫אבל אנחנו יודעים גם ‫עוד כל מיני מקרים,
[00:08:01 - 00:08:02] ‫אנחנו נדבר היום על בקרה אחרת,
[00:08:03 - 00:08:07] ‫שאנחנו יודעים משהו על ה-F הזה, ‫וזה נותן לנו איזושהי שיטה לדגום
[00:08:08 - 00:08:09] ‫בצורה יעילה.
[00:08:16 - 00:08:19] ‫דיברנו טיפה על ה... ‫קודם כל, זוכרים, הופקענו לדבר הזה?
[00:08:20 - 00:08:20] ‫מודל שנראה ככה?
[00:08:23 - 00:08:27] ‫כל מיני שמות, האפשרנות לגבי המרכאים ‫התבלגות וולצמן,
[00:08:29 - 00:08:31] ‫אם כאילו לוקחים איזושהי פונקציה ‫למעלה את הדבר
[00:08:32 - 00:08:36] בחזקה ועד סוף מנרנב, ‫או לפעמים התפלגות גיבס, ‫כן קוראים לזה?
[00:08:39 - 00:08:42] ‫לזה שאנחנו ממדלים רק את האיבר הזה,
[00:08:43 - 00:08:44] ‫ולא את כל ההתפלגות,
[00:08:45 - 00:08:47] ‫קוראים Energy Based Models.
[00:08:48 - 00:08:54] ‫בדבר הזה קוראים לדברים אנרגיה, ‫זה משהו שהוא לא מנורמל, פשוט הוא גדול ‫כשמשהו לא טוב קורה,
[00:08:54 - 00:08:55] ‫והוא קטן כשמשהו טוב קורה.
[00:08:57 - 00:09:01] ‫אז זה דרך להפוך את הפונקציה הזאת, ‫כל פונקציה שמתנהגת ככה,
[00:09:01 - 00:09:02] ‫להתפלגות על הדבר.
[00:09:04 - 00:09:06] ‫זאת אומרת, במקום מה לא טוב ומה כן טוב,
[00:09:06 - 00:09:09] ‫מה היותר סביר והפחות סביר,
[00:09:10 - 00:09:13] ‫באיזושהי שפה שהיא מתברמלת ‫לכל מה שאנחנו חושבים.
[00:09:14 - 00:09:15] ‫אז זה כמובן זה גם Energy Based Models,
[00:09:16 - 00:09:26] ‫ודרך, כשהדברים האלה, כשהפונקציה פה מתפרקת ‫לכל מיני איברים מעניינים ‫שאנחנו יכולים לעבוד איתם, ‫אז הרבה פעמים זה מתאים לגרף
[00:09:26 - 00:09:27] לא מכוון,
[00:09:28 - 00:09:30] ‫בניגוד לגרפים שראינו קודם,
[00:09:30 - 00:09:31] ‫שקוראים לו מרקוב נטוורק.
[00:09:34 - 00:09:39] ‫קודם בשיעורים השונים דיברנו ‫על פייז'ן נטוורק, שזה הילח מקוון,
[00:09:40 - 00:09:40] ‫שהילח הוא לא מכוון,
[00:09:41 - 00:09:42] ‫קוראים לזה מרקוב נטוורק.
[00:09:43 - 00:09:45] ‫בעצם כל קודקוד פה,
[00:09:45 - 00:09:47] ‫יש לו איזה שהם פקטורים שמשפיעים עליו.
[00:09:48 - 00:09:49] ‫למשל אם זה תמונה,
[00:09:50 - 00:09:52] ‫זו הרבה פעמים אנחנו אומרים, ‫הפיקסל הזה צריך להיות קרוב
[00:09:53 - 00:09:55] ‫לערכים של השכנים שלו.
[00:09:56 - 00:09:59] ‫כל פעם שיש איזשהו הבדל ‫בין הפיקסל הזה לשכן שלו, אנחנו...
[00:10:00 - 00:10:01] ‫רוצים לשלם על זה איזשהו מכין.
[00:10:02 - 00:10:03] ‫זה מעלה קצת את האנרגיה שלנו,
[00:10:04 - 00:10:07] ‫אז בעצם הדבר הזה משרה ‫איזושהי התפלגות על תמונותינו.
[00:10:08 - 00:10:12] ‫זאת אומרת, לא יודעים לחשב את ההתפלגות הזאת ‫בצורה ישירה, ‫כי יכול להיות איך לברמל את כל הדברים האלה.
[00:10:13 - 00:10:16] ‫בניגוד לגרפים שהיה לנו קודם, ‫יש לנו עכשיו פה מעגלים, ‫אז אין לנו דרך
[00:10:16 - 00:10:21] בעצם אחד-אחד לחשב את המקדם ‫מנימולד, כמו שהיה בשבילנו.
[00:10:23 - 00:10:24] ‫אז אני גם אמר פה בנצד הזה.
[00:10:25 - 00:10:26] אוקיי,
[00:10:27 - 00:10:31] ‫זה לא כזה, ואנחנו דרשנו אותה ‫למה אנחנו עושים NCLC, למשל את זה לשינותיות.
[00:10:37 - 00:10:40] ‫אוקיי, אז הדוגמה שראינו, ‫שהיא אולי הכי מפורסמת, ‫זו דגימת גיגס.
[00:10:46 - 00:10:49] ‫היא בעצם אומרת, אוקיי, ‫אולי אנחנו לא יודעים ‫לדגום מההתפלגות הזאת,
[00:10:50 - 00:10:52] ‫אנחנו יודעים לדגום ‫מהתפלגויות מותנות.
[00:10:52 - 00:10:54] ‫למשל, הדוגמה הזאת של ההגברה,
[00:10:55 - 00:10:57] ‫אם אני יודע את כל הערכים ‫של הפיקסלים,
[00:10:58 - 00:11:05] ‫אולי הייתי יכול לייצר מפונקציות ‫האנרגיה הזאת ‫את ההתפלגות של הפיקסל המרכזי המותנת.
[00:11:05 - 00:11:07] ‫ואם נתן שכל הפיקסלים האלה ‫לבנים, נגיד,
[00:11:08 - 00:11:18] ‫אז אני יודע שזה צריך להיות ‫לבן בהסדרות 0.9 ‫ושחור בהסדרות 0.1. ‫אז הדבר הזה הרבה פעמים הוא כן קל, ‫הההתפלגות המותנת, ‫להינתן כל השכנים.
[00:11:20 - 00:11:23] ‫אז בדיוק במצבים כאלה גיגס זה אלגוריתם נוח,
[00:11:25 - 00:11:26] ‫נוח.
[00:11:30 - 00:11:33] ‫באלגוריתם גיגס אנחנו מתחילים...
[00:11:37 - 00:11:41] ‫אתם זוכרים, אנחנו לא עובדים בעולם ההתפלגויות, ‫אנחנו עובדים על x מסוים,
[00:11:43 - 00:11:44] ‫השרשרת של x'ים,
[00:11:45 - 00:11:46] ‫ואני מייצר לנו בסופו של דבר ‫x'ים
[00:11:47 - 00:11:49] ‫שהם באו מההתפלגות שמעניינת אותנו.
[00:11:50 - 00:11:53] ‫אז אנחנו מתחילים את ה-x0 להיות משהו,
[00:11:55 - 00:11:59] ‫כל אחד מהמעמדים שלו, לא משנה, ‫אחד עוד xn,
[00:12:00 - 00:12:04] ‫ואז עושים איתרציות על הצעדים,
[00:12:06 - 00:12:07] ‫מ-אחד עוד איזשהו פי גדול.
[00:12:09 - 00:12:15] ‫אנחנו עושים את הדבר הבא, ‫אנחנו בוקרים קורדינטה פנדומלית.
[00:12:17 - 00:12:18] קליק רנדום
[00:12:19 - 00:12:22] עם i,
[00:12:23 - 00:12:27] ‫לפעמים הדלפון לא עשו את זה רנדומלית, ‫אלא לפי סדר שהוא רגילוני,
[00:12:28 - 00:12:30] ‫אבל גם רנדומלית זה עובד.
[00:12:33 - 00:12:35] ‫עכשיו אנחנו בעצם מעדכנים
[00:12:36 - 00:12:39] ‫את האיבר ה-i, את האינדקס ה-i, ‫בתוך הבדימה שלנו,
[00:12:40 - 00:12:43] ‫ועידי זה שאנחנו דוגמים אותו ‫מההתפלגות המותנית
[00:12:44 - 00:12:47] ‫של xi בהינתן
[00:12:48 - 00:12:49] ‫שאר ה-xים.
[00:12:50 - 00:12:51] ‫שימנו את זה במינוס i,
[00:12:53 - 00:12:56] ‫כל שאר האינדקסים, אוקיי, ‫אז זה יהיה בעצם xi באיתרציה t,
[00:12:57 - 00:13:01] ‫והוא יותנה על הערכים ‫ששאר ה-xים קיבלו באיתרציה הקודמת.
[00:13:06 - 00:13:07] ‫גם הוא ה-xi.
[00:13:09 - 00:13:15] ‫לא, זה לא i. ‫-pהוא זה ההתפלגות על xi, ‫בהינתן כל השאר,
[00:13:15 - 00:13:17] ‫אז יצא לנו איזה ערך,
[00:13:17 - 00:13:19] ‫מעדכנים את הקואורדינטה x,
[00:13:19 - 00:13:20] ‫קואורדינטה i, סליחה,
[00:13:21 - 00:13:28] ‫וכל שאר הקואורדינטות נשארות אותו דבר, ‫אז x לא i,p שווה פשוט x לא i,p.
[00:13:31 - 00:13:33] ‫אז זה באמצע אלגוריתם.
[00:13:35 - 00:13:36] ‫זה מאוד פשוט,
[00:13:36 - 00:13:38] ‫ובהינתן שאנחנו יודעים ‫לבדוק את ההתפלגויות המותנות האלה,
[00:13:40 - 00:13:46] ‫אז זה מובטח לנו שאנחנו מתכנסים, ‫יש תנאים אצלם, והתנאים הוא ש...
[00:13:48 - 00:13:48] ‫אין...
[00:13:56 - 00:13:58] ‫אתם צוחקים את התנאים ‫שבהם הדבר הזה מתכנס,
[00:13:59 - 00:14:02] ‫דיברנו על זה שהתהליך צריך להיות ‫גודי, זאת אומרת שמכל נקודה,
[00:14:03 - 00:14:04] ‫לא משנה,
[00:14:04 - 00:14:07] יש איזשהו מספר צעדים ‫שאפשר ממנו להגיע ‫לכל נקודה אחרת.
[00:14:08 - 00:14:11] ‫אז אין משהו ש... ‫אם התחלנו מאיזשהו מקום,
[00:14:11 - 00:14:12] ‫בטוח לא נגיע ל...
[00:14:13 - 00:14:16] ‫יש איזשהו x של אין לנו דרך להגיע,
[00:14:16 - 00:14:17] ‫אם זה המצב.
[00:14:17 - 00:14:23] ‫התקוד שהתחלנו בתיאוריות ‫במקום שהדרך ל-x עם ה... ‫שיש להם את צמירות היחסית גבוהה, ‫הן יהיו חסומות.
[00:14:25 - 00:14:27] ‫אוקיי, אה...
[00:14:28 - 00:14:29] ‫-לא גדול לזה?
[00:14:30 - 00:14:32] ‫כן, למה...
[00:14:33 - 00:14:40] ‫אם זה מתכנס, ‫אז ברור שההתפלגות הסטציונרית ‫היא ההתפלגות הנכונה, נכון? ‫כי אם אני יודע לבנות את ה... ‫את פעולות המותנות היה כמו פרצריד,
[00:14:40 - 00:14:43] ‫אבל בעצם, אם כבר הגעתי ‫להתפלגות הנכונה,
[00:14:44 - 00:14:50] ‫אני כל הזמן נשאר בהתפלגות הנכונה, נכון? ‫אני רק מפעיל כל פעם התפלגות מותנית ‫על ערכים שכבר הגיעו מההתפלגות הנכונה,
[00:14:51 - 00:14:54] ‫אז אני רק מתקן ערכים, ‫אבל לפי התפלגות נכונה כל פעם,
[00:14:54 - 00:14:55] ‫התפלגות מותנית נכונה,
[00:14:56 - 00:14:57] ‫אז ברור שאני נשאר בהתפלגות הנכונה.
[00:14:58 - 00:15:00] ‫אז ברור שההתפלגות הזאת ‫היא
[00:15:00 - 00:15:05] סטציונרית, ‫וגם ברור שהתהליך הזה הוא מרקובי, ‫כפה פעם אני מסתכל על xp מינוס 1.
[00:15:13 - 00:15:21] ‫אוקיי, הבעיה עם האלגוריתם הזה, ‫היתרון הזה הוא מאוד פשוט.
[00:15:23 - 00:15:24] ‫ברגע שיש לנו,
[00:15:24 - 00:15:26] ‫אתם יודעים להגיד את הדבר הזה, ‫זה מאוד קל להריץ אותו.
[00:15:27 - 00:15:29] ‫החיסרון זה שהרבה פעמים,
[00:15:29 - 00:15:34] ‫כשהגענו לזה מיקסינג טיים, ‫כמה זמן הוכיח השרשרת מרקוב הזאת להגיע,
[00:15:34 - 00:15:35] ‫או להתקרב לפחות,
[00:15:36 - 00:15:39] ‫תורה מספיק טובה להתפלגות שמעניינת אותנו,
[00:15:39 - 00:15:40] ‫זה יכול להיות מאוד ארוך,
[00:15:41 - 00:15:43] ‫וגם לפעמים אנחנו לא יודעים בדיוק לנתח את זה.
[00:15:44 - 00:15:50] ‫אם קלענו לבנות שרשרת כזאת, ‫שאנחנו יודעים שההתפלגות הזאת ‫הנכונה היא סטציונרית, ‫אבל לא יודעים בדיוק להגיד ‫כמה התפלגויות אנחנו צריכים.
[00:15:54 - 00:15:55] ‫זאת בעיה קשה שיש הרבה מחקר על זה,
[00:15:57 - 00:16:02] ‫והרבה בעיות בצורה פרקטית, ‫זה לא רק מהבעיה הזאת.
[00:16:04 - 00:16:05] ‫אוקיי, אז זו הייתה התפלגות,
[00:16:05 - 00:16:08] ‫זו הייתה שיטת דגימה שכבר דיברנו עליה,
[00:16:09 - 00:16:12] ‫ועכשיו נדבר על שיטה אחרת.
[00:16:14 - 00:16:21] ‫אני רק רוצה לפני זה להזכיר, ‫בעצם לא דיברנו על ה-NC השני, ‫למה אנחנו רוצים לדגום?
[00:16:23 - 00:16:25] ‫מה זה שיטת מרדכאו באופן כללי?
[00:16:26 - 00:16:31] ‫אז קודם כל, לפעמים אנחנו רוצים לדגום ‫בשביל לדגום, אנחנו צריכים את הדגימות האלה למשהו, ‫למשל, יש לנו התפלגות על תמונות,
[00:16:32 - 00:16:32] ‫אנחנו רוצים לייצר תמונות חדשות.
[00:16:33 - 00:16:36] ‫זה פשוט מה שמעניין אותנו, הדגימות עצמן.
[00:16:37 - 00:16:41] ‫והרבה פעמים הדגימות, אנחנו משתמשים בהן ‫כדי לעשות איזשהו אינפרנס.
[00:16:42 - 00:16:44] ‫האינפרנס זה חשב והסתברות של משהו.
[00:16:46 - 00:16:51] ‫בשביל שעבר התחלנו מזה, ‫שאנחנו רוצים רבותיים לחשב תוחלות, ‫אם יש לנו משתנה חבוי שאנחנו לא רואים,
[00:16:52 - 00:16:54] ‫אנחנו רוצים לחשב תוחלת ‫על המשתנה החבוי הזה,
[00:16:55 - 00:16:57] ‫והדרך לקרב את זה ‫זה על ידי הרבה דגימות.
[00:16:59 - 00:17:05] ‫זו סיבה שאנחנו לא חושבים את הדגימות האלה, ‫ואנחנו נדבר היום על שיטה אחרת, ‫שהיא לא מבוססת ממית דגימה,
[00:17:06 - 00:17:07] ‫גם כדי לכתוב את הבעיה.
[00:17:08 - 00:17:10] ‫אבל לפעמים הראש המקום ‫שמעניין אותנו זה הדגימות עצמן.
[00:17:12 - 00:17:15] ‫אוקיי, אז זו הייתה שיטה אחת, ‫היום אנחנו נדבר על שיטה אחרת,
[00:17:16 - 00:17:16] ‫שנקראת
[00:17:19 - 00:17:20] ‫לנג'ווין,
[00:17:22 - 00:17:23] ‫פיפנארג'ווין,
[00:17:25 - 00:17:25] ‫מימאת
[00:17:26 - 00:17:27] חברית מנג'ווין,
[00:17:38 - 00:17:48] ‫והרעיון הוא, זאת אומרת, ‫מתי זה כדאי לנו לעשות כזה דבר, ‫במצעי שההתפלגות שלנו רצוי,
[00:17:48 - 00:17:50] ‫ההתפלגות שנראית ככה,
[00:17:55 - 00:18:02] ‫אבל יכול להיות שעכשיו אנחנו לא יודעים ‫מתוך ה-fx הזה לבנות את ההתפלגויות המותנות האלה,
[00:18:03 - 00:18:03] ‫או קודם.
[00:18:04 - 00:18:06] ‫מצד שני, אנחנו יודעים לחשב ‫שנגזרת של
[00:18:07 - 00:18:08] f של x.
[00:18:10 - 00:18:13] ‫אם f של x היא פונקציה שהיא גזירה, ‫אנחנו יכולים לגזור אותה,
[00:18:15 - 00:18:16] ‫זה נותן לנו בעצם עוד מידע על f של x,
[00:18:17 - 00:18:20] ‫וזה המידע שנשתמש בו עכשיו בשביל התגימה.
[00:18:22 - 00:18:26] ‫קודם כול, אינטואיטיבית, ‫מה זה אומר שאני יודע ‫את הנגזרת של f של x?
[00:18:27 - 00:18:28] ‫מה זה אומר ש...
[00:18:29 - 00:18:30] ‫אני מדבר על הנגזרת של f של x,
[00:18:31 - 00:18:34] ‫זה פונקציה של x, כן? ‫לא פונקציה של הפרמטרים שלנו.
[00:18:34 - 00:18:41] ‫מה זה אומר, הפונקציה הזאת? ‫מה למשל זה אף פעם לא נותן לי?
[00:18:43 - 00:18:48] ‫כן, למשל תגיד שהפונקציה נראית ‫איזה משהו מורכב
[00:18:50 - 00:18:51] ‫מתאחד למנהיג.
[00:18:52 - 00:18:54] ‫אם הייתי, אם אני לא רק יודע ‫את הערך של
[00:18:55 - 00:18:59] עבור כל x, את הערך של הפונקציה הזאת, ‫אלא גם אתה יודע פי אמת נגזרת,
[00:19:00 - 00:19:00] ‫מה זה נותן לי?
[00:19:01 - 00:19:06] ‫טוב, gradient descent, אז למשל, ‫מה אני יכול לעשות ‫עם gradient descent?
[00:19:08 - 00:19:12] ‫כן, למצוא נקודות מקסימום, נכון? ‫אז אני יכול,
[00:19:13 - 00:19:16] אפילו שאני לא יודע ‫איך לנרמל ואיך לדגום מהפונקציה הזאת,
[00:19:17 - 00:19:22] ‫אני יכול לקוות ‫שאני אמצא נקודת מקסימום ‫לפחות לוקאלית, נכון? ‫לגידי gradient descent.
[00:19:23 - 00:19:24] ‫אני מתחיל מאיזשהו x, נכון?
[00:19:25 - 00:19:28] ‫זה פשוט לא מדובר ב-gradient descent, ‫זה gradient asset,
[00:19:29 - 00:19:30] ‫אפעם חשבת כאל רדיינט.
[00:19:31 - 00:19:34] ‫ואני אגיע לדגימה עם הסתברות גבוהה.
[00:19:36 - 00:19:36] ‫אוקיי?
[00:19:36 - 00:19:43] תראה איך נראה כאלה, ‫הכלל ל-XT שווה ל-XT מינוס 1,
[00:19:44 - 00:19:44] ‫ועוד
[00:19:47 - 00:19:48] ‫מישהו גודל נמצא,
[00:19:48 - 00:19:50] ‫למה פה זה תלוי ל-P,
[00:19:51 - 00:19:52] ‫הוא לא דודיינט,
[00:19:53 - 00:19:55] ‫חוץ לשמיים שלנו, הוא נותן לזה שאלה שאלה פה
[00:19:57 - 00:20:00] X. ‫הדודיינט הוא יהיה לפי אינס,
[00:20:00 - 00:20:01] ‫במקרה הקודמת.
[00:20:06 - 00:20:10] ‫אני מציב את ה-X בפונקציה של הגרדיאנט שם,
[00:20:11 - 00:20:14] ‫ואני יודעים שאני חוסיף ‫לניבוש המחקרי שלי,
[00:20:14 - 00:20:19] ‫וככה אני מתקדם. ‫אז אני לא מקבל דגימה על התפלגות הזאת, ‫אבל אני מקבל
[00:20:20 - 00:20:22] את הדגימות שיש להן הסתברות גבוהה.
[00:20:26 - 00:20:29] ‫יש איזשהו דמיון בין השניים, ‫אבל מה ההבדל?
[00:20:29 - 00:20:30] ‫מה בעצם חסר לי?
[00:20:31 - 00:20:38] ‫מה הקשר בין משהו שמייצר לי דגימה ‫עם הסתברות גבוהה ‫למשהו שמייצר לי דגימה
[00:20:39 - 00:20:43] ‫במגיעה מההתפלגות של הפונקציה הזאת?
[00:20:57 - 00:20:58] ‫כן, תמיד היא אותה דגימה,
[00:20:59 - 00:21:03] ‫לכאורה, מנקודת התחלה ה-CT, ‫כי תמיד אני אמור להגיע למקסימום.
[00:21:04 - 00:21:09] ‫זו בדיוק אותה נקודה, ‫זה לא טוב, נכון? ‫אני רוצה כמה נקודות. ‫אבל מה אני רוצה שיהיה לנקודות האלה?
[00:21:10 - 00:21:11] ‫אני רוצה שהן יהיו...
[00:21:11 - 00:21:14] ‫זה טוב שהגעתי לאזור הזה, נכון? ‫אבל אני לא רוצה להישאר בדיוק פה,
[00:21:15 - 00:21:16] ‫אני רוצה להיות באזור הזה.
[00:21:17 - 00:21:21] ‫אני רוצה קצת לא רק את נקודת המקסימום, ‫אלא את האזור של המקסימום,
[00:21:22 - 00:21:27] ‫שזה יהיה פרופורציוני לכמה שאני קרוב למקסימום, ‫כמה מהר לפונקציה הזאת יורדת,
[00:21:28 - 00:21:31] ‫וגם אני רוצה מדי פעם לקפוץ ‫לאיזה מקסימום אחר, נכון?
[00:21:31 - 00:21:32] ‫כי יש כמה נקודות מקסימום.
[00:21:34 - 00:21:37] ‫וגם, לעיתים מאוד מאוד מדהירות,
[00:21:38 - 00:21:39] ‫אני רוצה להגיע
[00:21:41 - 00:21:44] ‫לאיזשהו מקום עם התגרות יחסית נמוכה.
[00:21:46 - 00:21:51] ‫הן מאוד מדהירות, אוקיי? ‫פה אולי אף פעם לא, באזורים האלה, ‫אני כן רוצה מדי פעם שיהיה גדולה.
[00:21:52 - 00:21:57] ‫אם אני רוצה שההתפלגות שלי ‫באמת תהיה לפי הפונקציה הנכונה.
[00:21:58 - 00:22:01] ‫אני רוצה שההתפלגות של הדגימות שלי ‫תהיה באמת לפי הפונקציה.
[00:22:02 - 00:22:06] ‫אז זה בדיוק האינטואיציה ‫של דגימת בארכיבים.
[00:22:08 - 00:22:08] ‫ובעצם,
[00:22:12 - 00:22:15] ‫הדרך שעושים את זה, ‫זה פשוט עושים גדולים, ‫אבל מוסיפים רעש.
[00:22:18 - 00:22:20] ‫זה כלל עדכון, נראה.
[00:22:21 - 00:22:21] ‫ככה?
[00:22:22 - 00:22:23] ‫-9 ונראה.
[00:22:36 - 00:22:36] ‫אחד.
[00:22:38 - 00:22:38] ‫עוד.
[00:22:41 - 00:22:42] ‫אפסילון.
[00:22:43 - 00:22:45] ‫זה ככה כבר, ‫אז שאין פקעי 2.
[00:22:48 - 00:22:49] ‫נגזרת.
[00:22:49 - 00:22:50] ‫נגזרת כמו קודם.
[00:22:52 - 00:22:55] ‫אבל האם F זה לא B?
[00:22:57 - 00:23:00] ‫כן, זה לא אותו דבר, F זה B. ‫בדרך כלל,
[00:23:00 - 00:23:06] ‫בדרך כלל, אני לא יודע ‫לכתוב את פיקסל.
[00:23:07 - 00:23:09] ‫זה רק ידע לכתוב את F,
[00:23:10 - 00:23:12] ‫וחסר לי ה...
[00:23:12 - 00:23:13] ‫זה משכתב נרמול.
[00:23:13 - 00:23:17] ‫אני לא יכול להגיד למה, ‫הצביעו אותו בספיקות ההסתובבות של כל נקודה שלכם.
[00:23:18 - 00:23:22] ‫ולזה אני פשוט מוסיף רעש באוסיאני.
[00:23:24 - 00:23:25] ‫תוחלת אפס
[00:23:26 - 00:23:34] ‫זו שונות שהיא גם משתנה, ‫בפניקה של האפסינון.
[00:23:35 - 00:23:37] ‫היום האפסינון הזה הוא מספר קטן.
[00:23:39 - 00:23:42] ‫זה בעצם אותו דבר, ‫החלק הזה הוא אותו דבר,
[00:23:42 - 00:23:43] ‫אחר כך אני פשוט מוסיף כזה רעש.
[00:23:44 - 00:23:49] ‫אינטואיטיבית, מה זה אומר? ‫ברור שאני כן ללכת בכיוון של המקסימום,
[00:23:51 - 00:23:56] ‫ואלי קודם מתפסקים לי רעשים, ‫אז בדרך אני נמצא גם בסביבה של המקסימום.
[00:23:58 - 00:24:03] ‫מדי פעם, אגב, ‫תמיד זה נותן ערך מאוד מאוד גדול, ‫אז אני אקפוץ באיזשהו מקום כזה, ‫הסתברות מאוד נמוכה,
[00:24:04 - 00:24:06] ‫אבל אז מהר מאוד אני אגיע שוב פעם ‫לאיזשהו מקום רחוק.
[00:24:08 - 00:24:10] ‫ומדי פעם אני אקפוץ באיזשהו מקום רחוק,
[00:24:11 - 00:24:13] ‫ואז תמיד אני אשאר בסביבות של מקסימורים,
[00:24:14 - 00:24:15] ‫לאורך זמן,
[00:24:15 - 00:24:18] ‫אנחנו נראה בדיוק ‫לנקודות המקסימום ולסביבה.
[00:24:21 - 00:24:23] ‫זה הרעיון של הפנימה הזאת.
[00:24:27 - 00:24:30] ‫ובטח אפשר להוכיח שתחת תנאים מסוימים,
[00:24:34 - 00:24:37] ‫זה טיפה, אבל לא ניכנס יותר מדי לעומק,
[00:24:39 - 00:24:41] ‫אז X,
[00:24:43 - 00:24:47] ‫P גדול שואף לאינסוף,
[00:24:49 - 00:24:52] ‫ו-אפסילון T שואף לאפס,
[00:24:52 - 00:24:54] ‫רק, הנה, גודל הצד,
[00:24:54 - 00:24:56] ‫אז ה-X,T הזה מתפלג
[00:24:57 - 00:24:58] ‫לפי P של X,
[00:25:00 - 00:25:02] ‫כלומר לפי 1 חלקי Z,
[00:25:03 - 00:25:04] ‫P מינוס זה.
[00:25:11 - 00:25:12] טוב, אנחנו תכף נוכיח את זה,
[00:25:12 - 00:25:16] ‫כי זה בעצם אפשר להראות ש... ‫אפשר להפוך את זה לסוג של התפלגות גיבס,
[00:25:17 - 00:25:20] ‫בדרך כלל, ולפי זה להוכיח את זה ‫שבין להיכנס באנגלית.
[00:25:29 - 00:25:31] זהו, בעצם מה שנאמר, ‫שהדבר הזה זה תהליך MCMC,
[00:25:32 - 00:25:33] ‫סודם כל אתם רואים שזה תהליך MCMD,
[00:25:34 - 00:25:35] ‫זאת תהליך מרקובי,
[00:25:36 - 00:25:39] ‫ומה שאנחנו לוקח דרך ההתפלגות גיבס, ‫שהדבר הזה הוא...
[00:25:40 - 00:25:43] ‫כי זו ההתפלגות הסטרציונרית ‫של התאריך הקודם.
[00:25:56 - 00:25:57] ‫אז שוב, אם יש לנו איזושהי התפלגות,
[00:25:58 - 00:25:59] ‫אנחנו יודעים את F של X זה,
[00:26:00 - 00:26:01] ‫אנחנו רוצים לתגור ממנו,
[00:26:02 - 00:26:07] ‫אז זה נותן לנו את הדרך לעשות את זה. ‫מתי אנחנו רוצים, יש יכול להיות כמה סיבות.
[00:26:08 - 00:26:12] ‫אחת, אם אנחנו רוצים לייצר דימות, ‫אם הדבר הזה, נגיד, ‫התפלגות של תמונות.
[00:26:12 - 00:26:14] ‫שנייה, תחיל.
[00:26:14 - 00:26:17] ‫כאילו, יוצא שאנחנו כאילו דוגמים פי של אינטר.
[00:26:17 - 00:26:17] ‫כן.
[00:26:21 - 00:26:22] ‫הוא אין שלמדנו מודל
[00:26:23 - 00:26:25] ‫על תמונות.
[00:26:26 - 00:26:29] ‫יש לנו מודל שהוא טוב של תמונות, ‫אבל אנחנו לא יודעים לדגום ממנו.
[00:26:30 - 00:26:32] ‫רק יודעים לחשב את ה-F הזה,
[00:26:33 - 00:26:34] ‫התמודל הזה נראה ככה.
[00:26:34 - 00:26:36] ‫אנחנו יודעים את ה-F שלו,
[00:26:36 - 00:26:40] ‫ואנחנו יודעים את הנגברת שלה, ‫אנחנו נגזור.
[00:26:41 - 00:26:43] ‫אנחנו רוצים לייצר תמונות מהמודל הזה.
[00:26:44 - 00:26:45] ‫זו בעצם שיטה שהיא תייצר תמונות.
[00:26:46 - 00:26:51] ‫זו באמת השיטה שמשתמשים ‫בכל המודלים החדשים ‫של Diffusion Model.
[00:26:52 - 00:26:52] ‫אתם יודעים,
[00:26:53 - 00:26:56] כמו דלי שטיינינג,
[00:26:57 - 00:26:58] ‫אדימג'ן,
[00:26:59 - 00:26:59] פייטנט דיפיוזן,
[00:27:00 - 00:27:03] ‫כל המודלים האלה, ‫אז הם משתמשים בשיטה הזאת.
[00:27:03 - 00:27:05] ‫המודל הוא מודל שאנחנו נדבר עליו בקורס,
[00:27:06 - 00:27:11] ‫אבל בסופו של דבר הוא נותן לנו ‫איזה משהו כזה שאנחנו יכולים לדגום ממנו
[00:27:11 - 00:27:14] ‫על ידי השיטה הזאת של Diffusion.
[00:27:17 - 00:27:22] ‫אז זאת יכולה להיות סיבה אחת. ‫עוד סיבה שאנחנו רוצים לדגום, ‫זה מה שאמרתי קודם, ‫בשביל לחשב כל מיני תשובים.
[00:27:23 - 00:27:26] ‫אנחנו רוצים לדעת, ‫יש לנו תמונה חדשה, ‫אנחנו רוצים לדעת ‫מה ההסתברות של התמונה הזאת.
[00:27:27 - 00:27:31] ‫אז אחת מהשיטות זה לדגום ‫הרבה דגימות של תחת
[00:27:32 - 00:27:40] ‫המודל הזה, ‫בשיטה כזאת למשל, ‫ואז חשב נגיד למוצר ‫עם כל מיני תוכלות שאנחנו צריכים לעשות.
[00:27:40 - 00:27:44] ‫אנחנו משתמשים בזה כדי להשלים, ‫אם יש לנו חלק מתמונה, ‫אנחנו רוצים להשלים את החלק
[00:27:45 - 00:27:46] ‫השני של התמונה,
[00:27:46 - 00:27:52] ‫כדי לדעת מה ההסתברות השולית ‫של אחד מהפיקסלים בתמונה, ‫או שאחד מהמשתנים שאינם אמונים.
[00:27:52 - 00:28:03] ‫אם אוקיי, אולי נגיד על זה עוד כמה דברים ‫לפני שנראה דוגמה פשוטה.
[00:28:08 - 00:28:12] ‫אוקיי, מונח שצריך אולי להכיר.
[00:28:22 - 00:28:28] ‫זה מונח בסטטיסטיקה שהמשמעות שלו ‫זו פשוט הנגזרת
[00:28:30 - 00:28:31] ‫px של לוג
[00:28:33 - 00:28:34] ‫בהסתברות השולית.
[00:28:41 - 00:28:45] ‫זה פשוט שם שנותן לדבר הזה, כן? ‫נגזרת של לוג של v של x,
[00:28:45 - 00:28:46] ‫קוראים לזה סקור פונקשיון.
[00:28:49 - 00:28:49] עכשיו,
[00:28:50 - 00:28:54] ‫מה שכדאי לשים לב, ‫שהדבר הזה פשוט בדיוק שווה
[00:28:55 - 00:28:56] ‫למה שאנחנו מוסיפים כאן.
[00:28:58 - 00:28:59] ‫המודל שלנו נראה ככה.
[00:29:01 - 00:29:05] למה? ‫אם עכשיו נגזור לפי x,
[00:29:06 - 00:29:06] ‫לגלול
[00:29:09 - 00:29:11] ‫של התמונה שלי, ‫שזה כבר חלקי לזה,
[00:29:12 - 00:29:14] ‫אין מינוס fx.
[00:29:14 - 00:29:20] ‫אני יכול להפריד את זה למגזרת לפי x. ‫אני יכול להפריד את זה למגזרת לפי x.
[00:29:44 - 00:29:46] ‫המודל של הדבר הזה זה מינוס x.
[00:29:49 - 00:29:50] ‫זה נותן פשוט...
[00:29:52 - 00:29:53] ‫מה זה הנגזרת הזאת?
[00:29:55 - 00:29:56] ‫אפשר. ‫זה לא תלוי ב-x.
[00:29:57 - 00:29:59] ‫זה משהו שהוא תלוי באינטגרל, ‫כל ה-xים אפשריים,
[00:30:00 - 00:30:01] ‫לא פונקציה של x.
[00:30:02 - 00:30:04] ‫זה רב 0, ובעצם הנשאר היא מינוס...
[00:30:08 - 00:30:09] ‫זה סקור פונקשיון.
[00:30:15 - 00:30:16] ‫אז אנחנו משתמשים בו הרבה.
[00:30:24 - 00:30:26] ‫ועוד דבר שאולי להגיד,
[00:30:27 - 00:30:30] ‫כן, השיטה הזאתי של ‫לנג'ווין סנקלינג.
[00:30:44 - 00:31:13] ‫בקיצור אדוני ואווי.
[00:31:14 - 00:31:20] ‫בעצם המקור של ה...
[00:31:20 - 00:31:23] ‫זה בעצם השימוש SGLD,
[00:31:25 - 00:31:28] ‫השם הראשון שנתנו את זה ‫כשהתחיל להשתמש בזה ל-machine learning,
[00:31:28 - 00:31:33] ‫המקור של הדבר הזה, מה שלנג'רלין פיתח, ‫זה מה שנקרא לנג'רלין דיינאמיקס,
[00:31:34 - 00:31:37] ‫זה להשתמש בנוסחות האלה ‫כדי לחשב את המבנה
[00:31:38 - 00:31:42] התת-ממדי של חלבונים, ‫את השינוי במבנה התת-ממדי של חלבונים.
[00:31:42 - 00:31:46] ‫כן, אז עד היום משתמשים בזה ‫כדי לעשות סימולציה של
[00:31:47 - 00:31:56] ‫מה המבנה של החלבונים. ‫בעצם הפונקציית האנרגיה הזאתי ‫זו פונקציית האנרגיה שתלויה במרחקים ‫בין כל מיני אטומים בתוך החלבונים, ‫מולקולות, באופן כללי.
[00:31:58 - 00:31:58] ‫אז
[00:31:59 - 00:32:02] זה היה הפיתוח הראשון של זה, ‫זה נקרא ל-M�ג'רלין דיינאמיקס, כמובן, תאמין.
[00:32:03 - 00:32:08] ‫זה בערך למצוא את הדינמיקה הזאתי ‫שמשנה את המבנה של החלבונים.
[00:32:08 - 00:32:15] ‫אוקיי, אז בואו נראה דוגמה פתה לגרסיאן.
[00:32:17 - 00:32:19] ‫אמרנו שבגרסיאן אנחנו יודעים ‫לעשות בצורה ישירה כבר,
[00:32:20 - 00:32:24] ‫רק נראה איך זה נראה ב-M�ג'רלין דיינאמיקס.
[00:32:38 - 00:32:47] ‫זה נראה איך זה של איקס.
[00:32:51 - 00:32:52] ‫איקס היא נראה שזה מיון וסימה.
[00:32:55 - 00:32:55] ‫מה אני צריך לחשב?
[00:32:56 - 00:33:01] ‫אני צריך לחשוב על הדבר הזה ‫בתור אופציה כזאת של 1 חלקי Z.
[00:33:05 - 00:33:07] ‫P בחזקת, בחזקת מה זה יהיה?
[00:33:08 - 00:33:13] ‫חצי של איקס מיניאלס מיון ופורד,
[00:33:14 - 00:33:20] ‫הדיגמא דמיניאלס 1. ‫זה בעצם ה-Fx שלי.
[00:33:38 - 00:33:41] ‫כן, כן, כן, כל הצרכויות כאן, אה, כל הצרכויות
[00:33:42 - 00:33:43] ‫ב-Igredeine descent?
[00:33:45 - 00:33:46] ‫קצת, בקצרת.
[00:33:47 - 00:33:48] ‫כל מיני כזה צריך להיות מינוס,
[00:33:49 - 00:33:49] ‫כי
[00:33:52 - 00:33:53] P של X,
[00:33:53 - 00:33:59] ‫היא מינול לפי חלקי X. ‫אני מחפש,
[00:33:59 - 00:34:02] אם אני אסגר לי, בסדר, ‫אחפש בקופשי הזה, מחסימלי,
[00:34:03 - 00:34:05] ‫כי אני רואה שה-F הזאת היא מינימלית.
[00:34:06 - 00:34:06] ‫האנרגיה היא מינימלית.
[00:34:08 - 00:34:14] ‫אנחנו יכולים לעשות gradient descent על F של X. ‫פה, תודה רבה.
[00:34:15 - 00:34:25] ‫אוקיי, אז עכשיו אני רוצה לראות ‫מה יצא החישוב שלה.
[00:34:25 - 00:34:25] ‫אתה מאמין?
[00:34:26 - 00:34:29] ‫זה בעצם כל מה שאני צריך לחשב, ‫את הנגזרת הזאת, נכון?
[00:34:31 - 00:34:32] ‫אתה אחרי את הנגזרת הזאת?
[00:34:33 - 00:34:35] ‫מה היה הנגזרת של...
[00:34:38 - 00:34:48] ‫כמה מעניין אותי הדבר הזה? ‫כמה מעניין אותי רק הפונקציה הזאת? ‫אני רוצה לבנור אותה?
[00:34:51 - 00:34:52] ‫מה זה יוצא?
[00:34:54 - 00:34:55] ‫פעם אתה חוזרים לזה?
[00:35:00 - 00:35:05] ‫פעמיים נפל עם החצי, אז זה X מינוס חלקי עוד.
[00:35:08 - 00:35:20] ‫זה יוצא וקטור שורה, דרך אגב מידע פורקמן.
[00:35:38 - 00:35:42] ‫אבל אנחנו רואים את זה בסופו של דבר ‫אצל XT שווה ל-XT מינוס 1,
[00:35:45 - 00:35:45] ‫פחות
[00:35:46 - 00:35:48] Fse, t בין Kf2,
[00:35:49 - 00:35:52] ‫חלקי עוד מינוס מיון,
[00:35:53 - 00:35:55] ‫בעוד רגע שבועה שכנות יעניין.
[00:36:02 - 00:36:04] ‫אבל מי שרוצה, עוד מעט ננסות את זה.
[00:36:05 - 00:36:08] ‫לשיטה הישירה.
[00:36:10 - 00:36:14] ‫אתם רואים איזשהו גרסיאן, נכון? ‫אתם יכולים לצייר את האליפסות של הגרסיאן הזה,
[00:36:15 - 00:36:17] ‫לייצר הרבה דגימות לשיטה הישירה,
[00:36:18 - 00:36:21] ‫כלומר, יש לכם נקודות שהן פחות יותר ‫בתוך האליפסה,
[00:36:22 - 00:36:24] ‫והרבה דגימות מהשיטה הזאת.
[00:36:25 - 00:36:31] ‫אפשר להסתכל, בהתחלה, אם אתם מתחילים ‫במקום מאוד רחוק, ‫אולי אתם צריכים לחכות יותר זמן ‫עד שהדבר הזה מתכנס
[00:36:32 - 00:36:34] ‫לדגימות שמגיעות מתוך האליפסה.
[00:36:42 - 00:36:43] ‫זה כאילו לא משתמשים ב...
[00:36:45 - 00:36:47] ‫כי אנחנו יכולים לבנות ‫מאוניברסיה, נכון?
[00:36:48 - 00:36:49] ‫כן.
[00:36:52 - 00:36:57] ‫כשאנחנו נדבר על זה בסוף הקורס, ‫ה-F של X הזה, ‫זה יהיה רשת נאורלית חקוקה כזאת.
[00:36:58 - 00:37:01] ‫אנחנו לא יודעים לעשות את זה ‫שום דבר בצורה ישירה, ‫אנחנו יודעים לזור אותה.
[00:37:01 - 00:37:06] ‫יש פה עוד יותר שאנחנו יכולים לעשות ככה, ‫אז זה בגלל זה, זה לא מתאים.
[00:37:19 - 00:37:22] ‫אוקיי, עכשיו אנחנו נוכיח ‫שה-Lendry דיינאמיקס הוא
[00:37:25 - 00:37:29] ‫ההתפלגות הזאת, ‫זו התפלגות הפציונלית שלו.
[00:37:32 - 00:37:34] ‫ואז אנחנו נצביע לדבר על הנושא הבא.
[00:37:36 - 00:37:38] ‫שאלות יש להן למתכן את ההוכחה?
[00:37:41 - 00:37:42] ‫אוקיי.
[00:37:43 - 00:37:43] ‫אוקיי?
[00:37:51 - 00:37:54] ‫הרעיון של ההוכחה הזאת זה שאנחנו
[00:37:56 - 00:38:01] בעצם נראה את זה כדגימת גיפס, ‫אנחנו בעצם נהפוך את ה-X של ה-X,
[00:38:01 - 00:38:12] ‫אחרי כך הרבה איטרציות, ‫זו ההתפלגות הסטציונלית שלו.
[00:38:13 - 00:38:22] ‫הדרך שאנחנו נראה את זה, ‫אנחנו נהפוך את זה להתפלגות גיפס. ‫אבל התפלגות גיפס צריכה לתת לנו כמה משתנים. ‫כל פעם אנחנו עושים איזושהי משתנה התפלגות מותנית ‫בין אחד מהמשתנים
[00:38:23 - 00:38:25] למשתנים האחרים, ‫או בין קבוצה של משתנים
[00:38:26 - 00:38:27] ‫לפיילכן הקבוצה השנייה.
[00:38:27 - 00:38:31] ‫הדרך שאנחנו נעשה את זה, ‫אנחנו פשוט נפיל את X פעמיים,
[00:38:31 - 00:38:32] אין לנו שני עותקים של x,
[00:38:34 - 00:38:34] תכף נראה,
[00:38:35 - 00:38:37] ובעצם תהיה התפלגות גילס שכל פעם אנחנו דוגמת
[00:38:37 - 00:38:39] את אחד מהעותקים ויינתן העותק השני.
[00:38:42 - 00:38:45] אוקיי, אז מה, איך אנחנו בונים את ההתפלגות הזאת של שני עותקים?
[00:38:45 - 00:38:49] אנחנו נבנה את ההתפלגות הזאת, פי x1
[00:38:50 - 00:38:52] זה בעצם שני עותקים של x,
[00:38:53 - 00:38:54] וההתפלגות הזאת תהיה
[00:38:55 - 00:38:56] מוגדר ככה
[00:38:57 - 00:39:06] איך זה הפונקציה המקורית, אוקיי? אז ההתפלגות המקורית שאני רוצה להראות שאנחנו מגיעים אליה
[00:39:08 - 00:39:09] זה מה שכתוב שם,
[00:39:10 - 00:39:15] x שווה 1 לכל z, e במינוס x.
[00:39:17 - 00:39:19] עכשיו אני בונה התפלגות כזאתי
[00:39:20 - 00:39:24] על שני עותקים, שהיא נראית ככה, אז יש את הפונקציית מחיר הזאת
[00:39:26 - 00:39:31] אני מוסיף לפונקציות, אני יכול לתת לזה ככה, יש לי את הפונקציית המחיר הזאת, ועוד פונקציות, אותן פונקציית מחיר על x2
[00:39:33 - 00:39:38] ועוד פונקציית מחיר שהיא x1 צריכה להיות קרוב ל-x2
[00:39:40 - 00:39:42] במרחק, בנורמה שתיים
[00:39:43 - 00:39:44] מאיזשהו משקל
[00:39:45 - 00:39:46] קוראים לו בטא
[00:39:48 - 00:39:51] אז בניתי f חדש כזה, שעכשיו הוא מוגדר על פני ה-x
[00:39:53 - 00:39:55] הוא בנוי ככה שאני משלם את אותו מחיר
[00:39:55 - 00:39:56] כמו ששילמתי במקור
[00:39:57 - 00:40:01] על x1, אני משלם את אותו מחיר כמו ששילמתי במקור על x2
[00:40:02 - 00:40:05] ועכשיו יש לי גם עוד מחיר שעד כמה x1 ו-x2 קרובים
[00:40:07 - 00:40:09] אז מה התחומות של הפונקציה הזאת?
[00:40:10 - 00:40:11] כשבטא היא מאוד גדולה,
[00:40:13 - 00:40:14] אז בעצם המחיר הזה
[00:40:15 - 00:40:16] הופך להיות מאוד דומיננטי
[00:40:17 - 00:40:23] מה זה אומר? זה אומר שמבחינת ההסתברות הזאת תהיה הסתברות מאוד מאוד נמוכה ש-x1 ו-x2 יהיו שונים
[00:40:25 - 00:40:30] כי המחיר הזה הוא עוד פשוט מחיר גדול מדי, לא כדאי להתפלגות לתת משקל שם
[00:40:32 - 00:40:34] אז במקרה כזה בעצם עבור
[00:40:35 - 00:40:37] בית המספיק גדול
[00:40:42 - 00:40:46] אנחנו מגיעים למצב שההתפלגות השולית
[00:40:46 - 00:40:47] היא בין x1
[00:40:49 - 00:40:51] תבע ההתפלגות השולית של x2
[00:40:51 - 00:40:55] ושניהם פשוט שוברות להתפלגות המקורית שלנו
[00:41:01 - 00:41:04] כשבטא מספיק גדול הדבר הזה גורר לזה גם ככה להיות תמיד שווה
[00:41:05 - 00:41:06] אז חזרתי למקרה
[00:41:09 - 00:41:10] שהייתי כאן, זה יהיה חצי
[00:41:15 - 00:41:17] ואז כשזה תמיד קורה,
[00:41:18 - 00:41:20] והדבר הזה תמיד מקבל קרוב לאפס
[00:41:22 - 00:41:25] בעצם מה שזה אומר שאיתך את x2 זה אותו דבר
[00:41:25 - 00:41:27] ויש לי פה את שני החצאים האלה שהם נסכמים
[00:41:28 - 00:41:30] ואני פשוט חוזר לפונקציה המקורית שלי
[00:41:31 - 00:41:35] ועבור בית המספיק גדול הדבר הזה זה קירוב מספיק טוב לפונקציה המקורית שלי
[00:41:37 - 00:41:42] ועוד תנאי שקורה עבור בית המספיק גדול, לאו דווקא באותו גודל
[00:41:42 - 00:41:43] זה שאני יכול
[00:41:46 - 00:41:46] להתרגם שוב עבור
[00:41:47 - 00:41:48] בית המספיק גדול
[00:41:51 - 00:41:53] ‫אפשר
[00:41:55 - 00:41:56] ‫לקרב
[00:41:58 - 00:41:59] ל-F של x2
[00:42:02 - 00:42:10] ‫על ידי קירוב טיילור סביב X1.
[00:42:11 - 00:42:13] זאת אומרת, F של x2
[00:42:15 - 00:42:16] ‫יש על איזה ערך
[00:42:17 - 00:42:18] ל-F של x1
[00:42:21 - 00:42:24] ‫ועוד נגזרת של x2
[00:42:39 - 00:42:41] ‫זה העבר הראשון של שורותינו.
[00:42:51 - 00:42:55] ‫אוקיי, ריבור הבנייה,
[00:42:56 - 00:43:00] בעצם הפכנו את ההתפלגות הבקורית שלנו ‫להתפלגות על שני משתנים מקריים.
[00:43:00 - 00:43:03] ‫כל משתנה כזה יכול להיות וקטור בעצמו, כן, ‫עם הקורדינטים, אבל
[00:43:04 - 00:43:07] ‫לצורך העניין זה יהיה שתי הקורדינטות שלנו, ‫שאיתן אנחנו נעשה גיב סמפטינג.
[00:43:09 - 00:43:09] ‫אוקיי, אז יהיו לנו
[00:43:11 - 00:43:15] שתי קורדינטות, אבל אנחנו נראה ‫שכשווית המספיק גדול, ‫בעצם הן בורחות ומתקרבות
[00:43:18 - 00:43:18] לאותו ה-X.
[00:43:18 - 00:43:21] זו התפלגות חשובית שלהן, שכל אחת מהן
[00:43:21 - 00:43:26] ‫היא הולכת ומתקרבת, ההתפלגות השולית ‫שבעצם מבנית אותנו בסופו של דבר.
[00:43:27 - 00:43:36] אוקיי, אז מה אנחנו צריכים בשביל גיב סמפלינג? ‫אנחנו צריכים לבנות את ההתפלגות המותנית ‫של X1 ויינתן X2 ושל X2 ויינתן X1.
[00:43:42 - 00:43:45] התחלנו מהכיוון של X2 ויינתן X1,
[00:43:45 - 00:43:50] ובואו נחזור לוב של P של X2 בינתיים X1.
[00:43:51 - 00:43:52] למה הוא שווה?
[00:43:52 - 00:43:55] אז אני אסתכל על ההתפלגות הזאת.
[00:43:55 - 00:43:58] X2 עכשיו, X1, סליחה, זה פשוט קבוע.
[00:43:59 - 00:44:02] מה שנשאר לי זה X2 ויינתן X1,
[00:44:03 - 00:44:04] נכון?
[00:44:05 - 00:44:05] אז
[00:44:07 - 00:44:09] אני מניח שבטא הוא מספיק גדול,
[00:44:09 - 00:44:12] אז אני אשתמש בקירוב הזה שיש לי כאן.
[00:44:13 - 00:44:14] אז מה נשאר לי? נשאר לי
[00:44:21 - 00:44:30] קודם כל הקירוב הזה,
[00:44:31 - 00:44:32] אני מתחיל מהאיבר הזה,
[00:44:33 - 00:44:35] אז זה שווה
[00:44:37 - 00:44:38] מינוס חצי
[00:44:40 - 00:44:42] F של X1
[00:44:44 - 00:44:44] לעוד
[00:44:52 - 00:44:59] ועוד יש לי את האיבר הזה, אוקיי?
[00:45:07 - 00:45:08] במקור ומקור היה לי חצי
[00:45:09 - 00:45:11] לפני שהדברים יהיו יותר נחמדים אחר כך.
[00:45:12 - 00:45:15] כן, אז הייתם בטח לפני שתיים, זה לא משנה.
[00:45:15 - 00:45:18] תראה, בטח יותר גדול שיגרור לדבר הזה יהיה נכון.
[00:45:19 - 00:45:20] אז נשאר לי פה,
[00:45:20 - 00:45:22] ועוד
[00:45:24 - 00:45:26] ניתן פה נכון, שזה מינוס
[00:45:27 - 00:45:29] או למינוס
[00:45:30 - 00:45:33] מינוס חצי F1 להקירוב אלו פחות
[00:45:34 - 00:45:36] בתח חלקי שתיים
[00:45:42 - 00:45:42] קבוע
[00:45:44 - 00:45:44] ועוד
[00:45:45 - 00:45:48] הדבר הזה, נכון? אני רוצה להוסיף את הדבר הזה
[00:45:48 - 00:45:50] שגילגתי אליו, אבל הוא קבוע
[00:45:50 - 00:45:51] עכשיו מבחינתי
[00:45:52 - 00:45:55] לא אכפת לי, גם ככה אני הולך לנרמל את הדבר הזה
[00:45:59 - 00:46:01] וזה לא עוד איזה שיעור קבוע
[00:46:09 - 00:46:11] כן, זה לוג של ההסתברות
[00:46:12 - 00:46:14] זה כל החלקים שהאמת שגם זה
[00:46:15 - 00:46:17] הקצת יותר אבל זה גם הולך להיכנס לקבוע
[00:46:18 - 00:46:19] אין פה משהו שתביא ב-X2
[00:46:20 - 00:46:22] נכון, אז דברים שקוראים ב-X2 זה רק האיבר הזה והאיבר הזה
[00:46:24 - 00:46:26] אז בואו נפחק את זה, זה בעצם שווה ל...
[00:46:28 - 00:46:28] נשאר לי האיבר
[00:46:32 - 00:46:32] F של
[00:46:34 - 00:46:35] X1
[00:46:37 - 00:46:39] כפול X2
[00:46:44 - 00:46:46] זה וקטור, כן? וזה וקטור
[00:46:46 - 00:46:58] זה וקטור כפול וקטור, לדבר הזה גם אני יכול לכתוב את זה בתור וקטור כפול וקטור,
[00:47:00 - 00:47:01] נכון? אני יכול לכתוב, לפתוח את זה
[00:47:03 - 00:47:07] דילגתי כאן כמה דברים, על זה אני מדלג כי זה נכנס לי לתוך הקבוע
[00:47:07 - 00:47:10] גם על הנגזרת ה-X1 כפול X1 אני יכול לדלג,
[00:47:11 - 00:47:13] זה גם נכנס לי לקבוע, זה לא תביא ב-X2
[00:47:13 - 00:47:15] ופה יש לי
[00:47:16 - 00:47:16] את
[00:47:17 - 00:47:20] מינוס בטא חלקי 2 של X2
[00:47:22 - 00:47:25] 1 ופה יש לי מינוס בטא חלקי 1
[00:47:27 - 00:47:28] אוקיי, גם פה,
[00:47:28 - 00:47:31] זה כבר כמו ההשלמת לליבוע שראינו,
[00:47:32 - 00:47:34] גם פה יש לי איברים שהם פלוטסולים ב-X2
[00:47:34 - 00:47:39] והאיברים, בעצם מה שכן קורה ב-X2, יש לי איברים ליניאריים ואיברים ריבועיים
[00:47:41 - 00:47:44] מה זה אומר לי? אם הגעתי למצב שהלוג של ההסדרות
[00:47:46 - 00:47:46] הוא
[00:47:49 - 00:47:54] הוא שווה לאיזשהו קבוע ועוד דברים שהם עד איברים ריבועיים
[00:47:55 - 00:47:56] במשתמש שלי
[00:47:59 - 00:48:01] כן זה גם משהו שווה בתרקדים הראשון
[00:48:02 - 00:48:04] שלמר שהדבר הזה הוא גאוסיאני,
[00:48:04 - 00:48:06] אני יודע כבר שהדבר הזה הוא גאוסיאני
[00:48:06 - 00:48:10] ומה אני צריך כדי למצוא את ה... אז כמו שאני צריך למצוא את התוחלת ואת השונות של זה
[00:48:12 - 00:48:14] אתם זוכרים? יש כמה טריקים כדי לעשות את זה,
[00:48:14 - 00:48:19] יש טריק אחד של גזירה ויש טריק אחד של ראינו לפי המבנה הריבועי שזה נראה
[00:48:21 - 00:48:26] אז בואו נביא את זה רגע למבנה שכמו שראינו את זה בתרגיל, אז יש לי מקדם ליניארי של X2 שזה
[00:48:28 - 00:48:28] מינוס
[00:48:32 - 00:48:33] מגזרת הגרדיאנט הזה
[00:48:35 - 00:48:39] יש פה עוד מקדם ליניארי שזה יהיה X2 פחול X1
[00:48:41 - 00:48:42] מה אני מופיע נכון?
[00:48:43 - 00:48:44] זה יהיה בלי החצי הזה
[00:48:44 - 00:48:46] עוד פעם שכחתי גם חצי
[00:48:52 - 00:48:55] פה יש לי חצי ופה יש לי
[00:48:57 - 00:48:58] מינוס בטא
[00:49:01 - 00:49:01] X1
[00:49:03 - 00:49:08] כל זה זה המקדם הליניארי של X2
[00:49:10 - 00:49:13] והמקדם הריבועי זה X2 הזה כפול X2 הזה
[00:49:14 - 00:49:21] אפשר גם לכתוב את זה בתור תבנית ריבועית כמו שראינו בתרגיל זה יהיה X2 אחוז כפול
[00:49:24 - 00:49:24] בטא
[00:49:25 - 00:49:28] X2 I כפול X2
[00:49:34 - 00:49:37] וברגע שזה הצורה של העבוד קבוע
[00:49:40 - 00:49:43] ברגע שזו הצורה שהגעתי אליה, צורה הריבועית הכללית,
[00:49:43 - 00:49:50] אז אתה יודע אם אתם זוכרים בתרגיל ראינו שבעצם זה ה-covariance זה ה-precision זה ההופכי של ה-covariance
[00:49:51 - 00:49:52] מה שמורכב את התבנית הריבועית
[00:49:53 - 00:49:57] ומה שמורכב אותה חצי וזה כפול זה
[00:49:59 - 00:50:00] זה התוכלת
[00:50:00 - 00:50:01] זוכרים את זה מהתרגיל?
[00:50:05 - 00:50:09] יש עוד טריק שפשוט גוזרים, הנגזרת הראשונה נצטרך את התוכלת והנגזרת השנייה זה כבר
[00:50:11 - 00:50:11] גם רואים בי
[00:50:12 - 00:50:17] אוקיי, אז הדבר בשביל לבנות את התוכלת זה בסופו של דבר אני יודע שהדבר הזה שווה
[00:50:23 - 00:50:23] חץ
[00:50:24 - 00:50:26] פי של X2 בינתיים X1
[00:50:27 - 00:50:30] הוא קונסטיאן X2
[00:50:31 - 00:50:32] והתוכלת היא
[00:50:33 - 00:50:34] ההופכי של Z כפול Z
[00:50:35 - 00:50:38] ומה זה ההופכי של Z? זה פשוט לחלק בבית הר חלקי 2
[00:50:40 - 00:50:40] אז אני מקבל
[00:50:41 - 00:50:41] כאן
[00:50:46 - 00:50:48] נספור אם נקבל X1 פשוט
[00:50:56 - 00:51:01] X1 חלקי 1 חלקי בטא
[00:51:03 - 00:51:04] X2
[00:51:07 - 00:51:08] זה התוכלת
[00:51:09 - 00:51:11] והקוברנט זה פשוט
[00:51:12 - 00:51:14] ההפכי של זה, בלי החצי
[00:51:15 - 00:51:17] זה 1 חלקי בטא I
[00:51:20 - 00:51:24] אוקיי, אם אתה לא זוכר אם אנחנו רואים פה לפה, זה בתרגיל הראשון שלכם
[00:51:25 - 00:51:26] אוקיי
[00:51:27 - 00:51:37] אז בעצם מה גלינו? שה-X2 יינתן X1 ועל האופיין הזה אוקיי, אותו דבר הייתי יכול לעשות הפוך, רק פה הבדל בין X2 ל-X1, אותו דבר
[00:51:38 - 00:51:41] על גלגל X1 להינתן X2
[00:52:05 - 00:52:07] ועכשיו בעצם אני יכול להפעיל את זה איטרטיבית
[00:52:07 - 00:52:10] כמו בגיב סאמפלינג יש לי התפלגות
[00:52:10 - 00:52:11] כזאת
[00:52:12 - 00:52:15] ואני יודע לחשב את ההתפלגות המותנית של כל אחד מהם
[00:52:18 - 00:52:23] ועכשיו אני פשוט עושה גיב סאמפלינג, זאת אומרת אני מתחיל מאיזשהו משהו מונומלי ואז
[00:52:24 - 00:52:29] דוגמת X2 ונתן X1 מקבל איזשהו X2 ונתן X1 ועוד דוגמת X1 ונתן X2
[00:52:30 - 00:52:30] מקבל את הדבר הזה
[00:52:31 - 00:52:35] וככה עד שאני מתכנס להרבה צעדים
[00:52:36 - 00:52:37] כאשר בטא
[00:52:37 - 00:52:38] הולך וגדל
[00:52:39 - 00:52:43] ואז מובטח לי שההתפלגות השולמיות האלה הן פעלות לי את אותו דבר
[00:52:44 - 00:52:48] ושסך הכל ההתפלגות השולמית היא, ההתפלגות הסטציונרית
[00:52:49 - 00:52:50] היא אולי דבר רגע
[00:52:51 - 00:52:52] שבעצם מספיק כזה
[00:52:53 - 00:52:59] ובעצם הגעתי בדיוק לקהל העדכון של גיב סאמפלינג, אם אני מתעלם מזה שפעם אני קורא לזה 2 ופעם אני קורא לזה 1
[00:53:00 - 00:53:01] כי בעצם בכל איטרטיה מה אני עושה
[00:53:03 - 00:53:06] מה זה אומר שאני תביע מהגוונטיאנג הזה
[00:53:07 - 00:53:09] זו התוחלת שאני לוקח, אני לוקח את התוחלת הזאת
[00:53:10 - 00:53:14] ואני מוסיף לתוחלת הזאת רעש גאוסיאני עם ורנטס 1 חלקי בטא
[00:53:15 - 00:53:18] אז אם האפסילון שהיה לנו קודם
[00:53:18 - 00:53:20] יהיה שווה ל-1 חלקי בטא
[00:53:21 - 00:53:25] אז אנחנו נתבל את כלל העדכון
[00:53:25 - 00:53:36] רואים את הקשר ביניהם?
[00:53:40 - 00:53:41] זו בעצם התוחלת
[00:53:43 - 00:53:47] XT מינוס 1 זה בעצם העותק השני
[00:53:48 - 00:53:49] לפחות הנגזרת הזאת
[00:53:51 - 00:53:54] ועוד רעש גאוסיאני עם קובריאנס כזה
[00:53:55 - 00:53:59] ‫אז לדגום ההתפלגות הזאת ניתן דבר כמו
[00:53:59 - 00:54:03] להוסיף את התוחלת ואז לדגום ההתפלגות עם תוחלת על התחלת הזאת
[00:54:05 - 00:54:07] והעדכון הזה שווה לכלל ההתפלגות הזאת
[00:54:11 - 00:54:14] וזה אומר שבעצם יש כאן התפלגות
[00:54:15 - 00:54:18] MCM, התפלגות היא מרקוב צ'יין שההתפלגות
[00:54:18 - 00:54:21] הרציונרית שלה זה ההתפלגות המקורית שהתחלנו ממנה
[00:54:22 - 00:54:26] ‫עכשיו, יש פה כל מיני תנאים וכל מיני דברים
[00:54:30 - 00:54:32] ‫כל מיני קירובים שעשינו, נכון? עשינו קירוב טיילור,
[00:54:33 - 00:54:37] ‫אנחנו, שההיבט הזה הוא מספיק גדול ‫כדי שהעבר הזה יהיה מספיק דומיננטי,
[00:54:38 - 00:54:44] ‫אז התנאים של מתי הדבר הזה מתכנס, ‫הם תלויים למתי הדברים האלה, הקירובים האלה, ‫הם מספיק טובים,
[00:54:46 - 00:54:50] ‫וגם אנחנו צריכים את התנאים הבסיסיים ‫שבהם גיפס מתכנס,
[00:54:50 - 00:54:52] ‫שזה התנאי של האלגודי מספיק כבר.
[00:54:53 - 00:54:55] ‫אז אם התנאים האלה מתקיימים, ‫אפשר להוכיח,
[00:54:55 - 00:54:59] ‫זו בעצם ההוכחה של אנג'ל מידי דיינג ‫מתכנס.
[00:55:00 - 00:55:09] ‫לרוב עבור המקרים המעניינים, ‫למשל שהאלפיץ זה איזושהי רשת פירונים, ‫כמו שאמרנו, אי אפשר להוכיח את הדברים האלה, ‫ועוד יותר גרוע מזה, ‫אי אפשר לדעת מה הבטות שצריך
[00:55:10 - 00:55:12] ‫כדי להוכיח את זה, ‫או מה הקצב,
[00:55:13 - 00:55:16] ‫מה האפסינון שאנחנו בעצם רוצים ‫אותו דבר, מה הבטות זה כמו ‫מה האפסינון שאנחנו צריכים
[00:55:17 - 00:55:19] ‫כדי שהדבר הזה יתכנס.
[00:55:20 - 00:55:24] ‫שוב, זה נושא שיש עליו הרבה מחקר, ‫איך לתרום לזה, מה הקצבים הנכונים,
[00:55:25 - 00:55:31] ‫אבל גם הרבה פעמים, ‫אם לא יודעים להוכיח את ההוכחות, ‫בפועל זה עובד בהרבה מקרים.
[00:55:32 - 00:55:35] ‫אבל גם שם, צריך לדעת ‫באיזה קצבים להשתמש, ‫בשרבה מחקר,
[00:55:36 - 00:55:42] ‫לא רק על איך להוכיח שזה מתכנס, ‫אלא על איזה קצבים ואיזה סוגים של דעיכות
[00:55:42 - 00:55:47] שונות של האפסילון פה יעבדו יותר טוב ‫כשמפעילים את זה באמת,
[00:55:48 - 00:55:53] ‫וכשנגיע לשלב שנשתמש בזה, ‫אז נראה כל מיני שיטות שמשתמשים בזה.
[00:56:02 - 00:56:06] ‫טוב, סיימנו את השלב של הדגימה ‫ונשארנו על הנושא האחרון,
[00:56:07 - 00:56:12] ‫שנדברה כבר יש חלקים, ‫את ההפתקה של איזה עשר דקות לרבע שעה?
[00:56:12 - 00:56:12] ‫-אז אה...
[00:56:42 - 00:56:48] ‫דילגתי קצת. ‫יש את הלור בזה? ‫-לא, לא, לא, איך שאתה לא, ‫זאת אומרת, יש את ה...
[00:56:51 - 00:56:59] ‫אז מה זה אומר לעבור? ‫כשיש לי התרגלות משותפת כשהיא כתובה ככה, ‫אז מה זה אומר לעבור התרגלות מותנית? ‫זה אומר להגיד, ‫אני מניח שהדבר הזה הוא קבוע.
[00:56:59 - 00:57:01] ‫בעצם קבעתי כאן אותו דבר.
[00:57:02 - 00:57:04] ‫כן. טוב, במקום הקונסטרסט הזה, ‫אם הייתי כותב את הדבר הזה,
[00:57:06 - 00:57:12] ‫זה היה כתוב בדיוק אותו דבר. פשוט מה שרציתי להגיד, ‫אולי לעשות את זה הכי טוב, זה שכל מה שכתוב בו x1,
[00:57:12 - 00:57:18] ‫מבחירתי עכשיו הוא קבוע, ‫ולאט לאט נכנסתי עכשיו לתוך ה... ‫-אה, הבנתי, ידעת, ‫כי באת אותו בתור... ‫זה שנשארו לי רק האיברים האלה.
[00:57:19 - 00:57:19] ‫כאילו בגלל...
[00:57:21 - 00:57:24] ‫ומאמרתי שזה מאוד שווה בסוף זה, ‫כאילו אנחנו דוגמים...
[00:57:25 - 00:57:30] ‫אוקיי, אם אנחנו נלכת שנייה על החיזוקים, ‫אנחנו ניסינו ללמוד את הפוליסי הכי טובה, ‫לפצועות זה שערנו פוליסי
[00:57:31 - 00:57:37] ‫גנרציה של פוליסי סתם עם מלחין רנדומניה, ‫אבל אז קודם כל כך זה נראה ‫תוך כדי שאנחנו ניגמרו מהעולם,
[00:57:37 - 00:57:38] ‫לנסות לנוע ולראות
[00:57:39 - 00:57:40] ‫מה הפוליסי הכי טובה.
[00:57:41 - 00:57:46] ‫פה אני מנסה להבין מה אנחנו בעצם עומדים, ‫ללמד פה לייצר דנדה.
[00:57:47 - 00:57:49] ‫כי פה אתה רוצה לדגום, ‫יש לך מודל שאתה מחזיק אותו,
[00:57:50 - 00:57:57] ‫זה לא כמו ב-RL, ‫יש לי כבר תוך מודל. ‫יש לך, זאת אומרת, ‫אם אני מסתכל על האפיקס, ‫האפיקס נתונה לי,
[00:57:58 - 00:58:01] ‫ואני רוצה לדגום איתו פוליסט.
[00:58:01 - 00:58:05] ‫אוקיי, עכשיו יכול להיות שאתה רוצה ‫לדגום ממנו כחלק מתהליך התקדמי.
[00:58:06 - 00:58:08] ‫-מתהליך מתקדמי.
[00:58:13 - 00:58:19] ‫-יש כמה מצבים. קודם כל, יכול להיות שזה בדיוק מתקדמי עכשיו,
[00:58:19 - 00:58:21] ‫יש לי את המודל, ‫ואני רוצה לדגום ממנו.
[00:58:22 - 00:58:26] ‫אז זה, למשל, יש לי את המודל ‫דיפיוז'ן של זה, לא יודע, ‫אפשר לדגום ממנו ויוצר כמונו.
[00:58:27 - 00:58:28] ‫זה עכשיו משהו.
[00:58:28 - 00:58:32] ‫בשימוש בפאדג' הזה, ‫אבל בעצם מתוקבים ככה, כאילו,
[00:58:33 - 00:58:38] ‫כן, מתוקבים ככה, ‫בעצם מה שאנחנו מציגים ‫זה את ה-F הזאת,
[00:58:38 - 00:58:41] ‫זה איזושהי רשת שנותנת לנו את ה-F הזאת,
[00:58:42 - 00:58:45] ‫ופשוט מפעילים את האלבואית ‫באלף צעדים,
[00:58:46 - 00:58:48] ‫וזה מתקדמי יעשות הגולימות של הצפונים.
[00:58:50 - 00:58:51] ‫כי הוא רצה לי רק,
[00:58:52 - 00:58:54] ‫מה אם אני לוקח עכשיו שנייה לדליה, ‫אז אני נתן איזשהו פרונט,
[00:58:55 - 00:59:00] ‫הפרונט הזה... ‫-אז ה-F הזה הוא לא סתם F של X, ‫הוא F של X בהינתן טקסט.
[00:59:01 - 00:59:03] ‫היא נותנת טקסט, ‫כל טקסט בעצם מייצר F אחר.
[00:59:04 - 00:59:08] ‫-אוקיי, ה-F הזה זה בעצם הרעש שאנחנו,
[00:59:09 - 00:59:09] כאילו,
[00:59:10 - 00:59:12] ‫הרעש אנחנו מכניסים בעצם לאגן או...
[00:59:13 - 00:59:17] ‫לא, זה לא הרעש, ‫כי גם אם הוא טוב פרונט, ‫אתה יכול להפעיל אותו כמה פעמים, ‫כל פעם יש לך משהו אחר כבר.
[00:59:18 - 00:59:24] ‫אז זה לא הרעש. ‫הרעש זה... זה לא זה, ‫יש פה כאילו איזשהו משתנה חבוי, ‫איך לדבר כדוי...
[00:59:25 - 00:59:28] ‫שוב, הדבר הזה נותן לך התבלגות, ‫זה לא נותן לך תמונה.
[00:59:31 - 00:59:32] ‫הרעש הוא פה, הוא נכנס כאן, אוקיי?
[00:59:33 - 00:59:37] ‫אני עושה את הדבר הזה אלף פעמים, ‫אז כל פעם אתה מקבל הרבה רעש נכנס.
[00:59:39 - 00:59:46] ‫המימד הזה הוא גם מימד גדול, כן? ‫המימד הזה הוא גם מימד גדול. ‫אלף פעמים ואתה עושה מיליון פיקסלים, ‫שאתה דוגמת אוסיין.
[00:59:47 - 00:59:55] ‫ככה כל פעם אתה מקבל. ‫זה בעצם מה שקוראים לזה שכל פעם, ‫אפילו אם הפרומט שלך היה בדיוק אותו פונט, ‫זה תקבל תמונה אחרת.
[00:59:58 - 01:00:08] ‫אבל זה שאתה משנה את הפרומט, ‫הוא בעצם משנה את ה-F, ‫ה-F הוא הפונקציה של הפרומט, זה פונקציה של X. ‫אז הוא פונקציה שונה, הוא אופן משקל אחר, ‫לסוג אחר של תמונות.
[01:00:10 - 01:00:15] ‫התקופה שחשבתי פה זה הפרומט האפליקטיבי יותר, ‫שאני רוצה לדמיין בו ראש בי איך?
[01:00:16 - 01:00:17] ‫אז מהפונקציה הזאת אני עכשיו,
[01:00:18 - 01:00:25] ‫כאילו, כבר זאת עכשיו ה-Scapping הזה ‫שיש לך את ה-Lence-Aווין סאמפלגי,
[01:00:26 - 01:00:30] ‫אני בעצם עכשיו בחרונות X, ‫פה זה כל פעם אחרונה בסופו של דבר.
[01:00:31 - 01:00:36] ‫אנחנו נראה, לא דיברנו על זה, ‫אבל אנחנו נראה איך שזה, איך XT יתחיל מרעש לאוסייני,
[01:00:37 - 01:00:37] ‫מרעש של רעש,
[01:00:38 - 01:00:43] ‫שנפעיל על זה את אלף צעדים כאלה, ‫לאט לאט זה יתחיל מרעש,
[01:00:46 - 01:00:46] ‫טוב.
[01:00:48 - 01:00:49] ‫אבל כאילו, רק רעש,
[01:00:49 - 01:00:51] ‫אחרי זה נכנס כאילו לסיגנל,
[01:00:52 - 01:00:54] ‫וזה יפה להיות משהו ‫מהתפלגות של תמונות.
[01:00:56 - 01:00:58] ‫כאילו, מה שמודד בסופו של דבר, ‫אנחנו נראים לי...
[01:00:59 - 01:01:01] ‫אם אני לומד כבר רשת נוירונה מסוימת,
[01:01:02 - 01:01:09] ‫ויש לי כבר את רשת ה-F, ‫אז אני בעצם, מה שעכשיו אני עושה, ‫אני פשוט נותן רעש לרשת הזאתי ‫באבצעית שמונה.
[01:01:09 - 01:01:12] ‫אז זה בחר של מודד, או שמגנים כאלה,
[01:01:12 - 01:01:14] ‫אז זה מובן של F נותן לך את התמונה.
[01:01:14 - 01:01:15] ‫הפואר רצות של F זה ה...
[01:01:17 - 01:01:18] ‫האמת שזה...
[01:01:19 - 01:01:24] ‫יש כמה דרכים, כן? ‫יש איזו דקות שלא נכנסת לנגד, ‫אפשר ללמוד, אבל יש מודלים,
[01:01:24 - 01:01:29] ‫שאם זה באמת כמו שאמרתי, ‫אז בעצם הארצות של 0 זה מספר, ‫זה לא כל כך חשוב.
[01:01:30 - 01:01:31] ‫ואז
[01:01:32 - 01:01:33] בהינתן X,
[01:01:34 - 01:01:35] ‫זה ייתן איזושהי תמונה,
[01:01:35 - 01:01:37] ‫או אתה יכול להיות תמונה של רעש, ‫תמונה,
[01:01:37 - 01:01:39] ‫הדבר הזה נותן מספר שאומר,
[01:01:40 - 01:01:41] ‫האנרגיה של התמונה הזאת.
[01:01:41 - 01:01:46] ‫כפי של הדבר הזה, אלף פעמים, ‫אתה מקבל משהו שמגיע ‫מההתפלגות.
[01:01:48 - 01:01:52] ‫אם ההתפלגות הזאת ‫התפלגות טובה לתמונות, ‫אז אתה אמור לקבל משהו ‫שנראה תמונה טובה.
[01:01:54 - 01:01:58] ‫אה, אני מקבל פשוט הסתברות גמורה, ‫ועוד אני אדע בעצם ש...
[01:02:00 - 01:02:04] ‫אפונקציות אנרגיות אמורות לתת לך, ‫אם ביטשת בכלבים,
[01:02:05 - 01:02:08] ‫אז זה אמור לתת ציון מאוד נמוך ‫למשהו שהוא לא נראה בכלל כמו תמונה,
[01:02:09 - 01:02:11] ‫אבל גם ציון די נמוך למשהו שנראה כמו חתול,
[01:02:12 - 01:02:14] ‫אז כמו שנראה כוכב, הוא יקבל ציון דברי.
[01:02:15 - 01:02:20] ‫אתה רוצה לייצר תגומים מדהימות מזה, ‫אתה לא רוצה רק בשאול ‫האם זה נראה כוכב או לדבר.
[01:02:21 - 01:02:22] ‫זה נותן לך, יש לנו,
[01:02:22 - 01:02:23] ‫אם אז זה דגל.
[01:02:25 - 01:02:25] ‫אוקיי.
[01:02:31 - 01:02:35] ‫בואו זה כמו זה, אני מנסה לדמיין, ‫זה כמו תספין דומה, אבל זה לא בדיוק.
[01:02:35 - 01:02:40] ‫זה כן יכול להיות דומה, זה מיני דבר. ‫זה כן נותן פה, אתה אומר, איך זה דומה לי עבור...
[01:02:41 - 01:02:50] ‫תמונה, זה די מספין בין דיסקרימינטור קריטי. ‫- נכון דיסקרימינטור, ‫הם משתמשים גם לפעמים דיסקרימינטור ‫כדי להפוך את זה ללא...
[01:02:50 - 01:02:54] ‫אם יש לך F שהוא אחראי לזה שתהיה תמונה, ‫באופן כללי,
[01:02:54 - 01:02:56] ‫אתה מוסיף לזה F,
[01:02:57 - 01:03:01] ‫עוד F שהוא מגיע מאיזשהו דיסקרימינטור ‫שאומר למה תמונה של כלב, ‫יש לו חתול.
[01:03:02 - 01:03:03] ‫אז אתה יכול לבקש לפי זה כלב או חתול.
[01:03:06 - 01:03:08] ‫הם משתמשים בדיוק לדיסקרימינטור בפי זה.
[01:03:11 - 01:03:19] ‫אבל לא תמיד אנחנו רוצים, ‫הנקודה ב-NC and C זה ‫שלא תמיד אנחנו נותנים את הדגימות.
[01:03:20 - 01:03:22] ‫הפעמים אנחנו עושים את הדגימות ‫כדי לחשב משהו אחר.
[01:03:23 - 01:03:29] ‫כלומר, לדוגמה, זה היה NC הראשון ב-NC. ‫אהשני בעצם, אמרתי קרוב, נו תקרוב.
[01:03:29 - 01:03:30] זה ה...
[01:03:31 - 01:03:32] למשל, אתה צריך לחשב איזושהי תוכנית,
[01:03:33 - 01:03:34] אבל ההסתברות,
[01:03:34 - 01:03:37] ‫שאנחנו נדבר על זה עכשיו, ‫כי אנחנו נדבר על תשכה אחרת לעשות את הדברים האלה,
[01:03:38 - 01:03:38] אבל
[01:03:38 - 01:03:46] ‫מה שמעניין אותנו זה לא הדגימות עצמן, ‫אלא הדגימות כאמצעי,
[01:03:46 - 01:03:48] ‫אני חושב איזושהי תוכנית.
[01:03:49 - 01:03:55] ‫אבל זה היה יכול להיות התפלגות, ‫אם יש לי איזשהו משתנך רבוע,
[01:03:56 - 01:04:02] ‫מודל עם משתנך רבוע, ו-X תמיד זה התמונה, ו-Y זה משתנך רבוע, ‫חומר ל-Mu על התמונה.
[01:04:03 - 01:04:07] ‫אבל אני רוצה לדעת, ‫אני רוצה לדעת מה ההתפלגות על Y.
[01:04:09 - 01:04:13] ‫אם יש תוכנית תשובית של Y. ‫של Y ואיזה תמונה.
[01:04:14 - 01:04:15] ‫בכל הדברים האלה צריך לחשב אינטגרלים.
[01:04:17 - 01:04:18] ‫תשמע את נראית זה.
[01:04:21 - 01:04:24] ‫בישהו נדבר על זה, נדגים עוד כדי לחשב את התמונה.
[01:04:25 - 01:04:29] ‫ואז את ה-Y זה כמו מה שאז אנחנו רואים ‫עם התמונה, זה תשמע את המוצא של אדם,
[01:04:30 - 01:04:37] ‫כאילו איש, ואז... ‫-Y זה לא חייב להיות משהו שהוא במרחב תמונה. ‫-לא, לא, זה לא בטוח, נראה הרבה.
[01:04:37 - 01:04:49] ‫לנתן ה-Y הזה, זה יהיה המקשר ‫לנתן ה-Y. ‫כן, אתה יכול להשפיע, נגיד, ‫על המבוצע של X בהינתן Y. ‫אפשר ל-Y בהינתן E. ‫בהינתן שיש לך תמונה,
[01:04:50 - 01:04:51] ‫מה יכול להיות Y?
[01:04:53 - 01:04:56] ‫בכל הדברים האלה, שוב, ‫זה בהינתן שכבר יש לך את המודל,
[01:04:56 - 01:04:59] ‫אבל זה גם יכול להיות, ‫הרבה פעמים, תלולה פנימית,
[01:05:00 - 01:05:01] ‫בתוך למידה של המודל.
[01:05:03 - 01:05:06] ‫זה יש לך איזה ניחוש נוכחי למודל, ‫אתהוא תראה כמה הוא טוב,
[01:05:06 - 01:05:19] ‫הסיגנע של הלמידה יכול להיות משהו ‫בדל שאתה יודע לעבוד איתו. ‫זה יכול להיות לפעמים מולה פנימית, ‫בתוך מולה חיתונית, ‫שהיא שיפור המודל.
[01:05:19 - 01:05:26] ‫זה כאילו, אפשר לוודידציה ‫לעשות את פרנקלין ולאנגל. ‫-כן, זה יכול להיות גם אוטומטי ממש.
[01:05:26 - 01:05:28] ‫זאת אומרת שזה ממש אוטומטי.
[01:05:28 - 01:05:30] ‫בתוך חלק מהלמידה,
[01:05:30 - 01:05:32] ‫עושה אינפיים על ידי דגימות,
[01:05:33 - 01:05:33] ‫ואז אתה
[01:05:37 - 01:05:38] ‫בעצם, פרנקלסי קרדינסנד גם,
[01:05:39 - 01:05:40] ‫או פעם דוגן כמה דוגמאות,
[01:05:42 - 01:05:44] ‫משתמש בזה כדי לשפר את המוחלות,
[01:05:45 - 01:05:46] ‫אני גם יודע, ואז אתה משפר את זה.
[01:05:47 - 01:05:55] ‫במובן הזה, הקונספטואלית זה אותו דבר. ‫יש חבולה פנימית שעושה גימות, ‫מייצרת את חתימות שאיתה את המשרת מהתוכלת.
[01:05:56 - 01:06:00] ‫ואז אתה משפר גם את השיערות של התוכלת, ‫אני מקווה שזה גם משתמש בזה בתוכלת.
[01:06:01 - 01:06:08] ‫-טוב, נראה שפשוט זה אולי משהו עזר לי, ‫אולי יעזור לעוד כמה דרך
[01:06:09 - 01:06:13] ‫הקבלה אולי למשהו שאנחנו ניצרנו להתעסק, ‫אולי זה יכול לעזור לך.
[01:06:13 - 01:06:18] ‫-כן, פה אני... ‫-הפעלה הזאת היא חצי ראשון תיאורטית, חצי שמינתי. ‫-אני בעד התיאוריה, אבל כאילו...
[01:06:19 - 01:06:25] ‫מה עשית לזה? ‫-פייסרתי שאני פעם לראה, לקבל תמונה כללית, כאילו,
[01:06:25 - 01:06:30] ‫פהוא אני בא לך, אני שומד מנגן ואני אומר לך את זה, ‫כן, עושה טיפה להשתמש באלף בערך כדי
[01:06:31 - 01:06:32] ‫לראות איך אתה בא איתך בבית הביבליקה.
[01:06:33 - 01:06:33] ‫טוב,
[01:06:34 - 01:06:34] ‫אחרי ש...
[01:06:35 - 01:06:40] ‫אחרי שאתה רואה מה יהיה, ‫אחרי שאתה עושה ואתה כאילו...
[01:06:40 - 01:06:45] ‫-כן, מותר עכשיו קצת לראות את כל הדברים האלה, ‫והחלק מהחלקה השני, זה החלקה של היבורים.
[01:06:47 - 01:06:47] ‫כזה,
[01:06:47 - 01:06:47] ‫תודה רבה לכם,
[01:06:48 - 01:06:50] ‫אחרי כל האדם ‫שזה יהיה יותר מגבי הרבה יותר טובה.
[01:06:52 - 01:06:56] ‫עד שאני אגיד לזה, איך שהוא מופיע ‫אחרי שהוא מופיע ‫אבל אנחנו נתחיל ללמוד רגילה,
[01:06:56 - 01:06:58] ‫כן, כמו כאילו,
[01:06:58 - 01:07:02] ‫לסטיימן ואלדסטיימן ואלדסטייאלי,
[01:07:02 - 01:07:04] ‫סופר אחרי את...
[01:07:07 - 01:07:10] ‫נתחיל מ...אוטו-ריגרסיב מודל,
[01:07:10 - 01:07:12] ‫אז נדבר על יום שאילי.
[01:07:28 - 01:07:32] ‫-כן, ככה, אדם ידעים ואלדסטייאלי.
[01:07:58 - 01:08:02] ‫-כן, בבקשה, אדוני ידעים ואלדסטייאלי.
[01:08:28 - 01:08:31] ‫היא לאישה איזה מישהו מבין בלבד הזה.
[01:08:32 - 01:08:34] ‫זה לא יעד ככה, אדוני ידעים ואלדסטייאלי.
[01:08:36 - 01:08:36] ‫תודה רבה.
[01:08:37 - 01:08:38] ‫תודה רבה.
[01:08:39 - 01:08:40] ‫תודה רבה.
[01:08:40 - 01:08:53] ‫-תודה רבה.
[01:09:10 - 01:09:40] ‫-תודה רבה.
[01:09:40 - 01:09:40] ‫-תודה רבה.
[01:10:10 - 01:10:10] ‫-תודה רבה.
[01:10:40 - 01:10:40] ‫-תודה רבה.
[01:11:10 - 01:11:10] ‫-תודה רבה.
[01:11:40 - 01:11:43] ‫-תודה רבה.
[01:12:10 - 01:12:14] ‫-טוב, בואו נמשיך.
[01:12:37 - 01:12:39] ‫אוקיי, אז תראה,
[01:12:40 - 01:12:43] ‫באתי את השקפים האלה עכשיו באתר, ‫מי שלפני ראש שבת.
[01:12:44 - 01:12:47] ‫הרבה השקפים פה מבוססים על קורס ‫באדינגורד,
[01:12:47 - 01:12:49] ‫אני מדבר על מודלים אינטרנטיים
[01:12:54 - 01:12:54] ‫ב-BDD, אינטרנטס.
[01:12:56 - 01:12:58] ‫אני מסתכל גם שם, כשמתחם על גורם האינטרנטים.
[01:12:59 - 01:13:02] ‫אבל זה גם אומר שחלק ממה שקראנו ‫לא התפלמים לזה,
[01:13:03 - 01:13:08] ‫כי למשל יגרו לוואייל ושתנת את הדברים, ‫וודאי פה כמה פיצויות ‫הנקווה שהם מסתדרו עם זה.
[01:13:09 - 01:13:14] ‫אז דיברנו היום כבר בעצם, ‫רצינו חזרה ל-NCMC ולפעילת דנג'ווין.
[01:13:15 - 01:13:17] ‫הנושא הבא שלנו זה ‫ברישיונל אינפרנס.
[01:13:18 - 01:13:20] ‫אנחנו נעשה קצת הכנה לזה,
[01:13:21 - 01:13:24] ‫נדבר שוב פעם על קייל דייברג'נס, ‫לפחום לתכונות של לוג,
[01:13:26 - 01:13:29] ‫ואז נפתח את ה... בעצם הדבר הזה חסם,
[01:13:30 - 01:13:32] ‫נפתח אותו, ‫ואז נדבר על איך משתמשים בו.
[01:13:39 - 01:13:43] ‫אז שאלו אותי פה עכשיו במהלך קר, ‫אבל הייתי לחזור לזה ‫גם בהקשר של דגימה.
[01:13:45 - 01:13:47] ‫בעצם, למה אנחנו רוצים לדגום,
[01:13:52 - 01:13:58] ‫אז הרבה פעמים אנחנו רוצים לדגום ‫בשביל הדגימה עצמה, אוקיי? ‫נגיד שלמדנו מודל, טוב,
[01:13:58 - 01:14:00] ‫אנחנו רוצים להשתמש במודל הזה ‫כדי לייצר דאטה חדש.
[01:14:03 - 01:14:07] ‫את הדוגמה הזאת של מודלים של תמונות, ‫מודלים ו-visual models של תמונות,
[01:14:08 - 01:14:11] ‫ניתנו לנו איזשהו מודל שלמדנו אותו ‫באיזושהי דרך,
[01:14:12 - 01:14:14] ‫עכשיו אנחנו רוצים להשתמש בו ‫כדי לייצר תמונות חדשות.
[01:14:15 - 01:14:18] ‫יכול להיות שזה מודל שהוא מותנה, ‫למשל, הרבה פעמים ‫אנחנו נותנים את זה על טקסט,
[01:14:19 - 01:14:24] ‫אז זה מודל שהוא מקבל איזשהו אינפוט, ‫ואז בעצם מתנתן האינפוט, ‫יש לנו מודל הסתברותי על תמונות.
[01:14:25 - 01:14:29] ‫מהאינפוט שלנו היה התמונה של כלב רץ,
[01:14:30 - 01:14:35] ‫אז בעצם זה הופך את המודל שלנו ‫למודל הסתברותי על תמונות של כלבים רצים,
[01:14:36 - 01:14:37] ‫מה שאומר שתמונות
[01:14:38 - 01:14:41] ‫הרצים אמורים לקבל הסתברות גבוהה ‫במודל הזה,
[01:14:41 - 01:14:44] ‫ותמונות של דברים אחרים ‫אמורים לקבל הסתברות נמוכה,
[01:14:45 - 01:14:47] ‫ודברים שהם בכלל לא נראים כמו תמונות, ‫כמובן ההסתברות נמוכה,
[01:14:48 - 01:14:53] ‫וזה המודל שלנו, ‫ואנחנו רוצים לדגום ממנו עכשיו, ‫לייצר הרבה דוגמאות
[01:14:53 - 01:14:56] ‫של כלבים רצים.
[01:14:58 - 01:15:00] ‫אחת מהשיטות זה למשל להשתמש ‫בשיטות מונטה קרלו,
[01:15:01 - 01:15:03] ‫ספציפית בשיטת מנג'לין דייננט.
[01:15:04 - 01:15:09] ‫אבל לפעמים אנחנו רוצים להשתמש ‫בדגימה, ‫לא בשביל הדגימה עצמה,
[01:15:09 - 01:15:14] ‫אלא בתור איזושהי לולאה פנימית ‫שמחשבת לנו נגיד איזשהו אינטגרל,
[01:15:16 - 01:15:17] ‫שקשה לנו לחשב אותו אחרת,
[01:15:18 - 01:15:19] ‫למשל בזמן הלמידה.
[01:15:20 - 01:15:23] ‫בזמן הלמידה ראינו, ‫אנחנו רוצים לחשב תוחלת
[01:15:24 - 01:15:24] ‫על
[01:15:27 - 01:15:29] כל הדוגמאות של הדאטה,
[01:15:29 - 01:15:31] ‫אבל אנחנו לא יכולים לראות את כל הדוגמאות,
[01:15:31 - 01:15:36] ‫אנחנו יכולים רק לדגום מתוכם כמה דוגמאות, ‫במקרה הזה הדגימה מגיעה מהעולם, ‫מה-training set שאספנו,
[01:15:37 - 01:15:38] ‫אבל גם זו שיטה על שיערוך התוחלת.
[01:15:39 - 01:15:42] ‫בגלל שאנחנו דוגמים כמה דוגמאות מעולם, ‫אנחנו משכחים את התוחלת.
[01:15:42 - 01:15:47] ‫יכול להיות שגם בתוך הלולאה, ‫אנחנו נתחיל לדבר על זה היום קצת,
[01:15:47 - 01:15:51] ‫יש לנו משהו שאנחנו צריכים לחשב, ‫שאנחנו לא יודעים לחשב אותו,
[01:15:52 - 01:15:54] ‫זה חלק מהמודל שלנו שאנחנו רוצים ללמוד.
[01:15:55 - 01:15:57] ‫אז אנחנו דוגמים בתוך המודל שלנו,
[01:15:58 - 01:16:04] ‫וזה עוזר לנו לחשב את הדבר הזה ‫בתור איזושהי לולאה פנימית ‫שאחר כך היא נתפסת ‫על הלולאה החיצונית של האימון.
[01:16:05 - 01:16:08] ‫היום לא נראה בדיוק את הדוגמה הזאת, ‫אבל נדבר על הלולאה הפנימית,
[01:16:08 - 01:16:10] ‫ואז בהחל מ...
[01:16:10 - 01:16:14] ‫זה יהיה כנראה עוד שבועיים ‫נדבר על הלולאה החיצונית כזאת ‫שמערבת את הלולאה הפנימית.
[01:16:16 - 01:16:22] ‫אז עכשיו אנחנו נדבר על... ‫אנחנו לא מדברים יותר על בדימה, ‫אז אנחנו מדברים על המקרה הזה ‫שאנחנו, יש לנו איזשהו אינטגרל,
[01:16:23 - 01:16:24] ‫אנחנו רוצים לחשב אותו, ‫קשה לנו לחשב אותו.
[01:16:25 - 01:16:31] ‫אז נמצא את הדוגמה הזאתי כבר, ‫נגיד שיש לנו איזשהו משתנה,
[01:16:31 - 01:16:32] ‫שני משתנים,
[01:16:33 - 01:16:36] ‫ואנחנו רוצים לחשב רק את ההתפלגות השולית.
[01:16:37 - 01:16:43] ‫אנחנו רוצים, יש לנו את הפונקציית ‫התפלגות על שני משתנים, ‫אנחנו רק רוצים את הפונקציה ‫התפלגות על משתנה אחד.
[01:16:44 - 01:16:48] ‫בשביל לעבור מהתפלגות על שני משתנים ‫להתפלגות במשתנה אחד, ‫צריך לעשות אינטגרל.
[01:16:49 - 01:16:51] ‫לפעמים האינטגרל הזה הוא קשה, ‫ועדיף לנו פשוט לדגום,
[01:16:51 - 01:16:54] ‫אם אנחנו יכולים לדגום הרבה דוגמאות ‫ולעשות ממוצע,
[01:16:55 - 01:16:57] ‫אז זה ייתן לנו את מה שאנחנו רוצים.
[01:17:00 - 01:17:04] ‫למשל, אם אנחנו רוצים, ‫אחד מהמשתנים האלה הוא חבוי,
[01:17:04 - 01:17:11] ‫אז זה דוגמה לסוג של לולה אקלימית, ‫ויש, אנחנו רוצים שתהיה לנו, ‫בתוך לולה החיצוני.
[01:17:12 - 01:17:15] ‫אז ראינו שאנחנו רוצים ללמוד ‫את הפרמטרים, תדע,
[01:17:17 - 01:17:18] ‫של איזשהו מודל,
[01:17:18 - 01:17:20] ‫יש לנו דאטה, לי,
[01:17:20 - 01:17:24] ‫אוקיי? אבל המודל שלנו הוא מודל כזה ‫שהוא גם צריך לקבל זה,
[01:17:25 - 01:17:28] ‫איזשהו משתנה חבוי כדי לתת את הישיבות.
[01:17:29 - 01:17:32] ‫אין לנו דאטה על זה, ‫אנחנו לא רואים את המשתנה של זה,
[01:17:32 - 01:17:36] ‫אנחנו צריכים, כדי לחשב את הלייטליות ‫של הדאטה שאנחנו רואים,
[01:17:37 - 01:17:39] ‫תחת המודל שלנו, ‫אנחנו צריכים למצוא את האינטגרל הזה.
[01:17:40 - 01:17:48] ‫ובתוך הלולאה שבה אנחנו כל פעם, נגיד, עושים ‫מקסימום לייטליות עם gradient descent, נגיד, ‫אנחנו כל פעם צריכים לשפר את תטא,
[01:17:48 - 01:17:53] ‫ובתוך זה צריכה להיות אולי ‫איזושהי לולאה שמחשבת את האינטגרל הזה, ‫שאנחנו נוגמים כמה דגימות,
[01:17:54 - 01:17:59] ‫ולפי זה אנחנו מחשבים את האינטגרל הזה.
[01:18:00 - 01:18:02] ‫שיטה אחת היא מבוססת על דגימות, ‫מה שעכשיו אנחנו נדבר עליה,
[01:18:03 - 01:18:05] ‫זו שיטה אחרת שהיא לא מבוססת על דגימות,
[01:18:05 - 01:18:11] ‫אלא היא מבוססת על שיטת קירוב אחרת, ‫שנקראת variation of inference,
[01:18:12 - 01:18:13] ‫שהיא בעצמה שיטת אופטימיזציה.
[01:18:14 - 01:18:17] ‫אז הלולאה הפנימית הזאתי, ‫היא תהיה בעצמה שיטת אופטימיזציה,
[01:18:18 - 01:18:21] ‫לא לייצר הרבה דגימות ולעשות ממוצע.
[01:18:23 - 01:18:26] ‫אני מקווה שעד סוף השירות ‫אתם תבינו קצת את הקשר ביניהם.
[01:18:31 - 01:18:38] ‫בשביל הצוות אנחנו בעצם זה יהיה בעיית אופטימיזציה, ‫שהיא קשורה,
[01:18:38 - 01:18:41] ‫שהיא תהיה על גבי התפלגויות,
[01:18:42 - 01:18:43] ‫על גבי פונקציות שהן התפלגויות,
[01:18:44 - 01:18:49] ‫ולכן אנחנו הרבה פעמים ‫אנדוד מרחק בין התפלגויות,
[01:18:50 - 01:18:53] ‫והמדד כרגיל שאנחנו נעבוד איתו ‫זה ה-KL Divergence.
[01:18:53 - 01:18:56] ‫אז נתחיל בכמה שקפים ‫שמדברים על תחומות של KL Divergence,
[01:18:57 - 01:18:59] ‫שכבר נתקלנו בהן, ‫אבל רציתי לעשות את זה יותר מסודר,
[01:19:00 - 01:19:04] ‫ואז נדבר על הוואיון של מה אנחנו עושים פה.
[01:19:04 - 01:19:07] ‫כן, עכשיו כמה שקפים ‫שאנחנו מדברים על KL Divergence,
[01:19:10 - 01:19:13] ‫כאילו לפני זה על לורות.
[01:19:14 - 01:19:21] ‫-כן, זה השיטה. ‫אבל יש לי כאן איזה שקט לפני, ‫שבכל זאת אולי להגיד
[01:19:22 - 01:19:26] ‫שהשיטה הזאת שאנחנו מדברים עליה, ‫היא שיטה שהמקור שלה היא פיזיקה,
[01:19:28 - 01:19:31] ‫והרעיון זה לפתור כל מיני בעיות ‫על ידי אופטימיזציה.
[01:19:31 - 01:19:34] ‫זאת אומרת, אם יש לי משהו ‫שאני רוצה לשערך,
[01:19:34 - 01:19:36] ‫אז אני הופך את זה ‫לבעיית אופטימיזציה,
[01:19:36 - 01:19:40] ‫שככה שהאופטימום יהיה הפתרון ‫שאני רוצה.
[01:19:41 - 01:19:42] ‫אז נראה לי
[01:19:44 - 01:19:48] ‫הרדיט הראשון של השיטה הזאת ‫הוא אולי לא בטוח עם תרומה,
[01:19:49 - 01:19:49] ‫בחישוב של
[01:19:54 - 01:19:56] ‫המסלול של אור,
[01:19:57 - 01:20:02] ‫ובעצם החיפוש של הדרך הכי קצרה ‫לעבור משתי נקודות,
[01:20:03 - 01:20:06] ‫על ידי הפתיחה של הדבר הזה ‫למה שבעיית אופטימית.
[01:20:07 - 01:20:09] ‫באופן כללי, תחשבו על זה ככה,
[01:20:09 - 01:20:13] ‫השיטה הגישה הווריאציונלית, ‫שמה דבר דוביל בעברית,
[01:20:13 - 01:20:15] ‫אולי ציונית או לא ציונית,
[01:20:16 - 01:20:17] ‫ווריאציונל,
[01:20:17 - 01:20:22] ‫זה כשאנחנו הופכים איזושהי בעיה, ‫ואני חושב שזה יהיה
[01:20:22 - 01:20:24] ‫בעיה של לחשב איזושהי התפלגות,
[01:20:25 - 01:20:26] ‫לבעיית אופטימיזציה.
[01:20:26 - 01:20:28] ‫אנחנו נהפוך את זה לבעיה של,
[01:20:29 - 01:20:31] ‫יש לנו הרבה התפלגויות, ‫אנחנו מחפשים את ההתפלגות,
[01:20:32 - 01:20:35] ‫שממקסמת לנו איזושהי פונקציה,
[01:20:35 - 01:20:39] ‫וההתפלגות הזאת היא תעזור לנו ‫למצוא את הפתרון לבעיה המקומית.
[01:20:40 - 01:20:43] ‫אוקיי, תכף ניכנס יותר לעומק.
[01:20:44 - 01:20:48] ‫אז קודם כל מיני דברים ‫על כאלה דייברג'נס ועל פונקציית לוב.
[01:20:49 - 01:20:51] ‫אז נתחיל מלוב, פונקציית הלוב נראית ככה,
[01:20:52 - 01:20:54] ‫והפונקציה הזאת היא כאורה,
[01:20:54 - 01:20:55] ‫היא קונקייב.
[01:20:56 - 01:20:57] ‫מה זאת אומרת שהיא כאורה?
[01:20:58 - 01:20:59] ‫מה זאת אומרת שהיא
[01:21:00 - 01:21:00] נראית ככה?
[01:21:02 - 01:21:04] ‫זה ההפך מטערה,
[01:21:04 - 01:21:07] ‫ההפך משלבי דברים חמור,
[01:21:07 - 01:21:14] ‫ומה שאומרת, אם אני לוקח איזשהו מיתר ‫בין שתי נקודות של הפונקציה הזאת, ‫מחבר בין שתי נקודות של הפונקציה,
[01:21:15 - 01:21:19] ‫אז כל הנקודות במיתר הזה נמצאות ‫מתחת לפונקציה.
[01:21:21 - 01:21:22] ‫אוקיי, זהו משנה.
[01:21:24 - 01:21:26] ‫אפשר לכתוב את זה ככה.
[01:21:27 - 01:21:31] ‫הנוסחה הזאת כאן היא בעצם ‫נוסחה שמתארת את המיתר הזה.
[01:21:31 - 01:21:36] ‫ויש לנו את הערך הראשון, ‫u1, את היחס השבילי, u2,
[01:21:37 - 01:21:37] ‫במקום.
[01:21:44 - 01:21:47] ‫או שאני עושה קומבינציה לינארית,
[01:21:52 - 01:21:56] ‫או שאני עושה קומבינציה לינארית ‫בין log של u1 ולוג של u2,
[01:21:57 - 01:22:00] ‫שזה המקומות האלה,
[01:22:00 - 01:22:05] ‫או שאני עושה קומבינציה לינארית ‫של u1 ו-u2 במקור,
[01:22:05 - 01:22:07] ‫ואז נסתכל מה הערך של הפונקציה ‫בנקודה הזאת.
[01:22:08 - 01:22:11] ‫אז אם אני עושה קודם ‫את הקומבינציה הלינארית,
[01:22:12 - 01:22:14] ‫זה לא רק קומבינציה לינארית, ‫זו קומבינציה לינארית כמורה.
[01:22:15 - 01:22:19] זאת אומרת שהמקדמים הם סך הכול ‫שווים ל-1,
[01:22:20 - 01:22:21] ‫והם בין 0 ל-1.
[01:22:22 - 01:22:24] ‫אחרי זה היה יכול להיות כל הישר הזה.
[01:22:24 - 01:22:27] ‫כשזה יהיה בתוך התחום הזה, ‫צריך שהתנאי הזה יקפה,
[01:22:27 - 01:22:28] ‫כשזה יהיה a,
[01:22:29 - 01:22:31] ומקדם אחד יהיה a, ‫ומקדם השני יהיה 1 מינוס a,
[01:22:32 - 01:22:34] ו-a יהיה בין 0 ל-1.
[01:22:35 - 01:22:38] ‫אוקיי, אז זה מתאר לי את הישר הזה. ‫אז אם אני עושה את הקומבינציה הלינארית קודם,
[01:22:39 - 01:22:40] ‫ואז מפעיל את הלוג,
[01:22:41 - 01:22:42] ‫זה מגיע לנקודה כחולה הזאת,
[01:22:43 - 01:22:47] ‫אם אני קודם מפעיל את הלוג על זה, ‫קודם את הלוג על זה, ‫ואז עושה את הקומבינציה הלינארית,
[01:22:47 - 01:22:48] ‫אני מגיע לנקודה הזאת.
[01:22:49 - 01:22:54] ‫זה אומר שהבר הראשון שאני עושה ‫תמיד יהיה יותר גדול ‫מהדבר השני שלך.
[01:22:56 - 01:22:57] ‫אז אפשר קצת להכליל את זה,
[01:22:58 - 01:23:02] בגדול אפשר לחשוב על זה ככה לוג של ממוצע ובעוד שעתיים עם ממוצע של לוגים
[01:23:04 - 01:23:05] נכון גם ליותר משתי נקודות
[01:23:07 - 01:23:10] וזה נכון גם שהממוצע הזה הוא ממוצע משוכלל
[01:23:11 - 01:23:12] אצל הרבה נקודות
[01:23:12 - 01:23:14] שזה דרך אחרת להגיד תוחלת
[01:23:14 - 01:23:17] אז לוג של תוחלת של איזושהי פונקציה
[01:23:17 - 01:23:22] היא גדולה שווה לתוחלת של הלוג של הפונקציה למקציה
[01:23:23 - 01:23:27] זה צריך להיות חיובית כדי שאני מוכן להפעיל את הלוג הזה
[01:23:28 - 01:23:32] זה באופן כללי זה נקרא ינסן איני קואליטי
[01:23:33 - 01:23:33] אוקיי?
[01:23:34 - 01:23:34] משוויון ינסן
[01:23:35 - 01:23:37] זה גדול אומר שלוג של תוחלת
[01:23:38 - 01:23:39] תמיד גדול שווה לתוחלת של
[01:23:41 - 01:23:43] זה נובע מזה שלוג הוא כמוך
[01:23:46 - 01:23:48] מה זה אומר על קל דייברג'נס?
[01:23:49 - 01:23:51] אז קל דייברג'נס, אתם זוכרים ראינו את זה
[01:23:52 - 01:23:53] נדבר ככה
[01:23:55 - 01:23:59] זה אינטגרל פי איקס, לא לפי איקס חלקי קו איקס
[01:23:59 - 01:24:02] זה בעצם ניסוי של מדד מרחק בין שתי התחלויות
[01:24:03 - 01:24:04] אנחנו קוראים להם כאן פי וקיו
[01:24:05 - 01:24:07] ואפשר לעשות את זה בתור תוחלת
[01:24:08 - 01:24:10] לפי פי של לוג פי חלקי קיו
[01:24:13 - 01:24:15] וזה כאל דייברג'נס
[01:24:15 - 01:24:18] ודיברנו על כמה תכונות של זה,
[01:24:18 - 01:24:21] אחד זה שקודם כל זה לא סטימטרי,
[01:24:22 - 01:24:24] זה לא קוראים לזה מרחק, זה לא קוראים לזה דייברג'נס
[01:24:27 - 01:24:30] בעצם הבעיה זה שיש במבטן
[01:24:30 - 01:24:32] רק אחד מהמשתלגויות נשאר,
[01:24:33 - 01:24:33] זה לא שניהם
[01:24:34 - 01:24:35] זה לא סימטרי
[01:24:37 - 01:24:40] דבר שני זה שהדבר הזה זה תמיד חיובי, זו תכונה שנייה,
[01:24:41 - 01:24:42] תכונה שלישית זה שווה,
[01:24:42 - 01:24:43] תמיד E שלילי,
[01:24:44 - 01:24:46] תכונה שלישית זה שווה 0 רק כאשר P
[01:24:47 - 01:24:48] שווה ל-K
[01:24:49 - 01:24:53] אז למה זה תמיד איש ליבי?
[01:25:00 - 01:25:01] זה נובע בדיוק מה...
[01:25:06 - 01:25:07] איש ליביון של יאנסן שראינו
[01:25:12 - 01:25:14] נתחיל רגע ממינוס של KL
[01:25:14 - 01:25:15] בין T
[01:25:17 - 01:25:17] Q
[01:25:33 - 01:25:34] וזה לפי יאנסן
[01:25:39 - 01:25:41] מינוס, נכון? זה קטן שונה
[01:25:46 - 01:25:53] ‫אז בואו נסתכל על ידי כך עכשיו ‫לפי יאנסן מינוס, נכון?
[01:26:17 - 01:26:21] אוקיי? אז יש לנו כאן את המינוס של התוכרת הזאתי, ‫של הלוג ליחס,
[01:26:22 - 01:26:23] מינוס KL זה המברכה.
[01:26:24 - 01:26:28] עכשיו, במקום המינוס הזה, ‫אני יכול להפוך את Q ו-B,
[01:26:30 - 01:26:31] אוקיי?
[01:26:31 - 01:26:32] אותו דבר.
[01:26:32 - 01:26:34] עכשיו אני מפעיל את איש ליביון יאנסן,
[01:26:36 - 01:26:37] זאת אומרת שהדבר הזה תמיד
[01:26:37 - 01:26:38] קטן שווה
[01:26:39 - 01:26:40] מלהוציא את הלוג
[01:26:41 - 01:26:43] למיטה, אוקיי?
[01:26:44 - 01:26:45] ומה שכתוב כאן
[01:26:47 - 01:26:48] אנחנו אומרים את זה כאן באינטגרל
[01:26:49 - 01:26:50] של
[01:26:51 - 01:26:52] לפי
[01:26:52 - 01:26:53] P של X
[01:26:55 - 01:26:56] פרק Q של X
[01:26:57 - 01:26:58] פרק Q של X
[01:26:59 - 01:27:02] ולכן יש לי כאן פי של X סופר P של X
[01:27:02 - 01:27:03] ומתבטל
[01:27:04 - 01:27:06] ויש לי כאן אינטגרל איזושהי פונקציה.
[01:27:07 - 01:27:11] אם הפונקציה הזאת היא התפלגות, אז האינטגרל ההתפלגות הזאתי תמיד שווה 1,
[01:27:12 - 01:27:14] ואז יש לי לוג של 1 ושווה 1
[01:27:15 - 01:27:16] כן, לוג של הדבר הזה
[01:27:20 - 01:27:23] נראה לי שמינוס ה-KL תמיד קטן שווה F מ-F
[01:27:24 - 01:27:26] כן? מינוס ש-KL תמיד גדול שווה F
[01:27:27 - 01:27:32] בסדר? והדבר היחיד שהשתמשנו בעצם חוץ מהטריקים פה של ה... של התלנוקות
[01:27:33 - 01:27:34] זה ה-A שוויוני אל צמוד
[01:27:37 - 01:27:38] אוקיי
[01:27:41 - 01:27:44] בואו נדבר על החוסר סימטרי של זה,
[01:27:44 - 01:27:45] נפתור את זה להגדרה
[01:27:50 - 01:27:52] איך מתבטא את החוסר סימטרי הזה?
[01:27:56 - 01:28:02] מה ההבדל בין מצב שבו P יש לי איזשהו
[01:28:02 - 01:28:05] עבור איזשהו X, P נותן ערך מאוד נמוך
[01:28:06 - 01:28:08] אבל Q נותן ערך גבוה
[01:28:09 - 01:28:10] לעומת המצב ההפוך
[01:28:12 - 01:28:13] שבו P נותן עבור איזשהו ערך
[01:28:15 - 01:28:32] משהו גבוה ו-Q נותן ערך גבוה ו-P נותן ערך גבוה ו-P
[01:28:33 - 01:28:34] איפה אני משנה מכי יותר יקר
[01:28:35 - 01:28:37] כש-Q גבוה ו-P נמוך או כש-P נמוך ו-Q נמוך?
[01:28:45 - 01:28:48] P מופיע גם פה וגם פה, אוקיי?
[01:28:48 - 01:28:50] אז מה קורה אם P מקבל
[01:28:52 - 01:28:55] ערך מאוד נמוך, נגיד אפס דרך,
[01:28:56 - 01:28:57] ו-Q מקבל משהו גבוה?
[01:29:07 - 01:29:11] אולי כדאי לסגר טוב מהכיוון ההפוך, אם Q מקבל ערך מאוד נמוך
[01:29:12 - 01:29:13] ו-P מקבל ערך גבוה
[01:29:14 - 01:29:17] מה קורה? זה משהו שמשפיע יחסית הרבה על אינגדרה בזה
[01:29:19 - 01:29:21] כאילו הממוצע המשוכר הזה מקבל משקל גבוה, P הוא גבוה
[01:29:22 - 01:29:23] ומה הערך שהוא מקבל?
[01:29:25 - 01:29:27] השפעה מאוד גבוה, כי יש כאן אחד חלקי משהו מאוד קטן
[01:29:28 - 01:29:30] אז ה-K נמוך וזה יהיה מאוד גדול
[01:29:31 - 01:29:33] וזה משהו שמאוד יגדיל את ה-K נמוך
[01:29:33 - 01:29:34] אם מקומות ש-P
[01:29:35 - 01:29:37] יש להם ערך גבוה, Q נותן להם ערך נמוך
[01:29:38 - 01:29:39] אבל ההפך,
[01:29:39 - 01:29:41] לא כל כך יגרום
[01:29:43 - 01:29:48] תשלום, כי גם אם P הוא מקבל ערך מאוד גבוה,
[01:29:49 - 01:29:52] אם P מקבל ערך מאוד נמוך
[01:29:52 - 01:29:53] אז
[01:29:53 - 01:29:57] היחס לזה הוא מקבל איזשהו ערך מאוד שונה מאחד,
[01:29:58 - 01:30:01] זה ממושקל יחסית בצורה קטנה בתוך האינטגרל הזה
[01:30:03 - 01:30:05] כן אפשר לחשוב על זה ככה, אם שני
[01:30:09 - 01:30:13] התפלגות כזאת שיש לה אזורים קטנים ואזורים עם התפלגות גבוהה
[01:30:13 - 01:30:14] זה נגיד פי של איקס
[01:30:17 - 01:30:18] ועכשיו אם אני אעשה כי-אל
[01:30:19 - 01:30:19] בין
[01:30:25 - 01:30:27] אם יש לי כי-אל בין
[01:30:27 - 01:30:28] פי ל-q
[01:30:32 - 01:30:37] אני אגיד ש-q זה פונקציה שאני מחפש על כל הכי-אים שהיא גאוסיאן אחד,
[01:30:37 - 01:30:40] זה נגיד תערובת של שני גאוסיאנים, פי זה תערובת של שני גאוסיאנים
[01:30:41 - 01:30:44] ו-q זה תערובת, זה רק גאוסיאן אחד, זאת אומרת יכול להיות לו רק מוד אחד
[01:30:46 - 01:30:52] ואני רוצה למצוא את ה-p שממזער את ה-q שממזער את ה-k-l divergence הזה
[01:30:53 - 01:30:55] סליחה, את ה-q שממזער את ה-k-l divergence הזה
[01:30:55 - 01:30:56] זה p, שחור,
[01:30:57 - 01:31:00] אני רוצה למצוא את ה-q שממזער את ה-k-d divergence הזה
[01:31:01 - 01:31:02] זה איך הוא ייראה
[01:31:03 - 01:31:07] ה-q הזה עדיף לו לתת
[01:31:08 - 01:31:09] קודם כל הוא לא יכול להיות בדיוק,
[01:31:09 - 01:31:11] הכי טובים הוא היה מצליח להיות בדיוק זה, נכון?
[01:31:11 - 01:31:12] כי אז ה-k-l היה אפס
[01:31:13 - 01:31:15] אבל הוא לא יכול להיות, כי הוא רק כאוסיאן אחד
[01:31:15 - 01:31:16] הוא לא יכול להיות בצורה הזאת
[01:31:17 - 01:31:19] אז q יחפש איזה דרך
[01:31:20 - 01:31:22] למזער כמה שאפשר לקריאה ל-k-l היה אפס
[01:31:23 - 01:31:27] ומה הדרכים שהוא יכול לעשות, אז מה עדיף לו? עדיף לו לתת
[01:31:29 - 01:31:33] ציון גבוה מדי על מקום של p המקבל ערך נמוך
[01:31:34 - 01:31:38] או עדיף לו אפס, לתת ציון נמוך יותר ממקום של p מקבל משהו גבוה
[01:31:39 - 01:31:45] טוב, נו דרך, אז מה אמרנו קודם? אמרנו שעדיף ל-q
[01:31:49 - 01:31:53] עדיף יהיה q לתאות לכיוון שהוא יותר גדול מ-p
[01:31:55 - 01:31:56] נכון? גם במקומות ש-p
[01:31:57 - 01:32:01] סליחה, במקומות ש-p גדול, הפוך, במקומות ש-p גדול
[01:32:02 - 01:32:05] אסור ל-q להיות קטן מילה
[01:32:09 - 01:32:14] נכון, אז במקומות ש-p גדול אסור ל-q להיות קטן מדי, אז מה
[01:32:15 - 01:32:18] לעדיף לעשות משהו שנראה ככה
[01:32:20 - 01:32:21] במקומות ש-p גדול
[01:32:28 - 01:32:30] אני אצטרך ללכת בכיוון
[01:32:31 - 01:32:33] זה היה גרסיאן שהוא יבחר
[01:32:37 - 01:32:38] זה ברור למה זה הגרסיאן הכי טוב?
[01:32:39 - 01:32:41] אתם רוצים למזער בטנדר וינוס או רוצים?
[01:32:41 - 01:32:42] רוצים למזער את ה-kלר
[01:32:44 - 01:32:49] זה בעצם סוג של מדד מרחק שיש לנו בין ההתפלגויות, אנחנו רוצים למצוא את ההתפלגות הכי קרובה
[01:32:50 - 01:32:52] להתפלגות המקורית, אנחנו לא יכולים למצוא בדיוק את המקורית
[01:32:53 - 01:32:54] בשאלה מה ההתפלגות הכי קרובה
[01:32:56 - 01:32:58] עכשיו אם היינו עושים את זה בכיוון ההפוך
[01:32:59 - 01:33:03] אם היינו חושבים k בין q ל-p
[01:33:06 - 01:33:08] אז איך זה היה נראה, היינו מחפשים את ה-q
[01:33:09 - 01:33:15] ככה שהוא אסור לו לתת ציון גבוה במקום ש-p נותן ציון נמוך
[01:33:22 - 01:33:24] נכון? כי עכשיו q משחק על ה-p שם
[01:33:24 - 01:33:26] זה מה שכתוב כאן p
[01:33:27 - 01:33:30] אז אנחנו נחפש אם אני מזער את ה-k ל-verdium בכיוון הזה
[01:33:34 - 01:33:35] אז זה הפוך, אני צריך למצוא q
[01:33:39 - 01:33:44] והוא לא נותן בטעות פיון גבוה יותר מ-p
[01:33:45 - 01:33:48] אם קוראים ש-p הוא נכון, זאת אומרת הוא לא יכול לעבור באזור הזה
[01:33:51 - 01:33:52] וטייב יישאר או כאן או כאן
[01:33:55 - 01:33:57] עדיף לו לבחור איפה שיש יותר מסה מלא דעת
[01:33:57 - 01:33:59] כמו שציימתי מה זה על רגיל שלו
[01:34:00 - 01:34:02] אז הוא יצא גם סיימת שהוא כולו בתוך דעת
[01:34:06 - 01:34:08] בסדר, אז ההבדל בין לעשות למזער
[01:34:09 - 01:34:12] את ה-KL Divergence בכיוון הזה ולמודל את ה-KL Divergence בכיוון הזה
[01:34:14 - 01:34:16] ואנחנו עושים, אתם זוכרים שאמרתי
[01:34:17 - 01:34:18] באחד מהשאלות הראשונים
[01:34:19 - 01:34:23] שכשאנחנו עושים מקסימום לייקליות זה שקול למזעור KL Divergence
[01:34:24 - 01:34:27] עם ההתפלגות של הדאטה האמיתית
[01:34:28 - 01:34:29] אתם רואים את זה?
[01:34:31 - 01:34:32] אתם זוכרים לאיזה כיוון זה?
[01:34:35 - 01:34:38] זה היה במקסימום לייקליות
[01:34:39 - 01:34:42] זה היה כיוון הירוק בעצם
[01:34:43 - 01:34:44] NRE
[01:34:48 - 01:34:48] פול
[01:34:52 - 01:34:53] מין
[01:34:54 - 01:34:55] על
[01:34:56 - 01:34:56] כאילו
[01:35:01 - 01:35:01] קריטטה
[01:35:02 - 01:35:03] של ה-KL
[01:35:04 - 01:35:06] בין תנאי דאטה
[01:35:06 - 01:35:15] בעצם ההתפלגות שחיפשנו, המודל שחיפשנו היה במקום השני כאן
[01:35:16 - 01:35:18] מה שאומר שאם זו ההתפלגות של הדאטה השחורה
[01:35:19 - 01:35:25] אז היינו מוצאים את ההתפלגות, אנחנו נמצא את ההתפלגות הירוקה, זה משהו שהוא מכסה את כל האפשרויות בדאטה
[01:35:27 - 01:35:30] הוא גם נותן, אם זה מודל לא מספיק טוב
[01:35:30 - 01:35:34] אז הוא ייתן משקל יחסית גבוה למקומות שבהם הדאטה לא,
[01:35:35 - 01:35:36] ההסתברות שלנו מאוד נמוכה
[01:35:37 - 01:35:41] ונתנו את הדוגמה הזאת כי אמרנו שגאוסיאן לא יכול להיות מודל טוב לתמונות
[01:35:42 - 01:35:46] כלומר אם יש נגיד פיקסל שיכול לקבל או ערך מאוד קרוב לאפס או ערך מאוד קרוב לאחד
[01:35:47 - 01:35:49] אבל לא שום דבר באמצע, המודל הזה לא יכול להתפוס את זה
[01:35:51 - 01:35:52] זה פשוט קצת דיין, זה הדבר הזה
[01:35:53 - 01:35:55] אם היינו עושים את זה הפוך, אתה קייל הפוך
[01:35:57 - 01:36:00] במצב הזה של Machine Learning עם דאטה שמגיע מאימון אנחנו לא יכולים לעשות את זה
[01:36:01 - 01:36:05] אבל אם היינו עושים את זה הפוך אז היינו אולי מוציאים מודל שהוא רק טוב על סוג מסוים של תמונות,
[01:36:05 - 01:36:08] בגלל המשמעות אנחנו תופסים רק איזה מוד אחד בהתפלגות
[01:36:09 - 01:36:12] במקום למצע את כל המודים שיש לנו
[01:36:22 - 01:36:24] יש פה דיון הזה פחות או יותר,
[01:36:24 - 01:36:31] אז זה נקרא גם תורמי לזה מוד סיקינג לעומת מוד אבריג'ינג
[01:36:32 - 01:36:39] מה שכאן ירוק בא על הלוח זה בעצם מוד אבריג'ינג
[01:36:40 - 01:36:44] אנחנו בעצם הופכים יותר מודים ממה שאנחנו יכולים לתפוס
[01:36:44 - 01:36:45] ואנחנו מאחדים אותם
[01:36:47 - 01:36:48] ואחרת
[01:36:49 - 01:36:53] אם אנחנו עושים את הכאב בכיוון הפוך, אנחנו עושים מוד סיקינג ואנחנו מחפשים
[01:36:54 - 01:36:57] ואנחנו בוחרים בעצם חלק מהמודים אבל תופסים אותם טוב
[01:36:58 - 01:37:00] לפעמים יש יתרונות לאחד ולפעמים יש יתרונות לשני
[01:37:01 - 01:37:02] דובר על זה גם מעט
[01:37:05 - 01:37:09] זה לא קשור לזה, נסים את דברים, זה אותו דבר רק בדומראג
[01:37:12 - 01:37:17] אם ההתפלגות של המקורית שלי היא כחולה
[01:37:17 - 01:37:18] אני עושה את ה-KL
[01:37:19 - 01:37:20] בדרך כלל
[01:37:21 - 01:37:25] Q זה ההתפלגות שאנחנו מחפשים ו-P זה ההתפלגות המטומנית
[01:37:26 - 01:37:30] כן, אז אם אנחנו עושים KL בין P ל-Q זאת אומרת, מחפשים
[01:37:31 - 01:37:32] ‫בדימות השנייה
[01:37:35 - 01:37:36] ‫בתוך ה-KL,
[01:37:36 - 01:37:39] ‫אז אנחנו נמצא משהו כזה שמאחד את המודים
[01:37:40 - 01:37:41] ‫כל המודים של ההתפלגות,
[01:37:42 - 01:37:46] ‫ואם אנחנו עושים את ה-KL ככה בכיוון הזה, ‫ש-Q זה הראשון,
[01:37:47 - 01:37:48] ‫ככה כשעשינו את ה-Q,
[01:37:49 - 01:37:55] ‫אז הוא יבחר את אחד מהמודים וזה יכול להתראות אולי במצב ההתחלה שעשינו, ‫או באיזה מודים יותר נאסי.
[01:38:02 - 01:38:12] ‫אוקיי, זה היה לגבי KL. ‫עכשיו בואו נדבר על הפיתוח של החסם הזה, ‫של עבוד התפלגות, אז בעצם
[01:38:14 - 01:38:16] אנחנו נסביר גם מה אנחנו צריכים בקשר לחשוב, אוקיי?
[01:38:19 - 01:38:23] ‫שהופעה שלנו בגדול זה לעשות inference
[01:38:24 - 01:38:25] ‫על ידי אופטימיזציה.
[01:38:26 - 01:38:28] ‫בשביל זה אנחנו קודם נמצא איזשהו חסם,
[01:38:28 - 01:38:32] ‫הוא יהיה הכלי שאנחנו נשתמש בו.
[01:38:34 - 01:38:35] ‫-כן, יש לנו...
[01:38:36 - 01:38:38] ‫אנחנו נגיד, רוצים לחשב פי של איקס,
[01:38:44 - 01:38:45] ‫אוקיי, שזה אינטלראז.
[01:38:48 - 01:38:50] ‫אני חושב את זה כמו של אלן דארטה.
[01:38:52 - 01:38:58] אוקיי, אז פי של איקס, ‫אנחנו רואים שזה, אפשר לכתוב את זה, ‫בטוב.
[01:38:59 - 01:39:00] ‫בנצחת האינטגרלות השלמה, ‫בתור האינטגרל הזה.
[01:39:03 - 01:39:07] ‫נכון, אבל אנחנו לא יכולים ‫לחשב את האינטגרל הזה בצורה ישירה,
[01:39:07 - 01:39:12] ‫אז קודם אנחנו נראה ‫שאנחנו יכולים למצוא קירוב שהוא חסם,
[01:39:12 - 01:39:15] ‫חסם תחתון האינטגרל הזה, ‫ואנחנו נשתמש בו.
[01:39:19 - 01:39:21] ‫אוקיי, אז אנחנו יכולים לכתוב את זה כבר אחרון.
[01:39:25 - 01:39:27] ‫איזושהי פונקציה אחרת.
[01:39:29 - 01:39:38] ‫נכון, זה תמיד נכון.
[01:39:42 - 01:39:44] ‫הכפלתי וחילקתי באותה פונקציה ‫שאני קורא לה Q,
[01:39:45 - 01:39:47] ‫וזה פונקציה על פני Y,
[01:39:48 - 01:39:49] ‫שאולי היא תלויה גם ב-X,
[01:39:50 - 01:39:51] ‫המשתנה השני שיש לי פעם.
[01:39:52 - 01:39:54] אז Y זה המשתנה שאני עושה עליו את האינטגרל,
[01:39:55 - 01:39:58] ‫הוא לא מעניין אותי, ‫הוא רואה לו משתנה חזורי או משהו כזה,
[01:39:58 - 01:40:01] ‫ואני רוצה לעשות את האינטגרל עליו, ‫ו-X זה עוד איזשהו משתנה שיש לי.
[01:40:02 - 01:40:05] ‫למשל, זה ה... אני לא שאלתי מעט ‫אני רוצה לחשב את ההסתברות שלו.
[01:40:07 - 01:40:08] ‫אז אני יכול תמיד להכפיל ולחלק
[01:40:09 - 01:40:10] ‫בפונקציה כזאת.
[01:40:11 - 01:40:13] ‫הדבר הזה, בעצם אני יכול לכתוב אותו,
[01:40:14 - 01:40:20] ‫אני יכול להחליט שזה ההתפלגות ‫שנמצאת כאן בעצם, ‫שהיא לפיה אני עושה תוחלת.
[01:40:20 - 01:40:22] ‫אני יכול לקרוא לדבר בתוחלת
[01:40:23 - 01:40:27] ‫בפי Q. ‫עשינו את זה כבר, ‫שעשינו important something.
[01:40:28 - 01:40:29] ‫של P,
[01:40:31 - 01:40:47] ‫אנחנו נקבל פה, P, ספסיק וואי חלקי P. ‫כן, אז P של X אנחנו יכולים לכתוב ככה, ‫על ידי תוחלת,
[01:40:48 - 01:40:51] ‫לפי התפלגות אחרת שנמצאנו אותה, ‫איזושהי Q
[01:40:52 - 01:40:53] של המנה הזאת.
[01:40:56 - 01:40:57] עכשיו, אם אתם זוכרים, ‫השתמשנו כבר,
[01:40:58 - 01:41:01] ‫הדבר הזה, כשעשינו important something, ‫אמרנו, אוקיי, אנחנו רוצים לדגום,
[01:41:02 - 01:41:06] ‫אם עכשיו אנחנו רוצים לחשב את P של X, ‫אנחנו יכולים לכתוב אותו ככה,
[01:41:06 - 01:41:10] ‫ואז לדגום מה-Q הזה, אם יש לנו איזשהו Q,
[01:41:11 - 01:41:13] ‫אנחנו יכולים לדגום ממנו, ‫אז נדגום מה-Q הזה,
[01:41:13 - 01:41:17] ‫נחשב את המנה הזאת, ‫נעשה את הממוצע של הדגימות שלנו, ‫וזה יהיה
[01:41:17 - 01:41:22] מדד די טוב ל-P של X. ‫אבל זו שיטה אחת שראינו בשבוע שעבר.
[01:41:23 - 01:41:24] ‫היום אנחנו נדבר על שיטה אחרת,
[01:41:24 - 01:41:26] ‫שהיא בעצם לעשות קירוב אחר לדבר הזה.
[01:41:27 - 01:41:33] ‫במקום להשתמש בתכונה הזאתי ‫של חוק המספרים הגדולים, ‫שממוצע מתקרב בתוכנת,
[01:41:33 - 01:41:36] ‫אנחנו נשתמש ב-Yansom קודם,
[01:41:37 - 01:41:37] ‫ואני נכתוב,
[01:41:38 - 01:41:40] ‫בעצם נתקל על הלוג של ההתקדמות, ‫אז לוג
[01:41:42 - 01:41:43] של P של X
[01:41:45 - 01:41:49] ‫נתקבל לוג של הדבר הזה, ‫זה שווה בשביל לוג
[01:41:51 - 01:41:51] ‫בתוכנת הזאת.
[01:41:57 - 01:42:07] ‫לפי יאנסל אני יכול להגיד ‫שהדבר הזה
[01:42:12 - 01:42:14] ‫הוא תמיד גדול שווה
[01:42:15 - 01:42:17] ‫למקום לוג של תוחלת לתוחלת של לוג.
[01:42:20 - 01:42:21] ‫זה מה כאילו נכון?
[01:42:21 - 01:42:23] ‫לוג של תוחלת,
[01:42:23 - 01:42:27] ‫כן, בגדול לתוחלת של הלוג.
[01:42:31 - 01:42:32] ‫התוחלת היא אחת אחרת,
[01:42:33 - 01:42:33] ‫ה-PQ.
[01:42:37 - 01:42:38] ‫כן, לוג של המנה הזאת.
[01:42:38 - 01:42:54] ‫כן, זה מה שנקרא הטריק הווריאציוני,
[01:42:54 - 01:42:54] ‫אוקיי?
[01:42:56 - 01:43:01] ‫מה שקיבלנו כאן זה החסם הווריאציוני, ‫ווריאציונל באנד.
[01:43:02 - 01:43:08] ‫במקום לדגום מאיזושהי התפלגות Q ‫ולחשב ממוצע,
[01:43:09 - 01:43:12] ‫זה משהו שאנחנו יודעים שבתוחלת ‫אמור להתכנס למה שמעניין אותנו.
[01:43:13 - 01:43:20] ‫כאן אנחנו יכולים לחסום ‫את מה שמעניין אותנו, אם לא, כן? ‫אנחנו יכולים לחסום את הלוג ‫כן, ההסתמרות מלמטה,
[01:43:21 - 01:43:22] ‫על ידי הדבר הזה,
[01:43:23 - 01:43:26] ‫אוקיי? וזה, יש כמה מקשמות, ‫זה או הוורייאציונל באנד,
[01:43:27 - 01:43:28] ‫או כזה, איך אנחנו קוראים לזה M-Bose.
[01:43:31 - 01:43:33] ‫M-Bose זה Evidense lower bound.
[01:43:35 - 01:43:36] פי של איקס,
[01:43:37 - 01:43:38] ‫בהקשר, בהקשר של אה...
[01:43:40 - 01:43:41] ‫יש לנו עוד משתנה,
[01:43:42 - 01:43:44] ‫אנחנו עושים פתרונות תשולית ‫על מה שאנחנו רואים,
[01:43:45 - 01:43:45] ‫נקרא גם Evidense,
[01:43:46 - 01:43:48] ‫נכנה גם בכל בייס.
[01:43:50 - 01:43:52] ‫אוקיי? אז זה החסם.
[01:43:58 - 01:44:00] ‫ובעצם אנחנו יכולים להגדיר ‫מהדבר הזה,
[01:44:02 - 01:44:03] ככה, עם L,
[01:44:04 - 01:44:06] ‫כאילו, תלוי ב-X,
[01:44:07 - 01:44:09] ‫אבל הוא בעצם פונקציה של Q.
[01:44:34 - 01:44:41] ‫זה בדיוק בדיוק אותו דבר, ‫אבל למה קצת לי את זה שוב? ‫כי הדבר הזה, הוא בעצם, ‫אנחנו מתייחסים אליו עכשיו ‫בתור פונקציה של Q,
[01:44:42 - 01:44:45] ‫זה לא בדיוק פונקציה, ‫כי Q היא פונקציה בעצמה, ‫הדבר הזה זה פונקציונל.
[01:44:46 - 01:44:50] אוקיי? זה בעצם משהו שעבור ‫איזשהו ניחוש Q,
[01:44:50 - 01:44:51] ‫נותן לנו חסם
[01:44:54 - 01:44:56] ‫להתפלגות שמעניינת אותם, ‫להתפלגות שמעניינת אותם.
[01:44:57 - 01:44:59] ‫יש לנו X, יש לנו איזושהי נקודה,
[01:44:59 - 01:45:00] נותנים לחשב את ההסתברות שלה,
[01:45:01 - 01:45:03] אנחנו לא יודעים לחשב את ההסתברות של P של X,
[01:45:03 - 01:45:06] אנחנו רק יודעים לחשב P של X סיפ Y.
[01:45:09 - 01:45:11] הדבר הזה נותן לנו חסם
[01:45:12 - 01:45:13] על ההתפלגות שלנו.
[01:45:15 - 01:45:19] כמו שקודם, אם היינו מוצאים איזה Q שאנחנו יודעים לדגום ממנו,
[01:45:20 - 01:45:23] אז היינו יכולים לחשב את זה על ידי מרוטה של דגימות,
[01:45:24 - 01:45:26] פה יש לנו Q שאנחנו יכולים לחשב אותו,
[01:45:27 - 01:45:29] רוצים גם לדעת לדגום ממנו,
[01:45:29 - 01:45:31] נראה כזו חלק הזאת,
[01:45:31 - 01:45:33] אבל אם יש לנו איזשהו Q שאנחנו יודעים לעבוד איתו,
[01:45:34 - 01:45:37] זה מאפשר לנו לחסום את ההתפלגות הזאת.
[01:45:59 - 01:46:00] ‫זה לא עובר.
[01:46:14 - 01:46:15] ‫הבנתם מה המטרה פה?
[01:46:19 - 01:46:21] ‫המטרה שלנו זה למצוא איזושהי דרך לחשב,
[01:46:22 - 01:46:23] ‫משהו שאנחנו לא יודעים לחשב.
[01:46:24 - 01:46:26] ‫יש דרך אחת שזו דרך עם מדינות,
[01:46:26 - 01:46:29] ‫זו דרך אחרת שבה אנחנו חוסמים את הדבר הזה,
[01:46:29 - 01:46:30] ‫אנחנו מקבלים חסם
[01:46:31 - 01:46:31] בזה מלמד.
[01:46:33 - 01:46:38] ‫אנחנו לא יודעים לחשב את זה ישירות, ‫אנחנו מקבלים אבל משהו שהוא חסם של החסם של החסם.
[01:46:39 - 01:46:45] ‫אוקיי, עכשיו עוד נקודה שמעניין פי רוטליקטי זה ‫שאפשר לכתוב את זה בכמה דרכים, ‫בדיוק את אותו דבר.
[01:46:46 - 01:46:51] ‫יש כאן שלוש פורמולציות שונות ‫של בדיוק אותו החסם.
[01:46:54 - 01:46:56] ‫כן, אנחנו רואים את הדבר הזה, ‫ואפשר לכתוב את זה,
[01:46:57 - 01:46:58] טוב, נשמע אותה לשלושת, ארבעה עשרה וחמש,
[01:46:59 - 01:46:59] ‫ישראל.
[01:46:59 - 01:47:01] ‫אני רוצה לדבר לכם איך מגיעים ‫מאחד לשני,
[01:47:02 - 01:47:04] ‫ואחרים זה שיטות מאוד אמות.
[01:47:05 - 01:47:06] ‫תכף נראה מה...
[01:47:07 - 01:47:11] ‫קל לראות את המשמעות של החסם הזה, ‫כמה משמעויות שונות לפי
[01:47:13 - 01:47:18] כל פורמולציה כנראית, ‫קצת נקודת מבט שונה.
[01:47:19 - 01:47:20] ‫אוקיי, אז אם אני מתחיל מה...
[01:47:29 - 01:47:42] ‫נגיד כדי להגיע למה שכתוב כאן, ‫ב-13,
[01:47:45 - 01:47:46] ‫אני יכול לפתור את זה ככה,
[01:47:48 - 01:47:55] ‫אני יכול לפתור להפריד פה את הלואו ‫ולהפריד את הדבר הזה ‫להתפלגות מותנית, כיתה על השרשרת,
[01:47:56 - 01:47:58] ‫ויש לי כאן את התפלגת ביקורת תיאור,
[01:47:58 - 01:47:59] ‫אני רוצה לראות שאני מתחיל ל-Q,
[01:48:02 - 01:48:02] ‫בכן
[01:48:03 - 01:48:03] לוד
[01:48:05 - 01:48:07] טיור איקס
[01:48:09 - 01:48:10] ולוד
[01:48:11 - 01:48:15] לוד טיור יינתן איקס,
[01:48:19 - 01:48:20] ‫פחות
[01:48:20 - 01:48:21] לוד
[01:48:23 - 01:48:25] טיור יינתן איקס.
[01:48:26 - 01:48:29] ‫-כן, ברור שהגעתי מכאן לכאן.
[01:48:31 - 01:48:35] ‫זה דברים, עשיתי ביחד, ‫אחד זה הפקה של לוג של מפתלה וחילוק,
[01:48:35 - 01:48:39] ‫זה הדבר השני שכתבתי בתור X של X,
[01:48:40 - 01:48:42] ‫כתבתי ב-P של Y ב-Y.
[01:48:47 - 01:48:49] ‫אוקיי, עכשיו ברגע שיש לי את הדבר הזה,
[01:48:50 - 01:48:53] ‫אז האיבר הזה, יש לי כאן תוחלת על Q,
[01:48:54 - 01:48:55] ‫ה-Q זה התפלגות על Y.
[01:48:56 - 01:49:02] ‫-Y זה תוחלת על Y של איזושהי פונקציה ‫שהיא לא תקועה ב-Y.
[01:49:04 - 01:49:07] ‫התוחלת עוד פשוט נשארת רק האיבר הזה,
[01:49:08 - 01:49:11] ‫בממוצע משוכל, ‫שאתה כנראה מקבל את ה-Rever בעצם,
[01:49:12 - 01:49:20] ‫זה לוג פי X. ‫ועוד מה שכתוב כאן, ‫התוחלת לפי Q של לוג Q פחות לוג פי,
[01:49:22 - 01:49:23] ‫זה יוצא ה...
[01:49:24 - 01:49:31] ‫אוקיי, את האיברג'ינס, ‫אז אם אני מוכרח את זה, ‫אז זה מינוס האיברג'ינס ב-Q.
[01:49:49 - 01:49:52] ‫-כן, אז זו נקודה טובה, ‫איך שאנחנו כותבים את זה כאן,
[01:49:52 - 01:49:55] ‫זה לא שימושי בשביל לחשב את זה,
[01:49:56 - 01:49:57] ‫אבל זה שימושי כדי להבין ‫מה קורה כאן.
[01:49:58 - 01:50:00] ‫איך שאנחנו נחשב את זה, ‫אבל זה לא יהיה ככה.
[01:50:01 - 01:50:04] ‫זה יהיה משהו שיותר דומה ‫למה שכתוב כאן,
[01:50:04 - 01:50:05] ‫או...
[01:50:09 - 01:50:10] ‫או לפי ה-Y.
[01:50:14 - 01:50:15] ‫אנחנו נראה את זה בשביל יום הבאים.
[01:50:23 - 01:50:23] ‫או...
[01:50:31 - 01:50:43] ‫אז בעצם זה היה הדרך להגיע ל-13, ‫כדי להגיע ל-14 ו-15 זה אותו דבר, ‫רק שאנחנו מסדרים את הדברים קצת אחרת. ‫אפשר, אם אפשר לכתוב את זה הפוך כאן, P של Y כפי פי פי x בהינתן Y,
[01:50:45 - 01:50:47] ‫ואז לחבר את זה עם ה-P של Y,
[01:50:48 - 01:50:51] ‫עם Q שמובן בהינתן X, ‫אז אנחנו מקבלים את זה ב-14,
[01:50:52 - 01:50:54] ‫או להשאיר כאן את P של X ל-Y,
[01:50:54 - 01:50:58] ‫פשוט להוציא את Q החוצה, ‫אז זה יקבל את האנטרופיה הזאתי של Q.
[01:51:01 - 01:51:03] ‫זה עוד השיטה, אני מתאר לעבור על זה.
[01:51:04 - 01:51:07] ‫אבל זה שלושת הדרכים בעצם ‫שאפשר לסדר את ה... את העברים שמופיעים כאן בלוג,
[01:51:09 - 01:51:14] ‫ואפשר להסתכל קצת על המשמעות ‫של כל אחד מהפורמולציות האלה.
[01:51:16 - 01:51:20] אוקיי, אז אם נסתכל רגע על ה... איך שזה היה כתוב ב-13,
[01:51:20 - 01:51:27] ‫שזה הדבר הזה, זה נוח כדי לראות את הביטוי הזה, ‫בתור חסם למה שמעניין אותנו.
[01:51:28 - 01:51:31] ‫ובעצם מה שמופיע לנו כאן זה שמה שחישבנו,
[01:51:32 - 01:51:33] ‫כמו שאנחנו יודעים לחשב,
[01:51:34 - 01:51:36] ‫הוא שווה למה שמעניין אותנו,
[01:51:37 - 01:51:39] ‫בכל איזשהו קהיל דייברג'נס.
[01:51:41 - 01:51:43] ‫בקהיל דייברג'נס אנחנו יודעים ‫שהוא תמנית תיובי,
[01:51:44 - 01:51:46] ‫וזה בעצם דרך להוכיח ‫שהדבר הזה הוא חסם שחתום.
[01:51:47 - 01:51:52] ‫אוקיי? זה דבר תמיד יותר קטן, ללוג של פי של x,
[01:51:53 - 01:51:55] ‫מתי הוא יהיה בדיוק שווה ללוג של פי של x,
[01:51:55 - 01:51:58] ‫שזה מעניין אותנו, אנחנו רוצים לדעת ‫מתי החסם הזה הוא טוב.
[01:52:01 - 01:52:09] ‫בנוסחה הזאת, אתם רואים לדעת להגיד לי ‫מתי החסם, האלבו, הוא יהיה הדוק, ‫הוא יהיה קרוב לקריאה הלשווה הזה.
[01:52:10 - 01:52:12] ‫אז כשקריאה שווה 0, מתי זה קורה?
[01:52:12 - 01:52:15] ‫-Q,
[01:52:15 - 01:52:20] ‫כן, שהפונקציות שאנחנו מחפשים עליה, ‫את ה-Q שאנחנו מחפשים, ‫היא שווה בדיוק ל-P של y בנתן x.
[01:52:21 - 01:52:23] ‫זאת אומרת, כש-q הוא איזושהי,
[01:52:24 - 01:52:28] ‫אנחנו לא יודעים מה ה-q, ‫לא יודעים אם לקשב את זה, ‫אם היינו נקשב את זה, ‫אז היינו פותרים כבר את הבעיה.
[01:52:29 - 01:52:32] ‫אבל מה שזה אומר, שאם אנחנו עוברים על...
[01:52:33 - 01:52:36] איזושהי, נגיד, ‫מגדירים איזושהי משפחה של q-אין,
[01:52:36 - 01:52:38] ‫אז עושים אוקטימיזציה על ה-q-אין האלה,
[01:52:39 - 01:52:43] ‫כדי למזליח, כדי למקסם את האלבום,
[01:52:43 - 01:52:50] ‫אז אם אנחנו, ככל שנתקרב, ‫הקל הזה ילך באיתנו, ‫ואם אנחנו יכולים להתקרב
[01:52:51 - 01:52:52] ‫ההתפלגות הזאת,
[01:52:52 - 01:52:54] ‫אנחנו נגיע למצב שהחסם הזה יאמין.
[01:52:55 - 01:52:58] ‫אם בתוך המשפחה של ה-q שלנו, ‫יש את ה-q,
[01:52:58 - 01:53:00] יש את ה-prוספיריו האמיתי,
[01:53:00 - 01:53:01] ‫אז אנחנו נגיע אליו.
[01:53:04 - 01:53:10] ‫במובן הזה בעצם הפכנו את בעיית ה-Inference ‫לבעיית אופטימיזציה.
[01:53:11 - 01:53:12] ‫אנחנו רוצים למצוא
[01:53:13 - 01:53:20] את הדבר הזה, והדרך למצוא את זה, ‫זה להגדיר איזושהי משפחה ‫של פונקציות q ולעשות עליהם אופטימיזציה, ‫ככה שהם ימקסמו את הדבר הזה,
[01:53:21 - 01:53:23] ‫והם ייתנו לנו את החסם הכי טוב,
[01:53:24 - 01:53:26] ‫שהוא מוגדר על ידי המשפחה של ה-q-אין,
[01:53:26 - 01:53:28] ‫שעליהם עשינו אופטימיזציה,
[01:53:28 - 01:53:30] ‫על שיטת אופטימיזציה,
[01:53:31 - 01:53:35] ‫אבל אנחנו נקבל את החסם הכי טוב ‫שאנחנו יכולים על לוג פי שלו.
[01:53:43 - 01:53:46] ‫כל שתי נקודות שאמרות שבעצם זה אותו דבר ‫למצוא את
[01:53:47 - 01:53:50] פי של x ולמצוא את פי של y ונתן x,
[01:53:52 - 01:53:56] ‫זה אותו דבר. אם אני מניח שאני יודע ‫לחשב את פי של x, c-y,
[01:53:57 - 01:54:00] ‫אז למצוא או את פי של y ונתן x ‫או את פי של x זה אותו דבר.
[01:54:01 - 01:54:01] פשוט
[01:54:02 - 01:54:05] משנה כאן x שבין פי של xy חלקי פי של x.
[01:54:06 - 01:54:07] ‫אז אם זה ידוע לי,
[01:54:08 - 01:54:10] ‫מספיק לי למצוא אחד מהאחרים ‫כדי למצוא את השני.
[01:54:10 - 01:54:14] ‫וזה גם מה שיוצא מוועד הפנים זה אפשרי.
[01:54:31 - 01:54:33] ‫הנקודה הזאת היא ברורה,
[01:54:34 - 01:54:37] ‫לא לגמרי, למה הפכנו ועד אלפורנס ‫לוועד ארטימיגציה?
[01:54:38 - 01:54:39] ‫מה הכוונה כשאומרים לזה?
[01:54:41 - 01:54:54] ‫לא, אבל אולי שוב, אנחנו בעצם לא התחלנו ‫מבעיית אופטימיזציה. ‫עכשיו אנחנו נמצאים במצב שאנחנו נניח ‫שיש לנו כבר את P, ‫יש לנו את המודל של התמונות שלנו למשל,
[01:54:55 - 01:54:59] ‫אנחנו לא רוצים לעשות אינפרנס, ‫להינתן איזושהי תמונה X למשל,
[01:54:59 - 01:55:03] ‫אנחנו רוצים לדעת מה ההסתברות ‫של התמונה הזאת תחת המודל שלנו,
[01:55:04 - 01:55:08] ‫מה הלוג של P של X. ‫אנחנו לא יודעים לחשב את זה, ‫כדי לחשב את זה בצורה ישירה,
[01:55:08 - 01:55:09] ‫אנחנו צריכים לעשות אינטגרל.
[01:55:10 - 01:55:12] ‫המודל שלנו הוא מודל על משתנים חבויים,
[01:55:13 - 01:55:27] ‫אנחנו לא רואים אותם. ‫תכף קראנו את זה, ופה קוראים להם Y. ‫אנחנו לא רואים את Y, אנחנו רק רואים את X, ‫אנחנו רוצים לחשב את ההסתברות של X. ‫השיטה הזאת הופכת את הבעיה הזאת ‫לבעיית אופטימיזציה. ‫אנחנו אומרים, אוקיי,
[01:55:28 - 01:55:28] ‫אנחנו נגדיר את
[01:55:30 - 01:55:31] הדבר הזה,
[01:55:31 - 01:55:32] ‫ננסה למצוא Q,
[01:55:33 - 01:55:34] ‫את זה אנחנו יודעים לחשב,
[01:55:34 - 01:55:39] ‫בהינתן שיש לנו משפחה של Qים ‫אפשריים שאנחנו יודעים לחשב,
[01:55:40 - 01:55:46] ‫אנחנו מניחים שיש לנו משהו ‫אבל אנחנו מחפשים את ה-Qים ‫שעבורם אנחנו יכולים לעשות
[01:55:46 - 01:55:48] ‫אופטימיזציה בצורה נוחה,
[01:55:49 - 01:55:52] ‫ואני מבטח לנו שה-Q האופטימדי ‫הוא כזה שיתקרב כמה שיותר
[01:55:53 - 01:55:54] לדבר הזה.
[01:55:55 - 01:56:00] ‫וכמה שהתקרבנו זה כמה ה-Q שמצאנו ‫היה קרוב לפוסטריור שלנו.
[01:56:01 - 01:56:06] אם במקרה הייתה לנו בעיה יחסית קלה, ‫והיה לנו Qים שאנחנו יודעים ‫לעשות עליהם אופטימיזציה בצורה טובה,
[01:56:07 - 01:56:09] ‫ושמכילים גם את הפוסטריור האמיתי,
[01:56:10 - 01:56:15] ‫אז נוחת לנו שאנחנו נגיע ל-KL 0 ‫ונמצא בדיוק את מה שאנחנו מחפשים.
[01:56:16 - 01:56:21] ‫אם זה לא המצב, למשל, אם ה-costerיאור הוא, כמו שראינו קודם, ‫יש לו שני מודים,
[01:56:21 - 01:56:23] ‫אבל אנחנו מחפשים רק על Qים ‫שיש להם מוד אחד,
[01:56:24 - 01:56:28] ‫אז יהיה כאן איזשהו gap,
[01:56:28 - 01:56:32] ‫לא נמצא בדיוק את ההזדמנות שלנו, ‫אנחנו נמצא איזשהו חסם.
[01:56:35 - 01:56:38] אוקיי, מה אנחנו צריכים כדי ‫לעשות את האופטימיזציה הזאת?
[01:56:40 - 01:56:41] ‫אז זהו.
[01:56:43 - 01:56:49] ‫קודם כול, זו בעיית אופטימיזציה קשה. ‫בעצם אנחנו צריכים ליצור התפלגות
[01:56:52 - 01:56:53] ‫שממקסמת את הדבר הזה.
[01:56:55 - 01:56:55] אוקיי?
[01:56:56 - 01:57:01] ‫לפעמים זה קצת קודם דומה ‫לבעיית האופטימיזציה המקורית שהייתה לנו, ‫שרצינו ללמוד מודל של תמונות,
[01:57:02 - 01:57:03] ‫או אנחנו רוצים ללמוד
[01:57:04 - 01:57:07] איזשהו Q ‫רק כדי שנוכל להשתמש בו ‫כדי לעשות את החישוב על תמונה אחת.
[01:57:10 - 01:57:14] ‫אז זו יכולה להיות בעיה קשה, ‫ויש לזה כל מיני פתרונות ‫וכל מיני גישות.
[01:57:17 - 01:57:18] ‫לגדול יש שתי גישות,
[01:57:18 - 01:57:23] ‫שאנחנו נקפוץ לשנייה, ‫לא היום, אבל בהמשך הקורס.
[01:57:24 - 01:57:29] ‫הגישה הראשונה זה אומר ‫ממש לעשות כל מיני חישובים, ‫לראות עבור משפחה מסוימת ‫של התפלגויות,
[01:57:30 - 01:57:31] ‫של P, ההתפלגויות המקוריות.
[01:57:32 - 01:57:36] ‫אולי אני יכול להניח ‫כל מיני הנחות על Q ‫ולדעת שזה יהיה מספיק טוב
[01:57:37 - 01:57:38] בסופו של דבר.
[01:57:39 - 01:57:40] ‫אחת מהגישות האלה
[01:57:41 - 01:57:41] ‫נקראת מינפיד,
[01:57:42 - 01:57:54] ‫וההערכה שאומרת שה-Qים ‫שאנחנו עובדים איתם, ‫הם מתפרקים ל-Qים בלתי תלויים ‫אחד מהשני, כל מימד אחד מהשני, ‫ואז כל בעיה זו בעיה חצי פשוטה, ‫כי אם היא מימד אחד, ‫ואפשר לצורך בצורה קרה.
[01:57:55 - 01:57:57] ‫אז זו הגישה הקלאסית,
[01:57:58 - 01:58:00] ‫והגישה שאנחנו נדבר עליה, ‫והגישה הזאת,
[01:58:01 - 01:58:06] ‫שבה אנחנו מניחים שהמודל שלנו ‫בסופו של דבר נותן באוסיין, ‫שאנחנו יודעים לעבוד איתו,
[01:58:07 - 01:58:09] ‫אבל שהמודל הזה יכול להיות ‫מאוד מורכב,
[01:58:09 - 01:58:12] ‫למשל שהתוחלת של הגרסיון הזאת,
[01:58:13 - 01:58:15] ‫זה יכול להיות איזושהי רשת נוירונים ‫שצפויה ב-X,
[01:58:16 - 01:58:19] ‫אותו דבר לגבי הווריאנס של הגרסיון.
[01:58:20 - 01:58:22] ‫אז זו בעצם הגישה שאנחנו,
[01:58:22 - 01:58:27] ‫בכל סיבה שאני מדבר על פוריישן אינפרנס, ‫אבל בסופו של דבר אנחנו נשתמש בדבר הזה,
[01:58:28 - 01:58:32] ‫בעוד שני שיעורים, במודל שנקרא ‫בריאיישונל אוטום קוריון.
[01:58:37 - 01:58:42] ‫יש מקרים שממש אפשר לפתור את זה, ‫את הבעיה האנטגרסית, בצורה סגורה,
[01:58:43 - 01:58:47] ‫ואז זה אומר בעצם שאנחנו יודעים ‫לפתור את הבעיה המקורית בצורה סגורה.
[01:58:47 - 01:58:49] ‫זה אולי נוח בשביל פיתוח
[01:58:52 - 01:58:55] ‫הפתרון הסגור שלנו, ‫אבל זה בעצם אומר ‫שאנחנו לא עושים את הקירוב הזה, בסופו של דבר.
[01:59:07 - 01:59:13] ‫עוד דרך שאפשר לראות ‫שבעצם אנחנו עושים את האוטימיזציה הזאת,
[01:59:13 - 01:59:16] ‫אני דולג על זה, ‫אין מישהו שאני צריך לספר את זה ‫אחר כך.
[01:59:21 - 01:59:29] ‫אוקיי, בואו נסתכל רגע על עוד משמעויות ‫של ה-lbo, של החסם הזה.
[01:59:30 - 01:59:34] ‫בעצם מה זה אומר שאנחנו מחפשים ‫קוסטיריור שממקסם את ה-lbo?
[01:59:35 - 01:59:39] ‫אפשר לכתוב את ה-lbo, ראינו שלושת המונחים, ‫אז בכלל תומכים לכתוב אותו ככה, בתור
[01:59:39 - 01:59:43] ‫איזשהו איבר שבסומנתן באדום,
[01:59:44 - 01:59:45] ‫שאומר
[01:59:49 - 01:59:51] ‫אם y זה משתנה חבוי,
[01:59:51 - 02:00:02] ‫יש לי איזשהו משתנה חבוי ‫שהוא מייצר אחר כך את התמונה. ‫אני חושב על y בתור איזשהו משתנה חבוי ‫שיצר את התמונה. אתם זוכרים שהיה לנו את הדיבור הזאתי ‫של המשתנה החבוי שלנו היה
[02:00:05 - 02:00:11] ‫במקשר לתמונות של פנים, ‫זה היה באיזה מין, איזה צבע שיער, צבע עיניים,
[02:00:12 - 02:00:21] ‫זה משהו שמסביר את התמונה, נכון? ‫אז אם יש לנו תמונה שמסביר את התמונה, ‫אז בעצם אני מחפש את ה-Qים ‫שעבורם אני אקבל הסבר טוב של התמונה.
[02:00:23 - 02:00:25] ‫זו המשמעות של הדבר הזה גדול, ‫זאת אומרת שבהינתן
[02:00:26 - 02:00:27] ‫שעיניים כחולות, התמונה הזאתי היא סבירה.
[02:00:29 - 02:00:33] ‫אבל אני רוצה לעשות איזשהו trade-off ‫בין הדבר הזה לבין זה,
[02:00:34 - 02:00:41] ‫שההתפלגות שלי על צבעי העיניים, ‫למשל, היא תרובה לפריור,
[02:00:42 - 02:00:43] ‫לאיזשהו פריור על צבע עיניים,
[02:00:45 - 02:00:50] ‫לא יכול להיות מצב שאני חושב ‫שההתפלגות שלי היא מאוד שונה,
[02:00:51 - 02:00:54] ‫אם אני מסתכל על הרבה דוגמאות,
[02:00:54 - 02:00:58] ‫אני צריך שההתפלגות על צבעי העיניים ‫תהיה דומה להתפלגות שיש
[02:00:59 - 02:01:00] ‫באוכלוסייה על צבעי עיניים.
[02:01:01 - 02:01:06] ‫אז זה אחת מהמשמעויות שאפשר ‫לחשוב על הגישה בהריאציות.
[02:01:09 - 02:01:10] ‫אתם רואים את זה בפרקפון ‫בין שני הדברים האלה?
[02:01:12 - 02:01:14] ‫אפשר לקשר את זה למה שאני אומר?
[02:01:15 - 02:01:17] ‫זה נקרא לפני מיני קונסטרקשן term.
[02:01:18 - 02:01:22] ‫כמה אנחנו מצליחים לייצר מחדש ‫את הדאטה שאנחנו רואים
[02:01:23 - 02:01:26] ‫על ידי המשתנה החבוי שאנחנו מנחשים אותו.
[02:01:27 - 02:01:30] ‫כי הוא מנחש לנו התפלגות ‫על משתנים קבועים.
[02:01:31 - 02:01:35] ‫המשתנים החבויים האלה צריכים להיות ‫מסוגלים לייצר מחדש את התמונה בצורה טובה.
[02:01:36 - 02:01:45] ‫האיבר הזה בסוג של רגולריזציה, ‫כדומה אומרת, אוקיי, זה לא יכול סתם לייצר ‫כל מיני הסברים. ‫ההסברים האלה גם צריכים להתאים ‫לאיזשהו פרייר ‫על הסברים אפשריים.
[02:01:46 - 02:01:50] ‫זה B של Y. אתה יודע איך אנחנו צריכים לחשוב על זה.
[02:01:51 - 02:01:55] ‫יש יותר עוד דרך לחשוב על זה, ‫זה סידור אחר של האיברים.
[02:01:55 - 02:01:57] ‫אנחנו נחשוב על זה ככה,
[02:01:57 - 02:01:59] ‫כמו זה להצטברות של X ו-Y.
[02:02:00 - 02:02:05] ‫בעצם ההשלמה של הדאטה, ‫אתה לא מסתכל רק על X שאני ראיתי,
[02:02:06 - 02:02:10] ‫הרגישו את זה עם ה-O לגבי ה-X שאני רואה, נגיד, ‫ו-Y זה משהו שאתה לא רואה,
[02:02:11 - 02:02:15] ‫ואני רוצה להשלים את הדאטה ‫בצורה סבירה שההסתובבת בגדולת,
[02:02:16 - 02:02:18] ‫אבל גם אני רוצה שיהיה כמה שיותר
[02:02:19 - 02:02:20] ‫בריאבידיות
[02:02:21 - 02:02:22] ‫בהתפלגות שמשלימה,
[02:02:23 - 02:02:24] ‫בהתפלגות של הפוסטיריות.
[02:02:25 - 02:02:26] ‫זה האנתרופיה של קיר.
[02:02:27 - 02:02:28] ‫אני רוצה שזה יהיה כמה שיותר גדול,
[02:02:29 - 02:02:33] ‫וצריך למצוא טריידו. ‫בשני המקרים האלה, גם במקרה הקודם
[02:02:33 - 02:02:34] ‫וגם במקרה הזה,
[02:02:35 - 02:02:37] ‫מי שפותר את הטריידו ‫בצורה הכי טובה
[02:02:37 - 02:02:38] ‫זה הפוסטיריו האמיתי.
[02:02:40 - 02:02:44] ‫אבל יכול להיות שאני לא אצליח ‫למצוא אותו בבעיה של דיניים גם כן.
[02:02:52 - 02:02:58] ‫אוקיי, אז עכשיו בואו נדבר קצת על עצם זה ‫שאנחנו עושים פה אקרוקסימציה ‫ואנחנו עושים KL,
[02:02:58 - 02:03:01] ‫לא סתם שפוצים KL, ‫אנחנו עושים KL באחד מהכיוונים.
[02:03:02 - 02:03:03] ‫דיברנו עליהם.
[02:03:04 - 02:03:05] ‫זה נכון, אז אמרנו שאנחנו...
[02:03:07 - 02:03:08] ‫כאן,
[02:03:08 - 02:03:12] אנחנו יכולים לחשוב על הקירוב הזה ‫בתור מה שאנחנו רוצים, ‫פחות איזשהו KL,
[02:03:13 - 02:03:15] ‫הראשון ב-KL הזה, Q,
[02:03:16 - 02:03:19] ‫זה ההתפלגות שאנחנו מחפשים אותה.
[02:03:21 - 02:03:22] ‫עושים עליה את האופטימידציה.
[02:03:22 - 02:03:25] ‫מה זה אומר? זה אומר שאם הפוסטיריו האמיתי
[02:03:25 - 02:03:32] ‫הוא יותר מורכב ממה שאנחנו יכולים, ‫איך ייראה הקירוב הזה ‫שאנחנו עושים בין Q ל-P?
[02:03:35 - 02:03:36] ‫אני רוצה לתת בסוף הכול,
[02:03:37 - 02:03:39] ‫זה הפוך,
[02:03:41 - 02:03:42] ‫זה הכיוון ההפוך.
[02:03:42 - 02:03:44] ‫בואו ננסה למצוא רק חלק.
[02:03:44 - 02:03:47] ‫אז למשל, הדוגמה שאני נתקע, ‫אם יש לנו,
[02:03:47 - 02:03:49] ‫נגיד שהגאוסיאן שלנו הוא...
[02:03:50 - 02:03:54] ‫נגיד שה... סליחה, הפוסטיריו ‫הוא גאוסיאן כזה ביום מימדי,
[02:03:55 - 02:03:58] ‫אבל עם קורלציה מאוד גבוהה ‫בין X1 ל-X2.
[02:04:01 - 02:04:02] ‫בין Y1 ל-Y2, כן?
[02:04:02 - 02:04:08] ‫אבל מה שאנחנו מחפשים עליו, ה-Q,
[02:04:09 - 02:04:11] ‫אנחנו מניחים שהם בלתי תלויים.
[02:04:13 - 02:04:15] ‫אז אולי המשפחה יותר פשוטה ‫לעשות עליה אופטימיזציה,
[02:04:15 - 02:04:18] ‫אנחנו לא יכולים להגיע ‫להתפלגות כזאת.
[02:04:19 - 02:04:23] ‫מה זה אומר שאנחנו מניחים ‫שזה בלתי תלוי? ‫זה אומר שההתפלגות שנגיע אליה היא ספירה.
[02:04:23 - 02:04:30] ‫היא ספירה, אוקיי? ‫היא לא יכולה להיות קשר בין ציר ה... ‫בין ה-Q1 ל-Y2,
[02:04:31 - 02:04:34] ‫לכן זה צריך להיות מיושר לצירים.
[02:04:34 - 02:04:36] ‫אז זה לא חייב להיות ספירה, ‫זה צריך להיות מיושר לצירים.
[02:04:38 - 02:04:42] ‫אז מה אנחנו נקבל במקרה הזה? ‫אנחנו לא נרצה לשלם,
[02:04:43 - 02:04:45] ‫אנחנו לא נרצה לצאת החוצה יותר מדי.
[02:04:46 - 02:04:50] ‫אם אנחנו ננסה לתפוס את כל ההתפלגות הזאת, ‫בעצם יהיו הרבה אזורים,
[02:04:51 - 02:04:53] ‫שבין P הוא מאוד נמוך
[02:04:53 - 02:04:54] ‫ו-Q הוא מאוד גבוה,
[02:04:55 - 02:04:56] ‫וזה הכיוון שעליו נשלמים מחיר גדול.
[02:04:57 - 02:05:00] ‫עדיף לנו להיות בכיוון ההפוך, ‫עדיף להיות לנו בתוך ה...
[02:05:00 - 02:05:01] ‫להיות...
[02:05:02 - 02:05:06] בתוך ההתפלגות של P, ‫לא להקיף אותה, ‫היהיה להיות מוקפת על ידי ההשתלמות לפי P,
[02:05:07 - 02:05:08] ‫אבל בעצם מה שיימצא,
[02:05:09 - 02:05:10] ‫זה משהו כזה שהוא בפנים.
[02:05:13 - 02:05:14] ‫לא יצא החוצה.
[02:05:15 - 02:05:17] ‫אוקיי? ולכן זה יגמור,
[02:05:17 - 02:05:20] ‫בגלל זה שאנחנו לא יכולים ‫לתפוס את ההתפלגות הנכונה,
[02:05:20 - 02:05:25] ‫אנחנו נשלם על זה שההתפלגות שלנו ‫תמיד תהיה לו וריאנס יותר קטן ‫מההתפלגות הנכונה.
[02:05:26 - 02:05:27] ‫תהיה לו פיזור יותר קטן.
[02:05:30 - 02:05:32] ‫זה חלק מהתשלום שאנחנו עושים,
[02:05:34 - 02:05:35] ‫בזה שכאלו לא סימטרי.
[02:05:36 - 02:05:40] ‫אנחנו יודעים מראש שהפוסטריו שנמצא, ‫אם הוא לא פוסטריו הנכון,
[02:05:40 - 02:05:42] ‫תהיה לו נטייה להיות עם וריאנס קטן מדי.
[02:05:45 - 02:05:46] ‫אז זה דבר אחד.
[02:05:47 - 02:05:49] ‫ודבר שני, אם יש לנו נגיד כמה מוטים, ‫כמו שהיה לנו קודם,
[02:05:50 - 02:05:56] ‫אבל אם יש לנו כמה מוטים, ‫זה דבר אחד ממדרגה,
[02:05:56 - 02:05:58] ‫אז מה יקרה? אנחנו נישאר,
[02:05:59 - 02:06:06] ‫אנחנו לא נרצה לחצות מוטים, ‫שוב, אנחנו לא נרצה ש-Q ימצא משהו ‫שהוא עובר באזורים שההתפלגות נמוכה,
[02:06:07 - 02:06:11] ‫אנחנו נישאר בתוך מוט יחיד, ‫אז למשל נדפר את המוט הזה,
[02:06:11 - 02:06:12] ‫נישאר בתוכו.
[02:06:14 - 02:06:16] ‫אז נוסעים ב-Retion Inference,
[02:06:17 - 02:06:18] ‫הקוסטריאורס שמוצאים,
[02:06:19 - 02:06:21] ‫הם יהיו קירוב לפוסטריאור האמיתי,
[02:06:22 - 02:06:24] ‫שתהיה להם נטייה ‫גם להיות עם Vines קטן יותר,
[02:06:25 - 02:06:28] ‫וגם רק לתפוס חלק מהמודס ‫של ההתפלגות האמיתית.
[02:06:48 - 02:06:54] ‫יש רק עוד נקודה אחת.
[02:07:19 - 02:07:29] ‫אני רק רוצה שתבינו שיש קשר ‫בין הגישה הבריאציונית הזאת ל-EM.
[02:07:33 - 02:07:36] ‫אתם זוכרים איך הגדרנו את EM?
[02:07:48 - 02:07:55] ‫אם איך הגדרנו איך שיהיה ל-E step ‫איך למצוא את התוכלת
[02:07:55 - 02:08:00] ‫כן, גדול.
[02:08:25 - 02:08:35] ‫אני לא רוצה להיכנס לזה,
[02:08:36 - 02:08:39] ‫אבל אפשר לחשוב על EM בתור,
[02:08:40 - 02:08:44] ‫לפתח את EM מתוך הגישה ‫של Variational Interest,
[02:08:45 - 02:08:48] ‫ואז בעצם כל אחד מהצעדים ‫לצעד אופטימיזציה על משהו אחר.
[02:08:49 - 02:08:52] ‫אז אין ספק קודם שזה צעד אופטימיזציה עם תטא,
[02:08:52 - 02:08:55] ‫ואפשר לחשוב על הצעד הזה ‫בתור אופטימיזציה
[02:08:56 - 02:08:58] ‫על איזושהי פונקציית
[02:09:01 - 02:09:12] ‫אוסטריו שאפשר לקרוא לה Q. ‫במקרה של EM הקלאסי זה בעצם שקול לזה ‫שאנחנו יודעים לעשות את האופטימיזציה של Q ‫על הסוף בכל איתרציה.
[02:09:15 - 02:09:16] ‫נדלג על זה כרגע,
[02:09:17 - 02:09:22] ‫ואנחנו כנראה נחזור לזה ‫כשנדבר על variation and inference,
[02:09:23 - 02:09:26] ‫אני מבין שאם אני אכנס לזה עכשיו, ‫זה יהיה יותר מדי באוויר.
[02:09:33 - 02:09:36] ‫לסיכום, אני רק רציתי לחזור ‫על מה שכתוב כאן.
[02:09:37 - 02:09:39] ‫בעצם, מה זה אומר על סופר ראשון אינפלנס?
[02:09:40 - 02:09:42] ‫אנחנו רוצים לחשב איזושהי הסתברות.
[02:09:44 - 02:09:45] ‫לא חשב פה הכל בשקף.
[02:09:52 - 02:10:13] ‫אנחנו רוצים לחשב את לוג פי X. ‫אוקיי, יש לנו כבר איזשהו מודל, נגיד, שלמדנו,
[02:10:14 - 02:10:17] ‫איזה פי עם איזה שהם פרמטרים, ‫בטא נגיד.
[02:10:18 - 02:10:21] ‫עכשיו קיבלנו איזושהי תמונה X, ‫אנחנו רוצים לחשב את לוג פי X.
[02:10:22 - 02:10:24] ‫אנחנו לא יודעים לעשות את זה,
[02:10:24 - 02:10:33] ‫אנחנו רק יודעים לחשב לוג פי X פסיק Y,
[02:10:34 - 02:10:36] ‫עבור איזשהו משתנה חבוי שאנחנו לא רואים.
[02:10:37 - 02:10:40] ‫אנחנו יודעים שהדבר הזה ‫זה בעצם אינטגרל של הדבר הזה,
[02:10:40 - 02:10:42] ‫אנחנו לא יודעים לחשב את האינטגרל הזה.
[02:10:43 - 02:10:46] ‫גם אנחנו לא יודעים לחשב את האינטגרל.
[02:10:47 - 02:10:48] ‫מה אנחנו עושים?
[02:10:48 - 02:10:52] ‫אנחנו כותבים, מוצאים חסם.
[02:11:11 - 02:11:12] ‫נערב רק
[02:11:19 - 02:11:23] ‫זה מערב רק חישובים שאנחנו יודעים לעשות,
[02:11:24 - 02:11:26] ‫פי יש את x ו-y, אנחנו יודעים לעשות,
[02:11:26 - 02:11:30] ‫q של x ו-y זה איזה שהן פונקציות ‫שאנחנו מגדירים עכשיו ‫רק בשביל החסם הזה,
[02:11:32 - 02:11:33] ‫ועושים לחסם הזה אופטימיזציה.
[02:11:33 - 02:11:50] ‫ברגע שאם הצלחנו למצוא את החסם ‫הכי גבוה שאפשר,
[02:11:51 - 02:11:54] ‫אז התקרבנו כמה שאפשר ‫למה שרצינו לחשב.
[02:11:56 - 02:12:01] ‫זה הרעיון. ‫עכשיו השאלה איך אנחנו יכולים לעשות את זה, ‫איזה qים אנחנו צריכים לבחור,
[02:12:01 - 02:12:07] ‫כדי שמצד אחד הם יהיו יכילו אולי ‫את הפוסטיריור האמיתי,
[02:12:07 - 02:12:09] ‫או שאולי יתערבו אליו מספיק,
[02:12:09 - 02:12:12] ‫ומצד שני אנחנו נוכל לעשות עליהם ‫אופטימיזציה בצורה קלה.
[02:12:13 - 02:12:17] ‫אז גישה אחת זה ממש ‫למצוא את זה עבור כל התפלגות, ‫בצורה סופרת,
[02:12:17 - 02:12:19] ‫ספציפית להראות משנתכם ‫שכשעושים את זה,
[02:12:20 - 02:12:24] ‫בגישה הקלאסית הזאת, ‫אז מפרקים את ההתפלגויות ‫בהתפלגויות שוליות,
[02:12:25 - 02:12:26] ‫שקל לעבוד עליהן, ‫זה נקרא מין פיל.
[02:12:27 - 02:12:31] ‫שאלה אחרת זה שבונים איזושהי ‫פרמטריזציה ‫שיכולה להיות מאוד מורכבת,
[02:12:32 - 02:12:34] ‫אבל בסופו של דבר ‫נותנת איזשהו נגיד גאוסיאן,
[02:12:35 - 02:12:41] ‫אבל התלות הזאת של הגאוסיאן ב-X ‫יכולה להיות מאוד מורכבת, ‫היא יכולה להגיע ‫מאיזושהי מלבשת נוירונים.
[02:12:42 - 02:12:44] ‫עדיין, בסופו של דבר, ‫התוצאה תהיה גאוסיאן,
[02:12:45 - 02:12:50] ‫אבל אנחנו נראה שבמקרה של רשתות נוירונים ‫זה יכול אולי להיות בסדר,
[02:12:50 - 02:12:55] ‫כי יכול להיות שאנחנו בונים את המודל שלנו, ‫עם פי הוא כזה מורכב בעצמו,
[02:12:56 - 02:13:01] ככה שאנחנו יכולים להניח שהקירוב של הפוסטריור לגאוסיאן זה לא כל כך מרע.
[02:13:04 - 02:13:06] בסדר, אז כל זה יהיה עוד שבועיים כנראה.
[02:13:07 - 02:13:11] עוד שבוע אנחנו נתחיל לדבר על מה המשמעות של
[02:13:13 - 02:13:15] להתחיל לעבוד על מודיעין כאלה בצורה יותר פרקטית.
[02:13:18 - 02:13:22] אני חושב שכל החצי הראשון הזה היה טיפה באוויר מדי.
[02:13:23 - 02:13:33] אני מקווה שהצלחתם לספוג קצת לפחות מהמונחים ושיש לכם מספיק מידע פה,
[02:13:33 - 02:13:40] כך שכשתתקלו בדברים תוכלו לקשר את זה למה שראינו בחמשת ההרצאות האלה.
[02:13:41 - 02:13:49] אז בעצם אפשר לחשוב על כל חמשת ההרצאות האלה בתור איזושהי הקדמה ואני מניח שלאף אחד מכם מנותן מדי סדר בראש של מה קורה כאן
[02:13:50 - 02:13:54] ואחרי משבוע הבא אנחנו נתחיל קצת יותר מסודר לעבור על כל מיני מודלים
[02:13:56 - 02:14:00] שעובדים בפועל ושאתם גם תממשו אותם וממש תבינו איך הם עובדים
[02:14:00 - 02:14:10] ואני מקווה שתצליחו לקשר את מה שתראו לדברים שהיו פה ושזה יעשה לכם כל מיני חיבורים מה שעכשיו אני מניח שיחסר טוב.
[02:14:12 - 02:14:15] יש שאלות על מה שראינו היום
[02:14:15 - 02:14:17] או על עניין כבדי?
[02:14:19 - 02:14:27] תודה רבה, אדוני היושב-ראש, תודה רבה, אדוני היושב-ראש, תודה רבה, אדוני השר.
[02:14:49 - 02:15:19] תודה רבה.