[00:00:00 - 00:00:04] אני חושב שהסתדרתי עם כל הדברים, כבר נתחיל,
[00:00:05 - 00:00:06] אהלן,
[00:00:06 - 00:00:07] ברשותכם,
[00:00:08 - 00:00:10] אנחנו במפגש השלישי אז,
[00:00:10 - 00:00:14] וכבר הייתם אמורים להתחיל להסתכל על התרגיל השני,
[00:00:15 - 00:00:16] הצלחתם להתחיל להגיע אליו?
[00:00:17 - 00:00:18] לא כל כך רואה.
[00:00:21 - 00:00:22] טוב, שערות עליו או משהו?
[00:00:24 - 00:00:27] יש פה חלקים שהם די דומים לתרגיל הראשון, אם שמתם לב,
[00:00:28 - 00:00:34] והכל אבל יותר בנקודת מבט של שירוך בייזיאניק, שזה מה שעשינו בשבוע שעבר.
[00:00:36 - 00:00:39] אם יש שאלות אז אתם מוזמנים לשאול.
[00:00:40 - 00:00:44] היום מה שאנחנו נעשה, הנושאים שלנו זה שוב
[00:00:45 - 00:00:52] לסיים את הסוג של מבוא למודלים עם משתנים חבויים,
[00:00:52 - 00:00:54] שנקרא latent Perible Models,
[00:00:55 - 00:00:57] ואז אנחנו נדבר על סוג כזה של מודל
[00:00:57 - 00:00:59] שנקרא גרשן מינסטרל מודלס,
[00:01:00 - 00:01:04] ושיטה לעשות שירוך לפרמטרים
[00:01:05 - 00:01:06] של המודל הזה,
[00:01:06 - 00:01:08] השיטה נקראת אקספקטיישן מקסימיזיישן,
[00:01:09 - 00:01:10] E.M. בקיצור.
[00:01:11 - 00:01:16] אוקיי, אז שוב, בעצם זה יהיה הדוגמה שלנו למודל ולאיך אפשר להשתמש בו
[00:01:20 - 00:01:26] בעולם של מודלים שאנחנו מחשבים את האלגוריתם בצורה ידנית,
[00:01:27 - 00:01:28] ולא עושים
[00:01:31 - 00:01:36] שמקובלים ב-deep learning, שיש לנו איזושהי חבילה שמחשב לקרן אמברדיאנטים והכל קורה בצורה יחסית אוטומטית.
[00:01:38 - 00:01:43] אוקיי, זו הייתה הדוגמה שלנו של המודל הזה שאנחנו עושים אותו, אנחנו יודעים איך לחשב אותו ומבינים מה קורה שם,
[00:01:44 - 00:01:46] ובהמשך של הקורס עוד בערך
[00:01:47 - 00:01:48] בואיים אני חושב.
[00:01:48 - 00:01:54] אנחנו נראה כבר דוגמה ל-Latent Variable Models עם רשתות המוכרות, Deep-Latent Variable Models,
[00:01:54 - 00:01:57] דוגמה שנראה נקראת VE.
[00:01:57 - 00:01:58] יש לנו אותו עם כל רגע.
[00:02:01 - 00:02:03] אוקיי, אז מה שאנחנו נדבר היום,
[00:02:03 - 00:02:07] אז אני אעשה קצת חזרה על מה שהיה בשבוע שעבר ומה שהיה בכלל עד עכשיו,
[00:02:08 - 00:02:10] ושאני אכנס קצת לעניינים.
[00:02:11 - 00:02:14] יש פה כמה דברים שבשבוע שעבר לא הספקתי אז אני קצת אחזור אליהם.
[00:02:15 - 00:02:17] כשלא הספקתי אני אראה פעם ראשונה.
[00:02:17 - 00:02:20] חלק מהדברים אני אחזור אליהם בצורה, אני מקווה שתהיה לכם יותר ברורה.
[00:02:21 - 00:02:25] אז אנחנו שוב פעם נסביר מה הקשר של כל זה ל-Latent Variable Models,
[00:02:26 - 00:02:30] וכמו שאמרתי ניתן את הדוגמה הזאתי של GMM ו-explication maximum nation.
[00:02:30 - 00:02:32] זה סדר הדברים להיום.
[00:02:34 - 00:02:37] אז בואו נתחיל, למה אנחנו צריכים בכלל מודלים גנרטיביים?
[00:02:37 - 00:02:39] אז איפה שכבר ראיתם,
[00:02:40 - 00:02:42] יש בהתחלה כמה דקות של איגבים ואני לא מבין למה.
[00:02:43 - 00:02:44] זה בדרך כלל מסתדר.
[00:02:45 - 00:02:53] אוקיי, אז אמרנו שאנחנו יכולים להשתמש במודל גנרטיבי למשל בשביל לפתור איזושהי משימה שהיא כבר בזמן האימון מוגדרת,
[00:02:54 - 00:02:58] אז זה כבר ראיתם בעצם בקורס במבוא ל-Machine Learning,
[00:02:59 - 00:03:02] למשל עם Classifyים גנרטיביים. אם מראש יש לנו נגיד כמה קלאסים,
[00:03:03 - 00:03:09] אז שיטה להשתמש במודלים גנרטיביים זה ללמוד מודל עבור כל אחד מהקלאסים, ואז כשיש לנו דוגמה חדשה, לשאול
[00:03:10 - 00:03:14] תחת איזה מודל, הדוגמה החדשה הזאת תהיה הכי סבירה. ככה לעשות קלאסיפיקציה בעצם.
[00:03:15 - 00:03:20] אז זה שימוש אחד. שימוש אחר שמאוד נפוץ היום זה פשוט לייצר דאטה.
[00:03:20 - 00:03:22] יש לנו מודל של תמונות,
[00:03:22 - 00:03:25] אנחנו יכולים להשתמש בו כדי לדגום מתוך המודל ההסתברותי הזה ולייצר דאטה.
[00:03:27 - 00:03:32] שימוש שלישי שנדבר עליו קצת היום, מה שכתוב פה זה representation learning,
[00:03:33 - 00:03:39] שזה בעצם המודל שאנחנו לומדים איזשהו מודל, שאולי אפשר גם לדגום איתו דאטה, אבל המטרה בסופו של דבר זה בהינתן דאטה חדש,
[00:03:40 - 00:03:44] להשתמש במודל כדי להבין משהו על הדאטה הזה ולהבין משהו על הייצוג שלו,
[00:03:45 - 00:03:49] או לייצג אותו בצורה יותר טובה כדי שנוכל לעשות עליו דברים אחרים.
[00:03:49 - 00:03:51] למשל, אם אחרי זה אנחנו רוצים ללמוד עליו Classifier,
[00:03:52 - 00:03:55] משהו שלא היה מוגדר לנו מראש בזמן האימון,
[00:03:56 - 00:04:00] או שאנחנו רוצים, למשל אם יש לנו תמונה ואנחנו רוצים לתאר אותה,
[00:04:00 - 00:04:02] רוצים לייצר טקסט שיתאר אותה או משהו כזה,
[00:04:03 - 00:04:09] יכול להיות שהאינפוט לדבר הזה יש אינפוט יותר טוב מאשר לתמונה עצמה, לאיזשהו ייצוג שתופס דברים מעניינים.
[00:04:10 - 00:04:15] אז יש לנו מודל גנרטיבי טוב שיודע לייצר תמונות ומבין, התהליך הזה שמייצר תמונות,
[00:04:15 - 00:04:18] אולי איפשהו בתוכו מסתתר הייצוג הטוב הזה שאפשר להשתמש בו.
[00:04:20 - 00:04:21] אז נדבר על זה דיפה היום.
[00:04:22 - 00:04:23] ויש עוד
[00:04:24 - 00:04:35] שני דברים שזה בעצם משהו שברגע שיש לנו מודל הסתברותי ויש לנו לא רק אנחנו צריכים לעשות פרדיקציה של משהו אלא גם להגיד משהו על מה ההסתברות של הדבר הזה זה מאפשר לנו לעשות החלטות בצורה יותר מושכלת
[00:04:38 - 00:04:43] וזה מין מונח כללי כזה שאפשר להגיד שכל הדברים האלה הם סוג של
[00:04:44 - 00:04:46] פרובייליסטיק אינפורנס ברגע שהמודל שלנו הסתברותי
[00:04:47 - 00:04:49] אנחנו יכולים לתאר כל מיני משימות בתור
[00:04:50 - 00:04:52] בעיה שכל מה שאנחנו צריכים זה לחשב את ההסתברות של משהו
[00:04:53 - 00:04:59] ואז אנחנו משתמשים בשיטות כלליות כאלה של הסקה הסתברותית
[00:05:00 - 00:05:01] כדי לפתור את הבעיות האלה
[00:05:02 - 00:05:03] אוקיי להקדמה כללית
[00:05:04 - 00:05:08] מה שאמרנו שאיך אנחנו לומדים את המודל זה שיטה הקלאסית
[00:05:12 - 00:05:18] מה זה אומר ללמוד את המודל יש לנו נתון יש לנו דאטה שבעצם יוצר על ידי איזושהי התפלגות לא ידועה שאנחנו קוראים לה קי דאטה
[00:05:19 - 00:05:25] אנחנו עכשיו בונים איזשהו מודל פרמטרי של התפלגויות
[00:05:26 - 00:05:28] הפרמטרים באופן כללי אנחנו קוראים להם תטא
[00:05:29 - 00:05:32] ואנחנו מחפשים את התטא ככה שאת פי תטא יהיה כמה שיותר קרוב לפי דאטה
[00:05:33 - 00:05:39] נכון נתנו את התרשים הזה שזה איזשהו מודל מסובך של תמונות למשל
[00:05:39 - 00:05:44] ותמונות שאנחנו אוהבים אנחנו רוצים שהן יהיו בהסתברות גבוהה ותמונות שאנחנו
[00:05:45 - 00:05:48] חושבים שהן לא באמת תמונות מבחינתנו אנחנו רוצים שהן יהיו בהסתברות נמוכה
[00:05:49 - 00:05:50] לתוך המודל הזה.
[00:05:51 - 00:05:52] אוקיי,
[00:05:52 - 00:05:56] אז זה מה שדיברנו שבוע שעבר, אז אמרנו
[00:05:57 - 00:05:59] מה השיטה הקלאסית אם אנחנו רוצים לחפש
[00:06:00 - 00:06:02] את הפרמטרים האלה זה מקסימום לייקליות,
[00:06:03 - 00:06:05] אוקיי? אנחנו, יש לנו,
[00:06:05 - 00:06:13] ראינו גם שזה קשור למזעור ה-KL Divergence בין ה-P data,
[00:06:13 - 00:06:14] סליחה, בין ה-P data ל-P data,
[00:06:15 - 00:06:16] אוקיי? הספירו ש-KL Divergence זה לא סימטריה, אז
[00:06:17 - 00:06:18] חשוב איך אומרים את זה.
[00:06:19 - 00:06:24] זה ראינו שבוע שעבר, וגם ראינו גישה בייזיאנית שאומרת
[00:06:25 - 00:06:37] אוקיי אולי אני רוצה לחשוב על תתא, זה לא בתור איזה משהו פרמטר שהוא נכון ואמיתי, אלא איזשהו משתנה בפני עצמו, שאני לא יודע מה הערך שלו, שהוא יכול לקבל כל מיני ערכים.
[00:06:38 - 00:06:41] לפני שעשיתי ניסויים וראיתי הרבה תמונות,
[00:06:41 - 00:06:46] אז חשבתי שיכולים להיות ערכים כאלה וכאלה, יש לי איזושהי התפלגות שאנחנו רואים ל-Prior,
[00:06:46 - 00:06:49] ואחרי שאני רואה אני רוצה לחשב את ה-Posterior,
[00:06:49 - 00:06:49] על ההתפלגות הזו,
[00:06:50 - 00:06:51] על הפרמטרים האלה.
[00:06:53 - 00:06:54] כן, זו הגישה הבייזיאנית,
[00:06:55 - 00:07:02] ומה שאנחנו נראה היום שבעצם להשתמש במשתנים חבויים, מודלים משתנים חבויים, זה משהו שהוא באמצע,
[00:07:02 - 00:07:04] אוקיי? תכף תבינו למה אני מתכוון,
[00:07:04 - 00:07:06] בין שיטה קלאסית לשיטה בייזיאנית.
[00:07:09 - 00:07:11] אוקיי, אז טוב תזכור את מה אמרנו על הגישה הבייזיאנית,
[00:07:12 - 00:07:15] שכל משהו שהוא לא ידוע לנו מבחינתנו הוא משתנה מקרי,
[00:07:16 - 00:07:22] תמיד אנחנו מחזיקים איזושהי התפלגות על המשתנה המקרי הזה, על הפרמטרים שאנחנו לא יודעים,
[00:07:22 - 00:07:24] קוראים לזה לפעמים האמונה או ה-Belief,
[00:07:25 - 00:07:26] ובהינתן
[00:07:27 - 00:07:34] אבזרבציה אנחנו מעדכנים פשוט את ההתפלגות הזאת, בהתחלה, לפני אבזרבציה אנחנו קוראים לזה Prior, אחרי האבזרבציות אנחנו קוראים לזה Posterior,
[00:07:35 - 00:07:37] אם פתאום הגיעו עוד אבזרבציות אז אנחנו מעדכנים את הפוסטריאור.
[00:07:39 - 00:07:43] זה יתרונות שאנחנו בעצם יכולים לא רק לתפוס איזושהי,
[00:07:44 - 00:07:45] להגיד משהו על הפרמטר אלא גם לתת ממש
[00:07:46 - 00:07:51] איזשהו ציון של עד כמה אנחנו בטוחים בכל מיני דברים,
[00:07:51 - 00:07:53] אנחנו תופסים את האי-בדאות שיש לנו,
[00:07:54 - 00:08:01] אנחנו גם ראינו שבוע שעבר שאנחנו יכולים להגדיר תחת כל מיני קריטריונים מה הדבר האופטימלי שאפשר לעשות,
[00:08:02 - 00:08:03] מה שאי אפשר לעשות בגישה הקלאסית.
[00:08:06 - 00:08:08] חסרונות זה שאנחנו צריכים להניח איזשהו פריור,
[00:08:09 - 00:08:11] זאת אומרת כל מיני הנחות שאנחנו עושים משפיעים
[00:08:12 - 00:08:12] על ה...
[00:08:14 - 00:08:15] על התוצאה הסופית,
[00:08:16 - 00:08:19] יש אנשים שאומרים שזה ממש גרוע,
[00:08:20 - 00:08:20] הדבר הזה,
[00:08:21 - 00:08:23] שזה בדיוק מה שלא רוצים משיטה כזאת,
[00:08:24 - 00:08:27] מצד שני יש אנשים אחרים שאומרים תמיד אנחנו עושים איזשהן הנחות,
[00:08:27 - 00:08:31] פה כל ההבדל זה שההנחה היא מפורמלת בצורה של פריור,
[00:08:32 - 00:08:41] לעומת זאת אם אנחנו עושים linear regression אז אם הוא מוחבא באיך שאנחנו עושים בדיוק את האלגוריתם שלנו מוחבא כל מיני הנחות אחרות,
[00:08:41 - 00:08:42] פה ההנחות הן קצת יותר מפורשות.
[00:08:44 - 00:08:53] זה ויכוח כזה שיש על השימוש בפריור ואולי הקיצרון הכי גדול זה שהרבה פעמים אנחנו יכולים להגיד מה הדבר האופטימלי שצריך לעשות אבל בפועל לא יכולים לחשב אותו,
[00:08:54 - 00:09:00] זה לא כזה עוזר ובנוסף לכל ההנחות שאנחנו עושים אנחנו צריכים לעשות גם עוד מלא קירובים שלא ברור עד כמה הם טובים,
[00:09:01 - 00:09:03] בתוכם מסתדרים הרבה פעמים גם כל מיני הנחות.
[00:09:04 - 00:09:04] אוקיי,
[00:09:06 - 00:09:12] דוגמה שהתחלנו להסתכל עליה זה מודלים גאוסיאנים לינאריים שגם היה לכם בתרגיל,
[00:09:13 - 00:09:21] אז נניח שיש לנו y שהוא נחושב ככה על ידי איזושהי מטריצה a כפול פרמטרים שלנו תטא
[00:09:22 - 00:09:22] ועוד
[00:09:23 - 00:09:34] אתא רעש אפשר לחשוב על זה או איזשהו משתנה מקרי שאני מתעסק על זה שההתפגגות שלו היא גאוסיאנית עם תוכנית אפס
[00:09:34 - 00:09:38] ושונות כזאת, מה זאת אומרת השונות הזאת? שהשונות היא אלכסונית
[00:09:38 - 00:09:44] כלומר כל מימד באטא הוא בלתי תלוי בממדים האחרים
[00:09:45 - 00:09:47] והשונות של כל אחד מהם היא סיגמה בריבוע
[00:09:48 - 00:09:52] אין קו-וריאנס בין הממדים, קו-וריאנס הוא אקליסט
[00:09:54 - 00:10:04] אוקיי, אז אמרנו שבעצם זה משהו שהוא מאוד כללי ושהרבה בעיות אפשר לפרמל בצורה הזאת, למשל רידרסיה לינארית
[00:10:05 - 00:10:06] או רידרסיה
[00:10:06 - 00:10:10] פולינומיאלית, שאפשר לחשוב על זה כמו ריגרסיה לינארית עם איזשהו קרנל
[00:10:12 - 00:10:12] זאת אומרת,
[00:10:12 - 00:10:21] תטא הזה יכול להיות איזושהי פונקציה של ה-X'ים, משהו שאנחנו מראש מגדירים אוקיי? זה יהיה איזשהם פולינורים נגיד של X'ים או כל פונקציה
[00:10:21 - 00:10:25] ומה שאנחנו מחפשים זה את המקדמים הלינאריים של הפונקציות האלה של X
[00:10:26 - 00:10:32] ככה שאנחנו ממזערים פה אנחנו עושים מקסימום לייקליוןי. זאת אומרת זה שהכול למזער שגיאה ריבועית
[00:10:33 - 00:10:34] על ה-y שאנחנו מקבלים
[00:10:37 - 00:10:43] אוקיי אז זה משהו שהוא יחסית כללי וראינו את הפתרון של זה זה הפתרון מקסימום לייקליות
[00:10:44 - 00:10:48] חושבים את הלייקליות, גוזרים ואנחנו מקבלים פתרון אנליטי
[00:10:48 - 00:10:51] למה התטא שממקסם את הלייקליות, נמצאת לייקליות
[00:10:52 - 00:10:53] זה נראה את הדבר הזה
[00:10:54 - 00:10:55] A transpose A
[00:10:56 - 00:10:58] קוראים לזה במינוס 1 וקוראים ל-A transpose Y
[00:10:58 - 00:11:01] אז בהינתן Y אפשר להגיד מה התטות הכי טובות
[00:11:02 - 00:11:06] ברגל סלינארית אתם מכירים את זה, זה הפתרון של הרגל סלינארית
[00:11:06 - 00:11:12] שלא זוכר שהזכיר לעצמו זה נראה בדיוק ככה הפתרון של רגל סלינארית
[00:11:15 - 00:11:19] אוקיי מה עושים במקרה הבייזיאני
[00:11:20 - 00:11:24] זה משהו שלא הספקנו לדבר עליו שבוע שעבר אז בואו נדבר עליו עכשיו
[00:11:26 - 00:11:30] אז מה ההבדל בגישה הבייזיאנית שעכשיו תטא
[00:11:31 - 00:11:36] זה לא משהו שיש לו איזשהו ערך אמיתי שאני מחפש אותו זה משתנה מקרי בפני עצמו
[00:11:37 - 00:11:42] ואני יכול לשאול כל מיני שאלות על המשתנה במקרה האלה אז אחרי שאני אחשב את הפוסטריו עליו
[00:11:42 - 00:11:47] בהינתן Y אני רוצה לחשב מה ההתפלגות שנשארה לי על תטא
[00:11:48 - 00:11:56] יכול להיות שאני אהיה מאוד בטוח נגיד שתטא הוא באיזה ערך מסוים אבל יכול להיות שה-Yים שקיבלתי הם כאלה שלא ייתנו לי הרבה ביטחון על מה תטא נכון?
[00:11:57 - 00:12:00] ותכף אנחנו נראה שאנחנו יכולים עדיין לענות על כל מיני שאלות שם
[00:12:01 - 00:12:05] אז אוקיי אז בוא נכתוב את זה ככה אז תטא
[00:12:06 - 00:12:15] אוקיי אז תטא נניח שהוא מתפלג נורמלית
[00:12:16 - 00:12:20] עם איזשהו
[00:12:21 - 00:12:24] נגיד איזשהו מיו זאת איזושהי תותחלת
[00:12:24 - 00:12:25] זה מטריצות קובריאנס
[00:12:27 - 00:12:31] אוקיי ואתא זה בעצם עשיתם את זה ב...
[00:12:33 - 00:12:36] אוקיי לכתוב את זה כמו שעשיתם את זה בתרגיל בתרגיל זה היה נראה לכם ככה
[00:12:36 - 00:12:43] אתא הוא גם מתפלג נורמלית עם אפס ואיזשהו גמא
[00:12:44 - 00:12:46] אוקיי זה גם זה המטריצות קובריאנס של אתא
[00:12:47 - 00:12:54] וY כמו שאמרנו קודם Y שווה ל-A פותפו את תטא ועוד תטא
[00:12:55 - 00:12:55] ועוד
[00:12:57 - 00:12:59] ועכשיו אנחנו רוצים לשאול מה ההתפלגות
[00:13:00 - 00:13:02] של תטא בהינתן Y
[00:13:03 - 00:13:05] אם ראיתי Y זה וקטור של כל הנקודות
[00:13:06 - 00:13:06] רואה
[00:13:07 - 00:13:10] אוקיי בואו נחשוב רגע בראש אני אשאל לכם את הדוגמה של רגלסיה לינארית
[00:13:11 - 00:13:12] יש לי איקסים
[00:13:13 - 00:13:17] ה-A בעצם זה מורכב מהאיקסים אז A זה בעצם כל האיקסים
[00:13:17 - 00:13:21] וה-Y שאני רואה עבור כל X אני רואה איזשהו Y אחר
[00:13:23 - 00:13:25] ואני צריך למצוא איזשהו
[00:13:26 - 00:13:27] כלל ככה ש-A
[00:13:28 - 00:13:28] תטא
[00:13:31 - 00:13:35] נחשוב על זה ככה ש-Y פחות ה-A תטא שאני מוצא
[00:13:35 - 00:13:36] יהיה משהו שנראה הכי כמו
[00:13:39 - 00:13:39] ה...
[00:13:40 - 00:13:43] אבל יש לי גם איזשהו תנאי, איזשהו פרייר על ה-Teta
[00:13:44 - 00:13:48] אז אני רוצה לראות משהו שמתאים גם ל-Prior יש לי כאן איזשהו טריידור
[00:13:48 - 00:13:51] צריך להיות גם מתאים לדוגמאות שאני רואה וגם מתאים ל-Prior
[00:13:53 - 00:13:56] זה הרעיון ומה שאנחנו רוצים זה לחשב את הפוסטריאור של תטא בינתי Y
[00:13:58 - 00:14:00] הראיתם את זה בתרגיל
[00:14:01 - 00:14:04] שזה בעצמו באוסיאן
[00:14:05 - 00:14:05] נכון?
[00:14:06 - 00:14:07] הפוסטריאור
[00:14:08 - 00:14:15] לדבר הזה, הראיתם שהוא גם גאוסיאן, גם הפוסטריאור הראיתם שהוא גאוסיאן וגם ההתפלגות השולית על Y הראיתם שהוא גאוסיאן
[00:14:19 - 00:14:25] ואפשר לכתוב את זה ככה, ראיתם את זה ככה, זה גאוסיאן של תטא
[00:14:26 - 00:14:28] נקודה פסיק, עכשיו מה הפרמטרים של הגאוסיאן הזה,
[00:14:29 - 00:14:30] מה ה-Mu ומה הסיגמה שלו
[00:14:31 - 00:14:32] אז ה-Mu היה
[00:14:35 - 00:14:44] A טרנספוס A טרנספוס הוא גמא במינוס 1 A ועוד סיגמה במינוס 1,
[00:14:45 - 00:14:47] כל זה במינוס 1 כפול
[00:14:52 - 00:14:53] טיגמו u
[00:14:55 - 00:14:55] ועוד
[00:14:58 - 00:14:58] A
[00:14:59 - 00:15:00] טרנספוס Y
[00:15:02 - 00:15:03] זה התוחלת
[00:15:04 - 00:15:04] של הגרסיאן
[00:15:05 - 00:15:09] אם אתם לא זוכרים, תסתכלו בתרגיל שפתרתם, זה מה שהיה אמור לצאת לכם
[00:15:14 - 00:15:15] והוויריאנס
[00:15:16 - 00:15:17] מה מתנצלת ה-Covarיאנס?
[00:15:18 - 00:15:25] זה נראה הרבה פעמים אותו דבר, זה פשוט הסוגריים האלה
[00:15:26 - 00:15:28] הסוגריים האלה זה מוטל את ה-Covarיאנס
[00:15:28 - 00:15:33] אתם זוכרים בחוק הזה של השמה לביבוע יש לנו את משהו שהוא נמצא בין
[00:15:34 - 00:15:37] תוך התבלית הריבועית, זה יהיה ה-Covarיאנס שלנו
[00:15:37 - 00:15:40] ומה שהוא הדבר הלינארי, זה יהיה
[00:15:41 - 00:15:46] כשאנחנו מכפילים את ה-Covarיאנס כדי לתבות את ה-Covarיאנס שלנו, A
[00:15:48 - 00:15:49] גמא מינוס 1 A
[00:15:50 - 00:15:52] ועוד סיגמה מינוס 1
[00:15:53 - 00:15:55] ועוד מינוס 1 זה ה-Covarיאנס
[00:15:58 - 00:15:59] אוקיי? אז זה יצא לכם בתרגיל.
[00:16:00 - 00:16:02] מה זה אומר עכשיו לגבי ה...
[00:16:02 - 00:16:24] אוקיי, קודם כל זה שהראיתם את הדבר הזה לא אמרתי על זה בשבוע שעבר כלום אבל זה בעצם מקרה כללי, זה מראה שגם מה שהוכחנו בשיעור הראשון נכון, זאת אומרת שגם ההתפלגות השולית היא באוסיאן?
[00:16:26 - 00:16:31] למה? כי התפלגות שולית היא מקרה פרטי, אם A היה מטריצה שיש לה פשוט 1 בכל
[00:16:31 - 00:16:34] שורה יש לה 1 במקום אחר אז פשוט היא מסתכלת על
[00:16:37 - 00:16:42] סליחה אם יש לה פשוט 1 במקום בכל מקום זה פשוט מטריצה היחידה אבל אם היא רק שורה אחת
[00:16:42 - 00:16:43] שיש בה 1 במקום אחד
[00:16:44 - 00:16:48] אז זה יהיה פשוט מטריצה שבוחרת את אחד מהפרמטרים של תטא
[00:16:49 - 00:16:53] ואז בעצם Y זה יהיה אחד מהממדים של תטא
[00:16:54 - 00:16:57] התפלגות על Y תהיה התפלגות השולית על אחד מהממדים של תטא
[00:16:57 - 00:17:00] אז אם זה גאוסיאן אז זה אומר שבהכרח גם התפלגות שולית
[00:17:01 - 00:17:04] של אחד מהממדים של תטא זה גם גאוסיאן,
[00:17:04 - 00:17:06] זה מה שהוכחנו בשבוע הראשון,
[00:17:06 - 00:17:10] וגם יש סכום של משתנים מקריים,
[00:17:11 - 00:17:14] שני מגאוסיאנים וזה גם מראה את זה, כי אם A זה פשוט היה
[00:17:15 - 00:17:16] רצת היחידה,
[00:17:16 - 00:17:18] אז יש כאן סכום של שני משתנים מקריים.
[00:17:21 - 00:17:26] עוד נקודה שמעניינת שאפשר לחשוב על הדבר הזה, אני לא יודע כמה אתם רואים את זה,
[00:17:27 - 00:17:29] אבל זה איזושהי קומבינציה ליניארית בין מיור
[00:17:30 - 00:17:30] ל-Y
[00:17:32 - 00:17:34] יש כאן מיור שמוכפל באיזשהו מקדם
[00:17:36 - 00:17:38] בגלל שדילקטור הזה מקדם אז הוא מפריצה
[00:17:39 - 00:17:41] יש כאן Y שמוכפל באיזשהו מקדם
[00:17:41 - 00:17:44] ואז שניהם מחולקים בסכום המקדמים האלה
[00:18:00 - 00:18:04] אז זה קומבינציה ליניארית כזאת בין מיור ל-Y
[00:18:05 - 00:18:09] שזה הגיוני, אם אני יודע מראש שטטא אמור להיות קרוב לאיזשהו ערך
[00:18:11 - 00:18:16] ואז אני מקבל פתאום איזשהו Y שאומר שהוא משהו אחר עליו
[00:18:16 - 00:18:18] אז ההחלטה שלי תהיה איפשהו באמצע
[00:18:19 - 00:18:22] וכמה אני לוקח לי את צד אחד וכמה אני מושך לצד השני
[00:18:23 - 00:18:27] זה תלוי במטריצות covariance שאני מניח מראש על טטא
[00:18:28 - 00:18:31] או שאני מניח על הרעש שאני מסתכל עליו ועל האוזרבציה שלו
[00:18:33 - 00:18:34] זו הקומבינציה
[00:18:37 - 00:18:38] אני מעט אדבר עליהם עוד דברים שאפשר להגיד על זה
[00:18:44 - 00:18:47] אוקיי, אם אנחנו רוצים לחזור למה שהיה לנו פה קודם
[00:18:48 - 00:18:50] ישים את זה אבל יהיה קשה להבין מה קודם שם
[00:18:50 - 00:18:52] כשהריטובנציה היא כזאת
[00:18:53 - 00:18:56] אז מה שאני מקבל זה, זאת אומרת עבור המקרה
[00:18:57 - 00:19:01] עבור גמא
[00:19:02 - 00:19:04] ששווה לסיגמא בריבוע i
[00:19:06 - 00:19:09] ונגיד שגם מי הוא שווה 0
[00:19:11 - 00:19:14] ובראש הגאוסיין שלי סביב ה...
[00:19:15 - 00:19:17] של הפרייר של טטא הוא סביב ה-0
[00:19:17 - 00:19:21] גם כדי לפשט פה את הביטוי זה משהו שנייה עולה קצת יותר מוכר
[00:19:21 - 00:19:24] התוחלת של הדבר הזה, בוא נצביר רגע רק על האיבר הזה
[00:19:26 - 00:19:26] מה אני אכבל פה?
[00:19:27 - 00:19:29] אז אני צריך לשים סיגמא i במקום כל פעם שיש גמא
[00:19:30 - 00:19:32] אני יכול להתעלם מהעבר הזה של המיום
[00:19:34 - 00:19:35] זו תוחלת של
[00:19:38 - 00:19:39] גטא בהינתן y
[00:19:41 - 00:19:44] במקרה הזה תהיה a טרנספוז
[00:19:46 - 00:19:51] חד חלקי סיגמא בריבוע a ועוד סיגנול
[00:19:52 - 00:19:53] נוסחת
[00:19:53 - 00:19:55] בוא נראה בין נוסחת
[00:19:56 - 00:20:02] ואני יכול להתעלם אז נשאר לי a חלקי סיגמא בריבוע y
[00:20:03 - 00:20:05] אפשר לפשט את זה קצת
[00:20:05 - 00:20:10] יש לי פה רק סיגמא בריבוע ואני מחלק גם באחד מהאיברים כאן אז נשאר לי רק סיגמא בריבוע פה
[00:20:11 - 00:20:14] אוקיי, אוקיי, פוס,
[00:20:15 - 00:20:15] אוקיי,
[00:20:16 - 00:20:17] עוד
[00:20:18 - 00:20:19] סיגמא בריבוע
[00:20:21 - 00:20:23] ריבוע מינוס אחד,
[00:20:24 - 00:20:26] ריבוע מינוס אחד,
[00:20:27 - 00:20:29] אוקיי, פוס ווואט.
[00:20:31 - 00:20:32] נותר לכם מי קשור?
[00:20:33 - 00:20:33] נראה.
[00:20:36 - 00:20:39] נכון, יכולתם בשביל לרניב, מי שיצא לך לאחרונה.
[00:20:41 - 00:20:43] ריג' רגרשן, אתם זוכרים?
[00:20:48 - 00:20:53] הרבה פעמים מציגים את הדבר הזה בתור פתרון של רגרסיה ליניארית עם איזושהי רגולרידציה.
[00:20:54 - 00:20:58] שאלו פה שבוע שעבר, אני חושב, מה קורה אם a transpose a היא לא הפיכה?
[00:20:59 - 00:21:01] אוטומציה אולי קליארית, זו אחת מהשיטות,
[00:21:01 - 00:21:03] זה פשוט להוסיף לזה איזשהו משהו
[00:21:04 - 00:21:07] שהוא בעצמו להפעיל, זה הופך פה להפעיל.
[00:21:08 - 00:21:09] וגם דרך כלל לחשוב על גבול.
[00:21:10 - 00:21:13] אם המציגה היא לא הפיכה, זה אומר שאין לי פתרון,
[00:21:15 - 00:21:16] אין פתרון יחיד, יכול להיות הרבה דברים,
[00:21:17 - 00:21:19] ואם יש לי פריו על תטא,
[00:21:20 - 00:21:22] זה בעצם עוזר לי לבחור מה הפתרון הכי סביר לכולם.
[00:21:24 - 00:21:26] במקרה הזה בגלל שזה כבר סיין, זה דרך כלל התופלת
[00:21:27 - 00:21:29] היא גם הפתרון הכי סבירתי.
[00:21:29 - 00:21:31] זה גם מקסימום לייקליות
[00:21:37 - 00:21:39] מה זה אומר על האוניברסיה הלינארית?
[00:21:40 - 00:21:44] אם במקסימום לייקליות הייתי אומר אוקיי אני בוחר את זה, זה היה קו המקסימום לייקליות
[00:21:45 - 00:21:48] פה מה שאני אומר זה אוקיי יש לי אולי
[00:21:49 - 00:21:50] התוחלת
[00:21:51 - 00:21:56] של ההסתברות הזאת, שהיא גם הישר עם ההסתברות הכי גבוהה
[00:21:57 - 00:22:03] אבל יש לי התפלגות גם, אני יכול להגיד אוקיי יש הרבה דברים שסבירים פה
[00:22:05 - 00:22:07] יש כאלה שקצת יותר, יש כאלה שקצת פחות,
[00:22:07 - 00:22:08] אני יכול לדגור מההתפלגות הזאת
[00:22:09 - 00:22:12] אני יכול לחשב ולהינתן נקודה חדשה מה ההסתברות שהגיעה מפה,
[00:22:13 - 00:22:21] להינתן כל האפשרויות שיכולים להיות, אוקיי אני יכול להחזיק עכשיו איזושהי מערכת של אי ודאות
[00:22:26 - 00:22:32] מה הדבר האופטימלי דרך אגב לעשות, אתם זוכרים מה אמרנו שבוע שעבר?
[00:22:33 - 00:22:34] אז הגדרנו משהו שנקרא
[00:22:35 - 00:22:37] פטה bmse
[00:22:39 - 00:22:40] אתם זוכרים מה זה?
[00:22:44 - 00:22:49] זה מה שממזער את השגיאה הריבועית בתוחלת, ואם הפרייר שלי נכון
[00:22:50 - 00:22:54] והמודל כולו נכון אז מה יקרה אם אני אדגום הרבה פעמים
[00:22:55 - 00:23:02] את כל הבעיה הזאת, זאת אומרת אני אדגום תטא ואני אדגום נקודות שהגיעו מההתפלגות הזאת, ואני אנסה לשחזר את התטא שיצרה את זה,
[00:23:03 - 00:23:05] מה אם מזער את השגיאה הריבועית בתוחלת,
[00:23:05 - 00:23:07] זה פשוט התוחלת של הקוסטימליון,
[00:23:07 - 00:23:09] זה בדיוק הדבר הזה,
[00:23:10 - 00:23:13] וגם הגדרנו את תטא-מפ,
[00:23:14 - 00:23:18] וזה אם אני רוצה למזער במקום את השגיאה הריבועית,
[00:23:19 - 00:23:22] את השגיאה של מה שקראנו Epsilon nr או hit o miss,
[00:23:23 - 00:23:25] אם אני קרוב עד כדי Epsilon לדבר הנכון,
[00:23:26 - 00:23:30] אז אני משלם, לא משלם, אם אני קצת יותר רחוק אני משלם אחד,
[00:23:31 - 00:23:32] לא משנה עד כמה אני רחוק,
[00:23:33 - 00:23:35] אז תטא-קובע מאפ
[00:23:35 - 00:23:45] זה המוד של ההתפלגות, זה המקום, זה המקסימום של הקוסטריור, אבל במקרה הזה זה אותו דבר, כי זה גלוסיין, זה מין והמוד זה אותו דבר.
[00:23:46 - 00:23:50] אז כל הדברים האלה, שני הדברים האלה זה המשערך האופטימלי,
[00:23:50 - 00:23:53] ואולי אני בכלל לא רוצה להוציא משערך אחד, ישערך אחד,
[00:23:53 - 00:23:56] כל הרעיון הביזיאלי זה שאני מחזיק את ההתפלגות הזאת,
[00:23:57 - 00:23:59] ואני יכול לדגום כל מיני ישרים שמבחינתי
[00:24:00 - 00:24:03] יש הסתברות שהם ייצרו את הדאטה.
[00:24:05 - 00:24:08] העיגובים היום ממשיכים, אה? בדרך כלל זה נעצר באיזשהו שלב.
[00:24:16 - 00:24:18] אוקיי, שאלות על זה?
[00:24:18 - 00:24:25] זה היה אחר כך משהו שלא כל כך, לא סיימנו לעשות שבוע שעבר במסטטיסטיקה בנזיאנית.
[00:24:30 - 00:24:32] בואו נדבר עכשיו על לחזור רגע לתמונות,
[00:24:33 - 00:24:34] וזה גם משהו שכבר ככה דיברנו עליו.
[00:24:35 - 00:24:38] למה לא להשתמש כדארסיאנים בתמונות פשוט?
[00:24:43 - 00:24:44] זוכרים מה הייתה התשובה?
[00:24:47 - 00:24:47] יש שתי תשובות.
[00:24:48 - 00:24:49] התמונה לא מתפלגת.
[00:24:49 - 00:24:56] התמונה היא לא מתפלגת גאוסיאנית, נכון? זה מודל לא טוב פשוט לתמונות, אבל הייתה גם עוד סיבה?
[00:24:57 - 00:24:59] כן, אפילו
[00:25:01 - 00:25:04] המודל הפשוט הזה גאוסיאן,
[00:25:05 - 00:25:14] אנחנו צריכים לחשב קובריאנס שהוא D על D, ואם יש לנו תמונה ממיליון פיקסלים זה יהיה מטריצה בגודל מיליון על מיליון, זאת אומרת, אפילו המודל הזה אנחנו לא יכולים לעשות בצורה ישירה.
[00:25:16 - 00:25:17] דרך אגב,
[00:25:18 - 00:25:18] סתם
[00:25:20 - 00:25:27] איך מוכיחים שתמונות הן מולטי-מודליות? אמרתי לכם שבוע שעבר ראיתי לכם את ההתפלגות הזאת, אמרתי שזה נראה כמו משהו מסובך,
[00:25:28 - 00:25:32] אבל למשל אפשר להסתכל על פיקסל אחד בתוך תמונה,
[00:25:34 - 00:25:36] כן? אפילו אם נסתכל על,
[00:25:36 - 00:25:38] נגיד שאני רוצה מודלים של תמונות של שבע,
[00:25:39 - 00:25:40] של הספרה שבע, באמץ,
[00:25:41 - 00:25:45] לתופם את הדאטה של ספרות שאתם משתמשים בו הרבה בשביל
[00:25:48 - 00:25:51] לעשות כל מיני דברים משיניים, כן? תגיד שיש לי דאטה סט של
[00:25:54 - 00:25:55] מספרי שבע,
[00:25:57 - 00:25:59] אז לא יודעת, שתקראו לי פיקסלים,
[00:26:00 - 00:26:02] אז יש פיקסלים שנוטים להיות שחורים נגיד,
[00:26:04 - 00:26:05] שבע,
[00:26:07 - 00:26:07] אוקיי?
[00:26:07 - 00:26:11] אבל יש אפילו שבע, יש לי למשל שתי דרכים לתמונה שונות לכתוב אותה, נכון?
[00:26:12 - 00:26:16] אז יש דרך, זה הדרך האמריקאית נראה, וזה הדרך האירופאית.
[00:26:18 - 00:26:18] אוקיי?
[00:26:19 - 00:26:21] אז בתופסיה תהיו כאלה, ויהיו כאלה.
[00:26:21 - 00:26:23] נגיד, אם נסתכל על הפיקסל הזה,
[00:26:24 - 00:26:25] חצי מהפעמים הוא יהיה כחור,
[00:26:26 - 00:26:27] כאילו הוא יהיה שבע כזה,
[00:26:28 - 00:26:30] וחצי מהפעמים הוא יהיה משהו קרוב ללבן.
[00:26:32 - 00:26:36] אני מסתכל על פיקסל בודד, אין סיכוי שפיקסל בודד הוא יהיה גרסיני.
[00:26:37 - 00:26:40] כי הראינו פה, חייב להיות לו שני מודים בפיקסל הזה.
[00:26:41 - 00:26:44] זה אפילו בהנחה שכל השבים האלה הם תמיד מיושרים,
[00:26:45 - 00:26:46] אפילו אז,
[00:26:47 - 00:26:48] אפילו המודל הפשוט הזה הוא כבר לא גרסיאנט,
[00:26:49 - 00:26:50] כן, יש לו שני מודים.
[00:26:51 - 00:26:52] ואם פיקסל אחד הוא לא גרסיאנט,
[00:26:53 - 00:26:56] שזה התפלגות שולית של כל הפיקסלים,
[00:26:56 - 00:26:58] אם ההתפלגות השולית היא לא גרסיאנט,
[00:26:58 - 00:26:59] זה אומר שגם כל ההתפלגות המשותפת היא בודד.
[00:27:01 - 00:27:05] אני שומע את זה בתור הוכחה לזה שתמונות הן לא גרסיאנט.
[00:27:06 - 00:27:07] אין להם התפלגות גרסיאנטית.
[00:27:09 - 00:27:13] טוב, אז היום אנחנו נראה איך אפשר לפתור את שתי הדברים האלה.
[00:27:13 - 00:27:15] אז כבר התחלנו לדבר על איך אפשר לפתור את הבעיה
[00:27:16 - 00:27:19] אני חושב שהייתה ראשונה, הבעיה של
[00:27:21 - 00:27:22] שיש פני מדי פרמטרים
[00:27:22 - 00:27:24] שאנחנו לא יכולים להחזיק את זה.
[00:27:25 - 00:27:29] אז אמרנו נעשה כל מיני הנחות של אי-טלות
[00:27:29 - 00:27:34] בעצם איך אפשר לחשוב על זה? יש לנו הרבה משתנים, נגיד של איזה מיליון פיקסלים
[00:27:35 - 00:27:37] אנחנו יכולים לכתוב את ההתפלגות הזאת תמיד לפי כלל השרשרת
[00:27:38 - 00:27:39] בצורה כזאתי
[00:27:40 - 00:27:42] זאת אומרת, יש לנו, מסדרים את זה לפי איזשהו סדר, לא משנה,
[00:27:43 - 00:27:43] הכל סדר הוא חוקי
[00:27:44 - 00:27:46] ואז אני אומר הראשון יש התפלגות עליו,
[00:27:47 - 00:27:48] השני הוא בהינתן הראשון,
[00:27:49 - 00:27:54] השלישי בהינתן את פני הראשונים וככה הלאה, והאחרון הוא בהינתן כל הפיצולים הקודמים
[00:27:55 - 00:27:57] אז תמיד אפשר לחשוב ככה כל התפלגות
[00:27:57 - 00:28:01] זה כשלעצמו לא חוסך פרמטרים כי ההתפלגות האחרונה כבר יהיה לה
[00:28:02 - 00:28:07] שתיים לחזקת n-1 במצבים ואנחנו כל אחד מהם נצטרך להגיד מה ההסתברות של
[00:28:08 - 00:28:13] זה כשלעצמו לא יחסוך לנו אבל אם אני אתחיל לעשות המחות אני אתחיל למחוק מפה כל מיני איברים
[00:28:14 - 00:28:18] אז ייתן לנו בעצם יוריד לנו פרמטרים
[00:28:19 - 00:28:22] כן בוא נראה דוגמא בתמונות איך אפשר לעשות את זה
[00:28:31 - 00:28:39] אוקיי אז בואו לפני תמונות, תכף אני אחזור לתמונות, אבל לפני תמונות הגדרנו משהו שנקרא בייס נט שזה דרך גרפית לראות את הדבר הזה, פשוט יהיה נוח קודם שאני זוכר מה זה אומר
[00:28:40 - 00:28:41] אז בעצם
[00:28:41 - 00:28:44] אני, הדרך
[00:28:48 - 00:28:52] להראות את הדבר הזה בצורה גרפית זה לכתוב, אוקיי יש לי כל מיני משתנים
[00:28:57 - 00:28:59] שמונים ככה,
[00:28:59 - 00:29:04] אני מסדר אותם לפי איזשהו סדר זה הראשון, השני תלוי רק בראשון,
[00:29:05 - 00:29:08] השלישי תלוי גם בשני וגם בראשון,
[00:29:09 - 00:29:14] ככה יש לי עוד אחד שתלוי רק בזה ופה יש לי אחד שתלוי בשני דברים,
[00:29:15 - 00:29:21] אוקיי אז זה אפשר לתרגם כל סידור כזה
[00:29:22 - 00:29:25] ועם המחיקות שלו לגרף
[00:29:25 - 00:29:29] מכוון כזה בתנאי שהגרף הזה הוא בלי מעגלים,
[00:29:30 - 00:29:31] אוקיי דג
[00:29:35 - 00:29:38] אם אני עכשיו, אם אני מסתכל על הגרף,
[00:29:38 - 00:29:43] פשוט מאוד נוח להבין מתוך גרף מה המשמעות של זה, כן, כנראה אני מתעכב על זה,
[00:29:43 - 00:29:44] אז אם אני מסתכל על הגרף הזה,
[00:29:45 - 00:29:46] מה הוא אומר לי בעצם,
[00:29:46 - 00:29:50] הוא אומר לי ככה, קודם כל הוא אומר לי דרך אחת שאני יכול לכתוב את ההתפלגות הזאת,
[00:29:51 - 00:29:52] היא ככה,
[00:29:52 - 00:29:54] היא מכפלה של,
[00:29:55 - 00:29:58] לא לכתוב נגיד בקודם גרף, זה אני אגיד, אוקיי, יש לי התפלגות לייצר
[00:29:59 - 00:30:00] את המשתנה הזה,
[00:30:00 - 00:30:03] אחר כך לפי מה שיצא אני מייצר את המשתנה הזה,
[00:30:04 - 00:30:08] אחר כך לפי מה שיצא בשני אלה אני מייצר את המשתנה הזה,
[00:30:08 - 00:30:10] אחר כך לפי זה אני מייצר את זה,
[00:30:10 - 00:30:13] ואחר כך אני אסתכל מהצדקה ופה ולפי זה אני מייצר את המשתנה הזה.
[00:30:14 - 00:30:19] זה נותן לי ממש אלגוריתם איטרטיבי שאומר לי איך אני הולך לייצר את ההתפלגות של הדאטה.
[00:30:21 - 00:30:27] אז אם רק מסתכלו על הגרף זה אמור כבר להגיד לכם שזו דרך חוקית לייצר דאטה מהתפלגות.
[00:30:29 - 00:30:32] פורמלית איך אפשר לכתוב את זה זה שההתפלגות על x
[00:30:35 - 00:30:36] שווה למכפלה
[00:30:37 - 00:30:39] ולהתפלגויות של xI
[00:30:40 - 00:30:42] זאת אומרת אני עובר כל פעם
[00:30:43 - 00:30:44] על אחד מה-x'ים
[00:30:45 - 00:30:48] ואני מייצר אותו בהינתן ההורים שלו
[00:30:51 - 00:30:55] בהינתן ההורים של xI. אז בהינתן גרף כזה זה אומר לי שאני יכול לפרק את ההתפלגות על x
[00:30:56 - 00:30:57] כהתפלגות כזאת.
[00:30:59 - 00:30:59] בסדר?
[00:31:01 - 00:31:01] אם
[00:31:02 - 00:31:03] יכול להיות גרף
[00:31:03 - 00:31:04] מלא,
[00:31:04 - 00:31:06] כלל השרשרת גם אפשר לכתוב אותו ככה
[00:31:08 - 00:31:10] אם אני פשוט מסדר את הכל ככה
[00:31:11 - 00:31:14] וכל אחד הוא תלוי בכל אלה שבאו לפניו
[00:31:17 - 00:31:22] אם כל אחד תלוי בכל אלה שבאו לפניו זה גם יהיה חוקי, זה גם יהיה דג חוקי
[00:31:23 - 00:31:24] אבל הוא לא יחסוך לפרמטרים
[00:31:25 - 00:31:30] ברגע שיש מישהו שיש לו קודקודים לפניו שהוא לא תלוי בהם
[00:31:31 - 00:31:33] אוקיי שהוא לא, אין להם יחס שמחבר אותם,
[00:31:34 - 00:31:34] אז הפרספי פרמנט.
[00:31:36 - 00:31:37] זה אורן שמי, תודה רבה. מה?
[00:31:38 - 00:31:39] רואים זה אורן שמי,
[00:31:40 - 00:31:41] כן כן
[00:31:44 - 00:31:52] ואז צריך לחשוב באיזה סדר לחשב את המחקרה הזאת, אוקיי? תמיד צריך לחשב את זה לסידור של איזה שם גבר.
[00:31:54 - 00:31:57] שתמיד אדם מסתכל על מישהו אחרי שכבר ייצרת את כל ההורים שלו
[00:31:58 - 00:32:04] אוקיי? אבל אפשר לעשות את זה. ברגע שזה דג אז אפשר לעשות את זה. בגלל זה דג זה תירוג חוקי של כלל השרשייה.
[00:32:13 - 00:32:18] אוקיי, ברגע שאני יודע עכשיו בעצם שאם אני אסתכל על הגרף הזה, אז אני יודע להגיד
[00:32:19 - 00:32:21] ככה אני יכול לפרק את ההתפלגות
[00:32:22 - 00:32:23] של הגרף הזה?
[00:32:24 - 00:32:26] מה עוד זה אומר לי? איזה ניתנויות?
[00:32:26 - 00:32:28] מה זה אומר המערכות שאני עושה בעצם?
[00:32:29 - 00:32:32] נגיד שזה היה כל מיני דברים אמיתיים, לא יודע, נגיד שזה היה
[00:32:35 - 00:32:35] טמפרטורה,
[00:32:36 - 00:32:37] רוח,
[00:32:40 - 00:32:43] כמו הלכויות בטמפרטורה לחוץ,
[00:32:45 - 00:32:45] בגשם
[00:32:49 - 00:32:50] וכיף בחוץ.
[00:32:52 - 00:32:53] חשבו אני מתכוון גם בגשם.
[00:32:54 - 00:32:58] אוקיי, אז מה, ברגע שאני אומר, אוקיי, זה המודל שלי,
[00:33:00 - 00:33:00] אז
[00:33:01 - 00:33:03] מה זה אומר על האי תלויות שאני מניע?
[00:33:08 - 00:33:11] אתם יודעים להגיד לי פה משהו שהוא לא תלוי במשהו אחר?
[00:33:12 - 00:33:12] בוא נמחק גם
[00:33:16 - 00:33:17] בטמפרטורה בהינתן נכות.
[00:33:19 - 00:33:22] ישם לא תלוי טמפרטורה בהינתן נכות, נכון?
[00:33:24 - 00:33:27] עוד אה... נקויות שאתם יכולים להגיד?
[00:33:32 - 00:33:34] מה עם גשם וכיף בחוץ?
[00:33:37 - 00:33:37] אה?
[00:33:38 - 00:33:38] לא תלוי.
[00:33:39 - 00:33:41] אבל היו איזה בצורה מלאה?
[00:33:44 - 00:33:46] הם לא תלויים רק בהינתן על החוץ.
[00:33:49 - 00:33:50] מישהו יודע מה הכלל הכללי?
[00:33:50 - 00:33:53] אני חושב שאני חושב שזה בצורה כללית.
[00:33:53 - 00:34:01] זה טיפה מסובך, אבל אפשר להגיד ככה x, i
[00:34:02 - 00:34:03] זה פשוט x
[00:34:05 - 00:34:06] הוא לא תלוי,
[00:34:08 - 00:34:09] זה אחד מהצברים שאומרים שזה לא תלוי,
[00:34:10 - 00:34:10] במי?
[00:34:14 - 00:34:15] בכל ה-xים
[00:34:16 - 00:34:18] שהם לא ה...
[00:34:19 - 00:34:20] צאצאים שלו,
[00:34:21 - 00:34:22] זה צאצאים,
[00:34:23 - 00:34:25] הם לא רק צאצאים כללית,
[00:34:25 - 00:34:26] בהינתן
[00:34:28 - 00:34:30] ה-xים שהם ההורים.
[00:34:36 - 00:34:37] כן, אז זה משהו כללי.
[00:34:37 - 00:34:38] ספציפית עבור, יש...
[00:34:39 - 00:34:41] זאת, כאילו, כל מה שכתוב כאן תמיד יהיה נכון.
[00:34:42 - 00:34:44] ספציפית, אם מצליחים רק על, נגיד, שני אה...
[00:34:46 - 00:34:47] שני ממדים,
[00:34:48 - 00:34:49] יכול להיות שאפשר להגיד משהו אפילו יותר חזק,
[00:34:50 - 00:34:51] אבל כל ה-כל ההנחות פה,
[00:34:52 - 00:34:54] הן תמיד נכונות.
[00:34:55 - 00:34:56] אוקיי? זה אינטואיטיבית שתמיד נכונות.
[00:34:57 - 00:34:57] למשל,
[00:34:58 - 00:34:59] אה...
[00:34:59 - 00:35:00] F בחוץ,
[00:35:02 - 00:35:03] אפשר להגיד שהוא לא תלוי בטמפרטורה,
[00:35:05 - 00:35:06] בהינתן רוח ולחות.
[00:35:08 - 00:35:09] אוקיי?
[00:35:10 - 00:35:12] איך זה אינטואיטיבית? למה זה אינטואיטיבית?
[00:35:14 - 00:35:16] אם יש לי... אם אני יודע כבר את השכבה הזאת,
[00:35:16 - 00:35:18] הכול מעניין אותי מה גרם לה,
[00:35:18 - 00:35:20] כי זה מה שהיא ישפיעה על הכיף בחוץ.
[00:35:23 - 00:35:23] אוקיי?
[00:35:27 - 00:35:31] לקח קצת זמן להבין את הדברים האלה. שימו לב שזה מה שאמרתי עכשיו, מקיים את הדבר הזה.
[00:35:32 - 00:35:32] כיף בחוץ,
[00:35:33 - 00:35:35] מי ההורים של כיף בחוץ ולחות ורוח,
[00:35:36 - 00:35:40] ובהינתן ללחות ורוח הוא בלתי תלוי בכל השאר. הוא בלתי תלוי בגשם וגם באופן תלאפרטורה.
[00:35:41 - 00:35:42] אם לא,
[00:35:42 - 00:35:43] יצאו בכל השאר,
[00:35:43 - 00:35:44] כל מי שהוא לא צאצא שם.
[00:35:45 - 00:35:45] שני אלה לא צאצאים.
[00:35:46 - 00:35:48] קלם לחוץ,
[00:35:49 - 00:35:51] תראי פה דוגמה, אם אולי אתה
[00:35:54 - 00:35:56] צועק את הדבר הזה, אולי אתה יכול להגיד
[00:35:58 - 00:35:59] לחוץ הוא בלתי תלוי ברוח
[00:36:00 - 00:36:01] בהינתן טמפרטורה,
[00:36:02 - 00:36:04] אבל הוא לא בלתי תלוי בהיקף בחוץ ובגשם בהינתן טמפרטורה.
[00:36:08 - 00:36:09] כי הם צאצאים שלו.
[00:36:14 - 00:36:14] יש שאלה?
[00:36:14 - 00:36:15] יש שאלות יותר מעגל?
[00:36:16 - 00:36:23] מה? לא, אמרנו שמעגל יוצר התפלגות לא חוקית.
[00:36:27 - 00:36:30] חייבים שיהיה דאג כדי שזה יגדיר לנו איזושהי התפלגות.
[00:36:31 - 00:36:34] בתוך ההתפלגות זאת יכול להיות שיש תלויות בממדים ואין פעולות.
[00:36:35 - 00:36:38] לפעמים אני קורא לזה ממדים, דרך אגב, ולפעמים אני קורא לזה משתנים אחרים, זה לא משנה.
[00:36:41 - 00:36:44] וזה נקבע על ידי הכללים האלה.
[00:36:44 - 00:36:47] זה קצת כללי, יש סדרה של כללים
[00:36:50 - 00:36:54] אם אנחנו רוצים לשאול על שני דברים ספציפיים, שהם קצת יותר ספציפיים,
[00:36:55 - 00:36:58] שזה האם עובר מסלול שנחצם או לא נחצם,
[00:36:59 - 00:37:00] תביא בכיוונים, טיפה מורחב.
[00:37:01 - 00:37:03] מי שרוצה שיכול להתעגל על זה.
[00:37:06 - 00:37:10] מעניין אבל פחות חשוב בהמשך הפורס שלנו זה עובר טליפורס.
[00:37:12 - 00:37:12] אוקיי.
[00:37:13 - 00:37:18] דרך אגב ראיתם דוגמה כזאת גם בקורס של
[00:37:20 - 00:37:21] ה-ML של מנגול
[00:37:25 - 00:37:25] ב-Machine Learning
[00:37:31 - 00:37:33] נאיב-באס, אתם זוכרים?
[00:37:35 - 00:37:36] נאיב-באס
[00:37:37 - 00:37:45] היה לנו, הדוגמה שראינו את זה, זה היה דוגמה של נושאים של טקסטים.
[00:37:45 - 00:37:47] זה בדרך כלל דוגמה של נושאים של ניי בייס,
[00:37:47 - 00:37:48] אבל אפשר לעשות מזה את הדברים.
[00:37:49 - 00:37:51] נגיד שיש לנו איזשהו משתנה שאומר מה הנושא.
[00:37:56 - 00:37:58] ואז יש לי מילון של מילים.
[00:37:59 - 00:38:01] יש לי מילה אחת,
[00:38:02 - 00:38:03] מילה שנייה,
[00:38:04 - 00:38:06] כך כל המילים שיש לי במילון.
[00:38:06 - 00:38:12] תודה רבה. זה משתנה לכל מילה.
[00:38:13 - 00:38:14] ובעצם
[00:38:18 - 00:38:20] זה המודל בעצם הגרפית
[00:38:21 - 00:38:22] שמתאר שבתור
[00:38:23 - 00:38:24] בייז נטל.
[00:38:26 - 00:38:29] מה המודל הזה אומר? אז מה ההנחה שיש לי פה?
[00:38:36 - 00:38:39] מילים כאילו זה טרופי, כן, אבל ההנחות תמיד זה על אי-קלוט.
[00:38:41 - 00:38:43] אופן כללי אין לי הנחה, אני אומר שהכל תלוי בהכל,
[00:38:44 - 00:38:47] וכשאני נותן לך עץ כזה, שכבר עץ דג,
[00:38:48 - 00:38:54] אז בעצם אתה אמור להבין מתוכו מה ההנחות שאני אעשה, ומה ההנחות האלה זה אי-תלויות שהן מהתבדה.
[00:38:55 - 00:39:01] אבל ההנחה של נאיב-באס זה שכל המילים, שכל הקורדינטות, באופן כללי הן בלתי תלויות,
[00:39:02 - 00:39:03] להינתן הקלאס או נושאים.
[00:39:04 - 00:39:10] אוקיי, וזה מתוך הכלל שאמרנו קודם, אפשר לקרוא את זה מתוך הלח הזה.
[00:39:11 - 00:39:15] נכון? כי w2 למשל, ויינתן הפעורי שלו, זה אוטופיק,
[00:39:15 - 00:39:18] הוא בלתי תלוי בכל אלה שהם לא דיסמבלס שלו,
[00:39:18 - 00:39:19] לכל המילים האחרונים.
[00:39:21 - 00:39:23] כן, אז זה מודל שכבר ראינו.
[00:39:24 - 00:39:25] אוקיי, אז בואו נחזור רגע לתמונות.
[00:39:26 - 00:39:27] יש לי כאן
[00:39:30 - 00:39:31] פרט כזאת.
[00:39:32 - 00:39:33] איך אפשר לעשות את זה לתמונות?
[00:39:33 - 00:39:34] זאת אומרת יש לנו הרבה פיקסלים.
[00:39:39 - 00:39:40] מה אנחנו יכולים לעשות עם זה?
[00:39:41 - 00:39:42] אנחנו יכולים להגיד,
[00:39:43 - 00:39:53] נגיד שעץ נראה כך, אוקיי, יש לנו נגיד הרבה פיקסלים, כל פיקסל זה משתנה מקרים.
[00:39:57 - 00:39:58] ומה אני רוצה להגיד?
[00:39:59 - 00:40:01] אז הייתי יכול ממש לעשות כלל שרשנת מלא.
[00:40:03 - 00:40:05] דיברנו על זה שבוע שעבר, נגיד שהפיקסל הזה יש לו איזושהי התפלגות,
[00:40:07 - 00:40:08] זה בהינתן זה,
[00:40:08 - 00:40:10] זה יש לו התפלגות בהינתן שני הקודמים,
[00:40:11 - 00:40:13] ואז אני מסדר את זה נגיד לפי שורות,
[00:40:13 - 00:40:15] וכל אחד הוא תלוי בכל הקודמים.
[00:40:16 - 00:40:22] זה אומר שכשאני אצטייר את הפיקסל הזה, ייכנסו אליו החצים מכל הפיקסלים האחרים.
[00:40:24 - 00:40:27] אבל אני גם יכול להניח אי תלויות, למשל שרק הפיקסלים הקרובים משפיעים.
[00:40:28 - 00:40:36] למשל, לתוך זה אני אגיד שרק הפיקסלים בסביבה הזאת היא משפיעה.
[00:40:36 - 00:40:40] נגיד שאני בכל כיוון, אני אעזור את משהו כזה.
[00:40:45 - 00:40:48] אבל כל השאר שבאו לפניו לא משפיעים.
[00:40:50 - 00:40:54] למשל זה תמיד יכול להחזיק כדי שיש תמונה שכנים שמשפיעים על הפיקסל התשעילים.
[00:40:55 - 00:40:58] ואני יכול ככה לתפוס,
[00:40:59 - 00:41:02] אני יכול ככה לפרמן ממש את ההתפלגות על כל ה...
[00:41:03 - 00:41:05] בסך הכול זה יהיה
[00:41:06 - 00:41:09] ההתפלגות על כל פיקסל, על הפיקסל ה-I
[00:41:11 - 00:41:22] תהיה איזושהי התפלגות פרמטרית שעובדת מה ההתפלגות של XI, ויינתן שמונה השכנים שלו, שזה יהיה ל-XI מינוס 1 עד XI מינוס 8
[00:41:23 - 00:41:31] זה לא בדיוק ככה אתם סובלים את זה, אתה צריך להסתדר בדיוק בדיוק בדיור מימד, אבל זה משהו כזה. כל אחד יש לו 8 שמשפיעים עליו,
[00:41:32 - 00:41:33] וככה אני יכול לעבור על כל הפיקסלים,
[00:41:34 - 00:41:35] בהתחלה יהיה לי בעיה שהם
[00:41:36 - 00:41:38] קצת מופחות, כי הם בגבול של התמונה,
[00:41:39 - 00:41:42] אבל סך הכול לא יהיה לי אף אחד שהוא תלוי ביותר מישהו עם הפיקסלים,
[00:41:43 - 00:41:48] לכן אפילו אם אני עושה, נגיד שזה תמונות בינאריות, זה מתקדם על פרמטריזציה מלאה של זה,
[00:41:48 - 00:41:51] אז זה יהיה 2 בפסקת 8 אפשרויות,
[00:41:51 - 00:41:52] גם הוא יתור הרבה.
[00:41:52 - 00:41:55] כפול גודל הפיקסלים,
[00:41:55 - 00:41:58] אבל זה יהיה הרבה פחות מ-800 מיליון.
[00:42:04 - 00:42:05] זה למשל אפשרות
[00:42:06 - 00:42:08] למדל תמונות בצורה כזאת,
[00:42:10 - 00:42:14] אנחנו נראה דוגמה כזאת, למרות שזו אפשרות שעובדת לא רעה, אבל היא לא כזאת טובה,
[00:42:15 - 00:42:15] מה היא מתפספסת?
[00:42:18 - 00:42:20] מה רשות
[00:42:23 - 00:42:27] אמרנו שפיקסל הוא תלוי בדברים שבאו לפניו שלו, אבל תלוי
[00:42:28 - 00:42:31] הם לא צאצאים שלו, בהינתן ההורים שלו.
[00:42:32 - 00:42:35] זאת אומרת, זה שאלה טובה, הפיקסל הזה, בהינתן כל הפיקסלים האלה,
[00:42:36 - 00:42:39] לא יהיה תלוי בכל הפיקסלים שבאו לפניו.
[00:42:40 - 00:42:43] יש פה כמה שיבואו אחריו שהם יהיו הילדים שלו, צאצאים שלו.
[00:42:44 - 00:42:50] כל האזור הזה יהיו צאצאים שלו, אז הוא יהיה כן תלוי בהם, אבל כל השאר שבאו לפניו לא יהיה תלוי בהם.
[00:42:50 - 00:42:55] ‫זה יותר קל מתקדש במצוות, ‫במה שבאחריו, כמו שבאחריו, זה תלוי לאמצעים.
[00:42:56 - 00:43:06] ‫-כן, גם מה שיהיה פה איזשהו שלב, ‫שבו הפיקסלים שמגיעים כאן, ‫בהינתן אלה כבר לא יהיו תלויים. ‫איתלות זה סימנטרי.
[00:43:09 - 00:43:13] ‫אז הוא לא יהיה תלוי בכל אלה.
[00:43:15 - 00:43:20] ‫עדיין יהיה פה כמה פיקסלים ‫שגם יהיה ביניהם פיקלות, ‫והינתן מה שיש באמצע.
[00:43:20 - 00:43:25] ‫יש פה קצת דאגויות.
[00:43:26 - 00:43:29] ‫אני עשיתי קורס שלהם על הנושא הזה.
[00:43:30 - 00:43:31] ‫כן, קורס שלהם, יש ספר.
[00:43:34 - 00:43:37] ‫כאילו, מי שפיתח את הנושא הזה, ‫וגם תחום שנקרא
[00:43:41 - 00:43:45] ‫הסקה סיבתית בישראלי, ישראלי שעבר,
[00:43:45 - 00:43:48] ‫יש בנימין יהודה פר, ‫שגם קיבל פרפירום הוא נוסף.
[00:43:48 - 00:43:59] ‫כן, אבל יש ספרים שלמים על זה, ‫אבל צריך להיכנס לזה לאומית, ‫אני רק נותן טעימה של דברים ‫כדי שתבינו, ‫כי הרבה פעמים מצרים את המודלים ‫בתור גרפים כאלה,
[00:44:00 - 00:44:01] ‫שתבינו מה המשמעות של הגרפים.
[00:44:04 - 00:44:10] ‫אוקיי, אבל מה המודל הזה באמת מפספס? ‫מה זה אומר שאם הפיקסל הזה ‫הוא בלתי תלוי בפיקסל הזה, ‫והיא נותנת לסביבה שלו?
[00:44:11 - 00:44:17] ‫אז קודם כול, המודלים האלה, הם עובדים די טוב, ‫במובן הזה שאפשר להגיע אליהם ‫ללייטליות מאוד גבוה.
[00:44:18 - 00:44:22] ‫אחת מהסיבות, אם אתם זוכרים, ‫שבוע שעבר אמרנו שמקסימום לייטליות,
[00:44:23 - 00:44:26] ‫זה דומה למינימום כאל דייברג'נס,
[00:44:27 - 00:44:34] ‫ואפשר לכתוב את הלייטליות ‫בתור כאל דייברג'נס, ‫ועוד האנטרופיה של הדאטה, ‫האנטרופיה של התמונות.
[00:44:35 - 00:44:43] ‫אז בעצם אפשר לחשוב על המרחק, ‫על כאל דייברג'נס הזה, ‫בתור כמה אנטרופיה נוספת ‫אנחנו מוסיפים לאנטרופיה ‫שיש כבר בתמונות,
[00:44:44 - 00:44:47] ‫והרבה מהאנטרופיה בתמונות ‫היא את המידע הלוקאלי הזה,
[00:44:48 - 00:44:50] בעצם משהו שמאוד מבדיל תמונות
[00:44:51 - 00:44:53] טבעיות מאשר סתם
[00:44:54 - 00:44:56] ערכים רנדומליים בתוך מטריצה כזאת,
[00:44:57 - 00:45:07] זה מה שנקרא חלקות, בעצם שפיקסלים קרובים יש להם ערך מאוד קרוב כמעט תמיד, חוץ מבמקרים מסוימים שיש איזשהו אדג' נגיד בין שני אובייקטים בדיוק בתמונה.
[00:45:09 - 00:45:11] אז מודל כזה יכול לתפוס את התכונה הזאת ממש טוב,
[00:45:13 - 00:45:15] אבל שהוא לא יכול לתפוס את כל מיני תכונות גלובליות של התמונה,
[00:45:16 - 00:45:18] למשל אם זו תמונה של פנים,
[00:45:19 - 00:45:22] וזה בדיוק הפיקסל שאומר מה הצבע של העין.
[00:45:24 - 00:45:24] אז
[00:45:26 - 00:45:29] די ברור שהוא יהיה תלוי בצבע של העין פה, של העין השנייה,
[00:45:30 - 00:45:32] או שהיא נמצאת בצד השני אחד של התמונה.
[00:45:33 - 00:45:39] זה די ברור שיש פלוס בינינו. פה אני מניח שבהינתן שאני יודע מה הצבע של העור מסביב לעין,
[00:45:40 - 00:45:42] אז הצבע של העין הזאת היא לא תלויה בצבע של העין הזאת,
[00:45:43 - 00:45:44] שזה בבירור הנחה לא נכונה.
[00:45:45 - 00:45:50] כן, אז כל מיני תכונות גלובליות כאן של התמונות אנחנו מתפספסים ברגע שאנחנו בונים מודל.
[00:45:52 - 00:46:00] אבל נקודה שמעסיקה הרבה את התחום הזה זה שזה לא כל כך מתבטא בפונקציית ה-likelihood האלה.
[00:46:02 - 00:46:04] רוב הביטינים של ה-likelihood
[00:46:05 - 00:46:10] של האנטרופיה של תמונות, הם נמצאים בדיוק בתכונה הזאת שאזור לוקאלי הוא חלק,
[00:46:11 - 00:46:13] ורק קצת מאוד נדבר על כל מיני תכונות
[00:46:14 - 00:46:21] שבבחינתנו כשאנחנו מסתכלים על תמונות הן בעצם אולי הכי חשובות, שהן מדברות על כל מיני תכונות גלובליות של התמונות.
[00:46:26 - 00:46:29] אוקיי, אז איך אנחנו יכולים לחשוב על מודל קצת יותר טוב לתמונות?
[00:46:37 - 00:46:40] אז היינו רוצים משהו שהוא תופס קצת איזה משהו סמנטי,
[00:46:40 - 00:46:43] ושוב משהו שאומר איך התמונה הזאת נוצרה באמת.
[00:46:45 - 00:46:46] למשל,
[00:46:47 - 00:46:49] נרשום על דוגמא כזאת, נגיד שיש לי,
[00:46:49 - 00:46:52] אם אני מדבר על תמונות של פנים של אנשים,
[00:46:53 - 00:46:55] נגיד יש כל מיני פרמטרים שקובעים דברים בתמונה.
[00:46:56 - 00:46:58] אז אמרנו צבע העיניים
[00:47:03 - 00:47:04] ויכול להיות ש...
[00:47:05 - 00:47:06] לא יודע, יש צבע שיער
[00:47:11 - 00:47:14] ‫שני אלה אני מניח שאולי הם מגיעים ‫מאיזשהו משהו משוכח, לא יודע,
[00:47:15 - 00:47:17] ‫אתניות נגיד של האדם.
[00:47:23 - 00:47:25] ‫יכול להיות שיש לי משהו כזה.
[00:47:26 - 00:47:29] ‫יכול להיות שבהינתן אתניות, ‫הצבע עיניים וצבע שיער הם בלתי תלויים.
[00:47:31 - 00:47:33] ‫אבל אם אני לא יודע מה אתניות, ‫זה כן מאוד תלויים.
[00:47:34 - 00:47:36] ‫יכול להיות שיש עוד דברים, לא יודע, ‫מין,
[00:47:36 - 00:47:37] תיבה,
[00:47:38 - 00:47:43] ‫והזווית ההסתכלות,
[00:47:46 - 00:47:47] ‫משקפיים.
[00:47:50 - 00:47:57] ‫אני עכשיו בבלתי תלוי בצבע העיניים, ‫ואז בסופו של דבר יש לי את התמונה.
[00:47:57 - 00:48:10] ‫היא בעצם, איך שהתמונה הזאת נראית, ‫או כל פיקסי בתמונה הזאת, ‫היא פונקציה של
[00:48:11 - 00:48:12] כל הדברים האלה.
[00:48:14 - 00:48:19] ‫אבל נגיד מה שיש לי פה, ‫זה שבהינתן צבע עיניים וצבע שיער, ‫התמונה היא כבר בלתי תלויה באתניות.
[00:48:20 - 00:48:21] ‫במשל, אוקיי?
[00:48:23 - 00:48:26] ‫אם אפשר לחשוב על כל מיני מבנה כזה ‫היררכי שאומר לי מה...
[00:48:28 - 00:48:30] ‫איזה הנחות אני עושה על הדאטה הזה,
[00:48:32 - 00:48:36] ‫על הדרך שהדאטה מוצעת, ‫על ההתפלגות של הטבע הזאת ‫שמייצרת לי תמונות.
[00:48:38 - 00:48:45] ‫אז יהיו לי כל מיני פרמטרים כאן, ‫ואני אחפש את הפרמטרים ‫שממצמים את הלייקדומים.
[00:48:47 - 00:48:53] ‫זה נראה כמו משהו פשוט קצת יותר הגיוני ‫מאשר פשוט לסדר את הפיקסלים ‫באיזשהו סגנור נורמלי,
[00:48:54 - 00:48:56] ‫וזה תופס כל מיני דברים גלבליים, ‫משהו יוצא בעיניים,
[00:48:56 - 00:48:58] ‫ישפיע על שני הפיקסלים האלה בעיקר.
[00:48:59 - 00:49:03] ‫זה לא השיער, ‫ישפיע על הפיקסלים שקשורים לשיער.
[00:49:05 - 00:49:12] ‫ויש דברים שהם גלובליים, ‫יהיו דברים שהם לוקאליים, ‫אני יכול להגיד שגם פה, בתוך זה, ‫אני יכול עדיין לעשות ‫את המבנה הלוקאלי
[00:49:12 - 00:49:16] ‫בתוך כל ה... תמונה זה לא משתנה אחד, ‫הם משתנים הרבה מאוד מדהים,
[00:49:16 - 00:49:20] ‫גם בתוכו אני יכול לעשות עדיין ‫את מה שעשינו קודם,
[00:49:21 - 00:49:24] ‫אבל בנוסף יש פה מיני משתנים ‫מקרים גלובליים ‫שמשקיעים רחוב מאוד מאוד.
[00:49:27 - 00:49:36] אוקיי, מה הבעיה עם הדבר הזה?
[00:49:38 - 00:49:39] מודל טוב.
[00:49:39 - 00:49:43] למה אי אפשר לאמן ככה את ה-dataset של תמונות ולאמן מודל כזה?
[00:49:44 - 00:49:46] נגיד שאנחנו מניחים כל מיני הנחות כאלה,
[00:49:47 - 00:49:49] אנחנו רוצים לאמן את המודל.
[00:49:50 - 00:49:56] כן, אז קודם כל יש כמה בעיות, גם אם היה לנו את המידע אולי זה היה מאוד קשה לבנות מודל כזה.
[00:49:57 - 00:49:57] כאילו,
[00:49:58 - 00:50:02] נתתי מודל פה מאוד פשוט אבל כנראה שהמודל שיתפוס
[00:50:02 - 00:50:04] טוב את הדאטה הוא יהיה קצת יותר מורכב מזה,
[00:50:05 - 00:50:05] יכול להיות שזה יהיה קשה.
[00:50:06 - 00:50:09] ובעיה יותר חמורה זה שלאמת לרוב לא יהיה לנו את כל המידע הזה.
[00:50:11 - 00:50:14] והמידע הזה יהיה, יהיה לנו רק תמונות,
[00:50:14 - 00:50:19] יש לנו דאטה של תמונות ומתוכן אנחנו היינו רוצים איכשהו ללמוד אולי את הדבר הזה אבל אין לנו,
[00:50:20 - 00:50:24] כבר לא חייב להניח מראש את הדבר הזה ולעשות מקסימום להיקיות כי לא יהיה לנו את הערכים
[00:50:24 - 00:50:26] של כל המשתנים המקיימים.
[00:50:27 - 00:50:36] אוקיי, אז זה מביא אותנו בדיוק לנושא הזה של latent variable models, יש לנו משתנים
[00:50:38 - 00:50:41] מקראיים שאנחנו לא רואים, אנחנו רוצים להניח שיש משתנים מקראיים
[00:50:44 - 00:50:46] אבל אנחנו לא רואים אותם
[00:50:48 - 00:50:50] זה הרעיון ב-Latent Variable Models
[00:50:51 - 00:50:52] באופן כללי אפשר לכתוב את זה ככה
[00:50:56 - 00:51:00] יש לנו איזשהו משתנה חבוי, פה קוראים לו Z
[00:51:01 - 00:51:04] ו-X זה הדאטה הלא חבוי שלנו, הדאטה שאנחנו רואים
[00:51:05 - 00:51:05] הוא
[00:51:09 - 00:51:11] אנחנו יודעים את ההתגונות שלו בהינתן את זה
[00:51:13 - 00:51:19] שוב אנחנו יכולים לבנות משהו שהוא הרבה יותר מורכב כמו איזה אלף מלא משתנים חבויים שבסופו של דבר מייצרים את הדאטה שלנו
[00:51:20 - 00:51:23] אנחנו גם יכולים בתוך הדאטה לעשות כל מיני מבנים
[00:51:23 - 00:51:27] אבל הרבה פעמים מספיק פשוט לחשוב על הדאטה בתור כזה דבר
[00:51:28 - 00:51:30] יש לנו משתנה חבוי אחד
[00:51:31 - 00:51:33] והוא מייצר את הדעת
[00:51:34 - 00:51:34] זה מספיק
[00:51:35 - 00:51:39] כדי לפתור הרבה בעיות, תכף נפרט על זה
[00:51:43 - 00:51:47] אוקיי, מה זה אומר שוב? אנחנו מניחים שיש לנו משתנה מקרי נוסף
[00:51:48 - 00:51:50] זה נוסף לכל המשתנים שיש לנו בדאטה
[00:51:50 - 00:51:53] שאנחנו לא רואים, זאת אומרת עבור כל תמונה
[00:51:53 - 00:51:56] X יש לנו עוד Z שאנחנו לא רואים
[00:51:58 - 00:52:00] אוקיי קוראים לזה בדרך כלל latent,
[00:52:00 - 00:52:02] לפעמים גם קוראים לזה hidden
[00:52:03 - 00:52:06] בדרך כלל המונח, כשאומרים latent בדרך כלל מתכוונים לזה
[00:52:07 - 00:52:12] למשל hidden זה הרבה פעמים השכבות בתוך רשתות נוירונים
[00:52:14 - 00:52:18] ולטנט זה המשתנים החבויים, משתנים מקריים
[00:52:19 - 00:52:21] לפעמים עושים את ההפרדה הזאת, לפעמים לא, לפעמים זה מבלבל
[00:52:24 - 00:52:26] למה אנחנו משתמשים בזה בעצם?
[00:52:28 - 00:52:36] אז קודם כל אנחנו רוצים שיהיה לנו, אנחנו חושבים שזה מודל יותר טוב של הדאטה, אבל זה גם פותר לנו את הבעיות שהיה לנו קודם, זאת אומרת אנחנו רוצים בסופו של דבר
[00:52:37 - 00:52:41] ש-X יהיה מודל לא גאוסיאני ושיהיה לנו דרך יעילה לחשב אותו,
[00:52:42 - 00:52:49] ובעצם ההוספה הזאת של משתנים ממקרי חבוי, תכף אנחנו נראה דוגמה, היא עוזרת לנו לעשות את זה בצורה יעילה
[00:52:50 - 00:52:52] אנחנו יכולים בעזרת זה לייצר התפלגות
[00:52:53 - 00:52:55] סך הכל ההתפלגות ה-X לא תהיה גאוסיאן
[00:52:56 - 00:52:59] למרות שאולי זה למשל אולי כן יהיה גאוסיאן
[00:52:59 - 00:53:02] אבל בגלל שזה תלוי באיזשהו משתנה חבוי אחר,
[00:53:02 - 00:53:04] סך הכל זה לא יהיה גאוסיאן
[00:53:05 - 00:53:08] ואפשר לעשות את זה בצורה יעילה
[00:53:08 - 00:53:09] וללמוד את זה בצורה יעילה
[00:53:10 - 00:53:11] זה הרעיון
[00:53:15 - 00:53:18] כמו שאמרנו הרבה פעמים אם אנחנו יכולים
[00:53:19 - 00:53:21] ברגע שזה חבוי אנחנו לא יכולים לגרום לזה בדיוק
[00:53:21 - 00:53:24] להיות המבנה שאנחנו רוצים הרבה פעמים
[00:53:24 - 00:53:27] אבל עדיין זה יכול להוביל,
[00:53:27 - 00:53:29] תכף נראה דוגמאות, למצבים שבהם
[00:53:30 - 00:53:35] זה יותר טבעי מאשר להניח איזושהי הנחה אחרת כמו איזשהו סידור של הפיקסלים
[00:53:39 - 00:53:45] אוקיי אז אמרתי לכם קודם שגישה כזאת היא איפשהו באמצע בין גישה בייזיאנית לגישה קלאסית
[00:53:46 - 00:53:48] אז למה אמרתי את זה?
[00:53:49 - 00:53:51] בעצם אפשר להגיד ככה, יש לנו פה
[00:53:52 - 00:53:54] פרמטרים או דברים לא ידועים,
[00:53:54 - 00:53:56] יש לנו שני סוגים כאלה יש לנו את בטא
[00:53:57 - 00:54:04] נגיד שהדבר הזה הוא בעשייה, יש לנו את התוחלת שלו ואת הווריאנס שלו
[00:54:05 - 00:54:07] אבל יש לנו גם את Z
[00:54:09 - 00:54:16] זה גם משהו לא ידוע אבל אליו אנחנו מתייחסים לא בתור פרמטר לא ידוע אלא בתור משתנה מקרי
[00:54:18 - 00:54:21] אז יש לנו כאן משני הסוגים יש לנו פרמטרים
[00:54:21 - 00:54:23] שאנחנו חושבים אוקיי יש איזה של תטא שהם מבחינת NOM
[00:54:24 - 00:54:28] הם בטטאים הנכונים ויש לנו משתנים מקרים
[00:54:29 - 00:54:39] שאנחנו כן רוצים לחשב עליהם את כל ההתפלגות ולתפוס את הפוסטריאור שלהם ולהתחשב באנסרטנטי ובאי ודאות שיש לנו
[00:54:40 - 00:54:44] ואנחנו בעצם נעשה כשאומרים את דייבאל מודלס כמעט תמיד זה אומר שעושים את הדבר הזה
[00:54:45 - 00:54:48] בעצם על מה שנקרא כאן תטא עושים מקסימום לייקליות
[00:54:49 - 00:54:50] ומה שנקרא כאן Z
[00:54:51 - 00:54:53] עושים משהו בייזיאני
[00:54:54 - 00:54:57] לפעמים זה יהיה אמ״ק, לפעמים זה יהיה MNC, לפעמים זה יהיה לתפוס את כל ההתפלגות
[00:54:58 - 00:55:01] וברגע שאנחנו קוראים למשהו משתנה מקרי
[00:55:02 - 00:55:05] אז זה אומר אנחנו לא הולכים להגיד רק מה הדבר הכי טוב שם
[00:55:06 - 00:55:08] אלא איכשהו אנחנו צריכים לתפוס את ההתפלגות שיש
[00:55:11 - 00:55:12] זה ברור העניין הזה?
[00:55:12 - 00:55:15] אנחנו תכף נראה את הדוגמה דרך המודל GMM
[00:55:22 - 00:55:24] פה?
[00:55:24 - 00:55:29] לא זה כאילו המשך של הכוונה כאן שיכול להיות שגם פה יהיו פרמטרים לא ידועים
[00:55:30 - 00:55:36] אפשר לנות מודל שבו איכשהו יש גשר בין התטויות האלה, יכול להיות שפה אין בכלל דטה
[00:55:36 - 00:55:41] תגיד שאני מניח שהפררייו על זה הוא לא אוסיאן אההה לא אוסיאן אחי
[00:55:42 - 00:55:44] זה הפרמטרים כאילו של הsharework זה הפרמטרים של הפרייו
[00:55:45 - 00:55:49] יכול להיות מודל שבו אני רוצה שיש לי פרמטרים לא ידועים בשניהם יכול
[00:55:49 - 00:55:53] ‫יכול להיות מודל שבו זה מבחינתי ידוע, ‫ורק אם אני חיפש את זה, ‫יכול להיות גם ההפך.
[00:55:54 - 00:55:54] ‫למשל, אם אני יודע,
[00:55:55 - 00:56:00] ‫אפשר לחשוב על זה ‫שאני רואה איזה אונדובציות רועשות ‫של איזה משהו שאני רוצה, ‫אני יודע מה הפרמטרים של הרעש,
[00:56:00 - 00:56:02] ‫ואני לא יודע מה ההתפלגות ‫של הסיגרל שאני מוכרחס.
[00:56:03 - 00:56:05] ‫אבל מה שמעניין אותי זה דווקא ‫למצואות הפריור.
[00:56:08 - 00:56:15] ‫אבל באופן כללי אפשר להגיד ‫שטטא זה הפרמטרים של כל המודל שלי, ‫שחלק זה הפריור וחלק זה ה-RK.
[00:56:19 - 00:56:23] ‫אז השורה האחרונה, שוב פעם, ‫ברגע שאנחנו אומרים ‫להיטינגרנבל מודל,
[00:56:23 - 00:56:27] ‫זה לא לגמרי נובע מהשם הזה, ‫אבל זה בדרך כלל מה שמתכוונים,
[00:56:29 - 00:56:34] ‫שאנחנו מניחים שיש לנו משתנה מקרי, ‫דאט הקוראים לו Z, ‫מניחים שיש לנו פרמטרים לא ידועים,
[00:56:35 - 00:56:36] ‫של המודל, ‫שאנחנו יכולים להגיד את התטא,
[00:56:37 - 00:56:39] ‫ואז על תטא אנחנו עושים ‫מקסימום לייטליות,
[00:56:39 - 00:56:42] ‫ועל זה אנחנו עושים איזשהו שיערוך בייזיאני.
[00:56:43 - 00:56:49] ‫זאת אומרת, אנחנו נחשב את הפוסטריור שלו ‫או את ה...יכול להיות של הפוסטריור, ‫נעשה כמו מיני שיערוכים להתפלגות שלו,
[00:56:50 - 00:56:51] ‫נראה איך דוגמאות.
[00:56:52 - 00:57:17] ‫אז זה מה ששאלו קודם, ‫גם אני צריך להניח איזשהו פרמטר, ‫ברגע שזה משתנה מקרים, ‫בבחינתי אני צריך להניח עליו פריור, ‫אם אני רוצה לעשות עליו משהו בייזיאני,
[00:57:18 - 00:57:21] ‫ויכול להיות שאני גם, ‫שם בפריור זה פרמטרים שאני לא יודע.
[00:57:21 - 00:57:25] ‫שאני רוצה ללמוד תוך כדי הלמידה של המודל.
[00:57:32 - 00:57:32] ‫אוקיי, אז...
[00:57:36 - 00:57:37] כמה דברים.
[00:57:48 - 00:57:48] כן, אז שוב,
[00:57:48 - 00:57:51] ‫היינו יכולים להניח, כמו שאמרנו קודם, ‫נגיד שיש לנו את התמונה,
[00:57:52 - 00:57:58] ‫ולניח, אוקיי, יש לנו כל מיני ‫משתנים מקרים כאלה, איזשהו מבנה,
[00:57:59 - 00:58:03] ‫כל אחד מהם יש פרמטרים, ‫זד אחד, זד שתיים,
[00:58:05 - 00:58:18] ‫זד שתיים, וזד שתיים, וזד שתיים, וזד שתיים, וזד שתיים, וזד שתיים,
[00:58:19 - 00:58:21] ‫לא משנה.
[00:58:22 - 00:58:26] ‫אז אוקיי, יש לי כאן איזשהו ‫כמה משתנים מקריים, כמה תטו,
[00:58:27 - 00:58:31] ‫אני יכול לבנות איזשהו מבנה היררכי כזה, ‫מאוד מורכב, וללמוד
[00:58:32 - 00:58:36] ‫כל הפרמטרים האלה תוך כדי, ‫אז זה, נראה, בשיטה שאנחנו נדבר היום,
[00:58:37 - 00:58:38] ‫באקספיטיישן מוצאים, ‫בדרך כלל אפשר לעשות איזה דבר.
[00:58:41 - 00:58:43] ‫תלוי כמובן איך בדיוק אנחנו,
[00:58:44 - 00:58:46] ‫יש תנאים שבהם אפשר לעשות את זה.
[00:58:46 - 00:58:48] הרבה פעמים אנחנו לא נעשה את זה,
[00:58:48 - 00:58:52] ‫אלא פשוט אנחנו נגדיר איזשהו Z אחד,
[00:58:53 - 00:58:53] ‫שיהיה משהו אולי,
[00:58:54 - 00:58:55] ‫איזשהו וקטור,
[00:58:56 - 00:58:57] ‫מאוד גדול,
[00:58:58 - 00:58:59] איזשהו Z אחד,
[00:59:00 - 00:59:06] ‫ופשוט יהיה לנו התפלגות ‫שהיא יחסית מורכבת בין Z לתמונה.
[00:59:11 - 00:59:11] זה בשני שלבים.
[00:59:12 - 00:59:15] אז אוקיי, אני יכול להגיד כאילו דבר.
[00:59:15 - 00:59:19] ‫נגיד שיש לי משתנה מקרה אחד, ‫שאני קורא לזה זד,
[00:59:20 - 00:59:22] ‫ויש לי, כמו שהיה לנו קודם, פי של
[00:59:23 - 00:59:26] ‫איזשהו פרמטרן תטא של X בין ה-Z,
[00:59:26 - 00:59:27] ‫זה X, כן? זה התפלגות?
[00:59:29 - 00:59:32] ‫אולי יש לי פריור. ‫נגיד שאפילו הפריור ידוע על זה.
[00:59:34 - 00:59:37] ‫אז זה הרבה פעמים יכול להיות מספיק טוב, ‫אבל יש
[00:59:38 - 00:59:40] מקרים שבהם זה לא מספיק טוב.
[00:59:40 - 00:59:47] ‫למה זה יכול להיות? ‫למשל, מה קורה עם פי של...
[00:59:48 - 00:59:50] ‫עם הפריור על Z בגרסיאן?
[00:59:53 - 00:59:54] ‫איזשהו גרסיאן,
[00:59:55 - 00:59:56] ‫אוקיי, אם הוא שונה, זה סיגמאן,
[00:59:58 - 01:00:02] ‫ופי של תטא בינתי נגד Z זה גם איזשהו גרסיאן,
[01:00:03 - 01:00:08] ‫נגיד ש-X זה גרסיאן של X,
[01:00:08 - 01:00:11] ‫שהתוחלת שלו זה איזשהו משהו ליניארי,
[01:00:11 - 01:00:11] ‫וזהו.
[01:00:13 - 01:00:14] ‫ויש לו איזה...
[01:00:16 - 01:00:17] ‫גם קצת פרמטרן עצמי שלו.
[01:00:20 - 01:00:20] ‫מה הבעיה עם זה?
[01:00:21 - 01:00:23] ‫ואם אתה כתוב כאן?
[01:00:24 - 01:00:26] ‫יש לי משתנה חבוי, ‫זה, שאני לא יודע.
[01:00:26 - 01:00:27] פריור עליו,
[01:00:28 - 01:00:29] ‫אני מניח שאני יודע אותו.
[01:00:30 - 01:00:31] ‫אני לא יודע את Z, ‫אני יודע מה פריור עליו,
[01:00:32 - 01:00:33] ‫יש לו פרמטרים כאן.
[01:00:34 - 01:00:36] ‫ופה יש לי משהו שאני לא יודע ‫גידי את הפרמטרים שלו, נגיד.
[01:00:37 - 01:00:42] ‫יש לו או A, יש לו אולי יש לו כל מיני פרמטרים, ‫החובה אני לא יודע בדיוק,
[01:00:43 - 01:00:45] ‫אבל אני מניח שהוא גם גאוסיאן,
[01:00:46 - 01:00:49] ‫זאת אומרת ש-X בהינתן Z הוא גאוסיאן, ‫ככה שהתוחלת של X
[01:00:50 - 01:00:51] ‫היא A כפול Z,
[01:00:53 - 01:00:55] ‫ויש עוד איזשהו קצת פרמטרן.
[01:01:00 - 01:01:01] ‫נניח שזה גאוסיאן,
[01:01:02 - 01:01:05] ‫נכון, זה בדיוק הבעיה.
[01:01:05 - 01:01:09] ‫זה מה שהראינו קודם, ‫זה בדיוק מה שכתבנו קודם, ‫זה בדיוק המודל הליניארי הגאוסיאני,
[01:01:10 - 01:01:11] ‫והמשמעות של זה
[01:01:12 - 01:01:18] ‫זה שבסופו של דבר עשינו פה משהו ‫נורא מפורקם, משתנה מפרי וזה, ‫בסופו של דבר חזרנו להתפלגות גאוסיאנית ה-X.
[01:01:19 - 01:01:22] ‫אז אולי דווקא, אם Z הוא קצת יותר קטן מ-X,
[01:01:23 - 01:01:24] ‫אז יהיה לנו פחות פרמטרים,
[01:01:24 - 01:01:29] ‫אבל נוכל לתפוס את זה ‫בצורה אולי יותר יעילה, ‫אולי נפטרנו מהבעיה הראשונה שהייתה לנו,
[01:01:29 - 01:01:35] ‫שלא יעיל לסוג גאוסיאן מלא על החלטה, ‫אבל עדיין אנחנו נהיה במצב ‫שהמודל שלנו הוא גאוסיאני.
[01:01:35 - 01:01:37] ‫כמו שהראינו קודם, ‫במודל שלנו לא גאוסיאניות,
[01:01:38 - 01:01:41] ‫לא הרווחנו סיבוך של המודל.
[01:01:43 - 01:01:44] ‫אז איך אפשר לכתור את זה?
[01:01:45 - 01:01:46] ‫או שדברים לא יהיו גאוסיאניים,
[01:01:49 - 01:01:49] ‫או פה או פה,
[01:01:50 - 01:01:55] ‫בתרגיל שני כבר יש לכם דוגמאות ‫שמשהו פה לא גאוסיאני, ‫אם צער לא יוצא גאוסיאני,
[01:01:55 - 01:01:57] ‫או שהקשר הוא לא יהיה ליניארי.
[01:01:59 - 01:02:05] ‫אוקיי, אז היום נדבר על דוגמא אחת ‫שזה קורה, שזה תערוג אינפלגסיאני,
[01:02:06 - 01:02:13] ‫ובהמשך אנחנו נראה דוגמה ‫שבהם בשיטות עמוקות, ‫עם מה שנקרא VAE,
[01:02:14 - 01:02:22] ‫מה קורה ב-VAE, ‫ב-VAE אני בדיוק מניח ‫את ההנחה הזאת, ‫אפילו הרבה פעמים אנחנו מניחים ‫שהפריור זה 0 ל-I,
[01:02:26 - 01:02:27] ‫וכאן אני גם מניח שזה גאוסיאן.
[01:02:29 - 01:02:33] ‫אפילו אני מניח שזה גאוסיאנים, ‫זה מטריק שופטו וריאנס כלאכסונית.
[01:02:35 - 01:02:35] ‫אני יודע אותו.
[01:02:36 - 01:02:38] ‫כל ההבדל שהדבר הזה ‫הוא לא ליניארי,
[01:02:39 - 01:02:43] ‫אלא הוא איזושהי רשת נוירונים,
[01:02:45 - 01:02:47] ‫ברומטרים, תטא,
[01:02:47 - 01:02:51] ‫שמסתכלת על זה ומחשבת את התוחלת
[01:02:53 - 01:02:54] ‫של ההתפלגות שלנו.
[01:02:57 - 01:02:58] בעצם החץ הזה,
[01:03:00 - 01:03:01] ‫זה חץ מאוד מורכב כזה.
[01:03:03 - 01:03:05] ‫מציין אותה יותר ככה, כאילו,
[01:03:06 - 01:03:08] ‫כל מיני דבריוני,
[01:03:08 - 01:03:10] ‫אבל אצל עיגולים, ‫כן תחשבו שזה משתנה מקרית,
[01:03:11 - 01:03:12] ‫מתגבינים כאלה.
[01:03:13 - 01:03:15] ‫מתגבינים כל מיני דברים,
[01:03:16 - 01:03:20] ‫מערבבים כל מיני דברים בתוך זה, ‫בסוף יצא משהו בגודל התמונה,
[01:03:21 - 01:03:22] ‫שהוא התוחלת של התחלת.
[01:03:23 - 01:03:26] ‫אז אנחנו נראה עוד שניים או שלושה שאלות.
[01:03:26 - 01:03:32] ‫וזה כבר ישבור את ההנפת הגאוסיאניות ‫על איקס, אוקיי?
[01:03:32 - 01:03:33] ‫הדבר הזה הוא לא אינטרנטיבי.
[01:03:36 - 01:03:38] ‫היסטורנט הזה זה שאין ארגוריתם אינטרנטיבי,
[01:03:41 - 01:03:45] ‫גם יש לי רעיון זה מלינטרנטיבי, ‫אבל אין ארגוריתם אינטרנטיבי ‫שאפשר להראות,
[01:03:50 - 01:03:51] ‫נגיד את זה בצורה שהיא נכונה.
[01:03:51 - 01:03:54] ‫מה שאנחנו נראה שכן עובד פה ‫זה סוג של gradient descent,
[01:03:54 - 01:03:57] ‫בשילוב עם אקספטיישן וצמודי ‫של פנייה היום.
[01:03:58 - 01:04:01] ‫אז זה בשלב הבא בסיבוביות ‫של איך לחשב את הדבר.
[01:04:05 - 01:04:09] ‫אבל היום אנחנו נדבר על מודל ‫שאנחנו כן יכולים לחשב בצורה אינטרנטיבית בקפוצה.
[01:04:12 - 01:04:18] ‫אבל ברגע שהמודל לא גאוסיוני, ‫בדרך כלל זה בא ביחד עם זה ‫שאין לנו דרך אנליטית ‫לחשב את הפתרון של זה.
[01:04:19 - 01:04:22] ‫צריכים לחשוב על איזשהו קירוב ‫או איזושהי שיטה את האוויר.
[01:04:24 - 01:04:40] ‫אוקיי, אז ה-Z הזה, התקווה, ‫זה שהוא יתפוס,
[01:04:41 - 01:04:43] ‫שקיוק איך הוא יתפוס ‫את הדברים המעניינים האלה,
[01:04:44 - 01:04:45] ‫הדברים הסמנטיים האלה,
[01:04:46 - 01:04:52] ‫שאנחנו חושבים שהם הפקטורים ‫המשמעותיים ‫בייצור של ההתפלגות של תמונות,
[01:04:53 - 01:04:58] ‫אבל לא תמיד זה קורה, אוקיי? ‫לפעמים זה לא קורה, ‫ועדיין יש לנו מודל טוב.
[01:05:00 - 01:05:03] ‫במיוחד אם משתמשים בכל מיני רשתות, ‫כי לא תמיד אנחנו יודעים מה...
[01:05:04 - 01:05:10] ‫סוג של blackbox כזה, ‫אנחנו לא יודעים מה יצא ב-Z, ‫אפשר... הרבה מחקר זה ‫איך ה-Z הזה נראה, מה הוא תופס,
[01:05:11 - 01:05:13] ‫אבל לא תמיד זה תופס משהו מעניין,
[01:05:13 - 01:05:15] ‫או לא משהו שאנחנו יודעים לעבוד איתו,
[01:05:16 - 01:05:19] ‫אבל בהרבה מקרים דווקא כן אפשר ‫להראות שזה תופס משהו מעניין.
[01:05:20 - 01:05:21] ‫יש לי דוגמאות.
[01:05:22 - 01:05:46] ‫אז זה מראה את ה-Latence של מודל VAE, ‫בדיוק כזה מודל שתיארתי קודם, ‫שהתוחלת היא רשת נוירונים,
[01:05:47 - 01:05:51] ‫זה הכול גאוסיינים, אבל התוחלת ‫של הגאוסיין של X ‫היא רשת נוירונים שתלויה ב-Z.
[01:05:53 - 01:05:54] ‫וזה הוא פה דו-ממדי,
[01:05:54 - 01:05:57] ‫וזה מומן על מודל דאטה של אמניסט,
[01:05:58 - 01:06:04] ‫בלי סופרוויז'ן, ‫בלי להגיד כל ספרה ‫מאיזה קלאס היא הגיעה. ‫הסתכלו פשוט על כל התמונות,
[01:06:05 - 01:06:06] ‫ומצאו את הפרמטרים, את התטא
[01:06:07 - 01:06:11] ‫של המודל הזה, ‫במקרה הזה זה היה רשת, ‫התטא זה היה ‫משקולות של רשת נוירונים,
[01:06:12 - 01:06:16] ‫ועכשיו, בהינתן כל תמונה, ‫אפשר לחשב
[01:06:16 - 01:06:17] התפלגות על ה-Z-E.
[01:06:19 - 01:06:22] זה היה פוסטריאור, פוסטריאור,
[01:06:22 - 01:06:25] ‫זה מה שהם מציינים כאן, ‫זה לקחו כל מיני תמונות,
[01:06:27 - 01:06:30] ‫על כל תמונה ייצרו דגימה אחת ‫או אולי קצת יותר,
[01:06:31 - 01:06:38] ‫וצבעו את הדגימות. ‫בגלל שזה דודו-ממדי, ‫אז אפשר לשים את ה-Z שיצא, ‫אפשר לשים אותו בתוך מישור כזה.
[01:06:39 - 01:06:40] ‫זה המיקום של הנקודות האלה.
[01:06:41 - 01:06:45] ‫והצבע של הנקודות זה מאיזה קלאס זה הגיע.
[01:06:46 - 01:06:51] ‫המודל אף פעם לא נחשף למידע הזה, ‫אבל כשציירו את הגרף הזה, ‫הסתכלו על איזה קלאס זה יצא.
[01:06:52 - 01:06:53] ‫ולפי זה צבעו את הנקודה.
[01:06:55 - 01:06:56] ‫אז פה כל הנקודות האלה,
[01:06:57 - 01:07:00] ‫זה הרבה נקודות שהגיעו ‫מכל מיני תמונות,
[01:07:01 - 01:07:04] ‫זאת אומרת, זה הרבה Z שהגיעו ‫מכל מיני תמונות שהיו שתיים,
[01:07:04 - 01:07:06] ‫כל העבונים. ‫כבר היו מבינים תמונות שהגיעו במירכאי.
[01:07:08 - 01:07:11] ‫ואפשר לראות לפי איפה ה-Z במרחב,
[01:07:11 - 01:07:16] ‫שה-Z הזה תופס איזשהו משהו שיש בו, ‫איזושהי סמנטיקה שקשורה לספרה.
[01:07:17 - 01:07:23] ‫אוקיי, אז למשל יש כאן ‫האזור הזה, זה האזור שהוא ישראל 2, ‫אני לא יודע אם אתם רואים, ‫יש כאן קצת דברים אחרים,
[01:07:24 - 01:07:25] ‫אולי כשזה ירוק, ואין לאחדים,
[01:07:27 - 01:07:27] ‫כחול כזה,
[01:07:28 - 01:07:31] ‫לא יודע איזה מספר, ‫אבל יש פה כמה זלים שכן אכלו כאן.
[01:07:31 - 01:07:35] ‫זה גם התפלגות, ‫אז יכול להיות שמדי פעם יוצא משהו ‫שהוא מאוד לא סביר.
[01:07:36 - 01:07:37] ‫מדומים מתוך ההתפלגות.
[01:07:39 - 01:07:45] ‫אז כן רואים שיש אזור שהוא יותר ‫אזור של שתיים, ‫אזור שהוא אזור של אחדים,
[01:07:46 - 01:07:48] ‫אזור של צ'אשים, לא יודע, ‫בכל השאר הם לא קטעו כאן.
[01:07:49 - 01:07:54] ‫אז זה מחולק יחסית לאזורים, ‫יש דברים שהם קצת יותר ‫מתערבבים אחד עם השני, ‫ויש דברים שהם יותר נפרדים,
[01:07:55 - 01:07:59] ‫ויש גם דברים שבתוך מחלקה נפרדים, ‫יש פה שני סוגים של שתיים,
[01:08:00 - 01:08:07] ‫אז פה הם טוענים, לא יודע, ‫נראו רק דוגמה אחת, ‫אז אני לא יודע אם זה באמת פופון, ‫שכל השתיים כאן זה כאלה שתיים ‫שעשו עם עיגול,
[01:08:08 - 01:08:13] ‫ושתיים כאלה זה שתיים שעשו ‫בלי מסדרות עיגול, ‫עם שינוי כיוון כזה.
[01:08:14 - 01:08:20] ופה זו בלי הטענה זה אחד עם שהם אלכסוניים וזה אחד עם שהם ישרים
[01:08:22 - 01:08:25] החלשים הזה אני לא יודע אם הוא מספיק כדי להראות את הדבר הזה אבל
[01:08:26 - 01:08:31] יש הרבה מאמרים ומחקרים שמנסים לתפוס באמת מה הזדים האלה תופסים והרבה פעמים
[01:08:32 - 01:08:34] מבטיחים להגיע למצב שהזדים האלה תופסים משהו מעניין
[01:08:35 - 01:08:40] לפעמים צריך לעשות עוד קצת לא רק לעשות סתם מקסימום לקליות אלא לעשות עוד איזה משהו כדי לגרום
[01:08:40 - 01:08:43] לזיידים האלה לתפוס דברים בצורה יותר מהניות.
[01:08:45 - 01:08:46] זו דוגמא אחת.
[01:08:47 - 01:08:53] דוגמא שנייה זו דוגמא עם פרצופים שהיא זאת דוגמא עם גן,
[01:08:54 - 01:08:57] מודל שאני לא יודע אם מספיק לדבר עליו אבל גם אפשר לחשוב עליו
[01:08:58 - 01:09:00] בתור מודל כזה שיש לו latent variable Z
[01:09:02 - 01:09:03] ואז יש איזושהי רשת
[01:09:03 - 01:09:06] שמייצרת את התוחלת של ה-X בהינתן Z.
[01:09:07 - 01:09:12] השיטה שלי ללמוד את זה זה לא בדיוק מקסימום לייפליז, זו שיטה קצת אחרת
[01:09:13 - 01:09:22] אבל מה שמראים כאן על הזיידים זה מה שאפשר לעשות אריתמטיקה כזאת על הזיידים.
[01:09:23 - 01:09:24] אז פה למשל ייצרו
[01:09:29 - 01:09:31] אני לא זוכר בדיוק את הפרטים של איך הם ייצרו את הדוגמאות האלה
[01:09:32 - 01:09:35] אבל יכול להיות שיש להם לכל
[01:09:36 - 01:09:37] יש להם מודל שאיכשהו גם
[01:09:42 - 01:09:48] אני לא זוכר, יכול להיות שיש להם פרט טיפה שאומר משהו על התמונות האם זה איש או אישה, האם היא מחייכת או לא
[01:09:49 - 01:09:52] בכל מקרה הם ייצרו כמה תמונות
[01:09:52 - 01:10:03] של זה output של המודל שלהם כי אחרת אין להם גן זה קצת מסובך, בתוך תמונה אי אפשר לחזור חזרה לזה אבל אפשר לעשות את זה הפוך
[01:10:04 - 01:10:08] אז לא משנה כרגע הפרטים הם ייצרו כמה תמונות שלכל אחד מהם יש זד,
[01:10:09 - 01:10:10] אוקיי? זה כל התמונות האלה.
[01:10:10 - 01:10:13] נגיד הם לקחו את כל התמונות של אישה מחייכת
[01:10:14 - 01:10:20] את כל התמונות של אישה שהיא ניטרלית זאת אומרת היא לא מחייכת לא עושה שום, לא מביאה שום
[01:10:22 - 01:10:22] רגש
[01:10:23 - 01:10:25] וגבר הוא ניטרלי
[01:10:26 - 01:10:30] וחישבו את הזיידים של כל אחד מהם, אוקיי?
[01:10:31 - 01:10:33] יש לנו את הזייד של כל אחד מהתמונות האלה
[01:10:33 - 01:10:35] עשו ממוצע שלו,
[01:10:36 - 01:10:42] קודם כל ממוצע של הזדים הם מייצרים זד חדש שנראה גם כמו
[01:10:44 - 01:10:47] משהו באותה הסביבה, באותה קטגוריה,
[01:10:48 - 01:10:51] וזה פה הממוצע של הזדים האלה, ניצר את הזד הזה,
[01:10:52 - 01:11:01] ואז הם לקחו את הממוצע, אותו דבר בממוצע הזה, לקחו את הממוצע הזה, כל זה במרחב הזדים, כן? לקחו את הממוצע של הזדים האלה, פחות הממוצע של הזדים האלה ועוד הממוצע של הזדים האלה
[01:11:02 - 01:11:04] וזה נתן להם Z חדש,
[01:11:05 - 01:11:10] דגמו בתוך ה-Z הזה איזושהי סביבה קטנה וייצרו את התמונות החדשות שמתאימות ל-Z האלה,
[01:11:11 - 01:11:14] והם קיבלו משהו שהוא בערך נראה כמו גבר מחייבת,
[01:11:15 - 01:11:21] אוקיי? זה סוג של אריתמטיקה פגועה. זה מראה שיש איזשהו משהו סמנטי במרחב הזה של ה-Z, אוקיי? יש אזור שאחראי על
[01:11:22 - 01:11:25] ממדים מסוימים שאחראי על האם זה חיוך או לא,
[01:11:25 - 01:11:27] ממדים מסוימים שאחראים האם זה גבר או אישה,
[01:11:28 - 01:11:29] ולכן אם עושים את הדברים האלה,
[01:11:30 - 01:11:31] בעצם הממדים של ה...
[01:11:31 - 01:11:32] אישה
[01:11:33 - 01:11:34] מתבטלים,
[01:11:34 - 01:11:37] אבל הממדים של החיוך לא מתבטלים, כי פה אין חיוך,
[01:11:38 - 01:11:40] ואז אם מוסיפים את הממד של גבר,
[01:11:40 - 01:11:41] נשאר הממד של החיוך,
[01:11:42 - 01:11:45] והוא מתווסף לממד של הגבר, ואז כשהם יוצרים כמונה מקבלים את זה.
[01:11:48 - 01:11:55] יש כל מיני סוגים של מחקר שעושים על ה... על המרחב החבוי הזה, של המשתנה החבוי,
[01:11:56 - 01:12:01] וזה סוג ש... יש כל מיני סוגים, כל אחד מהם או רובם תחת השם של representation learning.
[01:12:02 - 01:12:05] בעצם למדנו משהו, יש לנו ייצוג יותר טוב לתמונות, כי עכשיו,
[01:12:05 - 01:12:10] תגיד אם אנחנו נרצה לבנות קלאסיפייה של האם תמונה היא של גבר או אישה,
[01:12:11 - 01:12:14] בתוך הזה זה כנראה נמצא במקום יותר ברור מאשר בתוך התמונה.
[01:12:15 - 01:12:17] אותו דבר לגבי חיוך או לא חיוך.
[01:12:19 - 01:12:20] יש פה עוד דוגמה עם משקפיים.
[01:12:24 - 01:12:28] זה לצורך השוואה, מה קורה כשעושים בדיוק את אותו תהליך על התמונות.
[01:12:28 - 01:12:31] אז קודם כל גם הממוצעים עצמם כבר לא נראים טוב,
[01:12:32 - 01:12:33] אתם רואים שזה מאוד מבושטש כזה,
[01:12:35 - 01:12:37] וגם התוצאה לא יוצאת.
[01:12:37 - 01:12:40] זה פשוט ממוצע, פה זה יהיה תוצאה על התמונות.
[01:12:41 - 01:12:41] תמונה,
[01:12:42 - 01:12:43] ממוצע של תמונות עם משקפיים,
[01:12:43 - 01:12:45] מורידים ממוצע של תמונות בלי משקפיים,
[01:12:46 - 01:12:48] ומוסיפים ממוצע של תמונות של נשים בלי משקפיים,
[01:12:49 - 01:12:50] ויוצא איזה משהו בלאגן.
[01:12:52 - 01:12:55] אז מרחב התמונות לא מפריד טוב בין הדברים האלה,
[01:12:55 - 01:12:59] הרבה פעמים אנחנו לומדים מודל עם מרחב, עם מימד
[01:13:00 - 01:13:02] משתנה חבוי,
[01:13:02 - 01:13:03] אז הוא כן יתפוס את הדברים האלה.
[01:13:11 - 01:13:11] כן,
[01:13:12 - 01:13:13] אז זה לומד משהו שבהינתן,
[01:13:14 - 01:13:17] המשהו הזה אפשר בקלות לבנות גאוסיין,
[01:13:18 - 01:13:26] ואז הרבה פעמים יוצא שהמשהו הזה הוא משהו שהוא מפריד כל מיני אספקטים שונים, כי הוא רוצה לעבור כל אספקט שונה, לבנות גאוסיין אחר,
[01:13:26 - 01:13:28] אז הוא צריך להיות מופרד באיזושהי צורה.
[01:13:30 - 01:13:33] כן, אבל זה, כל הדברים האלה זה, כשהקשר בין ה-Latent
[01:13:35 - 01:13:39] לתמונה, זה רשת נוירונים, שאנחנו נגיע לזה בהמשך.
[01:13:40 - 01:13:44] היום אנחנו נדבר על מודל אחר שנקרא Gוש and Mixture Model,
[01:13:45 - 01:13:50] שהוא גם אפשר עכשיו לעבוד תור משתנה חבוי, אז אני רק אגדיר אותו, ואז הוא יוצא לעשר דקות הפסקה.
[01:14:00 - 01:14:01] גארשן.
[01:14:15 - 01:14:19] יש מקומות שמקצרים, קוראים לזה Mixture of Gושן, ואז קוראים לזה M of GX,
[01:14:20 - 01:14:21] אני משתמש בזה,
[01:14:22 - 01:14:28] אז בעצם אני אגיד ש-X מתפלג לפי GMM עם איזה שהם פרמטרים, הפרמטרים הם
[01:14:29 - 01:14:30] הבאים.
[01:14:31 - 01:14:32] יש לנו Pi,
[01:14:33 - 01:14:38] שהוא סקלר, סקלר חיובי,
[01:14:41 - 01:14:42] ויש K כאלה,
[01:14:43 - 01:14:46] יש K שבין אחד עד K גדול,
[01:14:48 - 01:14:49] יש K גדול כאלה,
[01:14:49 - 01:14:53] עוד פרמטר זה U, זאת תוכלות,
[01:14:53 - 01:15:01] ומשתנה ב-RD, במימד של הבעיה של X שלנו,
[01:15:02 - 01:15:03] וגם יש K כאלה,
[01:15:04 - 01:15:06] שבין אחד עד K גדול,
[01:15:07 - 01:15:10] ורציג מאות
[01:15:12 - 01:15:14] זה ב-RDLD
[01:15:17 - 01:15:18] גם שבין K כאלה,
[01:15:19 - 01:15:25] זה הפרמטרים של ההתפלגות,
[01:15:26 - 01:15:28] התטא מורכבת בעצם משתנה את הדברים האלה,
[01:15:28 - 01:15:29] Pi,
[01:15:29 - 01:15:30] מיור וסיגמא,
[01:15:31 - 01:15:34] יש איזה שהם מילוצים, אז Pi לא רק שהוא חיובי,
[01:15:34 - 01:15:35] הסקום של כל ה-Pיים,
[01:15:37 - 01:15:38] X שווה 1 עוד K,
[01:15:40 - 01:15:41] X שווה 1,
[01:15:42 - 01:15:43] ומתוצאות ה-Covariance
[01:15:49 - 01:15:49] אוקיי?
[01:15:51 - 01:15:54] מטריצות ה-Covariance הן צריכות כבדת PSD
[01:16:01 - 01:16:01] אוקיי? זה היה פרמטרי X
[01:16:03 - 01:16:05] X וההתפלגות מוגזרת ככה
[01:16:18 - 01:16:25] זה לא משנה, X ככה פי של X
[01:16:27 - 01:16:27] שווה
[01:16:28 - 01:16:33] לסכום K שווה 1 עד K גדול
[01:16:35 - 01:16:40] של Pi K פפול הגרסיין ה-K בעצם
[01:16:43 - 01:16:44] X או X עם U K
[01:16:48 - 01:16:51] שווה 1 עד K גדול של איברסיין
[01:17:18 - 01:17:21] ‫שווה 1 עד K גדולה?
[01:17:48 - 01:17:51] ‫גרפיים זה הפועליות שיש לנו כמה גאוסיאנים,
[01:17:52 - 01:17:54] ‫כל אחד כזה זה גאוסיאן אחר.
[01:17:57 - 01:17:58] ‫זה אחד ממד הממצאי, נגיד יש גאוסיאנים,
[01:18:00 - 01:18:01] ‫התפחות מאוד שונות,
[01:18:01 - 01:18:03] ‫יש עוד גאוסיאן נגיד עם ה...
[01:18:04 - 01:18:05] ‫ווריאנס מאוד גדול.
[01:18:06 - 01:18:09] ‫ההתפלגות האחרית זה הסכום ‫של כל הפונקציות האלה.
[01:18:09 - 01:18:12] ‫וכל אחד מהם, ‫השטח שלו הוא אחד, נכון?
[01:18:13 - 01:18:14] ‫כל אחד מהתפלגות.
[01:18:14 - 01:18:17] ‫ובגלל שאני עושה ‫קומבינציה כמורה של כולם,
[01:18:18 - 01:18:21] ‫יש לי כאן מספרים שהם חיוביים,
[01:18:22 - 01:18:23] ‫והסכום שלהם הוא אחד,
[01:18:25 - 01:18:26] ‫בשטח של הסכום הזה גם יש הרטי,
[01:18:27 - 01:18:29] ‫גם התפלגות נכון.
[01:18:30 - 01:18:33] ‫בסך הכול, מה יוצא לי פה ‫איזה משהו שנראה...
[01:18:34 - 01:18:35] ‫ניקח,
[01:18:36 - 01:18:41] ‫הוא יותר קטן בעצם, ‫השטח צריך אותו גם. ‫זה יהיה פרופורציונלי למשהו כזה.
[01:18:48 - 01:18:51] ‫במקרה הזה יהיו לי שני צוואר אחר,
[01:18:53 - 01:18:55] ‫יהיו לי שני מודס,
[01:18:57 - 01:19:01] ‫וגם הזנב יהיה קצת יותר ארוך.
[01:19:08 - 01:19:11] ‫אז נמצא הפסקה של עשר דקות, ‫ואז נמשיך לדבר על
[01:19:12 - 01:19:15] ‫הנגדה של מודלס ואיך זה קשור ‫ללייטנד פריה במודלס.
[01:19:17 - 01:19:19] ‫-תודה רבה, אדוני.
[01:19:47 - 01:19:47] ‫-תודה רבה.
[01:20:17 - 01:20:17] ‫-תודה רבה.
[01:20:47 - 01:20:47] ‫-תודה רבה.
[01:21:17 - 01:21:17] ‫-תודה רבה.
[01:21:47 - 01:21:47] ‫-תודה רבה.
[01:22:17 - 01:22:17] ‫-תודה רבה.
[01:22:47 - 01:22:47] ‫-תודה רבה.
[01:23:17 - 01:23:17] ‫-תודה רבה.
[01:23:47 - 01:23:47] ‫-תודה רבה.
[01:24:17 - 01:24:17] ‫-תודה רבה.
[01:24:47 - 01:24:47] ‫-תודה רבה.
[01:25:17 - 01:25:17] ‫-תודה רבה.
[01:25:47 - 01:25:47] ‫-תודה רבה.
[01:26:17 - 01:26:17] ‫-תודה רבה.
[01:26:47 - 01:26:47] ‫-תודה רבה.
[01:27:17 - 01:27:17] ‫-תודה רבה.
[01:27:47 - 01:27:47] ‫-תודה רבה.
[01:28:17 - 01:28:17] ‫-תודה רבה.
[01:28:47 - 01:28:47] ‫-תודה רבה.
[01:29:17 - 01:29:17] ‫-תודה רבה.
[01:29:47 - 01:29:47] ‫-תודה רבה.
[01:30:17 - 01:30:17] ‫-תודה רבה.
[01:30:47 - 01:30:47] ‫-תודה רבה.
[01:31:17 - 01:31:17] ‫-תודה רבה.
[01:31:47 - 01:31:47] ‫-תודה רבה.
[01:32:17 - 01:32:17] ‫-תודה רבה.
[01:32:47 - 01:32:47] ‫-תודה רבה.
[01:33:17 - 01:33:17] ‫-תודה רבה.
[01:33:47 - 01:33:47] ‫-תודה רבה.
[01:34:17 - 01:34:17] ‫-תודה רבה.
[01:34:47 - 01:34:47] ‫-תודה רבה.
[01:35:17 - 01:35:29] ‫אוקיי, אז הגדרנו מה זה ‫ג'רשן מיקסצ'ר מודל,
[01:35:30 - 01:35:34] ‫בצורה ישירה כזאת, אוקיי? ‫נכון? יש לנו איקס,
[01:35:35 - 01:35:37] ‫יש לנו כל מיני פרמטרים ‫שמגדירים את ההתפלגות.
[01:35:38 - 01:35:38] ‫בעצם זה,
[01:35:40 - 01:35:42] ‫כלומר, עכשיו אני קיבלת תערובת, ‫זה שילוב,
[01:35:43 - 01:35:46] ‫תחום כמור, קובינציה כמורה שבתום רשמות,
[01:35:46 - 01:35:50] ‫של הרבה גאוסיאנים. ‫גאוסיאנים שונים, ההתפלגות,
[01:35:51 - 01:35:54] ‫שוב, זה לא סכום של משתנים מקרים ‫שכל אחד מהם הוא גאוסיאנים או משהו אחר,
[01:35:55 - 01:35:57] ‫כי זה נשאר גאוסיאן, ‫כמו שאתם כבר אמורים לדעת.
[01:35:58 - 01:36:08] ‫פה זה הפונקציית התפלגות, ‫היא בעצמה סכום של פונקציות התפלגויות ‫שכל אחד מהם היינו קוראים לו גאוסיאן, ‫אבל סך הכול יוצא לנו פונקציית התפלגות ‫שהיא
[01:36:09 - 01:36:09] לא גאוסיאן.
[01:36:10 - 01:36:13] כן, אז למשל, בגוגמה הזאת, לקחנו שלושה דברים שהם לא עשייה,
[01:36:14 - 01:36:14] וסכמנו אותם
[01:36:15 - 01:36:18] למנחה וההתפלגות, אוקיי? זה לא המשתנה המקרה עצמו
[01:36:19 - 01:36:19] של
[01:36:21 - 01:36:22] מסחם.
[01:36:24 - 01:36:28] אוקיי, אז זו הגדרה ישירה. איך זה קשור הדבר הזה למשתנים רבועים?
[01:36:30 - 01:36:35] בואו נראה הגדרה שקולה לדבר הזה,
[01:36:36 - 01:36:37] במיוחד לצערי.
[01:36:39 - 01:36:42] ‫הגדרה שקולה.
[01:37:04 - 01:37:07] ‫אז יש לנו משתנה שאנחנו נקרא לו K.
[01:37:10 - 01:37:13] ‫שהוא מתפלג בהתפלגות קטגורית.
[01:37:19 - 01:37:21] ‫פרמטר יפאי, מה זה התפלגות קטגורית, אתם יודעים?
[01:37:25 - 01:37:27] ‫זו הכללה של התפלגות בינומית,
[01:37:28 - 01:37:30] ‫הכללה של התפלגות ברנונית,
[01:37:31 - 01:37:32] ‫ליותר משני ערכים.
[01:37:34 - 01:37:37] ‫יש בו K אפשרויות,
[01:37:38 - 01:37:39] ‫וכל אחד מהם יש התפלגות.
[01:37:40 - 01:37:49] ‫אנחנו יכולים לקבל את הערכים ‫0, 1, 2, 3, 4, 5. ‫ויש פה חמישה קוביה, למשל, ‫בהתפלגות קטגורית, ‫שכל אחת מההתפלגויות הוא עם 6.
[01:37:51 - 01:37:52] ‫שזו התפלגות קטגורית.
[01:37:53 - 01:37:56] ‫ואז יש לי את X עם ה-NK,
[01:37:58 - 01:38:00] ‫שהוא מתפלג באנציאנית,
[01:38:03 - 01:38:07] ‫עם איזשהן תוכלות שתלויות ב-P,
[01:38:07 - 01:38:10] ‫וזה קוברריאנס שתלוי בפלגות.
[01:38:16 - 01:38:18] ‫כן, הדבר הזה מגדיר
[01:38:21 - 01:38:23] ‫מודל עם משתנה כתבוי.
[01:38:24 - 01:38:26] ‫אם אפשר לצייר עד ככה,
[01:38:27 - 01:38:30] ‫a עד שניים ו-X.
[01:38:34 - 01:38:36] ‫אנחנו רואים הרבה פעמים פה עם Z,
[01:38:36 - 01:38:40] ‫במקרה של תערוב איברסיאנים, ‫אני קוראים לזה K.
[01:38:44 - 01:38:48] ‫וזה שקול בהגדרה של התפלגות איברסיאנית. ‫אמור זה שקול?
[01:38:50 - 01:38:53] ‫אמור נוכח שזה שקול, ‫ההוכחה שזה שקול.
[01:39:00 - 01:39:01] ‫כביש של X.
[01:39:06 - 01:39:08] ‫אתם רואים פה?
[01:39:12 - 01:39:18] ‫P של X שווה לסכום K שווה 1,
[01:39:19 - 01:39:21] ‫את כל הערכים האפשריים של K,
[01:39:22 - 01:39:28] ‫של P של X פסיכו K. ‫איך זה נקרא, החוק הזה?
[01:39:29 - 01:39:35] ‫הסתברות השלמה, נכון? ‫גם כשאני רואה אם זה משתנה, ‫אני יכול להכניס אותו ‫ולספום לכל האיברים,
[01:39:35 - 01:39:36] ‫וזה רציני יותר.
[01:39:38 - 01:39:41] ‫זה ההתפלגות השולית ‫של ההתפלגות המשותפת.
[01:39:42 - 01:39:50] ‫זה שווה לסכום K שווה 1 על K של P של K,
[01:39:51 - 01:39:53] ‫כל P של X בין ה-K.
[01:39:55 - 01:39:56] ‫נכון? פרקתי את זה.
[01:39:58 - 01:40:02] ‫עכשיו, מה זה P של K?
[01:40:05 - 01:40:11] ‫כל אחד מהערכים של K זה ה-Pi ה-K, ‫נכון? זה הפרמטר כאן,
[01:40:12 - 01:40:13] פאי K,
[01:40:14 - 01:40:16] ‫ומה ההתפלגות של X בין ה-NK?
[01:40:17 - 01:40:18] ‫זה כתוב פה.
[01:40:21 - 01:40:22] ‫זה כתוב פה,
[01:40:23 - 01:40:25] ‫זה הגרסיין הזה, נכון? ‫אז יש לי פה שילוב
[01:40:27 - 01:40:28] ‫גרסיינים,
[01:40:29 - 01:40:43] ‫בנקוד שה-Mu איכשהו תלוי ב-K. ‫למשל, יכול להיות לי פשוט מu אחר לכל K. ‫או יכול להיות גם באיזשהו קשר אחר ‫בין מu ל-K, ‫אבל אנחנו נניח,
[01:40:45 - 01:40:52] ‫בדבר הכי קיצוני שאפשר, ‫שפשוט יש מu אחר לכל K. ‫נשים אחרת אותו.
[01:40:58 - 01:41:06] ‫ברור למה זה חזרנו בדיוק לאותה הגדרה, כן? ‫כך הקדמנו את האוציאנים, ‫שההתפלגות על X היא ספון
[01:41:07 - 01:41:09] ‫למוד של K באוציאנים.
[01:41:13 - 01:41:14] אוקיי?
[01:41:22 - 01:41:23] ‫אוקיי, אז
[01:41:26 - 01:41:28] להערה, שנקלוק קצת בתשובה.
[01:41:28 - 01:41:32] ‫אחר כך אני מתכוון לזה.
[01:41:38 - 01:41:38] ‫אחר כך
[01:41:39 - 01:41:40] ‫כל התגלגות
[01:41:45 - 01:41:46] ‫ניתנת
[01:41:48 - 01:41:49] ‫רוב
[01:41:51 - 01:41:52] ‫למודל מסגרת
[01:41:54 - 01:41:54] ‫התפתחים
[01:41:55 - 01:41:55] ‫בשיחד
[01:41:57 - 01:41:57] ‫לא השיאנים.
[01:41:59 - 01:42:02] ‫כן, המספר הסופי הזה ‫הוא יכול להיות גדול.
[01:42:11 - 01:42:15] ‫כן, בעצם עבור כל Epsilon ‫שניתנו לי, אני יכול למצוא,
[01:42:16 - 01:42:20] ‫ניתנו לי איזושהי התפלגות נורא מסובכת, ‫באפסילון יש מספר סופי של אוניברציאנים,
[01:42:21 - 01:42:23] ‫שיכול לקרב את ההתגלגות.
[01:42:24 - 01:42:26] ‫זאת התערובת שלהם לקרב את ההתגלגות.
[01:42:29 - 01:42:32] ‫-כן, זה מודל שהוא יכול להיות מאוד חזק.
[01:42:33 - 01:42:45] ‫יש מקרים שבהם צריך באמת מספר ‫שכבר אי אפשר לעבוד יותר טוב של אוניברציאנים, ‫ויש מקרים שבהם מספיק לנו מספר יחסית קטן ‫ועדיין אפשר להגיע לתוצאות מעניינות.
[01:42:46 - 01:42:50] ‫אז למשל, דוגמאות, ‫מתי רוצים להשתמש בהתפלגות כזאת?
[01:42:51 - 01:42:52] ‫אז דוגמה אחת,
[01:42:53 - 01:42:56] מה שכבר ראינו בעצם, אם אנחנו רוצים ‫לתפוס איזושהי התפלגות
[01:42:57 - 01:43:00] ‫מולטי-מודיאלית, שיש לה כמה
[01:43:03 - 01:43:04] מקסימומים.
[01:43:04 - 01:43:08] ‫בנושא ראינו את הדוגמה הזאת, נכון, ‫של ה-7 לעומת ה-7, ואמרנו,
[01:43:09 - 01:43:12] ‫הפיקסל הזה לפעמים הוא מאוד קרוב ל-1, ‫לפעמים הוא מאוד קרוב ל-0,
[01:43:13 - 01:43:14] ‫יש לו שני מודים.
[01:43:15 - 01:43:20] ‫אז אם את הרוב של גרסיאנים, ‫עם שני גרסיאנים אפשר לתפוס את הדבר הזה. ‫גרסיאנים אחד סביב ה-1,
[01:43:21 - 01:43:22] ‫גרסיאן אחר סביב ה-0,
[01:43:23 - 01:43:25] ‫הוא כבר יתפוס ב-1,
[01:43:26 - 01:43:28] ‫ההרפכה שהראינו כל מיני ‫שתמונות הן לא גרסיאניות,
[01:43:29 - 01:43:31] ‫כבר אנחנו יכולים לשבור ‫אותה עם התערובת שבגרסיאנים.
[01:43:33 - 01:43:34] ‫זה דוגמה אחת.
[01:43:35 - 01:43:40] ‫אבל דוגמה שמשתמשים הרבה, ‫למשל, אם רוצים למדל משהו ‫עם זנב ארוך,
[01:43:40 - 01:43:45] ‫שיכול לתפוס ארכייר, ‫דיברנו קודם על רוויזיה ליניארית,
[01:43:46 - 01:43:49] ‫שאנחנו שבעצם אפשר להגיד ‫שאנחנו מקסימום לייקיות,
[01:43:49 - 01:43:51] ‫שהרעש פה הוא גרסיאני.
[01:43:52 - 01:43:54] ‫בגרס ערך שגיאה ריבועית ‫זה כמו להגיד שהרעש פה הוא גרסיאני.
[01:43:55 - 01:43:59] ‫אם אנחנו נניח שהרעש הוא תערובת ‫של שני גרסיאנים,
[01:44:01 - 01:44:02] ‫אחד שהוא יחסי צר,
[01:44:03 - 01:44:04] ‫ועוד אחד שהוא מאוד רחב,
[01:44:06 - 01:44:08] ‫בעצם התערובת של זה, ‫זה יהיה משהו שלו.
[01:44:10 - 01:44:12] ‫אנחנו משלמים מחיר מאוד נמוך ‫כשאנחנו נמצאים קרוב ל-peak,
[01:44:14 - 01:44:18] ‫אבל כשאנחנו מתרחקים, ‫מאיזשהו ערך פעל, ‫אנחנו כבר משלמים בערך את אותו מחיר.
[01:44:19 - 01:44:25] ‫זה משהו שיהיה יותר עמיד ‫ל-outלייר כזה, כן? ‫אם יש פתאום איזה outלייר כאן,
[01:44:25 - 01:44:30] ‫המודל הרגיל שהוא רק השגיאה ריבועית, ‫כלומר, אני מניח שזה הרעש הוא גרסיאני,
[01:44:30 - 01:44:34] ‫היה לגמרי, זו דוגמה מאוד קיצונית, ‫הייתה מתה לגמרי את התוצאה הזאת.
[01:44:36 - 01:44:42] ‫אבל אם אני מניח שה-light-lue הגיע מתערובת ‫של שני גרסיאנים כאלה, ‫אני יכול להתמודד עם זה, להגיד,
[01:44:42 - 01:44:46] ‫אוקיי, מאיזשהו אזור מסוים זה כבר בערך, ‫אני משלם את אותו מחיר.
[01:44:47 - 01:44:49] ‫כן, משתמשים לזה בשביל brain-edic,
[01:44:50 - 01:44:51] ‫מודדים עם
[01:44:52 - 01:44:53] ‫מסתובבות פרידות, ארכאיב.
[01:44:55 - 01:44:56] ‫אוקיי, זה היה שתי דוגמאות.
[01:45:00 - 01:45:05] ‫עכשיו אנחנו נראה איך אפשר ‫ללמוד את המודל הזה. ‫אפשר לעוד על המודל עצמו, ‫לפני שאנחנו...
[01:45:16 - 01:45:34] אוקיי, אתם מבינים למה ‫אנחנו מדברים על G.M.M.M. פתאום?
[01:45:36 - 01:45:39] ‫היו לי פה כמה הערות ‫שחסר קצת תמונה גדולה ‫של אה אנחנו הולכים עם זה.
[01:45:40 - 01:45:44] כן, אנחנו אומרים שאנחנו רוצים ‫לבנות כל מיני תיאוריות ‫של מתי אנחנו יכולים לעבוד עם מודלים,
[01:45:44 - 01:45:50] ‫איזה סוג של מודלים אנחנו יכולים ‫לעבוד איתם. ‫אז גרסיאן, אם הכול היה גרסיאן, ‫היו יכולים לפתור הכול בצורה אנליפית.
[01:45:51 - 01:45:57] ‫אבל אנחנו רוצים משהו שהוא יותר ‫מורכב מגרסיאן, ‫שעדיין אנחנו יכולים לפתור בצורה טובה,
[01:45:57 - 01:45:58] ‫גם לא מדויקת,
[01:45:59 - 01:46:02] ‫ואחד מהדוגמאות זה ‫תערובת גרסיאן.
[01:46:03 - 01:46:07] ‫זה בעצם דוגמה למודל עם משתנה חבוי,
[01:46:08 - 01:46:11] ‫שהמשתנה החבוי זה איזה גרסיאן בעצם, ‫מאיזה גרסיאן הגיע הדאטה.
[01:46:12 - 01:46:12] בהינתן זה,
[01:46:13 - 01:46:15] ‫אני יודע שהמודל שלי הוא גאוסיאן,
[01:46:16 - 01:46:19] ‫אבל עבור כל נקודה ‫אני לא יודע מאיזה גאוסיאן הגיע, כשאני אומר.
[01:46:20 - 01:46:23] ‫כשאני אומר שאני מניח ‫שהדאטה מגיעה מתערובת גאוסיאנים,
[01:46:24 - 01:46:27] ‫זאת אומרת שאני לא יודע ‫כל נקודה מאיזה גאוסיאן הגיע,
[01:46:28 - 01:46:31] ‫אבל אני יכול להניח ‫שיש נגיד חמישה גאוסיאנים ‫שייצרו את כל הדאטה שלי,
[01:46:32 - 01:46:36] ‫כל מה שאני צריך לדעת זה ‫מה הפרמטרים של חמשת הגאוסיאנים.
[01:46:36 - 01:46:40] אז זה מה שאנחנו ננסה לעשות עכשיו, בהינתן שיש לנו דאטה, איך אני מוצא
[01:46:41 - 01:46:43] את החמשת הגאוסיאנים האלה.
[01:46:44 - 01:46:49] אם זה היה לגאוסיאן אחד זה לא היה בעיה, הייתי מחשב את הממוצע של כל הנקודות, זה היה נותן לי את התוחלת של הגאוסיאן,
[01:46:52 - 01:46:56] הממוצע של המרחק הריבועי של כל הנקודות מתוך הממוצע,
[01:46:57 - 01:46:58] וזה היה נותן לי את הווריאנס של הגאוסיאן הזה.
[01:46:59 - 01:47:05] אז זה לא בעיה למצוא את הפרמטרים של גאוסיאן אחד, ואם אני מניח שיש עכשיו כמה גאוסיאנים שמייצרים את הדאטה,
[01:47:05 - 01:47:09] איך אני מוצא את הפרמטרים של הבארסייניים
[01:47:11 - 01:47:12] זה מה שאני עושה עכשיו
[01:47:12 - 01:47:37] ‫אוקיי, אנחנו רוצים למצוא את המקסימום-לייקליות
[01:47:43 - 01:47:45] אוקיי, אז יש לנו דאטה,
[01:47:46 - 01:47:49] ‫דאטה שלנו זה פשוט הרבה איקסים.
[01:47:53 - 01:47:55] כן, יש כל אחד מהם הוא במימד D,
[01:47:56 - 01:47:57] ויש לנו N כאלה.
[01:47:58 - 01:48:00] I שווה XI, I שווה FRAD עד N.
[01:48:09 - 01:48:11] ומה שאנחנו רוצים זה למצוא את הפרמטרים,
[01:48:12 - 01:48:12] מה זה הפרמטרים?
[01:48:17 - 01:48:18] מה הפרמטרים של GMM?
[01:48:22 - 01:48:24] כן ספציפית אבל מה זה?
[01:48:25 - 01:48:25] FI, FI,
[01:48:27 - 01:48:31] כן זה כל הגרסיאנים, זאת אומרת התוכלות של הגרסיאנים והקובעת של הגרסיאנים,
[01:48:32 - 01:48:34] והמקדמים של האירבור שלהם,
[01:48:34 - 01:48:35] ה-PI,
[01:48:36 - 01:48:37] או זה מיקסינג וייטס,
[01:48:38 - 01:48:39] ארגמאקס של פטר,
[01:48:40 - 01:48:41] של ה-likelyer,
[01:48:41 - 01:48:42] מה ההסתברות, לראות
[01:48:43 - 01:48:44] את הדאטה שאני רואה
[01:48:45 - 01:48:46] בהינתן הפרמטרים.
[01:48:47 - 01:48:48] אוקיי, נחפש את הפרמטרים שעבורם.
[01:48:50 - 01:48:52] אוקיי, לפעמים כותבים את זה ככה,
[01:48:53 - 01:48:53] ולפעמים
[01:48:55 - 01:48:56] תקטיב שהוא קצת יותר ב-E's-I,
[01:48:57 - 01:48:58] אולי זה לא לגמרי נכון,
[01:48:59 - 01:49:00] זה תטא בתור
[01:49:01 - 01:49:02] פרמטרים של ההתקדמות.
[01:49:09 - 01:49:10] כן, נכון? זה מוציאום לייקליל.
[01:49:11 - 01:49:13] אני מחפש את הפרמטרים שאני מסיים את זה, אז
[01:49:17 - 01:49:18] למה זה שווה?
[01:49:19 - 01:49:20] אז פי של v
[01:49:23 - 01:49:23] שווה
[01:49:27 - 01:49:32] ל-v זה מורכב מהרבה איקסים, אני מניח שכל x הוא בלתי תלוי איכות בשני, נכון? אז יש לי מכפלה
[01:49:33 - 01:49:39] i של 1 עד n של כל אחד מהאיקסים, זה טעות של כל אחד מהאיקסים,
[01:49:39 - 01:49:45] שזה כל אחד מהם צריך לשבת את ההתפלגות של gm,
[01:49:46 - 01:49:50] זה סכום i שווה 1 עד k גדול,
[01:49:52 - 01:49:53] של i עד k
[01:49:58 - 01:49:59] n x i
[01:50:01 - 01:50:01] נכון,
[01:50:03 - 01:50:03] נכון?
[01:50:05 - 01:50:09] אתם מבינים מה זה הכתיב הזה? שאלו אותי פה קודם כל כך שאמרתי את זה פעם אחת,
[01:50:09 - 01:50:10] זה פשוט קיצור,
[01:50:11 - 01:50:13] כשאני אקח לכתוב את כל ה...
[01:50:14 - 01:50:17] f, g, שורש, n5, e ו-x,
[01:50:18 - 01:50:19] n,
[01:50:20 - 01:50:20] כזה,
[01:50:21 - 01:50:22] אז זה קיצור של הדבר הזה,
[01:50:23 - 01:50:27] בסדר? זה באוסיאן עם איזושהי תוחלת ואיזשהו סיגמא,
[01:50:27 - 01:50:29] שאני כותב אותו עבור משתנה x,
[01:50:30 - 01:50:31] מה משמעות לזה?
[01:50:31 - 01:50:32] יש כאן נקודה פסיק,
[01:50:33 - 01:50:33] אוקיי?
[01:50:33 - 01:50:48] אוקיי? אבל אנחנו בדרך כלל לא רוצים לעשות מקסימון למכפלה, נכון? זה מסבט אותנו בגזירה,
[01:50:48 - 01:50:50] אז אנחנו נצטרך לעשות לוג,
[01:50:52 - 01:50:52] מקסימון ללוג של
[01:50:54 - 01:50:56] gm, גם פה אנחנו נשים את הפתר הזה,
[01:51:00 - 01:51:02] אז זה הופך לסכום,
[01:51:03 - 01:51:05] i שווה 1 עד n
[01:51:06 - 01:51:07] של לוג
[01:51:08 - 01:51:09] כאילו הדבר הזה,
[01:51:10 - 01:51:10] ספורם
[01:51:11 - 01:51:13] k שווה 1 עד k גדול,
[01:51:14 - 01:51:16] yk, n,
[01:51:17 - 01:51:18] x, i,
[01:51:19 - 01:51:20] פסיק,
[01:51:21 - 01:51:22] m, k,
[01:51:23 - 01:51:23] פיגמא.
[01:51:26 - 01:51:31] אוקיי, מה עושים בדרך כלל? שאתם מקסימון הולך להיות? יש לנו את הלוג הסתברות, איך אני מוצא את המט?
[01:51:32 - 01:51:37] עוזרים, נכון? אז אנחנו צריכים למצוא איזשהו ביטוי אנליטי בניו וסי.
[01:51:37 - 01:51:38] אוקיי, אז בוא נראה את הנגזרת.
[01:51:40 - 01:51:40] תעשוי
[01:51:43 - 01:51:46] לא בקביל חף ל-a
[01:51:48 - 01:51:48] נגחתה
[01:51:50 - 01:51:54] ויכול להכניס את הנגזרת פה לתוך הסכום והסכם פה בחוץ,
[01:51:55 - 01:51:55] זה טוב
[01:51:56 - 01:52:02] ‫אז כאן אני מסתבך כי יש לי לוג של סכום.
[01:52:04 - 01:52:05] אוקיי, אז אני צריך לעשות
[01:52:06 - 01:52:07] קודם כל נגזרת של הלוג,
[01:52:08 - 01:52:09] ‫ואז נגזרת פנימית,
[01:52:11 - 01:52:14] ‫אז נגזרת של הלוג זה 1 חלקי
[01:52:15 - 01:52:16] מה שכתוב בלוג, נכון?
[01:52:17 - 01:52:18] ‫כפול הנגזרת הפנימית.
[01:52:19 - 01:52:21] קודם כל 1 חלקי מה שכתוב בלוג,
[01:52:21 - 01:52:22] 1 חלקי סכום.
[01:52:26 - 01:52:30] ‫אבל בוא נעשה את ה... לפי משהו ספציפי, ‫נגיד לפי
[01:52:30 - 01:52:31] new
[01:52:33 - 01:52:36] איזשהו new k, כן? ‫אני מחפש עכשיו את ה-new של הגאוסיאן השלישי.
[01:52:38 - 01:52:44] בסדר? וכמו שכשהם רצינו למצוא בו גאוסיאן אחד, ‫אז גאוסיאן הולכים new, ואז גאוסיאן הולכים סיגמה, ‫אז גם פה אני יכול לגזור
[01:52:45 - 01:52:46] לפי אחד מה-newים.
[01:52:49 - 01:52:52] אז כאן יש לי k,
[01:52:53 - 01:52:54] זה לא טוב k מפה,
[01:52:54 - 01:52:57] ‫כאן k-tag, שווה f חלק k גדול,
[01:52:58 - 01:52:59] ‫בייד כ-tag,
[01:53:01 - 01:53:02] ‫הגאוסיון הזה,
[01:53:03 - 01:53:04] ‫הוצאה
[01:53:06 - 01:53:07] k-tag,
[01:53:08 - 01:53:09] ‫דיגמה k-tag.
[01:53:11 - 01:53:15] זה רק היה הנגזרת החיצונית של הלוג, ‫עכשיו אנחנו נקבל הנגזרת הפנימית של הלוג,
[01:53:16 - 01:53:20] ‫זה נגזרת פנימית של הסכום הזה, ‫אז עכשיו אני יכול להיכנס לתוך הסכום,
[01:53:21 - 01:53:24] ‫אז כל העברים שהם לא k מפריעים.
[01:53:24 - 01:53:27] ‫הם לא פריעים ב-new k, נכון? ‫אז נשאר לי רק האיבר ה-k,
[01:53:29 - 01:53:31] ‫ושם יש לי את הנגזרת הפנימית ‫שהיא תהיה שווה ל...
[01:53:33 - 01:53:34] ‫תודה רבה, פחות לא,
[01:53:35 - 01:53:37] ‫שזה יהיה שווה ל-i, k,
[01:53:40 - 01:53:42] ‫בפול הנגזרת הפנימית של גאוסיאן.
[01:53:43 - 01:53:46] ‫אז מה הנגזרת הפנימית של גאוסיאן לפי uk?
[01:53:51 - 01:53:53] ‫הדבר של הלוג של הגאוסיאן זה של גאוסיאן,
[01:53:53 - 01:53:55] ‫אז איך נראה גאוסיאן?
[01:53:56 - 01:53:58] ‫פה, התאמצלתי לכתוב,
[01:53:59 - 01:54:01] ‫אני אפשר לשני הצבעה אישית.
[01:54:04 - 01:54:05] ‫פה,
[01:54:06 - 01:54:07] סיגנר ל...
[01:54:08 - 01:54:09] ‫שם אחד,
[01:54:10 - 01:54:10] ‫כי,
[01:54:10 - 01:54:14] ‫את הדבר הזה אני צריך לגזור,
[01:54:16 - 01:54:20] ‫לפי, עכשיו אני עושה, ‫לפי ה-new של ה-f, לפי uk,
[01:54:21 - 01:54:22] ‫זה מופיע לי,
[01:54:23 - 01:54:24] ‫מופיע לי כל האקספוננט הזה, שוב,
[01:54:26 - 01:54:27] ‫ואז הנגזרת הפנימית,
[01:54:29 - 01:54:30] ‫מה שבתוך האקספוננט.
[01:54:33 - 01:54:37] ‫אז כל זה מופיע לי, ‫אז בעצם אני קוצץ לכל התרפאות ה-n של xi,
[01:54:38 - 01:54:44] ‫נקודה פסיק מיוע סטיגמא k,
[01:54:48 - 01:54:50] ‫פול הנגזרת הפנימית לפי מיוע כ,
[01:54:51 - 01:54:52] ‫מה שבתוך האקספוננט,
[01:54:52 - 01:54:57] ‫שזה כבר אנחנו מכירים, ‫זו הנגזרת של ה-likelihood, ‫של ה-nobby's business הזה,
[01:54:58 - 01:55:04] ‫פתאום בתרגיל, זה יוצא פשוט x פחות מיוע כ,
[01:55:06 - 01:55:08] ‫פול סיגמא,
[01:55:08 - 01:55:13] ‫אז נכנס לפעם. נראה לי שפעם שעברה ‫שאת מדי הלוח, ‫שכחתי את הסיגמא בבישור המנגרי,
[01:55:14 - 01:55:15] ‫ואז נסכים לצד פה סיגמא.
[01:55:18 - 01:55:19] ‫כן, ברור למה זה היה נגזרת?
[01:55:21 - 01:55:22] זה היה קצת ממוקב קצת.
[01:55:22 - 01:55:26] ‫נכון, כשקדמנו לוג של גרסיאן, ‫הוא היה נורא קל, ‫זה היה רק הדבר הזה יצא ועכשיו.
[01:55:27 - 01:55:29] ‫עכשיו, בגלל שזו תערוג של גרסיאן,
[01:55:29 - 01:55:30] ‫הוספנו לנו כל הדבר הזה פה.
[01:55:35 - 01:55:37] ‫אוקיי, שימו לב מה כתוב כאן, ‫יש כאן את ה...
[01:55:38 - 01:55:41] תזכרו את הביטוי הזה, ‫כן, אנחנו תכף, כן, שוב.
[01:55:42 - 01:55:43] ‫יש כאן סכום,
[01:55:45 - 01:55:51] לא הסכום הזה, מה שכתוב, ‫השבר הזה, יש כאן עבור k מסוים, ‫יש כאן פאי k כפול הגרסיאן ה-k,
[01:55:51 - 01:55:54] ‫לפי הסכום של כל ה-fai kים ‫בארסיאנים כאילו.
[01:55:57 - 01:56:00] ‫תכף אנחנו נראה מה המשמעות של זה,
[01:56:01 - 01:56:06] ‫אבל לצורך מה שאנחנו עכשיו רוצים, ‫עכשיו רצינו לגזור, ‫ואז לחלץ את מול k ולראות למה הוא שווה.
[01:56:07 - 01:56:09] ‫אבל פה, בביטוי הזה,
[01:56:09 - 01:56:12] ‫כמו שפעם, כל אחד מהביטוי האלה ‫מסתתר כזה אקספוננט כזה,
[01:56:13 - 01:56:13] ‫בארסיאן,
[01:56:14 - 01:56:17] ‫זה נהיה קצת מורכב, ‫אנחנו לא יכולים לעשות את זה בשיטור העמדני.
[01:56:19 - 01:56:19] כן?
[01:56:19 - 01:56:22] ‫ה-x מינוס k וכולי זה לא מוריד ‫בתוך ה-fai למעלה?
[01:56:24 - 01:56:24] ‫בתוך ברדיאפל?
[01:56:25 - 01:56:26] ‫בברדיאפל הזה, כן.
[01:56:27 - 01:56:30] ‫כן לא משנה, זה כאילו בטוח. ‫קצאתי את זה מחוץ מהעדשי הדרדיאפל.
[01:56:34 - 01:56:41] ‫אז אנחנו לא יכולים לסתור את זה אנליטית. ‫בניגוד למקסימום לייקליות על לאוסיאן, ‫שפשוט מצאנו מה ה-mue האופטימלי,
[01:56:41 - 01:56:44] ‫כאן אנחנו לא יכולים. ‫אז אופציה אחת היא לעשות gradient descent.
[01:56:45 - 01:56:47] זה ה-gardian, בסדר, ‫אז אנחנו מוסיפים את זה
[01:56:48 - 01:56:51] ל-mue. ‫מתחילים איזשהו מue, מוסיפים את זה, ‫וכל פעם חושבים ש...
[01:56:53 - 01:56:55] ‫זו אופציה אחת, אבל במקרה הזה,
[01:56:56 - 01:56:58] ‫בכלל במקרה של משתנים חבועים,
[01:56:58 - 01:57:01] ‫יש שיטה יותר יעילה, נדבר עליה עכשיו,
[01:57:01 - 01:57:02] ‫שנקראת
[01:57:03 - 01:57:09] אקספייטיישן אקספייטיישן, ובקיצור E.M. אוקיי, אז כולם ברור למה אנחנו קטעים E.M?
[01:57:10 - 01:57:12] ‫אתם רוצים למצוא את הפרמטרים ‫של התערובת והאוסייאנים שלנו,
[01:57:13 - 01:57:15] ‫ואנחנו לא יכולים לעשות את זה בצורה אנליטית,
[01:57:16 - 01:57:20] ‫אז אנחנו לא רוצים לעשות דדי וסם, ‫זה יוצא פשוטי מדי,
[01:57:21 - 01:57:23] ‫אנחנו צריכים להשתמש בשיטה אחרת, יותר יעילה.
[01:57:24 - 01:57:33] ‫הרדתי פה, דרך אגב, רק עבור מיוק K, ‫אבל עבור Sigma K זה אותו דבר, ‫רק שפה יהיה את החישוב של ה... ‫בגלל ש Sigma K זה כל הדבר הזה אפשר אותו דבר.
[01:57:34 - 01:57:37] ‫עבור Pi זה יהיה קצת שונה, ‫אבל Pi כן אפשר למצוא אולי אלף.
[01:57:39 - 01:57:42] ‫אם אנחנו יודעים מי הוא Sigma, ‫אפשר למצוא את Pi בצורה אנוליגית.
[01:57:44 - 01:57:45] אוקיי,
[01:57:45 - 01:57:52] ‫אז בוא נכתוב עכשיו מה הבדלה של EL.
[01:58:15 - 01:58:26] ‫אוקיי, אז מה הרעיון של השיטה הזאת?
[01:58:34 - 01:58:36] ‫נתתי לך איזה שלישי, כאשר
[01:58:39 - 01:58:39] כשיש לנו
[01:58:45 - 01:58:53] E תתא פי תתא של X הוא קשה לאופטימיגציה,
[01:58:59 - 01:59:04] ‫אבל E תתא ביותר איקס וז
[01:59:07 - 01:59:08] ‫הוא קל לאופטימיגציה.
[01:59:08 - 01:59:22] ‫אם יש לנו התפלגות שאנחנו יכולים להגדיר אותה ‫בתור התפלגות עם איזשהו משתנך חבוי,
[01:59:23 - 01:59:26] ‫בדיוק כמו שהיה המצב עכשיו ‫בטהרות של גאוסיאנים,
[01:59:27 - 01:59:31] ‫כלומר שאנחנו יכולים להגדיר ‫את האור של גאוסיאנים בתור התפלגות
[01:59:31 - 01:59:34] ‫שיש לאיזשהו משתנך חבוי, זה אוקיי, ‫גם שאני איך נקרא לו,
[01:59:35 - 01:59:39] ואנחנו יכולים להגדיר אותה בתור התפלגות
[01:59:40 - 01:59:45] שולית של התפלגות משותפת על איזשהו משטרה שאנחנו רואים ומשטרה שאנחנו לא רואים
[01:59:47 - 01:59:49] אם יש לנו כזה דרך לתאר את ההתפלגות שלנו
[01:59:50 - 01:59:53] וקל לנו לעשות את האופטימיזציה על ההתפלגות המשותפת
[01:59:53 - 01:59:56] אבל קשה לעשות את האופטימיזציה על ההתפלגות השולית
[01:59:57 - 01:59:59] אז האלגוריתם הזה
[01:59:59 - 02:00:02] כאילו אינפקטיבי
[02:00:04 - 02:00:05] אוקיי? תכף נראה מה אלגורית בין הדברים
[02:00:07 - 02:00:10] ברור למה זה המצב בתערובת גאוסיאנים?
[02:00:11 - 02:00:19] נניח שלא, כאילו, טוב בדיוק אמרנו את זה, אבל זה קודם כל, זה ברור שזה קשה, למה שניסינו לעשות כאן, כן? לנסות למצוא את גאוסיאנציה, לא הצלחנו
[02:00:20 - 02:00:21] למצוא את ה...
[02:00:21 - 02:00:25] למה הדבר הזה הוא קל בתערובת גאוסיאנים?
[02:00:26 - 02:00:28] כי אם אני אומר לכם מה ה-Z,
[02:00:28 - 02:00:30] עבור Z מסוים ו-X מסוים, זה אומר
[02:00:31 - 02:00:33] פשוט למצוא את הפרמטרים של הגאוסיאנים,
[02:00:34 - 02:00:34] גאוסיאנים מסוים,
[02:00:36 - 02:00:39] כן, שעבור X, אוקיי? נגיד שאני אומר לכם, יש לי עכשיו
[02:00:40 - 02:00:41] אלף איקסים,
[02:00:42 - 02:00:45] יש לי עשרים שהגיעו מגאוסיאן הראשון, ועוד חמישים שהגיעו מהגאוסיאן השני,
[02:00:46 - 02:00:49] וכמה נשארו מגאוסיאן שלישי,
[02:00:50 - 02:00:54] אז פשוט אפשר למצוא כל פרמטרים של כל אחד מהגאוסיאנים האלה בצורה קלת,
[02:00:55 - 02:00:57] ודבר הזה הוא קל לאופטימיזציה.
[02:00:58 - 02:01:00] אז מה האלגוריתם ב-EIM אומר?
[02:01:01 - 02:01:04] הוא בעצם מחולק לשני חלקים, יש משהו שנקרא E-סטה,
[02:01:05 - 02:01:06] הצד של ה-expeakation,
[02:01:10 - 02:01:14] והוא אומר, הוא בעצם אנחנו מגדירים איזושהי פונקציה חדשה שקוראים לה Q-Teta
[02:01:15 - 02:01:18] בהינתן תתא T,
[02:01:18 - 02:01:24] זה אלגוריתם אינטרטיבי,
[02:01:26 - 02:01:28] כל צעד אנחנו עושים קודם E,
[02:01:29 - 02:01:30] שזה להגדיר את הפונקציה הזאתי,
[02:01:31 - 02:01:32] והיא מוגדרת ככה,
[02:01:34 - 02:01:35] תוחלת
[02:01:36 - 02:01:37] לפי ההתפלגות
[02:01:37 - 02:01:39] של Z בהינתן,
[02:01:40 - 02:01:42] הדאטה שאנחנו רואים
[02:01:49 - 02:01:50] והפטע הקודם שהיה לנו.
[02:01:50 - 02:01:52] אנחנו עכשיו בשלב ה-T,
[02:01:52 - 02:01:54] אבל פטע ה-T שחישבנו.
[02:01:59 - 02:02:04] זו ההתפלגות שלפיה אני עושה את התוחלת, והפונקציה שאני עושה עליה תוחלת זה לוג
[02:02:06 - 02:02:09] של P-Teta, ההתפלגות המשותפת הזאת.
[02:02:17 - 02:02:20] זה נקרא E-Sטה,
[02:02:21 - 02:02:22] תוחלת כזאת.
[02:02:23 - 02:02:27] זה לא מספר, כי זה משהו שיש בו משהו לא ידוע.
[02:02:28 - 02:02:29] זה תטע שאנחנו לא יודעים אותו,
[02:02:31 - 02:02:32] אבל זה פונקציה של תטא.
[02:02:34 - 02:02:35] אנחנו מוגדירים פונקציה חדשה על תטא,
[02:02:36 - 02:02:40] להינתן את התטא שקיבלנו באיתרציה הקודמת,
[02:02:41 - 02:02:42] מוגדירים פונקציה חדשה על תטא,
[02:02:43 - 02:02:44] וה-N-SET
[02:02:45 - 02:02:47] זה פשוט למצוא את המקסימום
[02:02:48 - 02:02:48] של הפונקציה הזאת.
[02:02:49 - 02:02:52] כלומר, התטא החדש שלנו, תטא t פלוס 1,
[02:02:53 - 02:02:54] הוא r
[02:02:54 - 02:02:56] Maps תטא
[02:02:58 - 02:03:00] של הפונקציה הזאת, של q,
[02:03:01 - 02:03:01] תטא,
[02:03:02 - 02:03:04] של פלוס תטא פלוס תטא.
[02:03:06 - 02:03:07] יש כאן
[02:03:08 - 02:03:14] תטות שהן עם פריפיקס כזה, ויש p או p עם יו עוד אחד, ויש כאלה שהן בלי.
[02:03:15 - 02:03:17] אוקיי, מה הכוונה?
[02:03:17 - 02:03:19] יש לנו תהליך אינטרטיבי,
[02:03:19 - 02:03:21] אנחנו שומרים כל פעם את התטות שאנחנו חושבים
[02:03:25 - 02:03:27] הניחוש הנוכחי שלנו
[02:03:28 - 02:03:29] לפרמטרים של המודל,
[02:03:30 - 02:03:31] וזה קוראים תטא t
[02:03:32 - 02:03:36] כמובן בכל צד אנחנו רוצים לשפר, אנחנו רוצים למצוא תטא t עוד אחד, שיהיה יותר טוב.
[02:03:37 - 02:03:39] אנחנו מגדירים פונקציה, שהיא פונקציה
[02:03:40 - 02:03:40] של תטא,
[02:03:41 - 02:03:42] תטא זה עכשיו
[02:03:42 - 02:03:45] בלי פריפיקס, זה משהו שאנחנו לא יודעים עדיין מה הערך שלו,
[02:03:46 - 02:03:47] ואנחנו מוצאים את המקסימום של הפונקציה הזאת.
[02:03:51 - 02:03:54] תזכרו מתי זה טוב,
[02:03:54 - 02:03:56] מתי שנמצא את המקסימום של הדבר הזה זה קל,
[02:03:58 - 02:04:00] ולכן גם המקסימום של התוחלת של הדבר הזה זה יהיה קל.
[02:04:05 - 02:04:08] תכף אנחנו נראה את המשמעות של זה ב-GMM, זה אולי טיפה יותר ברור, אבל
[02:04:09 - 02:04:10] האלגוריתם באופן כללי
[02:04:13 - 02:04:14] נשמע לכם הגיוני?
[02:04:16 - 02:04:17] נראה ככה מוזר.
[02:04:24 - 02:04:36] אני מסתובבת אם קודם
[02:04:41 - 02:04:45] להוכיח שהאלגוריתם הזה עובד, או קודם להגיד מה המשמעות של ה-GMM?
[02:04:54 - 02:05:16] כן, אנחנו לא נספיק כנראה להוכחה נעשה כבר לשיעור הבא.
[02:05:18 - 02:05:22] קודם כל אינטואיטיבית מה האלגוריתם הזה עושה, אוקיי? אז שוב, אנחנו, יש לנו data x,
[02:05:23 - 02:05:28] נכון? אנחנו הנחנו שהמודל של x הוא איזשהו מודל שתלוי גם ב-z.
[02:05:30 - 02:05:32] בנינו איזשהו מודל משותף ל-x ו-z.
[02:05:33 - 02:05:34] אבל z אנחנו לא רואים.
[02:05:35 - 02:05:41] אז מה האלגוריתם הזה בעצם אומר שכדאי לנו לעשות? כדאי לנו, אפשר עכשיו לצד הזה בתור אנחנו משלימים את z,
[02:05:43 - 02:05:46] רוצים איזושהי תוחלת על פני ה-zים האפשריים,
[02:05:47 - 02:05:49] ואז נגיד שמצאנו את ה-z,
[02:05:50 - 02:05:51] אז אנחנו עושים את האופטימילציה.
[02:05:53 - 02:05:54] למשל, אז על x ו-z.
[02:05:55 - 02:05:57] אנחנו עושים אופטימיזציה בעצם על הפונקציה הזאת,
[02:05:58 - 02:06:00] כל פעם משלימים את z לפי ה...
[02:06:00 - 02:06:02] ה...ניחוש הנוכחי שיש מולנו על t.
[02:06:03 - 02:06:06] אז אנחנו מתחילים מהתפלגות איזושהי התפלגות לא ידועה,
[02:06:07 - 02:06:10] סליחה, מאיזושהי תטא שהיא לא...
[02:06:11 - 02:06:16] אנחנו לא יודעים מה תטא צריך להיות, אנחנו מתחילים עם משהו שלרותי, ולכן ההתפלגות הזאת לא תהיה טובה.
[02:06:18 - 02:06:22] אז עדיין אנחנו בצורה אינטרטיבית לוקחים את ה-x לפי ההתפלגות הזאת שיש לנו על תטא,
[02:06:23 - 02:06:27] כן, הניחוש שיש לנו על תטא, אנחנו משלימים את הדאטה, זאת אומרת אנחנו משלימים את התוחלת
[02:06:28 - 02:06:32] על פני ה-zים האפשריים שיכולים להיות עבור כל x,
[02:06:34 - 02:06:35] ואז מחשבים את
[02:06:40 - 02:06:42] הפרמטרים הכי טובים, שימו אופטימיזציה,
[02:06:43 - 02:06:45] כשקורה כמו x ואיזשהו ניחוש כזה,
[02:06:45 - 02:06:48] ואז בצורה אינטרטיבית אומרים אוקיי, עכשיו יש לנו את הפרמטרים החדשים,
[02:06:48 - 02:06:51] אז נשלים בצורה יותר חכמה את ה-z שיש לנו.
[02:06:53 - 02:06:56] אנחנו משלימים בצורה יותר חכמה, ואז עוד פעם מוצאים את ה-teta שמותאים הכי טוב
[02:06:57 - 02:06:58] להשלמה החדשה שעשינו,
[02:07:00 - 02:07:04] אז תחשבו על זה בצורה כזאת, אנחנו משלימים את הדאטה שאנחנו לא רואים ואז מוצאים את הפרמטרים,
[02:07:05 - 02:07:18] ואז מוצאימים את הדאטה לפי הפרמטרים שמצאנו ואותו פעם משכרים את הפרמטרים שמצאנו, רק שההשלמה הזאת היא לא בדיוק שאנחנו בוחרים ערך אחד עם z אלא אנחנו מחשבים תוחלת על פני כל ה-zים האפשריים
[02:07:19 - 02:07:21] אוקיי, זה גם מה שכתוב כאן.
[02:07:21 - 02:07:23] בואו נראה איך זה מתרגם ל-GMM.
[02:07:26 - 02:07:30] אוקיי, אז נעשה רגע את ה-e-step הזה ל-GMM שזה ה...
[02:07:38 - 02:07:40] אני חושב שאולי גם את זה אני לא אספיק.
[02:07:48 - 02:08:02] בואו ננסה.
[02:08:03 - 02:08:03] אוקיי.
[02:08:10 - 02:08:11] אפשר לרגע, אבל אתה נמחק אז
[02:08:12 - 02:08:15] EM עבור GMM
[02:08:18 - 02:08:21] אז אני רוצה לחשב את ה-e-tech, אוקיי? אני רוצה לחשב את
[02:08:23 - 02:08:24] התוכלת הזאת
[02:08:25 - 02:08:26] שמה שכתוב כאן זה ה-GMM.
[02:08:35 - 02:08:37] קודם כל יש לי סכום על
[02:08:45 - 02:08:47] התוכלת הזאת על זה
[02:08:49 - 02:08:52] Z לפי V ו-Teta הקודם, T to P
[02:08:54 - 02:08:54] של
[02:09:00 - 02:09:03] הסכום על כל העין שאני רואה, כל הדאטה שיש לי
[02:09:05 - 02:09:07] 2 שווה 1 עד n
[02:09:09 - 02:09:10] של לוג
[02:09:13 - 02:09:14] לוג של P-Teta של ה...
[02:09:19 - 02:09:27] P-Teta של X ו-Z, אוקיי? אז מה ה-P-Teta של X ו-Z עבור תהרוגת גרסיינים?
[02:09:28 - 02:09:31] ההסתברות שאני רואה את Z
[02:09:33 - 02:09:38] שהגרסיין שלי הוא הגרסיין ה-Z ושהדאטה שלי הגיעה מהגרסיין הזה
[02:09:39 - 02:09:43] וזה עבור כל אחד מה-Z'ים הקשרים בתוכלת על פני כל ה-Z'ים
[02:09:44 - 02:09:46] ההסתברות שאני בגרסיין ה-Z
[02:09:49 - 02:09:49] בשלבים?
[02:09:51 - 02:09:52] פי של Z
[02:09:53 - 02:09:54] כפול פי של
[02:09:56 - 02:09:59] X ו-Z זה תמיד אפשר לכתוב
[02:10:00 - 02:10:07] במקום פי של X פסיק Z אפשר לכתוב את זה כאן, אוקיי? עכשיו כמה דברים שאפשר לעשות פה? אפשר להחליף בסדר של הדברים כאן
[02:10:09 - 02:10:10] ואפשר לכתוב בפירוש מה זה הדברים האלה,
[02:10:12 - 02:10:12] אוקיי? אז
[02:10:14 - 02:10:15] נחליף קודם את הסדר
[02:10:15 - 02:10:17] אז יש לי I שווה 1 עד N
[02:10:19 - 02:10:22] בו XI מלאות
[02:10:24 - 02:10:33] של התוכלת של Z, עכשיו התוכלת של Z היא נותנת רק XI מסוים
[02:10:34 - 02:10:37] כי הסכום על הנקודות הוא מחוץ לתוכלת
[02:10:38 - 02:10:39] T' ל-T'
[02:10:40 - 02:10:41] של לוג
[02:10:44 - 02:10:44] פי של Z
[02:10:46 - 02:10:48] ואיך נראה פי של XI
[02:10:49 - 02:10:53] נכון, נכון, בשלב הבא, P של XI ו-XI נותן את זה, כן
[02:10:54 - 02:10:55] עכשיו בשלב הבא
[02:11:01 - 02:11:03] איך אני יכול לחשב את התוכלת הזאת?
[02:11:03 - 02:11:10] איך אפשר לחשב את התוכלת? אפשר לחשב פשוט לעבור על כל העברים, כל הערכים האפשריים של Z כפול ההסתברות שלהם
[02:11:10 - 02:11:18] אז יש לי כאן סכום I שווה חי עד N
[02:11:20 - 02:11:22] סכום על כל הערכים האפשריים של Z
[02:11:23 - 02:11:27] קראנו לזה K בפזקי שווה 1 עד K גדול
[02:11:28 - 02:11:30] זה המשתנה החבוי שלנו שאומר נזק גאוסייני לנו
[02:11:31 - 02:11:35] ההסתברות לראות את הדבר הזה, זה פי של K
[02:11:36 - 02:11:37] כפול
[02:11:38 - 02:11:40] מה שכתוב כאן
[02:11:41 - 02:11:58] סליחה, דבר על ההתפלגות הזאת?
[02:11:59 - 02:12:03] זה הפריורייה להתפלגות הזאת ל-Z בהינתן XI פסיק תטא T
[02:12:05 - 02:12:06] אז
[02:12:08 - 02:12:09] ההסתברות של Z
[02:12:09 - 02:12:11] Z שווה K
[02:12:14 - 02:12:15] ויינתן XI
[02:12:16 - 02:12:17] וטט עבדי
[02:12:19 - 02:12:19] כפול
[02:12:21 - 02:12:23] לוב של ההתפלגות המשותפת
[02:12:24 - 02:12:40] ‫אז כניסתי לעשות את זה, ‫אז נוותר רגע על ההפרדה שעשיתי כאן.
[02:12:42 - 02:12:42] ‫אני חוזר רגע
[02:12:45 - 02:12:46] ‫לכותבת בצורה משותפת.
[02:12:46 - 02:12:57] ‫נחזור על זה לשיעור הבא, ‫אבל אני רוצה לסיים משהו קצר, ‫כדי שיהיה לכם על מה לחשוב השבוע.
[02:13:17 - 02:13:18] ‫אוקיי, נחוזר רגע על שלושת השלבים האלה.
[02:13:20 - 02:13:24] ‫זה ככה הקדמנו את הנוסחה שאנחנו, ‫האיסטר, כן? ‫אנחנו חושבים תוחלת
[02:13:26 - 02:13:30] על ה-likelydead על כל הדאטה,
[02:13:30 - 02:13:33] ‫לא רק של x, אלא של x ו-z.
[02:13:34 - 02:13:37] ‫התוחלת היא של z בהינתן כל הדאטה שראיתי, ‫והתטות
[02:13:38 - 02:13:39] מהאינטרציה הקודמת.
[02:13:40 - 02:13:41] ‫זו ההגדרה של האיסטר.
[02:13:42 - 02:13:44] ‫עכשיו אפשר להפוך פה את הסדר ‫בין הסכום לתוחלת.
[02:13:47 - 02:13:50] ‫וכאן אני פשוט מחשב את התוחלת ‫בצורה מפורשת.
[02:13:51 - 02:13:53] ‫אני סוכם את כל האיברים האפשריים של z,
[02:13:54 - 02:13:56] ‫כפול ההסתברות הזאת שכתובה כאן,
[02:13:58 - 02:14:00] ‫כפול הפונקציה שאני רוצה לעסוק לה,
[02:14:01 - 02:14:07] ‫לחשב את התוחלת שלה. ‫זה הלוג של ה-likelydead x וזה.
[02:14:09 - 02:14:10] ‫עכשיו, מה זה הדבר הזה שכתוב כאן?
[02:14:12 - 02:14:15] ‫זה ה-prosterיו של המשתנה החבוי ‫בהינתן הודעה.
[02:14:16 - 02:14:18] ‫לפי חוק בייס, הדבר הזה,
[02:14:19 - 02:14:20] ‫הוא גם שווה,
[02:14:21 - 02:14:23] ‫ה-prיור על z,
[02:14:23 - 02:14:24] ‫מה ה-prיור על z?
[02:14:25 - 02:14:26] ‫z ו-k זה יותר דבר.
[02:14:26 - 02:14:29] המשתנה החבוי שקובע איזה אינגרסייאנו.
[02:14:30 - 02:14:31] ‫מה ה-prיור עליו?
[02:14:37 - 02:14:44] ‫דוחים שאיך הגדלנו את ה-GMM בצורה השנייה? ‫אמרנו שיש לנו פלגות קטגורית ‫שמייצרת את z,
[02:14:45 - 02:14:46] ‫את ה-prיור על z.
[02:14:47 - 02:14:50] ‫ואז מתוך זה יש לנו התפלגות ‫שמייצרת את x ויינתן z.
[02:14:51 - 02:14:53] ‫אז ה-prיור זה פשוט pi,
[02:14:53 - 02:14:54] ‫אוקיי?
[02:14:56 - 02:14:58] ‫ואת זה אנחנו מכפילים ב-likelihood.
[02:14:58 - 02:15:02] ‫-likelihood זה התפלגות של x ויינתן z ויינתן k.
[02:15:03 - 02:15:05] ‫מה זה ההתפלגות של x ויינתן k?
[02:15:07 - 02:15:08] ‫כמה?
[02:15:08 - 02:15:12] כן, זה הגאוסייאנה k מתוך התערובת שלנו, ‫וזה n
[02:15:13 - 02:15:13] של xi
[02:15:17 - 02:15:22] שיש לו מיו k וסיגמא k, ‫זה הגאוסייאנה k,
[02:15:22 - 02:15:26] ‫ואת זה צריך לנרמל. ‫איך מנרמלים את הדבר הזה ביחס ל-k?
[02:15:27 - 02:15:29] ‫סוכמים על כל האפשרויות של k. ‫אז בואו נגיד
[02:15:30 - 02:15:33] סכום על k תג שווה אחד עם k גדול,
[02:15:34 - 02:15:35] ‫של pi k
[02:15:37 - 02:15:37] תג
[02:15:38 - 02:15:38] והנורמל
[02:15:40 - 02:15:45] ‫במוצעי בינתן מיו k תג סיגמא וז.
[02:15:47 - 02:15:51] ‫אוקיי, מה זה הדבר הזה? ‫כבר פגשנו מיקרופוזים,
[02:15:51 - 02:15:53] ‫זה בדיוק כמו שנוספת לנו כאן בגורדיאנט,
[02:15:54 - 02:15:55] ‫כפי שהיה לנו בגורדיאנט,
[02:15:56 - 02:15:58] ‫מה המשמעות של הדבר הזה?
[02:15:59 - 02:16:00] ‫זה מגדירים אותו,
[02:16:02 - 02:16:04] ‫קוראים לזה responsibilities,
[02:16:05 - 02:16:05] ‫בדרך כלל.
[02:16:11 - 02:16:16] רק מה ששכחתי כאן לעשות, כאן אנחנו עושים את זה עבור הפרמטר תטא שהיה לנו קודם, זאת אומרת,
[02:16:16 - 02:16:23] ‫צריך לתת גם את ה-T הזה, אוקיי? ‫זה עבור מיו וסיגמאות ופיים שהיה לנו באיטרציה הקודמת,
[02:16:24 - 02:16:25] ‫פרמטרים שהיו לנו באיטרציה הקודמת.
[02:16:27 - 02:16:33] ‫בדבר הזה אנחנו קוראים ה-responsibility של נקודה i
[02:16:35 - 02:16:37] ‫עבור גרסיאן k.
[02:16:38 - 02:16:41] זה אומר איזושהי השמה רכה ‫שאנחנו עושים לנקודות
[02:16:42 - 02:16:45] ‫בין הגרסיאנטים השונים.
[02:16:46 - 02:16:49] ‫זה אומר בעצם אנחנו חושבים ‫שיש לנו נקודה חלשה.
[02:16:49 - 02:16:51] ‫מאיזה גרסיאנטים אנחנו חושבים שהיא הגיעה?
[02:16:51 - 02:16:55] ‫אבל בצורה רכה. ‫הדבר הזה הוא דברים שהם מסחרים לאחד.
[02:16:56 - 02:16:59] ‫אז כל נקודה יש לנו התפלגות של ערכים
[02:17:00 - 02:17:02] ‫עבור כל אחד מהגרסיאנטים שלו.
[02:17:05 - 02:17:11] ‫ואז בעצם בואו נראה בדיוק את ההמשך של זה, ‫אבל אנחנו עכשיו נעשה לזה...
[02:17:12 - 02:17:13] ‫בצעד הבא זה יהיה לעשות,
[02:17:14 - 02:17:17] ‫בהינתן שהקדמנו ככה את האי שלנו, ‫אנחנו נרצה לעשות
[02:17:18 - 02:17:25] ‫אופטימיזציה לדבר הזה, ‫למצוא את הטנטות שממקסימים את זה. ‫זאת אומרת, למצוא את המיואים ואת הפיים ‫ואת הסימנות שממקסימים את זה,
[02:17:26 - 02:17:29] ‫בהינתן שההשמה הזאת נתונה.
[02:17:30 - 02:17:33] ‫אז ב-GMM אנחנו נעשה את זה ‫בקצת יותר מסודר בשבוע הבא,
[02:17:33 - 02:17:36] ‫אבל הדרך לחשוב על זה ‫שאנחנו מסתכלים על כל הנקודות,
[02:17:37 - 02:17:39] ‫אנחנו מנחשים בהתחלה איזה שהם גרסיאנטים,
[02:17:40 - 02:17:41] ‫פרמטרים של גרסיאנטים,
[02:17:42 - 02:17:43] ‫זה יעפה את האפס שלנו.
[02:17:44 - 02:17:49] ‫ואז אנחנו עושים השמה רכה של כל נקודה,
[02:17:50 - 02:17:53] ‫כלומר הנקודה הזאת היא פתוחה בגאוסיאן הזה, ‫הנקודה הזאת היא פתוחה בגאוסיאן הזה,
[02:17:54 - 02:17:57] ‫אבל כל נקודה היא סיכוי מסוים ‫בכל אחד מהגאוסיאנטים,
[02:17:58 - 02:18:00] ‫ואז בהינתן הסיכויים האלה, ‫אנחנו עושים,
[02:18:01 - 02:18:06] ‫עבור כל גאוסיאן, אנחנו מחשבים את הפרמטרים שלו,
[02:18:07 - 02:18:10] ‫מתוך מין ממוצע משוקלל כזה ‫של כל הנקודות
[02:18:12 - 02:18:14] בהסתברות מסוימת נמצאות אצלנו
[02:18:16 - 02:18:19] זה מזכיר לכם איזה אלגוריתם שפגשתם קודם?
[02:18:21 - 02:18:22] וגם
[02:18:25 - 02:18:26] כנראה ב-mode ו-machine-mode,
[02:18:27 - 02:18:32] זה קצת כמו K-means, אוקיי? אז אפשר לחשוב על זה בתור הכללה של K-means
[02:18:32 - 02:18:39] מין K-means דורך כזה, ו-K-means כל נקודה אנחנו רואים, אוקיי, בטוח מהגרסיון הזה, בטוח מהקלאסטרן הזה,
[02:18:40 - 02:18:45] ואז אנחנו עושים את האפטימידציה על כל הנקודות שהן הכי קרובות לפלאטום פלאסטר מסוים,
[02:18:46 - 02:18:47] מוצאים את המין שלנו.
[02:18:48 - 02:18:50] פה אנחנו עושים הכל בצורה יותר רכה,
[02:18:50 - 02:18:53] אוקיי? אז יש לזה, אנחנו נראה,
[02:18:54 - 02:18:56] כבר הוכיח שזה מתפלט בצורה יעילה,
[02:18:56 - 02:18:58] כדי לאשר דברים ש-K-means לא מאפשר,
[02:18:59 - 02:19:00] בסוף נותן לנו מודל הסתברותי,
[02:19:01 - 02:19:02] K-means רק יותר מודל פלאסטרי,
[02:19:03 - 02:19:04] שזה נותן לנו ממש מודל הסתברותי,
[02:19:10 - 02:19:13] כמו אולי להגיד עכשיו, לפני ש...
[02:19:14 - 02:19:15] בשיעור,
[02:19:16 - 02:19:21] אז מה שלא הספקתי לעשות זה קודם כל לעשות את זה עד הסוף, להראות לכם ממש מה האלגוריתם עבור
[02:19:22 - 02:19:23] Gartion Mixture Models,
[02:19:24 - 02:19:29] דבר שני, להוכיח למה זה מתכנס וזה גם יותר קצת אינטואיציה על מה האלגוריתם הזה עושה,
[02:19:30 - 02:19:32] אז את שני הדברים האלה נעשה כבר בשבוע הבא.
[02:19:33 - 02:19:44] אני רק אראה לכם אם יש פה קצת אנימציות שמראות מה גרסיאנים, איך זה נראה עבורי דאטה שונה.
[02:19:44 - 02:19:47] יש לכם שאלות על הדבר הזה? הצלחתם לעקוב קצת או ש...
[02:19:53 - 02:19:55] אני אראה לכם אנימציות של
[02:19:57 - 02:19:58] עובד לפועל
[02:20:03 - 02:20:13] ‫אוקיי, זה אנימציות שלקחתי מרועי פרידמן, הוא
[02:20:14 - 02:20:15] מתרגל של קורס
[02:20:18 - 02:20:20] ‫האוניברסיטה העברית שעושה, של בייז'ן ושין לרנינג.
[02:20:22 - 02:20:23] אז
[02:20:24 - 02:20:25] יש פה דאטה,
[02:20:25 - 02:20:26] כל הנקודות האלה,
[02:20:26 - 02:20:30] לצבעים שונים, אבל האלגוריתם לא רואה את הצבעים האלה, הוא פשוט רואה אוסף של הנקודות,
[02:20:31 - 02:20:37] ‫שבאמת הגיעו משלושה גרסיאנים שונים, אוקיי? ‫אז הורידה גם פה הרבה נקודות מדרסיאן אחד,
[02:20:37 - 02:20:41] ‫והרבה נקודות מגרסיאן אחר, ‫והרבה נקודות מגרסיאן שלישי, אוקיי? ‫זו דעת פדום ונדודי.
[02:20:43 - 02:20:45] ‫זה נותן את כל הנקודות האלה
[02:20:45 - 02:20:50] מהאלגוריתם E.M. ‫שמתחיל מאיזה שהם גרסיאנים לא נכונים,
[02:20:50 - 02:20:55] ‫ולאט לאט מתכנס. הצבע שאתם רואים זה בעצם ה...
[02:20:56 - 02:20:57] זה אמור להיות...
[02:20:58 - 02:21:06] ‫אני לא חושב שזה רך, ‫אז זה כנראה נותן את המספר הכי גד... ‫את הצבע, הוא צובע את זה לפי ה-responsibility ‫עם הערך הכי גבוה.
[02:21:07 - 02:21:09] ה-responsibility זה מה שכתבנו פה קודם,
[02:21:09 - 02:21:12] ‫זה בעצם עבור כל נקודה יהיו שלושה מספרים,
[02:21:13 - 02:21:18] ‫שנסחמים לאחד, ‫מה ההסתברות שזה הגיע מהירוק, ‫מה ההסתברות שזה הגיע מהאדום, ‫מה ההסתברות שזה הגיע מהפחול.
[02:21:20 - 02:21:24] ‫אז אפשר לצבוע... ‫היה אפשר לצבוע את זה בצבע רציף.
[02:21:24 - 02:21:33] ‫זה נראה לי לא משהו עושה, זה כנראה צבע ‫שהוא פשוט המקסימום של ה-Pai הזה.
[02:21:35 - 02:21:36] ‫כן, אנחנו רואים שזה די מהר מתכנס
[02:21:37 - 02:21:39] ‫לגאוסיאנים הנכונים שייצרו את הדאטה.
[02:21:41 - 02:21:44] ‫הליפסות שאנחנו רואים זה בעצם מראה ‫את הפרמטרים
[02:21:44 - 02:21:46] ‫של הגאוסיאן שאנחנו לומדים, את התטא.
[02:21:47 - 02:21:50] ‫לא רואים פה את ה-Pai, אבל
[02:21:50 - 02:21:51] ‫תטא אומר לנו...
[02:21:52 - 02:21:54] מה יש לנו? ‫בתטא יש לנו שלוש תוכלות,
[02:21:55 - 02:21:58] ‫וזה המרכזים של כל אחד מהאליפסות האלה,
[02:21:59 - 02:22:04] ‫ושלוש מטריצות covariance. ‫כמו שאתם זוכרים, אנחנו יכולים לחשוב ‫על מטריצת covariance בתור אליפסה,
[02:22:04 - 02:22:09] ‫על ידי זה שאנחנו מחשבים ‫את הערכים העצמיים והווקטורים העצמיים.
[02:22:09 - 02:22:13] ‫הווקטורים העצמיים נותנים לנו את הכיוון ‫של הצירים העיקריים של האליפסה,
[02:22:15 - 02:22:17] ‫והגודל של כל אחד מהם ‫זה הערכים העצמיים.
[02:22:18 - 02:22:19] זה מה שמצויר כאן.
[02:22:20 - 02:22:22] ‫זה עבור יותר גאוסיאנים ‫שקצת מעורבבים אחד בשני,
[02:22:23 - 02:22:25] ‫אתם רואים שזה יכול לקחת ‫קצת יותר זמן.
[02:22:30 - 02:22:37] ‫אבל עדיין, בדעת שהוא גם נוצר באמת מגאוסיאן, ‫אז ההתפלגות פה היא נכונה.
[02:22:38 - 02:22:39] ‫המודל עצמו הוא מודל נכון.
[02:22:40 - 02:22:42] ‫-כן, כמו כל אחד מהם זה עושה נפתרציות.
[02:22:42 - 02:22:50] ‫למשל אור מגבי האוסיאנים לא מודל או לא? ‫-כן, אז זה מבחינתנו זה המודל. ‫חלק מה... אתה בוחר... החלטת שאתה עושה גאוסיאן,
[02:22:50 - 02:22:52] ‫שאתה מבדל את הבעיה בתור GMM,
[02:22:52 - 02:22:58] ‫זה לא מספיק. ‫צריך להגיד, החלטתי שאני מודל את זה ‫בתור GMM עם עשרה אחים. זה המודל שלך.
[02:22:59 - 02:23:01] ‫זה מגדיר איזה תל אדם ‫אתה מחפש, גברתי.
[02:23:05 - 02:23:10] ‫אפשר לחשוב על כל מיני שיטות ‫שמוצאות איזה מודל הכי מתאים. ‫בעצם אתה יכול, יש משהו שנקרא ‫מודל סלקשן,
[02:23:11 - 02:23:12] ‫שגם יש כל מיני שיטות בייזיאנית ‫לעשות את זה,
[02:23:13 - 02:23:16] ‫להגדיר איזה מודל הוא הכי טוב, ‫מתי זה כבר מפסיק להשתפר.
[02:23:17 - 02:23:19] ‫בעיקרון, ככל שאתה עושים גאוסיאנים,
[02:23:19 - 02:23:22] ‫המודל שלך יהיה לו יותר capacity,
[02:23:25 - 02:23:27] ‫ואנוכל בעצם לתפוס ‫התפגות יותר מורכבות,
[02:23:27 - 02:23:29] ‫אבל בשלב מסוים זה כבר לא יעזור.
[02:23:30 - 02:23:32] ‫הלייק יוד יפה, יישאר אותו דבר.
[02:23:34 - 02:23:37] ‫אז יש כל מיני שיטות אוטומטיות ‫למצוא את זה, ‫אנחנו לא כל כך נדבר על זה.
[02:23:38 - 02:23:42] ‫פה זה דאטה שהוא לא הגיע במקור ‫מתערובת גאוסיאנים,
[02:23:43 - 02:23:48] ‫ואגודי, אני לא יודע כמה, ‫הודי הקירוב בעצם, ‫שתערובת גאוסיאנים עושה על הדאטה הזה.
[02:23:50 - 02:23:50] ‫לא יודע כמה יש פה.
[02:23:52 - 02:23:54] ‫זה שש, משהו כזה, גישה גאוסיאנים.
[02:23:54 - 02:23:57] ‫זה דאטה שהוא בפירוש לא גאוסיאני,
[02:23:58 - 02:24:00] ‫זאת אומרת, הוא גם לא תערובת של גאוסיאנים,
[02:24:01 - 02:24:05] ‫אבל אפשר עם מספר סופי של גאוסיאנים ‫לקרב את זה יחסיתו.
[02:24:10 - 02:24:12] ‫פה התחיל להשתגע גם קצת אחר כך.
[02:24:13 - 02:24:16] ‫יש פה דאטה שהוא גם מאוד מורכב,
[02:24:17 - 02:24:23] ‫אבל עם מספיק גאוסיאנים ‫אפשר להתחיל לקרב את זה. ‫אתם רואים שהיית פה כל מיני דברים ‫שכל גאוסיאן מוצא לעצמו איזה
[02:24:24 - 02:24:27] ‫מקום אחר, ולאט לאט הם ממלאים ‫את כל ההתפלגות שצריך למלא.
[02:24:33 - 02:24:37] ‫אמרנו בעצם שאפשר עם תערובת גאוסיאנים ‫לקרב כל התפלגות.
[02:24:39 - 02:24:40] ‫יכול להיות שצריך הרבה גאוסיאנים,
[02:24:41 - 02:24:51] ‫אבל עבור כל אפסילון בהתפלגות מוזרה, ‫אפשר למצוא את המספר הגאוסיאנים ‫שקרבו אותם מספיק טוב.
[02:24:52 - 02:24:53] ‫אה, יש פה אפילו עוד אחד.
[02:24:54 - 02:24:55] ‫הרבה גאוסיאנים.
[02:24:56 - 02:24:57] ‫די מגניב,
[02:24:58 - 02:24:58] מה שקורה פה.
[02:25:00 - 02:25:02] ‫-הנחה הזאת שקרבו את גאוסיאנים ‫על התמונות.
[02:25:11 - 02:25:16] ‫אז האמת שזו שאלה טובה, ‫אני אראה קצת דוגמאות ‫אבל בשבוע הבא.
[02:25:17 - 02:25:20] ‫זה היה חלק מהמחקר שלי ‫בית התורת האלה, ‫בגלל זה תערובת גאוסיאנים ‫על תמונות,
[02:25:21 - 02:25:23] ‫ואז אני יכול להראות קצת דוגמאות.
[02:25:25 - 02:25:29] ‫התשובה היא שזה מודל מאוד טוב ‫על פאצ'ים של תמונות.
[02:25:30 - 02:25:33] ‫באזורים קטנים של תמונות, ‫אתה רואה של גאוסיאנים ‫זה מודל טוב,
[02:25:34 - 02:25:35] ‫אבל זה לא כל כך מודל טוב ‫לתמונות שלמות.
[02:25:38 - 02:25:39] יש שיטות לטראות?
[02:25:40 - 02:25:40] כן.
[02:25:40 - 02:25:51] ‫אבל בעצם הקורס, כל הקורס הזה ‫ידבר על שיטות הכי מודרניות ומתקדמות ‫והכי טובות שמצאו ‫כדי למצוא מודלים טובים לתמונות.
[02:25:52 - 02:25:56] זה המודל שהגענו אליו בשיעור השלישי, ‫ואנחנו נגיע למודלים ‫היותר טובים בשיעור הבא.
[02:26:02 - 02:26:09] ‫טוב, אז לא הספקנו כמו שרצינו, ‫אבל נראה לי שקיבלתם ‫איזושהי תמונה של מה זה אומר, ‫ואנחנו נמשיך בשבוע הבא.
[02:26:10 - 02:26:12] ‫תודה רבה. תודה רבה.