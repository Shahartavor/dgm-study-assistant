[00:00:00 - 00:00:01] תודה רבה לכבוד.
[00:00:02 - 00:00:03] שלום לכולם.
[00:00:05 - 00:00:06] תודה רבה רגע.
[00:00:09 - 00:00:09] מעניינים.
[00:00:10 - 00:00:17] אז הגשתם לכולכם את התרגיל השלישי ואני לא יודע אם שמתם לב פה על התרגיל הרביעי שפורסם.
[00:00:19 - 00:00:22] אני מקווה שהוא לא יהיה הרבה פער מפחיד מדי.
[00:00:23 - 00:00:25] הוא אמור להיות יחסית פשוט
[00:00:26 - 00:00:29] מבחינת המימושים שאתם צריכים לעשות.
[00:00:31 - 00:00:34] המטרה של התרגיל זה להבין את הקוד.
[00:00:35 - 00:00:38] כאילו הקוד שם כבר נתון, יש כאילו מודל כבר שהוא
[00:00:39 - 00:00:41] מאומן כבר.
[00:00:42 - 00:00:48] אז המטרה היא להבין אותו, לעשות כל מיני צחקים כאלה שאתם מצליחים להבין מה קורה שם.
[00:00:48 - 00:00:51] ויש שם כמה סעיפים שצריך לחשב כל מיני דברים.
[00:00:51 - 00:00:53] גם בחישובים האלה הרוב הוא מאוד פשוט.
[00:00:54 - 00:00:58] יש שני סעיפים בעצם שהם נקשים ושם
[00:00:58 - 00:01:02] המטרה היא לא שתצליחו להגיע לתוצאות טובות,
[00:01:03 - 00:01:07] אלא שתציעו איזה דרכים אפשר לעשות כדי לעשות את החישובים האלה,
[00:01:08 - 00:01:12] ותראו איזה מימוש התחלתי כזה שעושה משהו
[00:01:13 - 00:01:14] לא טוב עכשיו.
[00:01:16 - 00:01:18] נראה לי שאלה חשבות שאלו.
[00:01:21 - 00:01:24] אנחנו ב... כתוב כאן שש, אנחנו בשבע כבר, נכון?
[00:01:25 - 00:01:25] בהצעה שבע.
[00:01:26 - 00:01:27] נסתכל על זה.
[00:01:28 - 00:01:34] אוקיי, אז היום הולך בהצעה שבע, אנחנו נדבר על מודל שנקרא Variational Auto-Encoder או VEAE,
[00:01:34 - 00:01:35] בקיצור.
[00:01:36 - 00:01:41] זה מודל שיקר לליבי, הוא יצא בעצם ב-2014,
[00:01:43 - 00:01:45] כשאני הייתי בסוף הדוקטורט,
[00:01:46 - 00:01:51] ועד אז אני התעסקתי בדברים שיותר קשורים למחשבת הראשון של הקורס,
[00:01:52 - 00:01:55] ובמודלים כמו גר של מיקשר מודל, GMM's,
[00:01:55 - 00:01:57] לשיטות כמו PM,
[00:01:58 - 00:01:59] Expectation ו-Experation,
[00:02:00 - 00:02:03] וזה היה מודל שחיבר בעצם את העולם הזה לעולם של
[00:02:04 - 00:02:05] Deep Learning,
[00:02:06 - 00:02:12] אז זה בעמוקי מה שהכניס אותי לעולם הזה של מודלים גנרטיביים עמוקים.
[00:02:14 - 00:02:15] אז זה מה שנדבר היום.
[00:02:16 - 00:02:22] התוכנית היא כזאת,
[00:02:23 - 00:02:26] אהבתי את השינויים ולא שנדברתי פה.
[00:02:29 - 00:02:33] בגדול זה אפשר להגיד שזו תוכנית אולי חוץ מה-Extensions פה.
[00:02:34 - 00:02:40] אז אנחנו נדבר, נעשה חזרה על בעצם מה זה אומר מודל משתנה חבוי,
[00:02:41 - 00:02:45] שאנחנו דיברנו עליו כבר כמה פעמים אבל בשבוע שעבר עשינו הפסקה מזה.
[00:02:46 - 00:02:50] נדבר על הגישה של Variational Inference לטפל
[00:02:50 - 00:02:52] בבעיה של משתנות חבועים.
[00:02:53 - 00:02:54] נדבר על מפניתה.
[00:02:55 - 00:03:04] אמורטאיזד Inference שזה כשאנחנו במקום לעשות את ה-Vertational Inference לכל דוגמה בנפרד
[00:03:04 - 00:03:06] אנחנו מאמנים איזשהו משהו משותף
[00:03:07 - 00:03:09] שיודע לעשות את זה לכולם ביחד.
[00:03:10 - 00:03:12] אמורטאיז זה בעצם מה שאנחנו חייבים.
[00:03:13 - 00:03:14] העול מתחלק
[00:03:15 - 00:03:19] על כל הנקודות בזמן האימון והוא לא משהו
[00:03:19 - 00:03:21] שצריך לעשות את עצמנו בנקודה,
[00:03:22 - 00:03:23] נסביר את זה.
[00:03:24 - 00:03:34] איך עושים, איך מאפשרים את כל המבנה הזה ועדיין שהכול יהיה גזיר, שאפשר יהיה לעשות גזירה ואז בסוף
[00:03:36 - 00:03:38] כשאני משלב את כל המרכיבים האלה נקבל את מה שנקרא ה-Varition
[00:03:38 - 00:03:40] Noto-Encoder BAE
[00:03:42 - 00:03:45] ונדבר על הרחבה אחרת ממה שכתוב פה,
[00:03:45 - 00:03:48] לא נדבר על ביתא BAE ו-Impotent Sampling A זה
[00:03:50 - 00:03:57] שני מודלים שיצאו אחר כך במקום מיני הרחבות, אנחנו נדבר על דף מודל אחר שהוא מדבר על היררכיה
[00:03:58 - 00:04:01] BAE עם היררכיה של משתנים חבורים
[00:04:02 - 00:04:03] לא נדבר על זה הרבה, כנראה לא נגיע,
[00:04:04 - 00:04:07] רק אני אציג את זה ואת התוצאות של המודל הזה
[00:04:09 - 00:04:10] זו התוכנית להיום
[00:04:11 - 00:04:14] אז שוב, פה תמיד אני אשם את השקפים הראשונים,
[00:04:15 - 00:04:15] מה זה מודל,
[00:04:16 - 00:04:18] תמיד יש לי קטע הזה עם המוזיקה שאני צריך לדייק,
[00:04:19 - 00:04:19] אוקיי.
[00:04:20 - 00:04:24] אנחנו רוצים לפתור עם מודלים גנרטיביים כל מיני דברים, נכון? לייצר דאטה,
[00:04:24 - 00:04:29] לקבל ייצוג חדש של דאטה שיכול לשמש אותנו לדברים אחרים,
[00:04:30 - 00:04:34] כל מיני החלטות שנובעות מה-certainty של הדאטה
[00:04:35 - 00:04:37] ולעשות פרוביוליסטיק אינפרנס באופן כללי
[00:04:40 - 00:04:43] להגיד משהו על המשתנה החבוי אולי,
[00:04:43 - 00:04:46] להגיד מה ההסתברות של דוגמה חדשה שקיבלנו
[00:04:49 - 00:04:50] אוקיי.
[00:04:51 - 00:04:53] תרשימה זה שתמיד אנחנו מראים
[00:04:54 - 00:04:55] שאפשר להתרגל עליו.
[00:04:57 - 00:05:04] ושוב, המרכיבים שלנו זה הדאטה, אז כבר דיברנו שבוע שעבר על סוג של דאטה שנעבוד איתו,
[00:05:05 - 00:05:07] על מרכיב אחר זה המודל,
[00:05:08 - 00:05:13] אז צריך לעודכן את זה כבר כי כבר דיברנו על עוד כל מיני מודלים,
[00:05:14 - 00:05:16] נכון? שבוע שעבר דיברנו על פיקסל cnn,
[00:05:17 - 00:05:24] שזה מודל שהוא לא ממדל את הבעיה בתור בעיה עם משתנה חבוי,
[00:05:25 - 00:05:33] אלא בתור פירוק של הבעיה לככה שכל פיקסל תלוי בכל הפיקסלים שבאו לפניו,
[00:05:33 - 00:05:39] נכון? פירוק לכלל השרשרת עם איזושהי פרמטריזציה שלא עובדת על כל האפשרויות.
[00:05:40 - 00:05:42] אז היום אנחנו נדבר על סוג אחר של המודלים שהוא
[00:05:42 - 00:05:50] הופך את הבעיה לבעיה שאפשר לחשב על ידי זה שאנחנו מוסיפים משתמש חבור.
[00:05:53 - 00:05:56] Objective function אנחנו נשארים היום במקסימום מייטליות,
[00:05:56 - 00:06:00] אבל אנחנו לא יכולים לחשב את זה בצורה מדויקת,
[00:06:01 - 00:06:10] בניגוד לשבוע שעבר, ולכן אנחנו צריכים להשתמש באיזשהו קירוב כשאנחנו עושים את האופטימיזציה והקירוב שאנחנו נשתמש שהוא יהיה
[00:06:11 - 00:06:17] בעיקר להסתמך על Variation and Inference, אבל יש בו גם מרכיבים של דגימה של מונטה קארו.
[00:06:20 - 00:06:23] אוקיי, פה כתוב האתגר שלנו באופן כללי,
[00:06:24 - 00:06:27] תזכורת, נכון? אנחנו לא יכולים סתם למדד את הכל לתור איזה בעיה פשוטה,
[00:06:28 - 00:06:36] כי המימדים גדלים בצורה מהירה, וצריך לחשוב על איזושהי פרמטריזציה חכמה של המודל.
[00:06:41 - 00:06:47] אוקיי, אז הדאטה שדיברנו עליו, שאנחנו נמשיך לדבר, אז אמניסט זה הדוגמה
[00:06:49 - 00:06:55] שאמרו דיינתה לעיקר, כי היא הכי פונה ופשוטה ולא ירוצו מהר כשתממשו את זה בקוד.
[00:06:55 - 00:06:59] אני מקווה שנגיע גם לקצת תמונות טבעיות,
[00:07:00 - 00:07:05] הדוגמה שמוציאה כאן זה מתוך אימג'נט, אבל מקווץ,
[00:07:06 - 00:07:07] זה יכול להיות של 32 ו-32.
[00:07:11 - 00:07:13] אוקיי, לדאטה אנחנו יכולים לייצג אותו,
[00:07:13 - 00:07:14] דאטה של תמונות,
[00:07:14 - 00:07:18] נהוג לייצג אותו בשתי שיטות, או שיטה דיסטרטית או שיטה רציפה.
[00:07:21 - 00:07:27] מודלים של פיקסל CNN ועוד מודלים יותר רגרסיביים, בדרך כלל נהוג
[00:07:28 - 00:07:29] להסתכל על הגישה הבדידה,
[00:07:32 - 00:07:33] ואז בעצם אנחנו,
[00:07:34 - 00:07:36] כל פיקסל הוא איזשהו ערך בין 0 ל-255,
[00:07:38 - 00:07:39] אם זה ציבוני אז יש לנו
[00:07:40 - 00:07:42] שלושה ערכים כאלה, אחד לכל ערוץ צבע,
[00:07:43 - 00:07:47] וגישה אחרת זה למדל את זה בתור דאטה רציף,
[00:07:49 - 00:07:53] ואז כל פיקסל יהיה ערך רציף בין 0 ל-1.
[00:08:00 - 00:08:04] אז זה פשוט משבוע שעבר.
[00:08:07 - 00:08:08] אוקיי, אז שתי אפשרויות,
[00:08:08 - 00:08:09] וגם ראינו פה שבוע שעבר,
[00:08:09 - 00:08:15] שתי אפשרויות להוריד את מספר הפרמטרים, להפוך את המודל הזה לאיזשהו מודל שאנחנו יכולים לעבוד איתו,
[00:08:16 - 00:08:20] אז גישה ראשונה זה לעשות איזשהו, להשתמש בצל השרשרת,
[00:08:21 - 00:08:25] ואז מכל אחד מהפקטורים האלה, צל השרשרת,
[00:08:26 - 00:08:28] איכשהו להוריד את מספר הפרמטרים שם,
[00:08:29 - 00:08:35] אז פה מה שכתוב זה להניח איזושהי אי תלות מותנית,
[00:08:36 - 00:08:38] זאת אומרת שאני לא תלוי בכל מה שבא לפניי אלא רק בחלק,
[00:08:40 - 00:08:49] בשבוע שעבר אנחנו ראינו בעצם גישה אחרת לעשות את זה, אנחנו עדיין, אולי כן יכולים להיות תלויים בהכל, אבל עם איזושהי פרמטריזציה שלא מסתכלת על כל האפשרויות, אלא
[00:08:50 - 00:08:52] למשל עושה logistic regression,
[00:08:54 - 00:08:56] שזה בעצם איזושהי פרמטריזציה ליניארית של כל מה שהיה קודם,
[00:08:57 - 00:08:58] או רשת נוירונים,
[00:08:59 - 00:09:00] זאת פרמטריזציה לא ליניארית,
[00:09:00 - 00:09:05] שלוקחת את כל מה שהיה קודם והופכת את זה להתפלגות על הפיקסל ה-XI.
[00:09:06 - 00:09:07] וזה מוריד לנו את מספר הפרמטרים,
[00:09:08 - 00:09:14] פה זה כתוב בשביל העניין של ה-conditionary independence, אז זה מוריד לנו את האקספורמנט הווטו מ-K
[00:09:16 - 00:09:18] מ-N-1 שהיה קודם,
[00:09:19 - 00:09:21] אז זו הגישה שדיברנו בשבוע שעבר,
[00:09:22 - 00:09:27] והגישה השנייה שנדבר עליה היום ובשיעורים הבאים זו הגישה שמתבססת על
[00:09:28 - 00:09:29] משתנה חבוי,
[00:09:30 - 00:09:36] בעצם אנחנו הופכים את הבעיה לבעיה כזאת שיש אי תלות בהינתן המשתנה החבוי.
[00:09:38 - 00:09:40] זה יינתן איזשהו משתנה Z,
[00:09:40 - 00:09:44] אני ממודל לכל אחד מהפיקסלים בנפרד.
[00:09:46 - 00:09:50] ואז בעצם מה שהיה כאן D בחזקת K או D בחזקת N-1,
[00:09:51 - 00:09:54] עכשיו זה פשוט הגודל של המשתנה החבוי.
[00:09:55 - 00:10:01] היום שוב אנחנו נדבר על פרמטריזציה אחרת, שהמשתנה החבוי שלנו יהיה רציף,
[00:10:03 - 00:10:06] ולכן אין אפשרות לדבר הזה,
[00:10:07 - 00:10:12] אבל תהיה איזושהי פרמטריזציה עם מספר סופי של פרמטרים עבור כל אחד מה-XI.
[00:10:14 - 00:10:15] אז אנחנו תכף נדעים את זה.
[00:10:18 - 00:10:25] אוקיי, אז יש פה גם את הדוגמאות של כל אחת מהגישות האלה, את הדוגמאות של המודלים, אז זה מה שראינו בשבוע שעבר, המודלים האלה,
[00:10:27 - 00:10:29] ואנחנו היום נדבר על זה,
[00:10:30 - 00:10:32] בשבוע הבא כנראה נגיע כבר לזה,
[00:10:33 - 00:10:34] ואולי נראה את זה.
[00:10:35 - 00:10:36] אולי כמה שאלות שראו לנו,
[00:10:37 - 00:10:41] פשוט כמה דברים שאנחנו רוצים אולי לעבור עליהם, אבל אחד מהם יהיה ל-Fusion Models,
[00:10:42 - 00:10:44] ואולי אם נספיק גם נדבר על גאנס,
[00:10:44 - 00:10:46] שזה גם אפשר לחשוב על זה בתור
[00:10:47 - 00:10:48] מודל לצורה הזאת.
[00:10:53 - 00:10:54] כן, אז אמניסט אמרנו,
[00:10:55 - 00:10:57] שבוע שעבר דיברנו על אמניסט ועל מידול בדיד
[00:10:59 - 00:11:00] של הדאטה,
[00:11:00 - 00:11:05] היום אנחנו נדבר על מידול רציף.
[00:11:11 - 00:11:13] היום אנחנו יכולים לדבר על מידול רציף.
[00:11:16 - 00:11:22] אז איזה מידולים רציפים אנחנו יכולים לחשוב עליהם? אז למשל אנחנו יכולים לחשוב על מודל שהוא גאוסיאן,
[00:11:23 - 00:11:24] זאת אומרת,
[00:11:24 - 00:11:27] כל פיקסל יש לו איזושהי פוחלת ווריאנס,
[00:11:27 - 00:11:30] אם הוא מסתכל על תמונה או על איזה איזשהו פאץ',
[00:11:31 - 00:11:32] אז זה קוראים של מיו זה יהיה וקטור
[00:11:33 - 00:11:36] של התוכלת, וסיגמה זה יהיה
[00:11:37 - 00:11:42] ה-covarions של ה-patch או של התמונה.
[00:11:43 - 00:11:47] אם אני חושב על פיקסל, אז מה אתם חושבים שמיו
[00:11:48 - 00:11:50] של פיקסל בודד בתמונה?
[00:11:50 - 00:11:51] מה התוכלת של פיקסל?
[00:11:52 - 00:11:52] הערך שלו.
[00:11:54 - 00:11:57] לא, זה עכשיו מודל, אני לא יודע מה הערך שלו.
[00:11:57 - 00:11:58] יש לי הרבה פיקסל בתמונה.
[00:11:59 - 00:12:03] אם אני רוצה למדל את הערך של פיקסל בתמונה בתור גאוסיאן,
[00:12:04 - 00:12:10] מה תהיה התוכלת של הממוצע של הממוצע בתמונת הוויקסל?
[00:12:10 - 00:12:10] כן,
[00:12:11 - 00:12:14] יש לי הרבה תמונות, מה לדעתכם זה יצא מהממוצע?
[00:12:15 - 00:12:17] מה שאתה אומר זה בעצם המקסימום לייקליות של התוכלת.
[00:12:18 - 00:12:19] כבר יהיה ה-28.
[00:12:22 - 00:12:25] כן, אנחנו מדברים על רציף, אז ברציף בדרך כלל אנחנו מדברים בין 0 ל-1,
[00:12:26 - 00:12:27] זה יהיה פשוט חצי.
[00:12:27 - 00:12:31] כן, בדרך כלל אם אנחנו מדברים על פיקסל 1 זה יהיה חצי,
[00:12:32 - 00:12:33] ומה יהיה ה-Variance?
[00:12:34 - 00:12:35] לא ברור, משהו
[00:12:38 - 00:12:39] שיוביל אותו בערך
[00:12:40 - 00:12:43] לתערכים בין 0 ל-1.
[00:12:46 - 00:12:52] אם אנחנו מדברים על איזשהו patch, נגיד אזור שהוא 2 על 2, פיקסלים בגודל 2 על 2,
[00:12:53 - 00:12:55] אבל התוכלות יהיו
[00:12:56 - 00:12:59] וקטור פשוט של 4 פיקסלים, נכון?
[00:13:00 - 00:13:01] אז יהיו שם 4 חצאים.
[00:13:02 - 00:13:03] איך תראה אם פריצת ה-Covariance?
[00:13:08 - 00:13:10] בעצם יהיה לנו את ה-Variance בה לחסום.
[00:13:11 - 00:13:13] נגיד שה-Variance הוא
[00:13:14 - 00:13:15] לשורש של
[00:13:17 - 00:13:21] לא, נגיד 0.3 בריבוע, לא יודע איך שזה
[00:13:22 - 00:13:23] שעשתה לנו ב-VH של 0.3
[00:13:25 - 00:13:26] ומה יהיה ה-Covariance?
[00:13:29 - 00:13:31] סתם בגדול, מה השמות של קוויריאנס?
[00:13:32 - 00:13:33] פיקסל 1,
[00:13:34 - 00:13:35] מה תהיה הקורולציה בין
[00:13:36 - 00:13:38] הפיקסלים השכנים?
[00:13:39 - 00:13:43] כן, אז כנראה שיהיה איזה קורולציה יחסית גבוהה בין,
[00:13:44 - 00:13:48] ככל שהפיקסל יותר קרוב כנראה שהקורולציות תהיה יותר קרובה, נכון? יש פיקסל 1 שקיבל ערך
[00:13:49 - 00:13:49] מאוד בהיר,
[00:13:50 - 00:13:52] רוב הסיפורים שהפיקסל לידו גם קיבל ערך תהיה.
[00:13:55 - 00:13:57] אז זה מה שמודל כזה יכול לתפוס.
[00:14:00 - 00:14:04] ראינו מודל אחר שנקרא GAR של ניקסל מודל, GMM,
[00:14:05 - 00:14:10] אז זה פשוט הרבה כאלה משולבים, אוקיי? זה יכול לתפוס כל מיני התנהגויות שונות של הדאטה.
[00:14:13 - 00:14:14] אז פה למשל יכול להיות,
[00:14:15 - 00:14:19] יש מקרים שבהם פיקסלים מאוד בהירים או מאוד קהיים, אז אני יכול,
[00:14:20 - 00:14:22] לתפוס את הגרסיונים שונים
[00:14:23 - 00:14:24] עבור מקרים שונים,
[00:14:25 - 00:14:30] יש מקרים שהקוברנטס הזה יגיד, אוקיי עכשיו זה patch שהוא חלק לגמרי,
[00:14:30 - 00:14:33] יש מקרים שהקוברנטס הזה יגיד,
[00:14:33 - 00:14:38] זה patch שיש בו איזשהו edge ורטיקלי נגיד,
[00:14:39 - 00:14:44] אז יהיה קוברנטס גבוה רק בין פיקסלים של אחד מתחת לשני, אבל יהיה קוברנטס נמוך בין
[00:14:45 - 00:14:47] פיקסלים שעוברים את הצדדים השונים של ה-edge.
[00:14:48 - 00:14:50] כל מיני דברים כאלה יכולים לקרות שם,
[00:14:51 - 00:14:53] אם היה לנו זמן היינו,
[00:14:53 - 00:14:57] והייתי מראה לכם ממש דוגמאות של מודלים כאלה של ג'ים עקרונות,
[00:14:58 - 00:15:00] אבל אנחנו נעבור למודלים יותר
[00:15:05 - 00:15:08] מורכבים, אפשר להגיד, זאת אומרת אם יש להם
[00:15:08 - 00:15:11] קפסטי יותר גבוה, הם יכולים לתפוס התפלגויות יותר מורכבות,
[00:15:14 - 00:15:16] והמודל שאנחנו נעבוד איתו היום זה
[00:15:16 - 00:15:21] מודל שהוא בעצם גאוסיאן, אבל הוא גאוסיאן שהוא
[00:15:24 - 00:15:25] מותנה באיזשהו אינפוט,
[00:15:28 - 00:15:31] והדרך שההתניה הזאת באה לידי ביטוי זה על ידי רשת נוירונית,
[00:15:32 - 00:15:34] אז מה כתוב כאן? יש כאן גאוסיאן,
[00:15:35 - 00:15:36] והתוחלת שלו
[00:15:38 - 00:15:41] היא לא איזשהו מספר קבוע כמו חצי שהיה לנו קודם,
[00:15:42 - 00:15:43] והיא לא איזשהו סט,
[00:15:43 - 00:15:45] ולא שנתנו לי איזשהו סט של תוחלות
[00:15:48 - 00:15:49] K תוחלות שונות,
[00:15:50 - 00:15:52] אלא זה איזושהי פונקציה של Z,
[00:15:53 - 00:15:56] ו-Z יהיה המשתנה החבוי שלנו,
[00:15:57 - 00:16:02] למרות שאנחנו גם, יהיה לנו את הדוגמה ההפוכה שבה נקודה כאן זה יהיה ה-input,
[00:16:02 - 00:16:05] אבל לא משנה, יש איזשהו input של איזושהי רשת,
[00:16:07 - 00:16:09] לרשת הזאת יש פרמטרים, תטא,
[00:16:09 - 00:16:12] וה-output של הרשת הזאת נותנת לנו את התוחלת.
[00:16:13 - 00:16:18] אוקיי, לרוב זה הדבר המשמעותי, לפעמים גם אנחנו רוצים
[00:16:19 - 00:16:22] שתהיה איזו רשת שלומדת את ה-Varience וה-Covarance.
[00:16:24 - 00:16:28] בדרך כלל אנחנו לא נוכל, אנחנו כן נרצה לדבר,
[00:16:29 - 00:16:33] בניגוד לגאוסיאנים ו-GMMים, ששם אפשר רק לדבר על פאצ'ים קטנים,
[00:16:33 - 00:16:36] אנחנו כן נרצה לדבר על גאוסיאנים על התמונה המלאה,
[00:16:37 - 00:16:39] אז אנחנו נניח שה-Covarance הוא
[00:16:39 - 00:16:44] אלכסוני, זאת אומרת כל הפיקסלים הם בלתי תלויים.
[00:16:45 - 00:16:47] זה בדיוק ההנחה שכתובה
[00:16:52 - 00:16:52] כאן,
[00:16:54 - 00:16:55] בהינתן איזשהו Z,
[00:16:56 - 00:16:59] כל הפיקסלים הם בלתי תלויים, זה מה שאנחנו מרוויחים מזה שאנחנו מכניסים את
[00:17:02 - 00:17:02] המשתנה החבוי הזה.
[00:17:05 - 00:17:07] אז הדרך שאנחנו נמדד את זה, זה על ידי הגאוסיאנים.
[00:17:08 - 00:17:11] עכשיו יש עוד דרכים שאפשר לעשות את זה, אבל זאת הדרך שאנחנו נעשה.
[00:17:12 - 00:17:13] רק נכתוב, בעצם מה זה אומר הגאוסיאנים הזה?
[00:17:14 - 00:17:14] אז
[00:17:15 - 00:17:17] תגיד שיש לי את המשתנה החבוי Z,
[00:17:19 - 00:17:23] מתוכו אני רוצה לייצר את ה-B של פועל ה-X.
[00:17:26 - 00:17:32] אני מניח כבר שאני רוצה שבהינתן Z זה יהיה בלתי תלוי.
[00:17:38 - 00:17:42] יהיה לי פי של X1 בלתי תלוי.
[00:17:44 - 00:17:47] עבור כל הפיקסלים של התמונה היא פי של X2 בלתי תלוי.
[00:17:48 - 00:17:49] ככה הלאה.
[00:17:52 - 00:17:53] פי XN בלתי תלוי.
[00:17:56 - 00:18:01] ואיך אני עובר מה-Z לפי של X בלתי תלוי? מה יהיה החצי הזה?
[00:18:02 - 00:18:04] והחצי הזה יהיה ככה, פשוט זה יהיה
[00:18:04 - 00:18:11] N, פרסיאן, שהתוכלת שלו היא פונקציה
[00:18:13 - 00:18:15] עם פרמטרי תטא של זה.
[00:18:16 - 00:18:18] ויש לי,
[00:18:20 - 00:18:22] איך גרסקי צריך לקטוב את זה? יש לי,
[00:18:23 - 00:18:25] אוקיי זה סקלר עכשיו, כן?
[00:18:25 - 00:18:28] ויש לי איזשהו סיגמא, זה אכתב סיגמא קטן כי זה סקלר,
[00:18:29 - 00:18:29] גם יהיה
[00:18:30 - 00:18:31] וקטורים, מי ש...
[00:18:32 - 00:18:33] רשת עם פרמטרים תטא?
[00:18:34 - 00:18:34] זה input שלו
[00:18:39 - 00:18:39] בסדר?
[00:18:40 - 00:18:41] ובעצם הדבר הזה
[00:18:43 - 00:18:44] נראה ככה נכנס Z,
[00:18:45 - 00:18:47] יש כזה מלאי Nodes,
[00:18:50 - 00:18:50] תשובים,
[00:18:51 - 00:18:52] רליוז,
[00:18:53 - 00:18:55] אקבולוציות אולי, כל מיני דברים.
[00:18:57 - 00:18:58] עכשיו יוצא
[00:19:00 - 00:19:00] מיוטי
[00:19:04 - 00:19:04] בסדר?
[00:19:05 - 00:19:05] הרבה פעמים אנחנו,
[00:19:06 - 00:19:09] בגלל שזה תמונה, אנחנו הרבה פעמים נחשב את הכל ביחד,
[00:19:09 - 00:19:10] אנחנו נניח שזה בלתי תלוי,
[00:19:11 - 00:19:16] אבל אנחנו נעשה איזושהי רשת שמוציאה את כל התמונה בעצם, שהיא תהיה תמונת התוכלות
[00:19:17 - 00:19:18] של כל הפיקסלים.
[00:19:20 - 00:19:23] סתם אצלי, הראיתי לכם קצת את החישוב בנפרד ב-NewI,
[00:19:24 - 00:19:29] אבל אופציה אחרת שמשתמשים בה הרבה פעמים זה ש-Z יש לו איזשהו מבנה גם אולי,
[00:19:30 - 00:19:32] איזושהי תמונה, משהו שהוא דו-ממדי,
[00:19:33 - 00:19:34] אבל לא חייב להיות,
[00:19:35 - 00:19:37] יש איזושהי רשת.
[00:19:41 - 00:19:42] הארטפוד של הרשת הזאת
[00:19:44 - 00:19:46] זה משהו שיש לו מבנה של תמונה.
[00:19:53 - 00:19:55] יש כל איבר בתמונה הזאת.
[00:19:58 - 00:20:01] זה מיור של פיקסל אחר.
[00:20:03 - 00:20:06] אוקיי, אז זה הדרך שאנחנו בדרך כלל
[00:20:07 - 00:20:08] נממש את הדבר הזה.
[00:20:09 - 00:20:11] ואותו דבר לגבי סיגמא, בגלל שאנחנו מניחים שהקובריאנס
[00:20:12 - 00:20:15] היא בלתי תלויה,
[00:20:16 - 00:20:19] המשקנים האלה היא בלתי תלויים, זאת אומרת הקובריאנס הזה מכיל רק את האלכסון,
[00:20:19 - 00:20:23] אז הגודל של הקובריאנס הזה זה גם אותו גודל כמו של מיור.
[00:20:24 - 00:20:27] בכל פיקסל יש לי רק את הווריאנס שלו, אין קובריאנס.
[00:20:27 - 00:20:30] קובריאנס עליון אפס, יש לי רק את האלכסון של המדריצת קובריאנס,
[00:20:30 - 00:20:32] ‫ולכן גם את זה אפשר לייצר בתור תמונה.
[00:20:33 - 00:20:36] ‫הרבה פעמים יהיה לי אפילו את אותה רשת ‫שמייצרת שתי תמונות,
[00:20:37 - 00:20:39] ‫אחת זו תהיה ה-new והשנייה זה יהיה ה-sigma.
[00:20:41 - 00:20:44] ‫הרבה פעמים ה-output פה זה יהיה לוב סיגמה,
[00:20:45 - 00:20:48] ‫כי סיגמה חייב להיות חיובי, ‫אז הוא טרי כזה כדי לגרום את זה להיות חיובי,
[00:20:49 - 00:20:50] ‫אז מוציאים את לוב סיגמה,
[00:20:50 - 00:20:50] ‫ואז
[00:20:51 - 00:20:53] ‫מתאם את זה באקספוננט וזה יוצא כזה.
[00:20:55 - 00:20:56] ‫זה בסדר, הפרנדריזציה ברורה.
[00:20:57 - 00:21:00] ‫אוקיי, אז איך שאנחנו נעבוד.
[00:21:06 - 00:21:11] ‫בעיה שיש עם דאטה רציף ‫זה המשמעות של לוג-לייקליות.
[00:21:13 - 00:21:17] ‫אתם זוכרים במודלים מסקרטיים ‫שכמו שעבר דיברנו ‫מה המשמעות של לוג-לייקליות,
[00:21:17 - 00:21:24] ‫שאפשר לחשוב על זה ‫בתור כמה ביטים אנחנו צריכים ‫כדי לשדר את התמונה שלנו
[00:21:25 - 00:21:28] ‫בצורה הכי יעילה לפי החסמים ‫של תורת האינפורמציות.
[00:21:30 - 00:21:37] ‫זו פשוט האנטרופיה של הדאטה ‫תחת המודל שאנחנו חושבים,
[00:21:38 - 00:21:40] ‫לא תחת המודל האמיתי, ‫זה שנקרא קוס אנטרופי.
[00:21:42 - 00:21:49] ‫בדאטה רציף זו בעיה קצת שונה, ‫אז היא יותר מורכבת, ‫אי אפשר לעשות את הפרפרטציה הזאת.
[00:21:51 - 00:21:54] ‫יש כאן דוגמה, למשל, מה קורה עם... ‫אם יש פיקסל אחד שוטר,
[00:21:54 - 00:21:57] ‫תמיד שחור, למשל, לדאטה שלנו.
[00:22:00 - 00:22:07] ‫לכאורה הוא לא... ‫אנחנו לא צריכים לשדר אותו, נכון? ‫אפס ביטים, כי בצד השני כבר יודע ‫מראש שהוא שחור,
[00:22:07 - 00:22:13] ‫אבל כשאנחנו נבנה את המודל הזה, ‫אנחנו נבנה את זה ‫בצורה של למשל באוסיאן כזה.
[00:22:14 - 00:22:16] ‫מה זה האוסיאן הכי טוב ‫שיתפוס משהו שהוא תמיד אותו דבר?
[00:22:18 - 00:22:21] ‫מה יהיה הודעתיות שלו? ‫-בואי נצמח לך.
[00:22:22 - 00:22:23] ‫הווריאנט יהיה...
[00:22:23 - 00:22:27] ‫שואף לאפס, נכון? ‫אז מה יהיה הערך שם בקצה של ה...
[00:22:28 - 00:22:29] ‫קצה של הלוייאנט?
[00:22:29 - 00:22:30] ‫טוב, זה אין סוף.
[00:22:31 - 00:22:36] ‫אם אנחנו רוצים ללוג של פי של איקס, ‫כאילו אנחנו עומדים במרחב הלוגים,
[00:22:37 - 00:22:40] ‫שהוא שווה ללוג של פי של איקס אחד,
[00:22:40 - 00:22:45] ‫ועוד ללוג של פי של איקס שתיים,
[00:22:46 - 00:22:47] ‫עוד ככה של כל הפיקסלים,
[00:22:48 - 00:22:49] ‫אז אחד מהדברים פה יהיה פשוט אין סוף.
[00:22:50 - 00:22:56] ‫זה לא יעזור לנו, זה בטוח לא אומר ‫כמה פיקסלים אנחנו צריכים בשביל ה...
[00:23:01 - 00:23:04] ‫השידור זה לא הלוגייפלית, ‫זה המינוס של הלוגייפלית, כן?
[00:23:04 - 00:23:07] ‫אחד מהארכים פה יהיה אין סוף, ‫היא בעצם אומרת שהם צריכים ‫מינוס
[00:23:08 - 00:23:12] אינסוף פיקסלים בשביל לשדר את הדאטה.
[00:23:13 - 00:23:13] זה בעצם,
[00:23:16 - 00:23:18] ‫אם היינו מתעלמים ממנו, ‫באי זה היינו מסתדרים,
[00:23:18 - 00:23:22] ‫אבל התופעה הזאת לא קורית רק ‫כשיש פיקסל אחד שיש לו,
[00:23:24 - 00:23:26] ‫שהוא ודאי שאין לו סטוכסטיות,
[00:23:26 - 00:23:28] ‫אלא באופן כללי, אין פה,
[00:23:29 - 00:23:36] אין בעצם המשמעות בדאטה רציף ‫של האינפורמציה היא יחסית,
[00:23:37 - 00:23:38] ‫אין לה משמעות אבסולוטית.
[00:23:39 - 00:23:42] ‫אז צריך לעשות כל מיני טריקים ‫אם אנחנו רוצים להבין ‫מה המשמעות של הלוג-לייקליות.
[00:23:44 - 00:23:46] ‫בדרך כלל הטריקים שעושים בזמן האימון,
[00:23:47 - 00:23:48] ‫מגבילים את הווריאנס פשוט,
[00:23:49 - 00:23:58] ‫ווריאנס יכול להיות מינימום 0.01. ‫אז זאת אופציה אחת,
[00:23:58 - 00:24:03] ‫ובזמן אבלואציה, הטריק שעושים בפניים, ‫זה מוסיפים רעש,
[00:24:06 - 00:24:12] רעש אוניפורמי שמתאים לדרגת אפור אחת,
[00:24:13 - 00:24:14] ‫או לדרגה אחת של צבע,
[00:24:14 - 00:24:20] ‫שזה 1 חלקי 256. ‫הדאטה בעצם, גם אם אנחנו ‫מתייחסים אליו יותר 0 ל-1,
[00:24:22 - 00:24:26] ‫במקור הוא איכשהו נשמר על המחשב, ‫זה מזכיר אותי, כל דבר נשמר בצורה נזכרת על המחשב,
[00:24:27 - 00:24:32] ‫ולכן יהיו לו ערכים מדויקים ‫בדיוק על הבינים שבין 0 ל-1.
[00:24:32 - 00:24:37] ‫זה בדיוק יכול לגרום למודל כזה ‫קצת להשתגע ולתת בארץ ‫מאוד בנמוך
[00:24:37 - 00:24:38] סביב הערכים האלה.
[00:24:39 - 00:24:43] ‫אז לרוב, מה שאנחנו, לפעמים גם בזמן אימון, ‫עושים את זה, מוסיפים רעש
[00:24:44 - 00:24:48] בעצם כשהופכים את הדאטה לדאטה רציף, ‫דואגים שהוא באמת יהיה רציף ‫על ידי זה שמוסיפים רעש
[00:24:49 - 00:25:03] ‫שגורם לו לא להיות בדיוק על ה-256 בינים בין 0.5. ‫אבל לפעמים יספיק לעשות את זה ‫רק לאבנואציה, לא? ‫-אם מגבילים את הווריאנס, למשל, באימון, ‫זה מספיק, לא צריך לעשות את זה ‫גם באימון.
[00:25:06 - 00:25:12] אוקיי, אז עכשיו זה היה לגבי דאטה רציף. ‫עכשיו בואו נדבר על הגישה הזאת של דאטה,
[00:25:12 - 00:25:13] ‫של משתנה חבוי.
[00:25:14 - 00:25:16] ‫דיברנו על זה כבר בשיעורים הקודמים.
[00:25:17 - 00:25:22] ‫אז היינו רוצים איזה משהו כזה, ‫שנכון שיש לנו את הדאטה הזה, ‫כל הפיקסלים בתמונה,
[00:25:22 - 00:25:27] ‫יש איזה סט של משתנים חבויים, ‫בדרך כלל אנחנו נקרא להם Z, ‫בדרך כלל אנחנו נקרא X,
[00:25:28 - 00:25:32] ‫אז המשתנים החבויים האלה Z, ‫הם איכשהו תופסים ‫את הדברים גם המעניינים בתמונה.
[00:25:35 - 00:25:40] ‫איזה שהם פקטורים שהם גרמו, ‫אולי בצורה סיבתית, ‫ייצרו את התמונה שלנו.
[00:25:41 - 00:25:43] ‫למה אנחנו רוצים שזה יתפוס את הדברים האלה?
[00:25:46 - 00:25:51] ‫אמרנו שזה, המשמעות שלו, ‫זה פשוט משהו שעוזר לנו ‫לצמצם את מספר הפרמטרים.
[00:25:52 - 00:25:54] ‫לאו דווקא אנחנו רוצים שהוא יהיה בדיוק
[00:25:55 - 00:25:56] ‫דברים כאלה.
[00:25:57 - 00:25:57] ‫אז
[00:25:58 - 00:25:59] אנחנו באמת לא,
[00:26:00 - 00:26:04] ‫תכף אנחנו חושבים שאנחנו לא יכולים ‫לאלץ את הדברים האלה, ‫כי אין לנו גישה לדברים האלה בדרך כלל.
[00:26:04 - 00:26:07] ‫אנחנו רוצים ללמוד בצורה Unsupervised, ‫רק מתוך התמונות.
[00:26:07 - 00:26:23] ‫אבל יש בעצם שתי סיבות שהיינו רוצים. ‫קודם כול, זה די... ‫אם Z כן יתפוס את הדברים האלה, ‫זה יכול להיות די מועיל, ‫אחר כך אנחנו יכולים בהינתן תמונה, ‫אולי להגיד מה היה צבע השיער, צבע העיניים,
[00:26:24 - 00:26:25] ‫בצורה יחסית פשוטה.
[00:26:26 - 00:26:30] ‫אבל גם יש סיבות להאמין, ‫יש תחום שלם שנקרא מחקר...
[00:26:32 - 00:26:33] ‫Cosal inference,
[00:26:34 - 00:26:35] ‫העסקה סיבתית,
[00:26:36 - 00:26:40] ‫שאומר שכשאנחנו מפרקים את המודל ‫לפקטורים
[00:26:41 - 00:26:42] ‫שבאמת ייצרו את התמונה,
[00:26:43 - 00:26:48] ‫אז המודל הוא יותר יעיל. ‫זאת אומרת, צריך פחות פרמטרים, ‫יש יותר התלויות באמת בין הדברים,
[00:26:49 - 00:26:53] ‫ולכן אפשר לתפוס את הדברים ‫בצורה יותר יעילה ‫וגם ללמוד עם פחות דוגמאות.
[00:26:55 - 00:26:57] ‫אבל זה לא... בואו ניכנס לזה.
[00:26:57 - 00:27:04] ‫אוקיי, אז זה בגדול התמונה שצריכה ‫לתתת לכם בראש ‫למה היינו רוצים ‫של latent variable במודה לתפוס.
[00:27:05 - 00:27:08] ‫בפועל, איך אנחנו עושים את זה? ‫אנחנו עושים את זה ככה.
[00:27:09 - 00:27:10] ‫אז יש לנו את ה-Z שלנו,
[00:27:12 - 00:27:16] ‫במשפנים החבועים, ‫יש לנו X שזה התמונות,
[00:27:19 - 00:27:20] ‫זה נראה כאן כאילו זה משהו אחר,
[00:27:25 - 00:27:26] ‫כן, אני חושב שזה לא את הכוונה כאן,
[00:27:27 - 00:27:29] ‫זה הדאטה שלנו שהוא,
[00:27:29 - 00:27:31] ‫זה הפיקסלים של התמונות.
[00:27:31 - 00:27:35] ‫אנחנו לומדים איזושהי רשת נוירונים, ‫ביניהם בעצם.
[00:27:36 - 00:27:41] ‫בין המשתנה החבוי למשתנה ‫שאנחנו רואים אותו, לפיקסלים,
[00:27:42 - 00:27:43] ‫דרך הדבר הזה.
[00:27:44 - 00:27:45] ‫זה יהיה הרשת הזאת.
[00:27:47 - 00:27:49] ‫הרשת שתופסת את ההסתברות,
[00:27:50 - 00:27:52] ‫ההסתברות של כל פיקסל, ‫את ההתפלגות על כל פיקסל,
[00:27:53 - 00:27:56] ‫בעידן זה שהיא תופסת את התוחלת ‫ואת הוואריאנס לכל פיקסל.
[00:27:57 - 00:28:00] ‫-כן, אנחנו לא
[00:28:04 - 00:28:10] ‫נאלץ את המשתנים החבויים ‫להיות תחת איזשהו מבנה כזה ‫שאולי יש לו איזושהי משמעות.
[00:28:15 - 00:28:19] ‫יש כמה דברים לעשות את זה, ‫אנחנו נתחיל מהדרך הכי פשוטה,
[00:28:20 - 00:28:22] ‫שבה אנחנו עושים את ההנחות הבאות.
[00:28:24 - 00:28:25] ‫הפריור שלנו זה,
[00:28:26 - 00:28:29] ‫אנחנו נניח שאין לו פרמטרים נלמדים, ‫הוא פשוט
[00:28:32 - 00:28:34] פיקסל, הוא קבוע מראש
[00:28:34 - 00:28:38] ‫להיות משתנה דרסיאני,
[00:28:39 - 00:28:44] ‫מתוחלת אפס, וקצת קוווריאנס בלתי תלויה.
[00:28:44 - 00:28:45] ‫קוווריאנס שווה ל-I
[00:28:55 - 00:29:05] ‫בעצם אנחנו נמודדים את הבעיה ככה,
[00:29:06 - 00:29:07] ‫איש איקס
[00:29:08 - 00:29:23] ‫אז זה בעצם משהו שאומר שזה משתנה ‫במודלים כאלה חבויים,
[00:29:24 - 00:29:26] ‫ואפשר לכתוב את זה ככה גם.
[00:29:27 - 00:29:29] ‫X ונז,
[00:29:30 - 00:29:31] ‫ו-T שווה זה,
[00:29:34 - 00:29:36] ‫יש לנו איזשהו פריו על זה,
[00:29:37 - 00:29:40] ‫אז יש לנו משהו שלוקח את זה, ‫אנחנו נותנים אותו להתפלגות על X.
[00:29:42 - 00:29:43] ‫גם מרחובים היום
[00:29:44 - 00:29:45] ‫במודל הגרפי.
[00:29:46 - 00:29:47] ‫ז,
[00:29:48 - 00:29:48] X,
[00:29:50 - 00:29:51] ‫ז יש לנו פריו על ז,
[00:29:52 - 00:29:55] ‫ואז יש לנו משהו מיותר את X ביחד ז.
[00:29:56 - 00:29:58] ‫-Z הוא חבוי ו-X הוא פורם יותר.
[00:30:00 - 00:30:04] ‫אז זה מסומן בתור עיגול מלא.
[00:30:07 - 00:30:11] ‫אוקיי, אז ההנחה שלנו ש-P של Z הוא קבוע,
[00:30:12 - 00:30:14] ‫וש-P של X זה מה שכתבנו קודם.
[00:30:14 - 00:30:17] ‫זה יהיה איזושהי רשת
[00:30:17 - 00:30:18] ‫שלוקחת את Z,
[00:30:20 - 00:30:22] ‫מחשבת את התוחלת של Z,
[00:30:23 - 00:30:26] ‫לוקחת את Z ומחשבת את הסיגמה של Z,
[00:30:27 - 00:30:32] ‫והתוצאה נותנת לנו את ההתפלגות הגרסיאנית ‫שממנה אנחנו יכולים לדגום את X.
[00:30:34 - 00:30:35] ‫נגיד שהיה לנו כבר את המודל הזה,
[00:30:35 - 00:30:38] ‫שאימנו כבר, איך היינו דוגמים תמונה ‫מתוך המודל הזה?
[00:30:48 - 00:30:48] ‫יציאן?
[00:30:52 - 00:30:53] ‫איך בוחרים את Z?
[00:30:55 - 00:31:01] ‫מדומה ליטרלית מה-Pyor, כן? ‫אז בוחרים Z מתוך ה-Pyor הזה, ‫זה נותן לנו איזשהו וקטור, ‫מייד של Z הוא בגודל 100?
[00:31:02 - 00:31:04] ‫יש לנו את ה-100 המספרים האלה.
[00:31:05 - 00:31:08] ‫זה ה-Input שנכנס לנו לרשת נוירונים ‫שמחקתי פה.
[00:31:09 - 00:31:16] ‫רשת נוירונים מחשבת את השאוב שלה, ‫יוצא ה-Output שהוא כל התוחלות, את כל התמונה.
[00:31:17 - 00:31:21] ‫נגיד שזו תמונת M-List, ‫זה 28-28 תוחלות, 28-28
[00:31:24 - 00:31:26] ‫ווריאנסים או לוג של הווריאנסים.
[00:31:26 - 00:31:31] ‫זה נותן לנו מטריצת התפלגות גרסיאנית פדשה,
[00:31:31 - 00:31:34] שהיא בלתי תלויה. זאת אומרת, כל פיקסל אנחנו יכולים לדגום אותו,
[00:31:34 - 00:31:35] ‫בצורה בלתי תלויה,
[00:31:36 - 00:31:37] ‫בלתי תלויה.
[00:31:38 - 00:31:39] ‫בצורה בלתי תלויה, אוקיי?
[00:31:40 - 00:31:42] ‫אז אנחנו יכולים לעבור פיקסל-פיקסל,
[00:31:42 - 00:31:46] ‫נדגום אותו לפי התוחלת שלו והווריאנס שלו,
[00:31:47 - 00:31:48] ‫וזה נותן לנו תמונה, דגימה של התמונה.
[00:31:52 - 00:31:55] אוקיי, אז איך אנחנו יכולים, הדבר הזה, ‫החלק השני הזה, ‫קוראים לו לפעמים דקודר.
[00:31:56 - 00:31:58] הוא לוקח את ה-Z,
[00:31:59 - 00:31:59] הופך אותו ל-X,
[00:32:01 - 00:32:03] הוא רואה עם Z, זה הקוד שלנו בעצם,
[00:32:03 - 00:32:07] ‫אז אנחנו עוברים Z חדרה ל-X, ‫אנחנו עושים דקודינג.
[00:32:09 - 00:32:11] ‫אז זה בעצם, הדבר היחיד ‫שאנחנו רוצים ללמוד זה הדקודר הזה.
[00:32:13 - 00:32:22] אוקיי? זה הפרמטרים של הרשת הניורונים הזאת, ‫שהיא הופכת לנו את Z להתפלגות על X. ‫זה איך אפשר ללמוד את הדקודר הזה, ‫איך אפשר לאמן אותו?
[00:32:25 - 00:32:27] ‫יש לנו רק X'ים, כן? אין לנו Z'ים.
[00:32:28 - 00:32:31] אם היה לנו זוגות של Z'ים ו-X'ים, ‫אז היינו יכולים לעשות סופרווייז לרנינג רגיל.
[00:32:32 - 00:32:37] ‫זה הסוף של דקודר שאני מתכבד ללמוד איקסים, ‫דאי עם איזשהו פצעה ועדיצות מהפצעה,
[00:32:38 - 00:32:40] ‫בין הבדל בלמוד איך אפשר ללמוד איך אפשר ללמוד?
[00:32:41 - 00:32:43] ‫-זה יהיה מה שאנחנו נעשה באמת, בסופו של דבר.
[00:32:44 - 00:32:45] ‫כן.
[00:32:46 - 00:32:51] אבל כאילו הפיקרפיטר, יש כמה פתרונות. ‫באמת אנחנו רוצים לאמן,
[00:32:51 - 00:32:53] קודם כל אנחנו רוצים לעשות ‫מקסימום לייטליות,
[00:32:53 - 00:32:57] ‫אז נגיד שיש לנו לוב של ה-P של X, ‫אנחנו רוצים למקסם את זה,
[00:32:57 - 00:33:04] ‫אז זה אני זוכר שכתבתי למעלה, ‫אז זה לוב של האינטגרל הזה,
[00:33:06 - 00:33:07] ‫אבל באופן ישיר,
[00:33:09 - 00:33:10] ‫אנחנו לא יכולים לחשב,
[00:33:11 - 00:33:13] ‫בדרך כלל לחשב את האינטגרל הזה,
[00:33:14 - 00:33:16] ‫זה לא יהיה משהו שאנחנו יכולים לעשות,
[00:33:16 - 00:33:17] ‫לחשב אותו אנוליקרי.
[00:33:18 - 00:33:22] אין לנו אובייקטיב פונקשן ‫שאנחנו יכולים לחשב בצורה אנוליקרית.
[00:33:23 - 00:33:26] ויש מקרים שזה כן, ‫למשל ב-GMM שדיברנו זה כן,
[00:33:28 - 00:33:34] ‫אנליטי. למה? כי האינטגרל הזה ‫הוא בעצם סכום על מספר סופי של איברים.
[00:33:35 - 00:33:37] ‫ואז התחלנו לחשב אותו.
[00:33:38 - 00:33:43] ‫אפשר לעשות קרדיאנט דיסנט על GMM, ‫ראים שיש דרך יותר יעילה ‫להגמוד אותו עם EEM,
[00:33:44 - 00:33:50] ‫אבל אפשר לחשב את האינטגרל הזה ב-GMM. ‫יש אולי עוד כמה מודלים ‫שאפשר לחשב את זה,
[00:33:51 - 00:33:53] ‫אבל ברוב המודלים אי אפשר לחשב את זה.
[00:33:54 - 00:33:59] ‫וגם במודל שלנו, הוא פשוט, ‫אבל יש פה את הרשת טיורלין הזאת באמצע,
[00:34:00 - 00:34:04] ‫אז היא בעצם גורמת לזה ‫שאנחנו לא יודעים לחשב את האינטגרל הזה ‫בצורה סגורה.
[00:34:08 - 00:34:11] ‫אופציה אחת שאנחנו לא נעשה היום ‫זה לעשות מונטה קרלו.
[00:34:11 - 00:34:16] ‫כי יש לנו אינטגרל כזה ‫שאנחנו לא יודעים לחשב, ‫אנחנו יכולים לעשות דגימת מונטה קרלו,
[00:34:18 - 00:34:22] ‫שמה זה אומר? ‫זה אומר אנחנו דוגמים ‫מתוך הפריור של זה,
[00:34:24 - 00:34:26] ‫אז אנחנו בעצם מקרבים את האינטגרל הזה.
[00:34:28 - 00:34:33] ‫אנחנו מקרבים את האינטגרל הזה ‫על ידי זה שאנחנו דוגמים ‫הרבה פעמים מ-z,
[00:34:35 - 00:34:37] ‫ומחשבים את הממוצע של הדבר הזה.
[00:34:39 - 00:34:42] ‫נכון? כי אפשר לחשוב על הדבר הזה, ‫אם אני מסתכל רק על אינטגרל לפי z,
[00:34:43 - 00:34:44] ‫זה כמו תוחלת לפי p של z,
[00:34:45 - 00:34:46] ‫של הפונקציה הזאת,
[00:34:47 - 00:34:48] ‫אז אם מונטה קרלו אני יכול לשערך
[00:34:49 - 00:34:51] ‫תוחלות של פונקציות ‫על ידי זה שאני דוגם
[00:34:52 - 00:34:55] ‫מההתפלגות מספר מסוים של גימות, ‫ואתה עושה ממוצע.
[00:34:57 - 00:35:01] ‫אז אפשר לנסות לעשות את זה, ‫אבל זה לא יעבוד טוב ‫כשהדאטה הוא
[00:35:03 - 00:35:04] ‫במימד כל כך גבוה,
[00:35:05 - 00:35:10] ‫הווריאנס בעצם של המשארך הזה ‫הוא גבוה מדי.
[00:35:11 - 00:35:13] ‫אבל אפשר להיות גבוה מדי, ‫אז אנחנו לא נגיע למשהו שהוא...
[00:35:14 - 00:35:18] ‫זה יהיה בעצם לעשות גדיני ‫דיסנט על הדבר הזה, ‫זה יהיה מאוד מאוד רועש.
[00:35:22 - 00:35:26] ‫אוקיי, אז האופציה השנייה ‫שאנחנו נדבר עליה ‫זה לעשות את הגישה שדיברנו עליה כבר,
[00:35:27 - 00:35:33] ‫בתור אופציה אחרת ‫לחשב את האינטגרל הזה, ‫זה על ידי החסם הווריאציוני.
[00:35:33 - 00:35:40] ‫ווריאשן הבאונד, בעצם אנחנו לא משארכים ‫את האינטגרל הזה, ‫אנחנו חוסמים את הערך שלו מלמטה.
[00:35:42 - 00:35:43] ‫אז בואו נזוכר איך זה נראה.
[00:35:45 - 00:35:49] אוקיי, אז יש לנו פי איקס,
[00:35:49 - 00:36:01] ‫אנחנו יכולים לחשב אותו ככה במקור, ‫בלי מה שכתוב כאן, זה מה שכתוב שם, נכון? ‫זה אינטגרל של פי איקס, אה, סליחה, יש פה... ‫קחתי את זה מהשקפים שלנו,
[00:36:01 - 00:36:02] ‫שיהיו למקור אחר,
[00:36:03 - 00:36:05] ‫יש שם לייטנס ווואי.
[00:36:06 - 00:36:08] ‫יש פה גם אחר כך אייש, רגע, נכון מכאן.
[00:36:09 - 00:36:14] ‫לייטנס, אנחנו קוראים לו לפעמים ז, ‫לפעמים y ולפעמים h.
[00:36:16 - 00:36:17] אוקיי, אז תסלחו לי.
[00:36:19 - 00:36:33] ‫יש לנו עוד כמה דברים שאולי מבלבלים, ‫אני לא יודע אם כבר שמתם לב, ‫שכשיש לנו פרמטרים בהתפלגויות, ‫לפעמים אנחנו כותבים את זה פי פטא ל-איקס בינתי נזד נגיד,
[00:36:35 - 00:36:38] ‫ולפעמים זה פי ל-איקס בינתי נזד,
[00:36:40 - 00:36:40] ‫בפקודה פסית פטא.
[00:36:49 - 00:36:49] ‫בלאגן קצת.
[00:36:50 - 00:36:53] ‫כן, אז כמו y זה בעצם המשתנה החבוי שלי.
[00:36:54 - 00:36:59] ‫אז אוקיי, אז בלי הדבר הזה, ‫זה כמו שכתוב שם, נכון? ‫זה היה פשוט האינטגרציה
[00:37:00 - 00:37:02] ‫על כל ה-y מהקשרים.
[00:37:02 - 00:37:08] ‫עכשיו, אני יכול תמיד לחלק ולסבול ‫בפונקציה כל דבר,
[00:37:09 - 00:37:12] ‫בפרט בפונקציה של y ובפרט בהתפלגות על y.
[00:37:13 - 00:37:14] ‫זה מה שכתוב כאן,
[00:37:15 - 00:37:17] ‫זו מישהי התפלגות שאני גם רוצה ‫לאפשר לה להיות תלויה ב-x.
[00:37:18 - 00:37:20] ‫זה לא משנה, כל דבר שהייתי כותב כאן ‫זה היה נכון,
[00:37:22 - 00:37:23] ‫כי אני מכפיל על לחלק באותו דבר.
[00:37:24 - 00:37:30] ‫עכשיו, כשאני מסתכל על זה ככה, ‫אז אני יכול לחשוב על הדבר הזה ‫בתור תוחלת לפי ההתפלגות הזאת
[00:37:31 - 00:37:32] ‫של מה שנשאר, של זה וזה.
[00:37:33 - 00:37:35] ‫זה מה שכתוב כאן, ‫תוחלת לפי ההתפלגות הזאת,
[00:37:36 - 00:37:43] ‫בין P חלקי Q. ‫ואתם זוכרים איך חישבנו את החסם הוורייציוני? ‫במה השתמשנו? איזה חוק?
[00:37:44 - 00:37:45] איזה אי-שוויון?
[00:37:46 - 00:37:47] מה?
[00:37:48 - 00:37:48] ‫אנגון.
[00:37:49 - 00:37:53] ‫כן, קוראים לזה, בהקשר של תוחלת, ‫קוראים לזה ינסן, אי-שוויון ינסן,
[00:37:54 - 00:37:59] ‫שאומר שאנחנו יכולים, אם אנחנו, יש לנו לוג, ‫לוג של ההסתברות זה לוג של התוחלת,
[00:38:00 - 00:38:02] ‫זה גדול שווה מתוחלת של המון.
[00:38:02 - 00:38:03] ‫אנחנו יכולים להכניס את הלוג
[00:38:04 - 00:38:05] ‫בתוך הפונקציה.
[00:38:06 - 00:38:14] אוקיי? אז זה בעצם הדבר הזה נקרא החסם הוורייציוני-באונד,
[00:38:14 - 00:38:19] זה אבידנס לואוורבאונד והרבה פעמים משתמשים בקיצור אלבואו
[00:38:20 - 00:38:21] יש לי אבידנס לואוורבאונד
[00:38:25 - 00:38:26] זוכרים את זה? היה לנו את זה לפני
[00:38:30 - 00:38:31] שלושה שבועות
[00:38:32 - 00:38:32] אוקיי
[00:38:33 - 00:38:40] עוד משהו שאני מקווה שאתם זוכרים זה שאפשר לכתוב את החסם הזה בשלושת הדרכים האלה
[00:38:41 - 00:38:44] זה פשוט סידור אחר של האיברים בתוך הלוג אפשר להפריד את זה
[00:38:44 - 00:38:47] ל-X בינתן Y או Y בינתן X
[00:38:47 - 00:38:49] ואז נחשוב על זה בתור שני איברים שונים
[00:38:50 - 00:38:56] כל סידור אחר נותן לנו את אחד משלושת החסמים האלה מי שלא זוכר זה היה לנו את זה גם לפני שבועיים
[00:38:58 - 00:39:00] אז אחת מהדרכים זה מה שכתוב כאן
[00:39:02 - 00:39:05] של לוג פי-איקס, שזה מה שרצינו מלכתחילה לחשב
[00:39:06 - 00:39:07] פחות איזשהו KL דייברג'נס
[00:39:09 - 00:39:14] בין Q ל-Y ל-X, Q זה פשוט סתם ההתפלגות הזאת שהצענו
[00:39:15 - 00:39:20] ו-P זה הפוסטריאור האמיתי של Y בינתן X
[00:39:22 - 00:39:28] אז הצורה הזאת היא מדגישה את האופי של האלבוא בתור חסם
[00:39:29 - 00:39:36] בעצם זה מה שאנחנו היינו רוצים לחשב והחסם הזה הוא בטוח יותר קטן ממה שהיינו רוצים
[00:39:37 - 00:39:41] בגלל שאנחנו מורידים KL דייברג'נס שהוא תמיד מספר חיובי ומתי החסם הזה יהיה מדויק?
[00:39:41 - 00:39:44] כשה-Q הזה שאנחנו מציעים
[00:39:46 - 00:39:48] אם במקרה הוא שווה בדיוק
[00:39:48 - 00:39:50] לפוסטריאור לפי שווה בינתיים X
[00:39:51 - 00:39:56] אז החסם הזה יהיה הדוק, זאת אומרת הערך של האלבואו יהיה בדיוק הערך של מה שרצינו לחסום אותו
[00:39:57 - 00:40:03] זה ההסתברות של X, לוג היסטברות של X. אוקיי, זאת אחת מהצורות, צורה אחרת
[00:40:05 - 00:40:07] שאנחנו מגיעה אליה בסוף השיעור היום זה שההסתברות
[00:40:08 - 00:40:11] אפשר לכתוב את זה בתור תוחלת של לוג ההסתברות של X
[00:40:11 - 00:40:12] בהינתן Y,
[00:40:12 - 00:40:14] זה בדיוק הדקודר שלנו
[00:40:15 - 00:40:17] בהינתן ה-Latent,
[00:40:17 - 00:40:19] מה ההסתברות על הפיקסלים,
[00:40:20 - 00:40:24] אז התוחלת של הדקודר הזה, הדקודינג,
[00:40:24 - 00:40:30] פחות KL שהוא בין ה-Q הזה שאנחנו מציעים ל... מה זה הדבר הזה?
[00:40:32 - 00:40:34] P של Y זה כמו P של Z,
[00:40:35 - 00:40:36] לפריור על ה-Latent,
[00:40:38 - 00:40:38] אוקיי?
[00:40:39 - 00:40:40] אז זה גם הדרך אחת,
[00:40:41 - 00:40:46] וזאת הדרך שאנחנו נשתמש בה קצת אולי תראו אותה בפיתוחים,
[00:40:46 - 00:40:49] לוג של ההסתברות של X ו-Y המשותף, פחות
[00:40:50 - 00:40:55] האנתרופיה של Q. אוקיי.
[00:40:55 - 00:40:56] דן, למה זה אפליה?
[00:40:57 - 00:40:58] למה Q של Y זה אפליה?
[00:40:59 - 00:41:03] ככה Y זה בעצם זה, נכון? מה זה נגיד Q של Y?
[00:41:04 - 00:41:07] Q של Y, Q זה איזושהי פונקציה שאנחנו מציעים,
[00:41:09 - 00:41:10] אין לה עדיין משמעות,
[00:41:11 - 00:41:12] בפני עצמה,
[00:41:13 - 00:41:16] אבל היא מקווה, מה שאנחנו יכולים לראות כאן זה שה-Q האופטימלי,
[00:41:19 - 00:41:21] כדי שהדבר הזה יהיה שווה למה שאנחנו רוצים,
[00:41:21 - 00:41:22] זה ה-Posterior.
[00:41:23 - 00:41:26] אז הרבה פעמים קוראים ל-Q הזה ה-Posterior לעצמו,
[00:41:26 - 00:41:27] ה-Proximate Posterior.
[00:41:28 - 00:41:30] כי ה-Q הכי טוב זה Q שהוא שווה ל-Posterior.
[00:41:32 - 00:41:34] אבל הדברים האלה יכולים לכל Q, מה שכתוב כאן.
[00:41:35 - 00:41:36] Q יכול להיות כל דבר.
[00:41:37 - 00:41:38] זאת אומרת, צריך להיות כאיזושהי התפלגות של Y.
[00:41:41 - 00:41:47] אוקיי,
[00:41:48 - 00:41:54] אני חושב שזה כן חשוב להגיד את הקשר של הגישה הזאתי,
[00:41:55 - 00:41:57] של לחשב חישוב
[00:42:00 - 00:42:01] Variational Bounds ב-EM,
[00:42:02 - 00:42:03] אוקיי? אז
[00:42:05 - 00:42:09] דילגתי כאן על משהו, בעצם אנחנו עושים את ה-Variousional הזה, אנחנו רוצים
[00:42:11 - 00:42:15] לחשב את ה-T�חלת הזאת איכשהו, נכון?
[00:42:18 - 00:42:26] התוחלת הזאת זה בעצם שלב שני, או שלב פנימי בתוך זה מה שאנחנו רוצים לעשות בגדול, שזה לחשב את ה-P הכי טוב.
[00:42:27 - 00:42:31] אנחנו רוצים לעשות מקסימום לייקליוד על המודל שלנו, יש לנו כל מיני פרמטרים במודל,
[00:42:32 - 00:42:35] אנחנו רוצים לעשות את הפרמטרים שייתנו לנו את ה-Likely וכי טוב,
[00:42:36 - 00:42:37] אבל אנחנו לא יכולים לחשב אפילו את ה-Likelyוד,
[00:42:38 - 00:42:39] אז כדי לחשב את ה-Likelyוד אנחנו צריכים לעשות,
[00:42:40 - 00:42:45] לחשב את זה, ואנחנו מציעים לחשב את הדבר הזה בתור בעיית אופטימיזציה בפני עצמם.
[00:42:45 - 00:42:47] יש לנו בעיית אופטימיזציה בתוך בעיית אופטימיזציה.
[00:42:48 - 00:42:49] אז יש לנו,
[00:42:50 - 00:42:55] ה- maximum likely זה בעיית אופטימיזציה, ובתוך זה אנחנו צריכים לעשות inference,
[00:42:56 - 00:42:59] תכף אנחנו נראה, לכל נקודה בנפרד,
[00:43:00 - 00:43:03] או אנחנו נראה גם גישה שעושה את זה ביחד,
[00:43:03 - 00:43:05] לנקודות השונות.
[00:43:07 - 00:43:09] אז מה הקשר של הדבר הזה ל-EM?
[00:43:10 - 00:43:11] אז
[00:43:12 - 00:43:18] כשאנחנו רוצים לחשב את האלבו, אפשר לחשוב על דרך, לעשות אופטימיזציה, סליחה, לאלבו, אנחנו יכולים לעשות,
[00:43:19 - 00:43:22] יכולים לחשוב על הדרך הבאה לעשות את האופטימיזציה לאלבו,
[00:43:23 - 00:43:25] לעשות אופטימיזציה כל פעם,
[00:43:26 - 00:43:30] לעשות משהו איטרטיבי, וכל פעם לעשות אופטימיזציה למשהו אחר.
[00:43:31 - 00:43:33] זה נקרא Coordinate Assense, שאנחנו כל פעם,
[00:43:34 - 00:43:36] יש לנו כל מיני פרמטרים שאנחנו יכולים לשנות,
[00:43:36 - 00:43:37] אבל אנחנו לא משנים את כולם בבת אחת,
[00:43:37 - 00:43:40] אלא הפעם אחת האחרונה שאין להם חלק אחד,
[00:43:40 - 00:43:42] פעם שנייה חלק אחר של הפרמטרים.
[00:43:43 - 00:43:49] אז ההרצאה כאן זה לעשות אופטימיזציה של Q,
[00:43:50 - 00:43:52] ואז אופטימיזציה של הפרמטרים שלנו, של המודל קטע.
[00:44:01 - 00:44:04] נכנסת לטייל בו עם הפרמטרים בסדר פה.
[00:44:07 - 00:44:08] ‫כן.
[00:44:14 - 00:44:14] ‫בעצם ה-N ברור,
[00:44:15 - 00:44:15] אם זה
[00:44:37 - 00:45:06] ‫האלבו הזה המטרה שלנו זה בסופו של דבר,
[00:45:07 - 00:45:09] ‫למרסם את ה...
[00:45:15 - 00:45:18] ‫נדלק, יש לי את זה כמה סוגרות בהמשך, ‫אבל בסדר.
[00:45:19 - 00:45:20] אני אגיד את זה קודם בעל פה.
[00:45:21 - 00:45:25] ‫אנחנו רוצים גם לעשות אופטימיזציה ל-Q ‫וגם לעשות אופטימיזציה ל-T.
[00:45:26 - 00:45:28] ‫אופטימיזציה ל-Q, ‫למה אנחנו רוצים לעשות?
[00:45:28 - 00:45:36] ‫כי כשאנסה אופטימיזציה ה-Q הכי טוב, ‫ייתן לנו את הקירוב הכי טוב ל-objective function ‫המקורי שאנחנו רוצים, ‫שזה לוג של P של X.
[00:45:38 - 00:45:41] ‫זה מה שאומרים כאן. ‫אם נעשה אופטימיזציה ל-Q, ‫אז ה-Q הזה יקטן,
[00:45:42 - 00:45:45] ‫ונקבל את מה שאנחנו רוצים ‫במקור שיהיה ה-objective function.
[00:45:45 - 00:45:52] ‫ועכשיו, אם נעשה אופטימיזציה לתטה, ‫אז אנחנו נשפר את המודל שלנו ‫תחת ה-objective function שלנו.
[00:45:53 - 00:45:54] ‫נמצא פרמטרים יותר טובים של המודל.
[00:45:56 - 00:46:04] ‫אפשר לחשוב על האלבו הזה בתור פונקציה מעל ‫של שני הדברים האלה, ‫של הפונקציה Q ושל הפרמטרים תטא.
[00:46:05 - 00:46:08] ‫אנחנו רוצים לעשות אופטימיזציה ‫לפי שניהם.
[00:46:10 - 00:46:19] ‫הקשר ל-EM זה אם אנחנו חושבים ‫על לעשות אופטימיזציה 1-1. ‫קודם לפי Q ואז לפי תטא, ‫בצורה איטרטיבית, ‫לפי Q ולפי תטא. ‫לפי Q ולפי תטא.
[00:46:19 - 00:46:25] ‫אנחנו מקבעים את תטא ועושים אופטימיזציה לפי Q, ‫ואז מקבעים את Q ועושים אופטימיזציה לפי תטא.
[00:46:26 - 00:46:27] ‫זה נקרא קורדינט אסנט.
[00:46:28 - 00:46:31] ‫ואם בשלב שבו אנחנו עושים אופטימיזציה לפי Q,
[00:46:32 - 00:46:35] אנחנו יכולים לחשב את זה בצורה סגורה עד הסוף
[00:46:36 - 00:46:36] זאת אומרת
[00:46:37 - 00:46:39] מה ה-Q האופטימלי? ה-Q האופטימלי זה הפוסטריאור
[00:46:41 - 00:46:45] זה הפוסטריאור שפה הוא ב-H במקום Z או UI
[00:46:46 - 00:46:49] זה ההתפלגות של ה-Effedance בהינתן הדאטה שראינו
[00:46:50 - 00:46:55] אז אם אנחנו יכולים לחשב את הצעד הזה בצורה אופטימלית כל פעם
[00:46:56 - 00:46:57] אז הדבר הזה יוצא שקול ל-EM
[00:46:58 - 00:47:01] כי בעצם החישוב הזה של ה-Q האופטימלי
[00:47:02 - 00:47:06] מציב את הפוסטריאור בתוך הנוסחה הזאת של האלבום
[00:47:07 - 00:47:09] אתם יכולים להשוות את זה למה שראינו ב-EM
[00:47:10 - 00:47:14] ב-EM ה-E-Step זה היה לחשב את התוחלת
[00:47:14 - 00:47:16] לפי הפוסטריאור
[00:47:18 - 00:47:21] פוסטריאור שהיה תלוי בפרמטרים הקודמים של תטא
[00:47:22 - 00:47:25] של הפונקציה הזאת לוג פ של dh
[00:47:27 - 00:47:28] פחות
[00:47:29 - 00:47:31] לוג פ של h בהינתן
[00:47:31 - 00:47:37] זה הדבר הזה, תשוו את זה למה שכתוב כאן
[00:47:38 - 00:47:41] זה אותו דבר רק שפה כתוב
[00:47:42 - 00:47:44] הפוסטריאור במקום Q
[00:47:45 - 00:47:53] אבל ה-Q הכי טוב זה הפוסטריאור אז אם אני יכול לעשות את האופטימיזציה הזאת בצורה סגורה בכל איטרציה למצוא את ה-Q הכי טוב זה בעצם שקול ל-E-Step
[00:47:54 - 00:48:01] אוקיי, אז זה פה איך שנראה ה-E-Step וה-N-Step
[00:48:02 - 00:48:04] אז אם הייתי יכול לעשות את זה בצורה שקולה זה היה נראה ככה
[00:48:05 - 00:48:06] כשאני לא יכול לעשות את זה,
[00:48:07 - 00:48:08] את האופטימיזציה הזאת עד הסוף
[00:48:08 - 00:48:17] עדיין לפעמים כדאי לחשוב על זה בתור קורדינט אסנט כזה, אני קודם עושה אופטימיזציה לפי Q כמה שאני יכול, לפעמים אני לא יכול לעשות את זה למצוא את הפוסטריאור
[00:48:18 - 00:48:20] ואחר כך אני עושה אופטימיזציה לפי תטא
[00:48:21 - 00:48:24] אוקיי, אז הדבר הזה קוראים לו לפעמים Variational EM
[00:48:25 - 00:48:27] כי זה מאוד דומה ל-EM, אנחנו עושים את האופטימיזציות
[00:48:29 - 00:48:32] עושים שני צעדים שונים בכל איטרציה
[00:48:33 - 00:48:38] רק שאת הצעד הזה שקראו לו במקור E-Step אנחנו לא עושים עד הסוף, אנחנו לא בדיוק מחשבים את התוחלת
[00:48:39 - 00:48:42] אין לנו דרך לחשב את הפוסטריאור הזה כי זה לא,
[00:48:43 - 00:48:47] זה כבר, בניגוד למודלים כמו GMM אנחנו לא יכולים לחשב את זה בצורה יעילה
[00:48:48 - 00:48:50] אבל עדיין אולי אנחנו יכולים לעשות אופטימיזציה על Q
[00:48:51 - 00:48:52] ולשפר בקצת
[00:48:52 - 00:48:55] ש-Q יהיה קצת יותר קרוב, שה-KL שפה
[00:48:57 - 00:48:59] יהיה קצת יותר קטן
[00:49:02 - 00:49:02] אז
[00:49:03 - 00:49:07] פה פשוט לא כתוב תטא, אז אם אנחנו מסתכלים על מה שכתוב כאן אז אנחנו עושים,
[00:49:07 - 00:49:09] תחשבו על זה שאנחנו עושים את האופטימיזציה של האלגו הזה,
[00:49:10 - 00:49:12] פעם על ידי שיפור ה-Q בקצת
[00:49:13 - 00:49:14] ופעם על ידי שיפור ה-Teta
[00:49:15 - 00:49:17] כל הזמן אנחנו רוצים להגדיל את מה שכתוב כאן
[00:49:18 - 00:49:20] לפעמים אנחנו מחפשים Q שיגדיל את מה שכתוב כאן
[00:49:20 - 00:49:23] ולפעמים אנחנו מוציאים את תטע שיגדיל את מה שכתוב כאן
[00:49:34 - 00:49:37] אוקיי, זה היה אמור קצת לחבר אתכם ל-EM אבל
[00:49:38 - 00:49:42] קצת שנייה אתם עדיין לא מבינים מה הולך פה, אז בואו נחזור רגע לבעיה שלנו
[00:49:42 - 00:49:43] אולי אחר כך
[00:49:44 - 00:49:44] נצליח
[00:49:45 - 00:49:48] להבין מה היה החיבור הזה ל-EM שדיברתי עליו
[00:49:49 - 00:49:51] אוקיי, אז עוד פעם אנחנו רוצים לחשב
[00:49:51 - 00:49:52] את
[00:49:55 - 00:49:58] ה-Nightlylyde שלנו, לוג של P של X
[00:50:00 - 00:50:03] היינו רוצים לחשב את זה ויש פה איזה שהם פרמטרים תטא
[00:50:04 - 00:50:06] היינו רוצים למצוא את התטא שימקסימו את הדבר הזה
[00:50:07 - 00:50:09] נכון? זו המשמעות של מקסימום נייטליות
[00:50:10 - 00:50:12] אבל אנחנו לא יכולים לחשב את זה בצורה ישירה
[00:50:14 - 00:50:14] למה?
[00:50:14 - 00:50:18] בגלל שהמודל שלנו הוא מודל שבנוי ככה
[00:50:19 - 00:50:24] הוא דורש איזשהו אינטגרל כדי לחשב את הדבר הזה, אין לנו דרך לחשב את זה מתוך המודל
[00:50:25 - 00:50:26] המודל שלנו הוא בנוי פשוט על ידי
[00:50:28 - 00:50:30] P של X בהינתן זה,
[00:50:30 - 00:50:35] לזה יש לנו פרמטרים תטא כפול P של Z שזה אנחנו שאין לו פרמטרים
[00:50:37 - 00:50:38] וכדי לחשב מתוך זה
[00:50:39 - 00:50:43] את objective function שלנו אנחנו צריכים לעשות את האינטגרל הזה שאנחנו לא יודעים לעשות
[00:50:45 - 00:50:48] אוקיי עדיין אנחנו רוצים למקסם את מה שכתוב כאן
[00:50:48 - 00:50:51] אז האלבואו נותן לנו דרך למקסם את מה שכתוב כאן
[00:50:54 - 00:51:02] על ידי זה שאנחנו עושים אופטימיזציה של האלבואו עם איזושהי התפלגות שאנחנו מציעים
[00:51:03 - 00:51:03] Q
[00:51:05 - 00:51:09] יכול להיות ש-Q יותר קרוב לפוסטריור החסם הזה הוא יותר טוב
[00:51:09 - 00:51:11] אבל החסם הזה הוא נכון עבור כל Q
[00:51:15 - 00:51:20] בצורה קלאסית שעושים וראשונל אינפרנס לכל נקודה X אנחנו מוצאים Q משלו
[00:51:22 - 00:51:25] תכף תראו האלגוריתם שיסביר איך זה עובד
[00:51:26 - 00:51:34] אפשר לחשוב על התפקיד של Q בתור משהו שהופך את החץ הזה במודול הגנרטיבי
[00:51:36 - 00:51:37] בהינתן X
[00:51:38 - 00:51:39] הוא מוצא איזושהי התפלגות
[00:51:45 - 00:51:50] XI ויש לי כאן איזשהו QI שנותן לי את ההתפלגות
[00:51:51 - 00:51:53] על ה-ZD
[00:51:55 - 00:52:00] בשאיפה, ה-Q הזה יודע למצוא את ה-Z שיצר X
[00:52:01 - 00:52:05] זה המשמעות של הפוסטריור, בהינתן X מה ההתפלגות על זה?
[00:52:10 - 00:52:13] ב-EM היה לנו דרך לחשב את הדבר הזה בצורה יעילה,
[00:52:14 - 00:52:14] כן?
[00:52:14 - 00:52:16] אתם זוכרים ב-GML בראשה, והיה לנו דרך לחשב
[00:52:17 - 00:52:23] מתוך כל נקודה חדשה מה ההסתברות שהגיעה מכל אחת מהגרסיאניק שלו באתר עומת.
[00:52:24 - 00:52:25] זה היה בדיוק ה-Q הזה,
[00:52:26 - 00:52:26] ה-Q האופטימלי,
[00:52:27 - 00:52:30] זה דרך אגב לחשב את זה בצורה מדויקת בהינתן המודל שלנו כרגע.
[00:52:31 - 00:52:38] פה אנחנו לא יכולים לחשב את זה, אז אנחנו צריכים לעשות אופטימיזציה, איזושהי לולה פנימית שתעשו את אופטימיזציה על Q.
[00:52:40 - 00:52:41] כן, אז איך זה ייראה? זה ייראה ככה,
[00:52:42 - 00:52:47] אנחנו בכל איטרציה נגיד, אנחנו לוקחים איזושהי נקודה X מתוך הדאטה סט שלנו,
[00:52:49 - 00:52:53] רוצים לחשב איתה את הלוג פי X ולמקסם אותו ואנחנו לא יכולים,
[00:52:53 - 00:53:02] אז אנחנו עושים את השלולה הקטנה שעושה אופטימיזציה על הפרמטרים של Q. נגיד לכל XI אני עושה אופטימיזציה
[00:53:03 - 00:53:04] על הפרמטרים
[00:53:05 - 00:53:05] של ה-Q הזה,
[00:53:06 - 00:53:08] כדי שייתן לי איזשהו Q טוב,
[00:53:09 - 00:53:15] אז אני ממקסם את האלבו הזה ככה שרק על פני הפרמטרים של Q בהתחלה,
[00:53:16 - 00:53:18] כשהוא לאט לאט יתקרב
[00:53:19 - 00:53:19] למה שאני רוצה,
[00:53:20 - 00:53:22] ואז כשהוא סיים את האופטימיזציה,
[00:53:23 - 00:53:25] אז אני עושה אופטימיזציה על הפרמטרים,
[00:53:25 - 00:53:27] עושה איזשהו עדכון לתטא.
[00:53:31 - 00:53:34] זו הגישה הקלאסית של Variational Inference,
[00:53:34 - 00:53:41] שבה יש לנו לכל X אנחנו עושים אופטימיזציה לפרמטרים שלו.
[00:53:42 - 00:53:44] לכל X יהיה לו את ההתפלגות, את ה-Q שלו,
[00:53:44 - 00:53:48] שהוא הכי קרוב לפוסטריאור שלו, ככה גם עשינו ב-EM.
[00:53:48 - 00:53:52] לכל נקודה היה את ה-Responsיביליטיז שלה,
[00:53:52 - 00:53:59] היה את הסט של ההתפלגויות שאנחנו חושבים שכל אחד מהגאוסיאנים ייצר אותה.
[00:54:00 - 00:54:06] אוקיי, יש כאן איזה תרשים כזה שאולי מראה קצת יותר ברור.
[00:54:07 - 00:54:08] אז נגיד שיש לנו, זה הדאטה שלנו,
[00:54:09 - 00:54:10] איזה ה-Xים יושבים פה,
[00:54:11 - 00:54:14] יש להם התפלגות כזאת מאוד מורכבת, זה התפלגות של התמונות.
[00:54:14 - 00:54:16] זה לא התפלגות יפה כזאתי,
[00:54:17 - 00:54:21] משהו מאוד מורכב, אנחנו לא יכולים לתפוס אותו, אין לנו מודל טוב שתופס את זה בצורה ישירה.
[00:54:23 - 00:54:28] אבל המודל שלנו הוא מהכיוון הזה, אוקיי? הוא אומר, אוקיי, יש לנו איזשהו פריוב על זה,
[00:54:29 - 00:54:30] שוב, פריוב קבוע או אחיד,
[00:54:31 - 00:54:33] התפלגות גאוסיאנית, יחידה,
[00:54:34 - 00:54:37] וממנה אנחנו יכולים לחשב
[00:54:38 - 00:54:40] איזושהי התפלגות על X.
[00:54:41 - 00:54:43] אז בהינתן נקודה ב-Z,
[00:54:43 - 00:54:47] אז נגיד הנקודה הזאת אחראית על האזור הזה בהתפלגות של ה-Xים,
[00:54:48 - 00:54:50] על כל התמונות שנראות ככה,
[00:54:51 - 00:54:52] שנמצאות פה.
[00:54:54 - 00:54:55] אז זה מה שהדיקודר הזה עושה,
[00:54:55 - 00:55:04] הוא יודע לחשב את התוחלת של הדבר הזה ואת ה-Variances של כל הנקודות, וזה נותן גם פלגות שנראית יפה, אבל היא רלוונטית רק לאחד מה-Z'ים האלה.
[00:55:05 - 00:55:10] לחשב את תחת כל זה, היינו צריכים לעשות אינטגרל ובעצם לחשב את זה עבור כל ה-Z'ים שיש בו.
[00:55:12 - 00:55:17] אז זה המודל הגנרטיבי שלנו, ואיך אנחנו לומדים אותו?
[00:55:17 - 00:55:22] אז בעצם אנחנו מחליפים את הפריוב הגדול הזה, שאנחנו לא יודעים לחשב עליו שום דבר,
[00:55:23 - 00:55:25] באיזשהו posterior שהוא משהו קטן.
[00:55:26 - 00:55:26] אוקיי?
[00:55:27 - 00:55:29] אז בעצם המודל הזה כאן,
[00:55:29 - 00:55:40] אנחנו עושים אופטימיזציה לאיזשהו Q כדי שנמצא איזשהו posterior שאומר לנו מה האזור שאחראי על הנקודה הזאת.
[00:55:41 - 00:55:45] למשל כאן הוא טעה לגמרי, נכון? זה נגיד, אולי בתחילת האימון זה ייראה ככה.
[00:55:46 - 00:55:49] הנקודה שייצרה את הדאטה היא זאת,
[00:55:51 - 00:55:54] למרות שאין נקודה שייצרה את הדאטה, זה latency,
[00:55:55 - 00:55:59] זה לא משהו שבאמת קשור למשהו בייצור של הדאטה.
[00:56:01 - 00:56:07] אבל נגיד שאם היינו מריצים את המודל שלנו מהנקודה הזאת, היינו מקבלים משהו שנראה כמו הנקודה שאנחנו רואים.
[00:56:09 - 00:56:13] אבל ה-Q שלנו שעשינו לאופטימיזציה הגיע לאזור הזה.
[00:56:14 - 00:56:21] ובשאיפה היא שאחרי אופטימיזציה של Q הוא כן ידע למצוא את האזור היותר מתאים.
[00:56:26 - 00:56:36] אה, מה אתם אומרים על ההנחה הזאת ש-Q זה פשוט גאוסיאן, תגיד,
[00:56:36 - 00:56:39] Q זה גאוסיאן שאני מחפש לכל X איזשהו גאוסיאן
[00:56:41 - 00:56:41] שתופס את
[00:56:46 - 00:56:47] הפוסטריאור על Z.
[00:56:48 - 00:56:49] הטובה או לא טובה?
[00:56:56 - 00:57:14] כן, אז יש מקרים שזה בעייתי, אבל לרוב, בוא נאמר, בסדר ראשון של המודלים זה לא בעיה כל כך לאופטימית, כי באמת אם המטרה של Q זה להפוך את הדבר הזה ולמצוא, נגיד, את האזור שממנו ה-Z שייצר את הדאטה נמצא,
[00:57:15 - 00:57:19] אז בעצם מה זה אומר שאנחנו מדלים את זה בתאורי גאוסיאן? שאנחנו חושבים שהגענו לסביבה
[00:57:20 - 00:57:24] של הדבר הזה ושיש פחות או יותר נקודה אחת שיכולה לייצר את זה.
[00:57:25 - 00:57:28] מתי זה יכול להיות בעייתי אם לכל דוגמא יכול להיות כמה,
[00:57:28 - 00:57:30] נגיד גם Z1 פה וגם Z פה,
[00:57:31 - 00:57:35] כי שניהם יוצרים לנו אזור מאוד דומה כאן, שניהם מתמפים לתוך האזור מאוד דומה,
[00:57:36 - 00:57:37] אז אם יש כאלה דברים,
[00:57:38 - 00:57:39] אז זה יכול לעשות לנו בעיות.
[00:57:41 - 00:57:43] אבל הכיוון של ה-KL, אני לא זוכר אם אתם זוכרים,
[00:57:43 - 00:57:46] שדיברנו שלכיוון של QL יש משמעות אם הוא תופס
[00:57:46 - 00:57:48] אחד מהמודים או שהוא עושה ממוצע,
[00:57:48 - 00:57:51] אז פה הכיוון של QL הוא כזה שהוא תופס את אחד מהמודים,
[00:57:52 - 00:57:53] אז הוא פשוט יתפוס אחד מהם.
[00:57:53 - 00:57:55] אם יש כמה מודים, זה גם לא כל כך נורא.
[00:57:57 - 00:57:58] אז לרוב זה בסדר.
[00:58:01 - 00:58:08] באמת, כאילו כל מיני פיתוחים מאוחרים יותר של VE, אז כן מנסים לשכלל את זה, וכן אפשר להגיע לתוצאות יותר טובות, אבל
[00:58:11 - 00:58:14] בגרסאות המקוריות של המודלים פשוט משתמשים בגאוסיאן.
[00:58:16 - 00:58:17] אוקיי.
[00:58:24 - 00:58:24] למה?
[00:58:24 - 00:58:25] ל-P?
[00:58:25 - 00:58:33] נכון. אז Q צריך להיות כמה שיותר דומה ל-P של Z בהינתן X,
[00:58:35 - 00:58:40] וההנחה ש-P של Z בהינתן X היא גאוסיאן היא אולי לא כל כך נוראית.
[00:58:41 - 00:58:43] תלוי בדיוק במרחב של Z.
[00:58:44 - 00:58:49] נגיד ש-Z כן תפס משהו כמו מה שהיינו רוצים כזה בהתחלה, שזה כל ה... לא יודע.
[00:58:50 - 00:58:52] אם הפרצוף מחייך ומה הכיוון,
[00:58:53 - 00:58:59] אז בהינתן ש-אם בהינתן תמונה יש רק הסבר אחד כזה, נגיד, שמסביר את התמונה,
[00:59:00 - 00:59:05] אז להגיד שזה גאוסיאן זה אולי בסדר, כי זה בעצם אומר לתפוס,
[00:59:05 - 00:59:11] יכול, לפחות זה יכול, לתפוס את המקום הנכון עם איזושהי סביבה של רעש מסביב.
[00:59:12 - 00:59:20] אבל אם בעצם יכולות להיות שתי אפשרויות שהן מאוד שונות, בהינתן תמונה יכול להיות שזה מישהו מחייך בזווית כזאת,
[00:59:20 - 00:59:23] או מישהו שהוא כועס אבל בזווית אחרת,
[00:59:24 - 00:59:26] ואומר שצריך לתפוס כאילו שתי אפשרויות מאוד שונות,
[00:59:26 - 00:59:27] אז זה יכול להיות בעיה.
[00:59:30 - 00:59:32] ככל שהמודל שלנו יותר טוב,
[00:59:32 - 00:59:35] אז ככל שהמודל שלנו בהתחלה המודל עצמו הוא לא טוב,
[00:59:36 - 00:59:37] אז זה פוסטרניות תחת המודל,
[00:59:38 - 00:59:40] זה גם לא ברור אם זה גורם לזה דברים להיות יותר פחות
[00:59:44 - 00:59:45] דיונים מודליים כאלה,
[00:59:46 - 00:59:47] או שזה דווקא מחליק הכול,
[00:59:48 - 00:59:49] יש גם מחקר על הדברים האלה.
[00:59:50 - 00:59:52] זה נכון להגיד שבסוף האמון
[00:59:53 - 00:59:56] ה-Z-Z שעבר ראש וגורסיין הוא מאוד חדים,
[00:59:57 - 00:59:58] שבוע אחד מהם
[00:59:58 - 01:00:05] שם בידו פרסיין, כן? כן, אז המטרה היא בעצם שהדבר הזה,
[01:00:06 - 01:00:13] לפחות זאת המטרה, לא בטוח שזה קורה, אבל המטרה היא שהפריור של Z הוא קבוע בפני שלנו,
[01:00:14 - 01:00:15] הוא לא פשוט גורסיין,
[01:00:15 - 01:00:20] ובעצם אתה רוצה שהמודל שלך אחרי האימון,
[01:00:21 - 01:00:27] יהיה מודל כזה שכל נקודה היא בעצם, שכל הגאוסיין הזה מכוסה,
[01:00:27 - 01:00:29] ושכל נקודה יש לה,
[01:00:29 - 01:00:34] בעצם כיסית את הגאוסיין הזה עם הרבה גאוסיינים קטנים,
[01:00:35 - 01:00:36] שאין חורים באמצע.
[01:00:37 - 01:00:43] כי מה זה אומר שיש חור? זה אומר שכשאתה דוגם מתוך ה...כשאתה תדגום איזה נקודה, ובמקרה תגיע לחור הזה,
[01:00:43 - 01:00:45] זה יהיה אזור שאף פעם המודל לא רע.
[01:00:46 - 01:00:48] גאוסיין זה לא קורה,
[01:00:49 - 01:00:50] גאוסיין יכסה את הכול.
[01:00:51 - 01:01:01] כן, גם גאוסיין אחד זה יכסה הכול, אבל אתה רוצה שזה יכסה את הכול בצורה כזאתי שההתפלגות של סכום הגאוסיינים הזאת, אם אתה יכול לחשוב עליה בתור תערובת גאוסיינים, אם עשית את זה על הרבה איקסים,
[01:01:01 - 01:01:04] שהתערובת הגאוסיינים הזאת תקרב את הגאוסיין הזה.
[01:01:09 - 01:01:11] טוב, בואי נעשה הפסקה עכשיו של 10 דקות,
[01:01:11 - 01:01:15] ואז נדבר על אמורטייזד אינפלנטס.
[01:15:41 - 01:16:11] תודה רבה, אדוני.
[01:16:11 - 01:16:11] אדוני היושב-ראש,
[01:16:11 - 01:16:13] אדוני היושב-ראש,
[01:16:13 - 01:16:13] ‫של אמי?
[01:16:14 - 01:16:15] ‫זה פשוט דבר שונה,
[01:16:15 - 01:16:17] ‫זה פשוט דבר שונה.
[01:16:43 - 01:17:12] ‫אוקיי, אז אמרנו שברישן אינפרנס ‫בדרך כלל מה שעושים לכל נקודה X, ‫אנחנו עושים אופטימיזציה בנפרד ל-Q,
[01:17:14 - 01:17:18] ‫כדי למצוא את האוג'קטיב הכי טוב, ‫ואז
[01:17:18 - 01:17:22] עושים אופטימיזציה לתטא, נכון? ‫שתי לולות כאלה, אחת בתוך השנייה,
[01:17:23 - 01:17:25] ‫מה שמופיע פה.
[01:17:26 - 01:17:35] ‫לא, שאלו אותי קצת איך עושים את זה, ‫לא ניכנס לפרטים, ‫כי זה לא מה שנעשה עכשיו. ‫עכשיו אנחנו נעשה מה שנקרא אמורטייזד אינפרנס,
[01:17:35 - 01:17:38] ‫והסיבה היא שבעצם לעשות את הלולאה ‫הפנימית הזאת
[01:17:39 - 01:17:42] עבור כל נקודה שאנחנו רואים בדאטה-סט,
[01:17:42 - 01:17:54] ‫זה לא משהו שהוא יעיל בסט-אפ כזה ‫של Deep Learning, ‫שאנחנו רוצים לסקר על הרבה דאטה, ‫ואנחנו צריכים לעשות הרבה איטרציות ‫לעדכן את הפרמטרים שלנו.
[01:17:56 - 01:18:02] ‫אז הרעיון הוא במקום זה, ‫זה להחליף את הדבר הזה ‫בסט אחד של פרמטרים,
[01:18:03 - 01:18:08] ‫שהוא יהיה אחראי על ה-Q זה, ‫אז למפות בחדש את ה-X ל-Z.
[01:18:09 - 01:18:16] ‫במקום שיהיה לנו Q עבור כל אחד מה-Xים, ‫יהיה לנו Q שיהיה פונקציה של ה-Xים,
[01:18:17 - 01:18:23] ‫והוא יהיה, יהיו לו פרמטרים, ‫שלהם אנחנו נעשה אופטימיזציה,
[01:18:23 - 01:18:25] ‫אבל זה יהיה אותם הפרמטרים ‫זה כל הנקודות.
[01:18:29 - 01:18:34] ושוב, אנחנו נשתמש באותה שיטה ‫כדי לעשות פרמטריזציה, ‫ה-Z זה מרחב רציף,
[01:18:35 - 01:18:40] ‫אז ההיפלגות הזאת של Q של Z בהינתן X,
[01:18:41 - 01:18:44] ‫זה גם יהיה גרסיאן, ‫שהתוחלת שלו תהיה פונקציה של X,
[01:18:45 - 01:18:46] ‫איזושהי רשת נוירונים שהתוחלת שלה זה יהיה.
[01:18:47 - 01:18:50] ‫זה יהיה כל התוחלות של כל ה-Zים,
[01:18:52 - 01:18:54] ‫והווריאנס יהיה אלכסוני,
[01:18:55 - 01:18:59] ‫זאת אומרת, אנחנו נניח ‫שכל המימדים של Z יהיו בלתי תלויים אחד מהשני,
[01:19:00 - 01:19:03] ‫וכל אחד מהערכים שם גם יהיה התוצאה ‫של איזושהי
[01:19:04 - 01:19:10] רשת נוירונים שלוקחת את ה-X ‫ונותנת לנו את כל הווריאנסים ‫על כל ה...ממדים של Z.
[01:19:12 - 01:19:13] ‫אוקיי? אז Q זה גם יהיה,
[01:19:15 - 01:19:19] ‫אנחנו נעשה פרמטריזציה ל-Q על ידי רשת, ‫או מימוש של Q על ידי רשת נוירונים,
[01:19:19 - 01:19:26] ‫ולפרמטרים של הרשת הזאת ‫אנחנו נקרא FI. ‫זה פרמטרים של המודל הגנרטיבי, ‫אנחנו קוראים פתא,
[01:19:26 - 01:19:28] ‫ושל המודל ה...
[01:19:30 - 01:19:33] ‫ההופך את זה, ‫שהוא הולך מ-Z, מ-X ל-Z,
[01:19:34 - 01:19:45] ‫לפרמטרים שם נקרא FI. ‫אם נחזור לתרשים שנשארנו פה, ‫אז עכשיו החלפתי את מה שכתוב כאן ‫בפונקציה של פרמטרים FI,
[01:19:45 - 01:19:50] ‫פונקציה FI גדול של Z בינתיים X. ‫אז פה יש לנו דקודר,
[01:19:51 - 01:19:54] ‫אפשר לחשוב על הדבר הזה בתור דקודר, ‫על הדבר הזה בתור אנקודר.
[01:19:54 - 01:19:56] ‫אם זה הקוד שלנו,
[01:19:56 - 01:20:01] ‫יש לנו רשת אחת שהיא לוקחת נקודה פה ‫ונותנת לנו התפלגות
[01:20:02 - 01:20:04] ‫על הנקודות פה,
[01:20:05 - 01:20:10] ‫ויש לנו רשת אחרת שנקראת ‫אנקודר שלוקחת נקודה פה ‫ונותנת לנו התפלגות על הנקודות פה.
[01:20:12 - 01:20:18] ‫שתי הרשתות האלה נראות די דומה ‫בגישה הקלאסית, ‫כי יש כל מיני פיתוחים של זה, ‫שזה שונה,
[01:20:18 - 01:20:23] ‫שבין שתי הדוגמאות האלה, ‫בדוגמה שאני מדבר היום, ‫זה יהיה גאוסיאנים שניהם.
[01:20:24 - 01:20:25] זאת אומרת,
[01:20:26 - 01:20:31] ‫ההתפלגות המורכבת הזאתי, ‫אנחנו בעצם מנסים למדל אותה ‫על ידי הרבה גאוסיאנים קטנים,
[01:20:31 - 01:20:33] ‫שמגיעים מכל נקודה ב-Z.
[01:20:33 - 01:20:35] ‫ופה ההתפלגות הזאתי יחסית פשוטה,
[01:20:36 - 01:20:44] ‫אבל עדיין אנחנו ננסה, ‫במקום שעבור כל X אנחנו לא נדע בכלל ‫מתוך ה-Z הזה מאיפה ה-Z שלו הגיע,
[01:20:44 - 01:20:47] ‫אנחנו נאמן איזושהי רשת ‫שהמטרה שלה
[01:20:47 - 01:20:51] ‫זה למצוא איזשהו אזור ‫שכנראה משם ה-Z הביעו.
[01:20:52 - 01:20:57] נראה משם ה-Z שיצר את ה-X הגיע, אוקיי? ‫שני הדברים האלה זה רשת אחת,
[01:20:57 - 01:20:58] ‫פשוט מה שישתנה זה האינפוט של הרשת הזאת.
[01:20:59 - 01:21:01] ‫פה אני יכול לחשב את התוצאה הזאת,
[01:21:02 - 01:21:03] ‫לתת Zים שונים,
[01:21:04 - 01:21:06] ‫וכאן אני יכול לחשב Qים שונים,
[01:21:06 - 01:21:09] ‫כל אחד על סמך Xים שונים,
[01:21:09 - 01:21:10] ‫זה מה שישתנה.
[01:21:12 - 01:21:15] ‫אם מסתכלים שוב על ה-Alvo, ‫המטרה של ה...
[01:21:20 - 01:21:21] טוב, נדבר על זה תכף.
[01:21:22 - 01:21:25] ‫אוקיי, אז זה Decoder ו-Encoder.
[01:21:26 - 01:21:34] ‫אם בוא נסתכל רגע על איך נראה ‫האימון של ה-Alvo, אוקיי? ‫אז יש לנו... זה ה-objective function שלנו, שוב, ‫זה חד-מדרכי לכתוב את ה-Alvo.
[01:21:35 - 01:21:37] ‫עכשיו יש פה... זה כתוב פה עם כל ה...
[01:21:38 - 01:21:39] פרמטרים, אוקיי?
[01:21:39 - 01:21:44] ‫אז יש לנו את המודל הגנרטיבי שלנו,
[01:21:45 - 01:21:47] ‫פה כתוב X פסיק Z, ‫אבל אפשר לפרק אותו
[01:21:47 - 01:21:51] להתפלגות על Z, ‫וההתפלגות של XP נתן Z,
[01:21:52 - 01:21:59] ‫אוקיי? ול... זה יש פרמטרים תטא. ‫אנחנו הנחנו שהחלק של ה-Priar של Z, ‫אין לו פרמטרים,
[01:21:59 - 01:22:01] ‫אבל אפשר גם שם לשים פרמטרים,
[01:22:02 - 01:22:04] ‫שנתמדים תוך כדי.
[01:22:05 - 01:22:06] ‫אז זה המודל יותר הרגנרטיבי.
[01:22:07 - 01:22:07] ‫ו-Q,
[01:22:08 - 01:22:13] שהוא בעצם המודל שמחשב את הכיוון ההפוך,
[01:22:14 - 01:22:19] ‫הוא מופיע פעמיים, הוא מופיע גם פה בפנים, ‫וגם הוא ההתפלגות שעליה ‫אנחנו עושים את התוחלת.
[01:22:22 - 01:22:27] ‫בסדר, אז זה הלוס שלנו עבור נקודה פת, ‫עבור נקודה X.
[01:22:30 - 01:22:32] ‫יש פה שני סטינגים של פרמטרים, ‫תטא ופי.
[01:22:34 - 01:22:35] ‫זה בסדר?
[01:22:35 - 01:22:36] עכשיו,
[01:22:45 - 01:22:45] כן, אוקיי, אז
[01:22:46 - 01:22:54] ‫דרך אחרת לכתוב את ה-Lבואו, ‫ראינו את זה גם קודם, נכון? ‫אנחנו נחשוב על זה בתור ה-Marginal,
[01:22:56 - 01:22:57] ‫לייטליות של X,
[01:22:58 - 01:23:02] ‫ואם הייתה לנו גישה לזה, ‫אז זה היה הכי טוב, ‫זה מה שהתחלנו ממנו,
[01:23:03 - 01:23:10] ‫ועוד כדייוורגנטס בין ההתפלגות ‫שאנחנו לומדים אותה גם, ‫עם הפרמטרים C,
[01:23:11 - 01:23:14] ‫ל-Posterior שנובע מהמודל שלנו.
[01:23:16 - 01:23:17] ‫-Posterior של ממהפה חזרה מ-Z ב-X.
[01:23:19 - 01:23:22] ‫שוב, מה זה הדבר הזה אומר? ‫אמרנו את זה קודם, ואז עושים את זה שוב,
[01:23:23 - 01:23:24] ‫שמה ה-Q הכי טוב?
[01:23:25 - 01:23:30] אם ה-Q הכי טוב היה יודע ‫בדיוק למצוא את ה-Posterior של המודל שלנו,
[01:23:30 - 01:23:39] ‫אז זה היה מצמצם את ה-KL Divergence הזה, ‫והיינו פשוט עושים, אם היינו עושים אופטימיזציה ‫לפי תטא, זה היה יוצא שאנחנו עושים אופטימיזציה ‫בדיוק ל-objective שאנחנו רוצים.
[01:23:40 - 01:23:41] ‫כל עוד זה לא קורה,
[01:23:41 - 01:23:44] ‫אז אנחנו לא עושים אופטימיזציה ל-objective שלנו,
[01:23:44 - 01:23:47] ‫אלא לאיזשהו חסם תחתון ‫של האובייקטיב שלו.
[01:23:48 - 01:23:51] ‫עדיין, אם אנחנו משפרים ‫את החסם התחתון,
[01:23:52 - 01:23:53] ‫זה אומר שאנחנו...
[01:23:55 - 01:24:04] ‫זה לא אומר כלום, אבל לפחות אנחנו יודעים ‫שמה שאנחנו רוצים לעשות לאופטימיזציה ‫הוא יותר גדול מאיזשהו ערך שהולך ועולה.
[01:24:06 - 01:24:10] ‫אוקיי, אז יש לנו איזה משהו, ‫היינו רוצים להגדיל אותו,
[01:24:11 - 01:24:13] ‫אנחנו לא יודעים, ‫אנחנו יודעים להגדיל ‫איזשהו חסם תחתון.
[01:24:14 - 01:24:15] ‫אנחנו מגדילים אותו לכאן.
[01:24:15 - 01:24:16] ‫אוקיי, הגענו לכאן.
[01:24:17 - 01:24:22] ‫אז כבר אנחנו יודעים, למשל, ‫שאנחנו יותר טובים מכל האזור הזה, ‫אנחנו לא יודעים בדיוק איפה אנחנו כאן,
[01:24:23 - 01:24:23] ‫אבל
[01:24:26 - 01:24:28] עדיין זה נותן לנו איזושהי אינפורמציה ‫על
[01:24:29 - 01:24:32] מה ש...על האובייקטיב המקורי ‫שהיינו רוצים לעשות.
[01:24:33 - 01:24:35] ‫ואם אנחנו יכולים לעשות מקסום טוב ל-Q,
[01:24:36 - 01:24:40] ‫אז אנחנו יודעים שאנחנו בעצם, ‫הדבר הזה, זה בדיוק המרחק הזה,
[01:24:41 - 01:24:43] ‫בין מה שהיינו רוצים לעשות אופטימיזציה,
[01:24:43 - 01:24:45] ‫ולא שלחנו אותה פרויקט סימברנטיביזם.
[01:24:48 - 01:24:53] ‫אז אפשר לחשוב על ה-KL הזה ‫בתור שני מרחקים שונים. ‫אחד זה המרחק בין ה-Q,
[01:24:54 - 01:24:57] ‫שקוראים לו לפעמים ‫אפרוקסימט פוסטריאור,
[01:24:57 - 01:25:02] ‫או לפעמים קוראים לו פוסטריאור נטוורף, ‫כי זה פשוט רשת נורליים ‫שמחשבת את ההתפלגות הזאת,
[01:25:03 - 01:25:04] ‫לפעמים קוראים לזה אנקודר,
[01:25:05 - 01:25:07] ‫לפעמים קוראים לזה אינפרנס נטוורק,
[01:25:07 - 01:25:09] ‫אתם תראו את כל השאלות האלה, ‫אינפרנס נטוורק,
[01:25:13 - 01:25:24] ‫משהו, היה עוד אחד. ‫אז ה-KL הזה בעצם, ‫הוא תופס שני דברים, ‫אחד זה המרחק בין ה-Q ‫והפוסטריאור האמיתי,
[01:25:25 - 01:25:30] ‫והשני זה המרחק בין האלבו, ‫מה שאנחנו עושים לו מקסימיזציה,
[01:25:31 - 01:25:32] ‫מה שהיינו רוצים לעשות במקסימיזציה.
[01:25:40 - 01:25:41] ‫אוקיי, אז כשאנחנו עושים אופטימיזציה,
[01:25:43 - 01:25:45] ‫של הדבר הזה,
[01:25:46 - 01:25:49] ‫על פני הפרמטרים תטא ו-Fי,
[01:25:50 - 01:25:52] ‫אנחנו בעצם משפרים את זה,
[01:25:53 - 01:25:55] ‫כשאנחנו עושים אופטימיזציה על-פי,
[01:25:55 - 01:25:59] ‫אנחנו משפרים את המרחק הזה,
[01:26:00 - 01:26:05] ‫כאילו, כשאנחנו עושים אופטימיזציה על תטא, ‫אנחנו מעלים את הכול.
[01:26:08 - 01:26:11] ‫אז אנחנו בעצם, יש לנו דרך ככה,
[01:26:14 - 01:26:20] ‫לקוות, שום דבר לא מובטח, נכון? ‫כי יכול להיות שעלינו פה משהו כאן גדל, ‫אבל זה גרום דווקא למשהו כאן כאן,
[01:26:22 - 01:26:23] ‫בתוך הגט הזה.
[01:26:24 - 01:26:30] ‫לא מובטח לנו שום דבר על ה-P של תטא, ‫אבל אנחנו יכולים לקוות ‫שהנקסום הזה של ה-Bounds
[01:26:32 - 01:26:35] ‫ימקסם גם את מה שאנחנו רוצים במקור.
[01:26:39 - 01:26:43] ‫אוקיי, אז זה מה שעושים, זה מה שאנחנו נעשה. ‫עכשיו בוא נראה איך אנחנו יכולים לעשות את זה.
[01:26:43 - 01:26:45] ‫את המקסימיזציה של ה-Album,
[01:26:45 - 01:26:47] ‫אוקטימיזציה על הפונקציה הזאת.
[01:26:48 - 01:26:50] ‫אז יש לנו פה שני פרמטרים ‫שאנחנו צריכים לעשות עליהם אופטימיזציה,
[01:26:51 - 01:26:51] ‫תטא ופי.
[01:26:53 - 01:26:59] ‫אוקיי, אז אופטימיזציה לפי תטא, ‫אנחנו רוצים לעשות הכול עם gradient descent, אוקיי? ‫אנחנו רוצים לעשות הכול עם gradient descent, אוקיי? ‫כל זה רשתות ניורונים בסופו של דבר,
[01:27:00 - 01:27:03] ‫וזה הדרך הכי יעילה ‫שאנחנו מכירים לעשות באופטימיזציה.
[01:27:04 - 01:27:05] ‫אז אנחנו רוצים לעשות gradient descent,
[01:27:06 - 01:27:08] אנחנו עושים לך שאתה נגזרת לפי תטא ולפי פי
[01:27:09 - 01:27:10] זה הדבר הזה.
[01:27:10 - 01:27:15] אז נגזרת לפי תטא זה לא כל כך בעיה, אוקיי? אז יש לנו נגזרת
[01:27:16 - 01:27:16] של
[01:27:17 - 01:27:18] תוחלת
[01:27:19 - 01:27:19] של משהו,
[01:27:20 - 01:27:21] נכון? ואנחנו יכולים,
[01:27:22 - 01:27:25] זה הכול פוליניארי, אז אנחנו יכולים להכניס את הנגזרת לתוך התוחלת,
[01:27:26 - 01:27:28] נכון? ויש לנו תוחלת
[01:27:28 - 01:27:31] של נגזרות,
[01:27:32 - 01:27:33] והתוחלת הזאת היא לפי q,
[01:27:35 - 01:27:37] אז איך אנחנו יכולים לשערך את
[01:27:39 - 01:27:40] הנגזרת הזאת?
[01:27:41 - 01:27:42] תוחלת של הנגזרת?
[01:27:46 - 01:27:49] כשיש לנו Objective Function שהוא מורכב מתוחלת, אז בדרך כלל מה אנחנו עושים?
[01:27:54 - 01:27:58] כן, אנחנו לוקחים דגימות מתוך ההתפלגות הזאת,
[01:27:59 - 01:28:02] מחשבים את הנגזרת הזאת על הדגימות וממצאים.
[01:28:02 - 01:28:07] אוקיי, אז פה זה כתוב עם דגימה אחת,
[01:28:07 - 01:28:08] אוקיי? אפשר לעשות את זה עם דגימה אחת.
[01:28:11 - 01:28:15] זה בדיוק אותו שיעור כשאנחנו עושים מיני-בצ'ים במקום להסתכל על כל ה-train.
[01:28:17 - 01:28:19] בעצם פה זה הכול כתוב על x1,
[01:28:20 - 01:28:22] אבל יש פה עוד איזה תוחלת שהיא על כל ה-x'ים גם של הטרנינגסים.
[01:28:23 - 01:28:29] אז בדיוק באותו אופן אנחנו מכניסים את התוחלת לתוך ה-nגזרת לתוך התוחלת, ולוקחים רק דגימה אחת
[01:28:30 - 01:28:31] לתוך התוחלת הזאת.
[01:28:31 - 01:28:35] אז בעצם, והחלק השני כאן הוא לא תלוי בתטא,
[01:28:35 - 01:28:37] יש כאן רק חלק אחד שתלוי בתטא.
[01:28:38 - 01:28:43] אז בעצם כדי לגזור את האובג'קטיב הזה לפי תטא,
[01:28:44 - 01:28:45] מה נצטרך לעשות? בהינתן x,
[01:28:46 - 01:28:47] נצטרך לחשב את q
[01:28:48 - 01:28:49] ל-z בהינתן x,
[01:28:50 - 01:28:51] לקחת דגימה,
[01:28:52 - 01:28:52] כמה דגימות,
[01:28:53 - 01:28:55] לחשב את התוחלת של x,
[01:28:56 - 01:28:58] דגימה זאת אומרת, זה נותן לי z מסוים,
[01:28:59 - 01:29:02] ועכשיו אני יכול לחשב את ההתפלגות, יש לי גם את ה-x וגם את ה-z,
[01:29:02 - 01:29:03] אני יכול לחשש
[01:29:04 - 01:29:06] התפלגות של x ו-z תחת המודל שלי,
[01:29:07 - 01:29:12] ולחשב נגזרת של z, נכון? זה כמו סופרוויזר לרנן, יש לי את האינפוט של הרשת,
[01:29:12 - 01:29:13] יש לי את הארטפוט,
[01:29:14 - 01:29:15] אני מחשב את האינפוט,
[01:29:16 - 01:29:21] יצא בין זה גאוסיאן, נכון? אז ה-likely-out זה פשוט יהיה השגיאה הריבועית בין הארטפוט שיצא לי
[01:29:23 - 01:29:24] בתוחלת למה שיש באמת,
[01:29:25 - 01:29:29] ולזה אני עושה backpropagation דרך הרשת הזאת של התוחלת,
[01:29:29 - 01:29:31] בשביל הרשת של הגאוסיאן זה יוצא משהו אחר,
[01:29:33 - 01:29:35] הרשת של ה-co-veriance זה יוצא משהו אחר,
[01:29:36 - 01:29:37] אבל גם דומה,
[01:29:38 - 01:29:45] זה מתחיל פשוט מהמגזרת של ה-lighting לפי ה-veriance,
[01:29:46 - 01:29:50] זהו, אוקיי, אז ככה אני מחשב את זה בשביל תטא.
[01:29:52 - 01:29:52] זה ברור?
[01:29:55 - 01:29:56] תכף נראה את התרשים שמראו את הכל, אבל
[01:29:58 - 01:30:01] ניקח את ה-X ונחשב את ההתפלגות על Z,
[01:30:02 - 01:30:03] בואו נענות את התרשים הזה כאן,
[01:30:06 - 01:30:07] ניקח X והדאטה,
[01:30:07 - 01:30:12] נחשב את ההתפלגות על Z, נותן את ההתפלגות הזאת, דוגם להתפלגות הזאת, דוגמה,
[01:30:14 - 01:30:15] עכשיו יש לי Z ויש לי X,
[01:30:16 - 01:30:16] ואני אומר מה,
[01:30:17 - 01:30:20] אני מריץ את ה-Z, זה נתן לי נגיד איזשהו X אחר, אני מחשב את ה-loss,
[01:30:21 - 01:30:22] זה נותן לי את ה...
[01:30:23 - 01:30:27] את הגרדיאנטים שאני צריך, בשביל תטא מופיע פה.
[01:30:29 - 01:30:32] אוקיי, אז זה החלק הקל, מה קורה עם פי?
[01:30:32 - 01:30:34] איך אני מחשב את הגרדיאנטים בפי-פי?
[01:30:40 - 01:30:44] הבעיה כאן זה שפי מופיע פעמיים, הוא לא כמו תטא שמופיע רק פה,
[01:30:45 - 01:30:48] פי מופיע גם פה, בתוך הפונקציה שאני אעשה עליה תוכלת,
[01:30:49 - 01:30:51] אבל גם בתוך ההסתברות,
[01:30:52 - 01:30:54] שאיתה אני מחשב את התוכלת.
[01:30:56 - 01:31:00] אני לא יכול להכניס פשוט את הגרדיאנט כאן, כי בעצם דילגתי על משהו שהוא תלוי בפי,
[01:31:01 - 01:31:05] אני אשנה את פי, זה לא רק לשנות פה את הדברים האלה לפנים, זה גם לשנה איך אני עושה את התוכלת הזאת.
[01:31:07 - 01:31:07] אז
[01:31:08 - 01:31:11] אני לא יכול לעשות כבר את זה, זאת אומרת, אני לא יכול לקרב את
[01:31:15 - 01:31:17] הנגזרת של האינטגרל הזה,
[01:31:17 - 01:31:20] לנגזרת של התוכלת הזאת, על ידי איזושהי תוכלת של נגזרת,
[01:31:21 - 01:31:22] ועל ידי דגימה מתוך התוכלת.
[01:31:24 - 01:31:27] תהליך הדגימה עצמה גם תלוי בפרמטר שאני רוצה לעשות באופטימיזציה.
[01:31:29 - 01:31:30] מה אפשר לעשות?
[01:31:32 - 01:31:33] אז יש שתי שיטות,
[01:31:33 - 01:31:35] שיטה של לא להשתמש בה, שאני אעבור על זה קצרה,
[01:31:35 - 01:31:40] ושיטה של להשתמש בה, שאני אעבור על זה קצת יותר. שיטה שלא נשתמש בה נקראת סקור פונקשן אסטימטור.
[01:31:41 - 01:31:48] אולי נתקלתם בזה בקורס של למידה מחיזוקים, זה גם מה שמשתמשים שם,
[01:31:48 - 01:31:53] עוד שם של זה זה likelihood ratio estimator או reinforce שזה האלגוריתם הראשון שאני אשתמש בזה.
[01:31:54 - 01:31:56] והרעיון הוא כזה,
[01:31:57 - 01:31:57] זה בעצם
[01:32:01 - 01:32:03] אם יש לי תוכלת של איזושהי פונקציה,
[01:32:04 - 01:32:07] כשהתוכלת היא על פני איזושהי התפלגות
[01:32:08 - 01:32:11] עם פרמטרים של PM אני רוצה לעשות את הנגזרת,
[01:32:12 - 01:32:14] אני יכול לעשות
[01:32:15 - 01:32:21] כן, אז זה הדבר, ואני יכול להגיד שהכתוב פה הטובריק עצמו.
[01:32:21 - 01:32:24] הנגזרת הוא שאם הנגזרת של לוג
[01:32:24 - 01:32:25] של ההסתברות
[01:32:27 - 01:32:29] שווה לאחד חלקי
[01:32:33 - 01:32:33] ההסתברות,
[01:32:34 - 01:32:36] כפול הנגזרת של ההסתברות, נכון? יש לי
[01:32:39 - 01:32:40] נגזרת של לוג
[01:32:41 - 01:32:44] פרמטרופולוגיה, פרמטרופולוגיה,
[01:33:09 - 01:33:10] פרמטרופולוגיה,
[01:33:10 - 01:33:13] של הדבר הזה לפי Q של F,
[01:33:14 - 01:33:15] אני יכול
[01:33:40 - 01:33:46] לפתחים את זה, אז מה שכתוב בתוכלת זה אינטגרל Q פי
[01:33:47 - 01:33:48] באיזושהי אינפולוגיה
[01:34:10 - 01:34:13] אין פה ההוכחה של זה כבר, אבל זה לא כתוב
[01:34:15 - 01:34:17] אתה זוכר את זה מ-reinforce-converning?
[01:34:36 - 01:34:39] זה מה שכתוב כאן, פתחתי פשוט את התוכלת הזאת
[01:34:40 - 01:34:59] זה לא מגזרת של לוג, זה מה שכתוב כאן, זה שווה לפי החוקים של מגזרת של לוג
[01:35:10 - 01:35:40] ‫אז אני מקבל אינטגרל של נגזרת של Q
[01:35:41 - 01:35:43] ‫זה עד שאני יודע שאם אני הופך לסדר,
[01:36:05 - 01:36:08] ‫בדעת תוכלת זה מה שכתוב נתן צנות, ‫זו ההגדרה של התוכלת.
[01:36:09 - 01:36:13] ‫אז זה טריק כזה, אם אני פשוט עובר ‫למרחב הלוג,
[01:36:14 - 01:36:18] ‫אני יכול, פשוט הנגזרת של תוכלת נשארת עדיין,
[01:36:19 - 01:36:21] ‫אני הופך את נגזרת של תוכלת ‫לתוכלת של נגזרת.
[01:36:23 - 01:36:26] ‫אני צריך לשנות קצת את הפונקיה ‫שבפנים כדי שזה יתקיים.
[01:36:27 - 01:36:29] ‫למה זה טוב לי? ‫כי שוב, עכשיו יש לי,
[01:36:29 - 01:36:31] ‫הנגזרת המקורית שלי היא תוכלת
[01:36:32 - 01:36:36] של משהו שאני צריך לזור רק את הבפנים שלו,
[01:36:36 - 01:36:40] ‫אז אני יכול לשרך את זה ‫על ידי דגימת מונטקאוו.
[01:36:40 - 01:36:42] ‫אני אקח לדגימה אחת מתוך התוכלת הזאת,
[01:36:43 - 01:36:44] ‫ולחשבת הנגזרת עליה.
[01:36:46 - 01:36:51] ‫אז זה טריק שמשתמשים בו הרבה ב-Enforcement Learning,
[01:36:51 - 01:36:56] ‫אבל גם במודל גנרטיבי ‫יש כל מיני מודלים שמשתמשים בזה.
[01:36:56 - 01:36:57] ‫אנחנו לא נשתמש בזה.
[01:36:58 - 01:36:59] ‫אז אם לא הבנתם כלום, אז לא נורא,
[01:37:01 - 01:37:03] ‫אבל כדאי לכם להסתכל על זה, ‫זה טריק כן של אנושי.
[01:37:06 - 01:37:09] ‫אז כשאנחנו נשתמש, בואו נקרא,
[01:37:10 - 01:37:12] ‫צריך להגיד אותו, ‫reparמטריזיישן.
[01:37:13 - 01:37:17] ‫-נאי, אתה יכול להסביר עוד פעם ‫למה עכשיו אתה פשוט בבינטניקס,
[01:37:18 - 01:37:22] ‫ולא כאילו למה ביקור של זל בינטניקס ‫ולא ביקור של זל בינטניקס ולא ביקור של זל?
[01:37:23 - 01:37:23] ‫איפה?
[01:37:24 - 01:37:30] ‫בעמודה הקודמת, רשמו שהם דומים עם פיור של זל בינטניקס,
[01:37:30 - 01:37:35] ‫אבל פה יצא אחרי ההבדיל של זל. ‫-אה, לא כתבתי תמיד, ‫אפן איקס בכל מקום, סתם לא נראה לי להקפיד את זה.
[01:37:36 - 01:37:41] ‫-אתה יכול לחזור על השווה האחרון, ‫למה אין לנו תוחלת? ‫השווה בערך?
[01:37:42 - 01:37:43] ‫זה יש יורו ומונטקרלו.
[01:37:43 - 01:37:51] ‫יש לי תוחלת לפי איזשהו משהו של משהו. ‫עכשיו הנגזרת היא רק בפנים. ‫הנקודה היא שכאן זה כבר...
[01:37:52 - 01:37:57] ‫הבעיה אצלנו במקום השני, ‫רצינו לגזור במשהו שהתוחלת הייתה על פי.
[01:37:58 - 01:38:02] ‫עכשיו, אנחנו רואים שזה שווה למשהו ‫שבו התוחלת היא רק בפנים, בתוך התוחלת.
[01:38:03 - 01:38:05] ‫סליחה, הנגזרת היא רק בפנים, ‫בתוך התוחלת.
[01:38:06 - 01:38:13] ‫בזה אין בעיה לשעריך ‫על ידי מונטקרלו, שזה מה ש... ‫-אה, אז הסימן הזה הכוונה לש... ‫כן, זה פשוט דגימה אחת ‫של מה שכתוב פה בפנים.
[01:38:14 - 01:38:19] ‫בתוחלת של זה, ‫זה בערך שווה לדגימה אחת. ‫היהיה חלק שאתם כתוב בכלל חלקי ארץ, סכום,
[01:38:19 - 01:38:20] ‫לפני כמה כאלה.
[01:38:21 - 01:38:28] בסדר. ‫-אז הטריק שאנחנו כן נשתמש בו ‫נקרא re-parמטריזיישן,
[01:38:28 - 01:38:34] ‫והרעיון הוא לפרק בתוך ה... ‫בתוך המודל הזה, Q שתלוי בפי,
[01:38:35 - 01:38:38] ‫לפרק את שני מרכיבים, ‫אחד זה המרכיב שמביא את הסטוכסטיות,
[01:38:39 - 01:38:42] ‫השני זה המרכיב שמביא את הפרמטרים ‫שאנחנו רוצים ללמוד.
[01:38:43 - 01:38:47] ‫ויש הרבה מודלים שאפשר ממש לפרק את זה ‫לשני ערוצי חישוב נפרדים,
[01:38:48 - 01:38:56] ‫ואז בעצם אין לנו את הבעיה הזאתי, ‫אנחנו יכולים לעשות פשוט gradient descent ‫דרך הערוץ הווטרמיניסטי ‫שמוביל לפרמטרים,
[01:38:57 - 01:39:03] ‫והרעש הוא פשוט משהו שנכנס מהצד, ‫הסטוכסטיות זה משהו שנכנס מהצד. ‫אני מקווה שתכף תבינו את זה.
[01:39:03 - 01:39:06] ‫אז הרעיון הוא ככה, אם יש לנו מודל ‫שאנחנו יכולים
[01:39:07 - 01:39:10] ‫לדגום אותו ככה, אוקיי? ‫אנחנו רוצים לשנות את Q,
[01:39:11 - 01:39:13] ‫לכתוב את Q בצורה כזאתי,
[01:39:13 - 01:39:16] ‫שזה, זה בעצם איזושהי פונקציה,
[01:39:16 - 01:39:20] ‫דטרמיניסטית של Epsilon, Fי, ‫זה הפרמטרים כאן ב-X,
[01:39:21 - 01:39:24] ‫זה ה-input שלה, ‫אם זה רשת נוראי למשל, זה ה-input שלה,
[01:39:25 - 01:39:28] ‫אנחנו רוצים שזה יהיה פונקציה ‫דטרמיניסטית של הדברים האלה,
[01:39:28 - 01:39:36] ‫כאשר Epsilon הוא המשתנה הקראי פה, ‫שאין לו פרמטרים שאנחנו רוצים ללמוד.
[01:39:38 - 01:39:41] ‫אז הרעיון, אם אנחנו יכולים לפרק את זה ככה,
[01:39:42 - 01:39:43] ‫אז התוחלת
[01:39:44 - 01:39:45] שראינו קודם,
[01:39:45 - 01:39:48] ‫אנחנו יכולים לכתוב אותה בתור תוחלת ‫על פני ההתפלגות של Epsilon,
[01:39:49 - 01:39:51] ‫אז כל הפרמטרים רק מופיעים בתוך התוחלת.
[01:39:53 - 01:39:53] ‫תיכף נראה את זה.
[01:40:02 - 01:40:06] מה שאמרתי כאן זה ברור, ‫מה אני רוצה לעשות? ‫אם יש לי איזושהי פונקציה התפלגות,
[01:40:07 - 01:40:08] ‫אני רוצה לכתוב ככה את ה-Z,
[01:40:10 - 01:40:13] ‫דרך לדגום Z לתוך ההתפלגות הזאת, ‫זה יהיה איזושהי פונקציה דטרמיניסטית
[01:40:14 - 01:40:20] ‫שמקבלת את הפרמטרים ואת ההסטרוכסטיות בנפרד.
[01:40:22 - 01:40:28] ‫אז אני יכול לחשב את הנגזרת של זה ‫לפי הפרמטרים F,
[01:40:29 - 01:40:31] ‫והתוחלת תהיה על פני Epsilon.
[01:40:33 - 01:40:34] ‫זאת אומרת, אני מקווה שעוד רגע זה יהיה ברור.
[01:40:38 - 01:40:42] ‫אוקיי, אני רוצה לחשב את הנגזרת של התוחלת ‫של הדבר הזה, לפי F,
[01:40:43 - 01:40:44] ‫על איזושהי פונקציה.
[01:40:45 - 01:40:47] ‫אני יכול לחתור עכשיו, אם ה-A...
[01:40:50 - 01:40:54] ‫אוקיי, אז אם אני יכול לפרק את זה ‫כמו שכתבתי קודם, ‫אז אני יכול לכתוב את זה ככה.
[01:40:54 - 01:40:57] ‫הנגזרת על תוחלת לפי Epsilon,
[01:40:58 - 01:41:00] ‫זה המשתנה האקראי שלי בעצם עכשיו, ‫לא Z, Epsilon,
[01:41:01 - 01:41:03] ‫וז Z הוא איזושהי פונקציה של Epsilon.
[01:41:04 - 01:41:07] ‫Z יהיה מחושב על ידי הפונקציה הזאת ‫שתוארנו קודם.
[01:41:08 - 01:41:10] ‫ואז ברגע שאם זה המצב,
[01:41:11 - 01:41:13] ‫אז אין לי בעיה יותר, ‫כי הבעיה,
[01:41:13 - 01:41:19] ‫הדבר שהייתה לי קודם זה ש-P הופיע לי פה למטה ‫בסאבסקריט של האקספוננט,
[01:41:20 - 01:41:22] ‫זה של האקספקטיישן של התוחלת.
[01:41:23 - 01:41:31] ‫ברגע שזה לא מופיע ו-P הוא רק פה, ‫בתוך הזדר ובתוך הפונקציה הזאת, ‫אז אין לי בעיה, ‫אני יכול לעשות חישוב ‫מוטה קרלו כמו קודם, ‫לדגום דגימה אחת,
[01:41:32 - 01:41:34] ‫כי התהליך הזה של הדגימה ‫הוא לא תלוי בפרמטרים.
[01:41:34 - 01:41:35] ‫אני לא צריך
[01:41:36 - 01:41:38] לחשב את התדכון ‫על התהליך של הדגימה עצמה.
[01:41:39 - 01:41:42] ‫אז הפרדתי בין התהליך של הדגימה ‫לפרמטרים עצמם.
[01:41:44 - 01:41:46] ‫עוד שנייה, אני מקווה שזה יותר ברור.
[01:41:58 - 01:42:00] טוב, אוקיי, נעבור על מה שכתוב כאן. אוקיי, ‫אז איך זה ייראה ב-Albo?
[01:42:03 - 01:42:07] ב-Albo זה ייראה ככה. ‫אני מזכיר, הבעיה שלנו, ‫אנחנו רוצים לדגום.
[01:42:08 - 01:42:09] ‫אנחנו רוצים לחשב את הדבר,
[01:42:10 - 01:42:13] ‫אני חושב, נגזרת של הדבר הזה,
[01:42:14 - 01:42:15] ‫של התוחלת הזאת.
[01:42:15 - 01:42:21] ‫נגזרת לפי תטא אמרנו שאין לנו בעיה, ‫אבל נגזרת לפי P, הסתבכנו, ‫כי הפי לובוק מופיע גם פה וגם פה.
[01:42:22 - 01:42:25] ‫אנחנו אומרים, אוקיי, ‫אם אנחנו הולכים לפנות את ה-Albo בצורה הזאתי,
[01:42:25 - 01:42:29] ‫זאת אומרת שהנגזרת תהיה לפי איזשהו Epsilon, ‫ש-Epsilon יהיה משהו שאין לו פרמטרים,
[01:42:30 - 01:42:32] ‫לפחות לא פרמטרים ‫שאנחנו ממש מנסים ללמוד,
[01:42:33 - 01:42:42] ‫ועכשיו זה נשאר אותו דבר, ‫רק שכל מקום שכתוב כאן Z, ‫האם צריכים בעצם לדגום את ה-Z, ‫או לשקלל את ה-Z לפי התוחלת הזאת,
[01:42:43 - 01:42:45] ‫אנחנו מחליפים את ה-Z בפונקציה הזאתי,
[01:42:46 - 01:42:47] ‫שהיא פונקציה של ה-Epsilon ‫שמגיעה למקום.
[01:42:48 - 01:42:50] ‫והפונקציה הזאת כן תלויה בפרמטרים.
[01:42:51 - 01:42:54] ‫ועכשיו מה שקורה זה שהפרמטרים ‫נמצאים פה ופה.
[01:42:54 - 01:43:02] ‫אז זה תלוי בפי בצורה ישירה, ‫אבל גם זה תלוי בפי כי ה-Z הזה ‫הוא פונקציה של Epsilon ‫שתלויה בפרמטרים.
[01:43:03 - 01:43:05] ‫ברגע שהפרמטרים נמצאים רק בתוך
[01:43:07 - 01:43:09] ‫מה שהתוחלת מחושבת על גביו,
[01:43:10 - 01:43:13] ‫אז אין בעיה לעשות מונטי קו-סמפטינג, ‫אז אני דוגם את Epsilon,
[01:43:15 - 01:43:17] ‫מחשב את Z מתוך Epsilon,
[01:43:19 - 01:43:23] ‫ומחשב את ה-objective function ‫ומחשב את הנגזרת.
[01:43:24 - 01:43:32] ‫תשימו לב משהו שלא ברור כאן, ‫זה שכשאני אחשב את הנגזרת, ‫זה לא יהיה לא רק האיבר הזה ישתתף, ‫אלא גם האיבר הזה ישתתף, ‫כי פה, בתוך זה,
[01:43:33 - 01:43:34] ‫מתחבא גם הפרמטרים האלה.
[01:43:41 - 01:43:45] ‫אז זה איך שאני אשתמש, ‫בואו נראה ספציפית ‫איך זה ייראה לגאוסיאנים, ‫שזה הדרך הסטנדרטית לעשות את זה.
[01:43:46 - 01:43:48] ‫וגאוסיאנים זה מאוד פשוט, אוקיי? ‫אז Q,
[01:43:50 - 01:43:53] זה מה שעד עכשיו היה לנו. ‫ה-Q שלנו היה גאוסיאן,
[01:43:54 - 01:43:58] ‫לפני Z, בהינתן X. ‫איך הוא היה מחושב?
[01:43:59 - 01:44:08] ‫זה מה שכתוב כאן, כן? ‫הוא היה עושה לנו לפני Z. ‫בהינתן... כשהתוחלת והווריאנס שלו ‫מחושבים על ידי איזושהי רשת, אוקיי? ‫הרשת הזאת יש לה פרמטרים C,
[01:44:08 - 01:44:10] ‫שהיא מקבלת בתור input את X,
[01:44:11 - 01:44:14] ‫והארטפוט שלה זה התוחלות, ‫כל הפיקסלים,
[01:44:14 - 01:44:17] ‫זה לא פיקסלים, סליחה, ‫כל הממדים של Z,
[01:44:17 - 01:44:19] ‫והווריאנסים לכל הממדים של Z,
[01:44:20 - 01:44:20] ‫הפועלות של הווריאנסים.
[01:44:22 - 01:44:23] ‫ואז זה, לא משנה,
[01:44:24 - 01:44:27] ‫בגלל שזה דייג, ‫אנחנו יכולים לחשב על זה ‫בתור מחשבלם.
[01:44:28 - 01:44:33] זה לא פריטי. ‫אבל זה המידול שלנו של Q, ‫איך אנחנו עושים את ה-reparמטריזיישן על הדבר הזה?
[01:44:34 - 01:44:35] ‫אז בעצם,
[01:44:36 - 01:44:38] ‫אפסילון שלנו יהיה גאוסיאן, ‫אבל בלי פרמטרים.
[01:44:39 - 01:44:44] ‫זה פשוט גאוסיאן קנוני, 0 ו-I,
[01:44:45 - 01:44:48] ‫תוחלת 0 וכל באזן ספי I, היחידה.
[01:44:49 - 01:44:52] ‫הרשת שלנו תהיה ביום אותה רשת ‫כמו קודם,
[01:44:52 - 01:44:53] ‫היא מקבלת X,
[01:44:54 - 01:45:02] ‫היא נותנת לנו את התוחלת של Q ‫ואת ה-sיגמא של Q. ‫אז פה נמצאים כל הפרמטרים שלנו.
[01:45:02 - 01:45:04] ‫עכשיו, איך אנחנו דוגמים Z?
[01:45:05 - 01:45:07] ‫אנחנו פשוט לוקחים את מU שיצא כאן,
[01:45:08 - 01:45:16] ‫ומוסיפים לו את הרש שהיה לנו, ‫כפול ה-sיגמא, כל מימד של ה-ε, ‫אנחנו מכפילים אותו ב-sיגמא.
[01:45:17 - 01:45:19] ‫אז יש לנו כאן דרך לייצר את Z,
[01:45:20 - 01:45:23] ‫בלי שהפונקציה... ‫אנחנו כאילו מוציאים את
[01:45:23 - 01:45:25] ‫הקטע ההסתברותי,
[01:45:25 - 01:45:29] את הקטע הסטרוכסטי, ‫מתוך הפונקציה עם הפרמטרים.
[01:45:31 - 01:45:35] ‫יש כאן דרך גרפית ‫שגם עוזרת קצת ל-OART הזה, אוקיי? ‫אז אם במקור היה לנו X,
[01:45:36 - 01:45:39] ‫ה-input של הרשת שלנו, ‫ב-P זה הפרמטרים של הרשת,
[01:45:40 - 01:45:42] ‫הם מייצרים לנו את F, ‫אבל לא יכלנו לעשות
[01:45:42 - 01:45:49] Back Propagation דרך זה, ‫כי ה-Node הזה בחישוב ‫זה בעצם נוד שדוגם מתוך Q. ‫אנחנו לא יודעים איך לעשות
[01:45:50 - 01:45:52] Back Propagation דרך דגימה.
[01:45:53 - 01:45:59] ‫אז כאן זה מראה איך אנחנו יכולים ‫בעצם לעשות Back Propagation דרך דגימה, ‫וזה בדיוק כמו שאמרנו קודם.
[01:46:00 - 01:46:04] ‫יש לנו, תחשבו, על הדבר הזה תחשבו עליו ‫בתור הגרף של החישוב, ‫מה שקורה
[01:46:08 - 01:46:14] ‫בכל החבילות, נגיד, ה-Tensor Flow, ‫בונים איזה גרף חישוב, ‫ולפיו עושים את ה-Back Propagation.
[01:46:15 - 01:46:16] ‫אז אם יש לנו פה את...
[01:46:17 - 01:46:19] כאן אנחנו לא יודעים ‫לעשות חישוב דרך הדרג הזה,
[01:46:19 - 01:46:23] ‫אבל פה בעצם הפרדנו את הסטרוכסטיות ‫החוצה,
[01:46:24 - 01:46:29] ‫אז משתנה הסטרוכסטיות, ‫ה-Epsilon הזה זה איזשהו input נוסף ‫שהרשת הזאת מקבלת.
[01:46:30 - 01:46:33] ‫אז הרשת הזאת מקבלת את V ואת X פה קודם, ‫אבל היא גם מקבלת את Epsilon,
[01:46:34 - 01:46:36] ‫זה איזשהו משתנה גאוסיאני ‫בלי פרמטרים.
[01:46:36 - 01:46:41] ‫ועכשיו, בהינתן ה-Output, ‫אנחנו יכולים לחשב ‫ב-Back Propagation של הדבר הזה, ‫כי מה שכתוב כאן זה רק
[01:46:42 - 01:46:44] משהו מאוד פשוט, אוקיי? ‫זו הנוסחה הזאת.
[01:46:45 - 01:46:49] ‫וכדי לחשב גרדיאנטים לפי ניו,
[01:46:50 - 01:46:55] ‫זה לא בעיה להכניס את זה, ‫כאילו פשוט יש כאן סכום של דברים, ‫אז הדבר הזה נופל, לגמרי,
[01:46:56 - 01:46:57] ‫וכדי לחשב גרדיאנטים לפי סיגמא,
[01:46:58 - 01:47:03] ‫אז זה נופל ונשאר רק הערך של סיגמא, ‫זה בעצם מה שמתווסף, קודם,
[01:47:04 - 01:47:05] ‫לחישוב גרדיאנט.
[01:47:05 - 01:47:15] ‫ברור ה-Reparמטריזיישן.
[01:47:17 - 01:47:19] ‫מה הייתה המטרה? שוב, אני מזכיר,
[01:47:20 - 01:47:25] ‫המטרה היא לחשב את הנגזרת של הדבר הזה,
[01:47:26 - 01:47:34] ‫ולהכניס את הנגזרת פנימה, ‫כדי שנוכל לחשב את זה ‫על ידי זה שנדגום בשיטת מונטה קרלו ‫מתוך המקלפת תוחלת הזאת.
[01:47:36 - 01:47:36] ‫אוקיי,
[01:47:38 - 01:47:43] ‫אז זהו, אנחנו מוכנים עכשיו ‫למה שנקרא ורישנל אוטו-אנקודר, V-A-E.
[01:47:44 - 01:47:49] ‫יש פה את האבסודוקוד, ‫תכף נראה את הרשים שיותר יותר עוזר,
[01:47:49 - 01:48:00] ‫אבל בואו נראה רגע האבסודוקוד. ‫יש לנו דאטה-סט, יש לנו איזושהי פרמטריזציה ‫מתחול של רשת שבינתי איקס, ‫נותנת לנו את ההתפלגות על Z,
[01:48:01 - 01:48:02] ‫בדרך כלל זה יהיה האוסיין,
[01:48:02 - 01:48:06] ‫ואז הרשת הזאת תיתן לנו את התוחלות ‫ואת הווריאנס של כל מימד ב-Z.
[01:48:07 - 01:48:10] ‫יש לנו את המודל הגנרטיבי, ‫שהוא בהינתן,
[01:48:12 - 01:48:14] ‫כזה שהוא יכול לשאול גם את הפריור,
[01:48:15 - 01:48:18] ‫אבל בעיקר זה יהיה הרשת של X בהינתן Z,
[01:48:19 - 01:48:20] ‫שזה יהיה גם,
[01:48:21 - 01:48:23] ‫בדוגמה הקלאסית זה יהיה גרסיאן,
[01:48:24 - 01:48:25] ‫שמקבל את Z בתור אינפוט,
[01:48:26 - 01:48:29] ‫והארטפוט שלו זה כל התוחלות והווריאנסים של X.
[01:48:30 - 01:48:32] ‫אנחנו רוצים לאמן את המודלים האלה,
[01:48:33 - 01:48:35] ‫אוקיי? התוצאה תהיה בעצם הפרמטרים,
[01:48:36 - 01:48:37] ‫בטא ופילט של המודלים האלה.
[01:48:39 - 01:48:44] ‫איך זה עובד? אנחנו עושים SGD. ‫מה זה SGD? זאת אומרת שאנחנו כל פעם ‫לוקחים
[01:48:45 - 01:48:48] מיני-באץ' אקראי מתוך הדאטה-סט שלנו, ‫של X'ים.
[01:48:50 - 01:48:54] ‫אז זה קורה עם M פה, מתוך B, B זה כל הדאטה, M זה המיני-באץ'.
[01:48:55 - 01:48:58] ‫לכל נקודה אנחנו דוגמים Epsilon.
[01:49:00 - 01:49:06] ‫זה יהיה ההערה שנכניס בתור הדגימת Q שלנו, ‫הדגימה של ה-Z'ים.
[01:49:07 - 01:49:08] ‫אנחנו מחשבים את הדגימה,
[01:49:11 - 01:49:14] ‫את ה-Monte Carlo estimate של האלבום,
[01:49:15 - 01:49:17] ‫אנחנו לא רואים בנוסחה, ‫מתכנירה מה זה אומר,
[01:49:18 - 01:49:21] ‫ואת הגרדיאנטים שלה, ‫גם לפי Fי וגם לפי דאטה,
[01:49:21 - 01:49:23] ‫באמת כנים, נוסעים לזה.
[01:49:23 - 01:49:28] ‫אפשר להשתמש ב-SGD או באדם ‫או כל כלל עדכון כזה של
[01:49:30 - 01:49:32] ‫התבסס על גרדיאנט דרסנט.
[01:49:35 - 01:49:36] ‫אוקיי.
[01:49:38 - 01:49:39] ‫בוא נראה איך זה נראה בפרשים.
[01:49:40 - 01:49:43] ‫נגיד שהתחלנו מאיזושהי דאטה פונק ‫של איזושהי תמונה,
[01:49:45 - 01:49:47] ‫אנחנו מריצים את התמונה הזאת ‫בתוך הרשת הזאתי,
[01:49:48 - 01:49:52] ‫היא נותנת לנו עכשיו תוכלות וווריאנסים של Z.
[01:49:54 - 01:49:55] ‫אנחנו דוגמים מתוך הרשת הזאת,
[01:49:57 - 01:49:57] אוקיי?
[01:49:58 - 01:50:04] ‫אנחנו מחשבים את המודל הגנרטיבי שלנו, ‫זאת אומרת, אנחנו מחשבים את ה-Prior על Z,
[01:50:06 - 01:50:13] ‫שיצרנו Z, בסדר? ‫אנחנו חושבים מה ההסתברות שלו ‫תחת המודל שלנו, זה P של Z,
[01:50:13 - 01:50:15] ‫ואת P של X בהינתן Z,
[01:50:16 - 01:50:19] ‫זו הרשת השנייה שלוקחת Z בתור input,
[01:50:19 - 01:50:23] ‫שמייצרת תוכלות וווריאנסים על X.
[01:50:24 - 01:50:30] ‫ואנחנו מחשבים, בעצם מה שאמרתי עכשיו, ‫זה כל המרכיבים של ה-objective function הזה.
[01:50:31 - 01:50:32] ‫חשבנו את ההסתברות של Z,
[01:50:33 - 01:50:35] ‫ההסתברות של X בנתנזל,
[01:50:36 - 01:50:38] ‫וגם אנחנו צריכים לחשב פה את Q של Z,
[01:50:39 - 01:50:41] ‫ואנחנו עושים פשוט,
[01:50:41 - 01:50:43] ‫זה מה שאנחנו גוזרים,
[01:50:44 - 01:50:47] אוקיי? ומוסיפים ל... גוזרים לפי פנטה, גוזרים לפי פי פי,
[01:50:47 - 01:50:49] ‫ונוסיפים לפי פי.
[01:50:50 - 01:50:52] ‫בשביל לגזור את זה לפי פי, ‫אנחנו צריכים לעשות כאן את ה-Prior,
[01:50:52 - 01:50:56] ‫שהדגימה של Z היא בעצם ‫לא דגימה של Z, אלא דגימה של Epsilon,
[01:50:56 - 01:51:01] ‫ואנחנו מפעילים עליה את הפונקציה G הזאת, ‫שהופכת את Epsilon ו-Z.
[01:51:05 - 01:51:06] ‫יש פה עוד תרשים כזה,
[01:51:07 - 01:51:09] ‫אני מראה את זה קצת יותר ספציפית לגאוסיין,
[01:51:09 - 01:51:12] ‫אני דומה, יש לנו X,
[01:51:13 - 01:51:15] ‫לתוך ה-X אנחנו מחשבים עם
[01:51:17 - 01:51:19] ‫האנקודר הזה, כמו מקום קוראים לזה משהו אחר,
[01:51:19 - 01:51:21] ‫אנקודר או inference network,
[01:51:22 - 01:51:26] ‫אנחנו מחשבים את ההתפגרות של Z ו-X, ‫שזה בדרך כלל תוחלת סיגמא.
[01:51:27 - 01:51:29] ‫אנחנו מבקמים את Epsilon,
[01:51:30 - 01:51:34] ‫מחשבים את Z לידי הפונקציה הזאת, ‫-U ועוד סיגמא אפסילון,
[01:51:35 - 01:51:36] ‫זה נותן לנו את Z,
[01:51:37 - 01:51:39] ‫זה נותן לנו את Z למדגם,
[01:51:41 - 01:51:43] ‫עכשיו אנחנו מחשבים את X בינתיים את Z,
[01:51:44 - 01:51:47] ‫פהיא נניח גם שה-Prior של Z בנגיד קבוע, ‫אז לא צריך לחשב אותו,
[01:51:48 - 01:51:50] ‫וזה נותן לנו בעצם את ה-X,
[01:51:51 - 01:51:55] ‫איזושהי התפלגות על X,
[01:51:55 - 01:51:59] ‫שאנחנו יכולים לחשוב עליה ‫בתור איזושהי פרדיקציה על X, ‫שלפי הפרדיקציה הזאת אנחנו עושים,
[01:52:00 - 01:52:01] ‫מחשבים את ה...
[01:52:02 - 01:52:07] ‫את הגרדיאנטים של תטא ושל V.
[01:52:14 - 01:52:16] ‫אוקיי, הרבה גם אנחנו אמרנו
[01:52:22 - 01:52:33] ‫של X. ‫משהו שאני רוצה להבהיר, ‫ההתפלגות של X בהינתן Z
[01:52:35 - 01:52:37] ‫היא גאוסיאן,
[01:52:38 - 01:52:38] ‫כן?
[01:52:42 - 01:52:46] ‫שתוחלת שלו היא איזושהי פונקציה של X. ‫-זה נכון?
[01:52:47 - 01:52:48] ‫ויש גם דייג
[01:52:49 - 01:53:05] ‫לסיגמה של G, פונקציה של X. ‫ואז אנחנו מחשבים את ה... חלק מה...
[01:53:08 - 01:53:11] ‫חלק מהאובייקטיב, ‫הפשרה שלנו, זה לוב-P של XZ.
[01:53:15 - 01:53:18] זה שווה ללוב-P של Z.
[01:53:19 - 01:53:25] זה ה-Prior, נניח שה-Prior שלנו הוא פיקס, ‫כמו שאמרנו בהתחלה, ‫אז אין לו בכלל פרמטרים,
[01:53:25 - 01:53:28] ‫אבל הוא לא משתתף בתוך ה...
[01:53:29 - 01:53:29] ‫בתוך ה...
[01:53:32 - 01:53:33] ‫אופטינדציה, בתוך ה...
[01:53:35 - 01:53:35] ‫הנגזרת,
[01:53:36 - 01:53:36] ‫זה
[01:53:39 - 01:53:44] לוב-P של פיקס, זה יש לו את הפרמטרים, אוקיי? ‫זה הפרמטרים האלה.
[01:53:46 - 01:53:46] ‫זה
[01:53:46 - 01:53:47] המדעים האלה.
[01:53:48 - 01:53:50] ‫עכשיו, אם עוד הנחה שעושים לפעמים,
[01:53:51 - 01:53:54] ‫וגם לצורך ההסבר פה, ‫בואו נניח שהסיגמה הם גם קבועים.
[01:53:55 - 01:53:57] ‫כל מה שהרשת לומדת זה את התוחלת.
[01:53:58 - 01:54:04] ‫מה זה אומר? זה אומר, אני חושב את התרשים הזה, ‫שזה היה משהו מורכב כזה, ‫שאני רואה מוזר,
[01:54:04 - 01:54:07] ‫ו-P של X תרם איזשהו עיגול כזה,
[01:54:07 - 01:54:11] ‫אז זה אומר שמה שהרשת לומדת, ‫זה רק את המרכז של העיגול,
[01:54:12 - 01:54:14] ‫והעיגול נראה אותו דבר בכל מקום.
[01:54:15 - 01:54:17] ‫אוקיי? אז אני מניח ‫שזה יהיה דבר יותר קבוע.
[01:54:19 - 01:54:19] ‫אז
[01:54:21 - 01:54:22] איזשהו סיגמה,
[01:54:23 - 01:54:24] ‫כל מה שאני דומה זה רק את התוחלת.
[01:54:26 - 01:54:27] סיגמה חי.
[01:54:29 - 01:54:32] אז מה בעצם זה אומר ה-objective function ‫שתלוי בתטא,
[01:54:33 - 01:54:34] ‫הדבר הזה? איך זה נראה?
[01:54:40 - 01:54:41] יש לי איזושהי X,
[01:54:42 - 01:54:45] ‫אחרי כל התהליך הזה יצרתי את התוחלת
[01:54:46 - 01:54:48] ‫של הגאוסיאן של ה-X הזה, ‫ואני רוצה לעשות
[01:54:49 - 01:54:52] לוג של הגאוסיאן הזה,
[01:54:55 - 01:54:57] ‫לראות את זה ביחס לתוחלת.
[01:55:00 - 01:55:01] ‫איפה התוחלת נמצאת בגאוסיאן?
[01:55:03 - 01:55:06] ‫באקספונט שם למעלה, נכון? ‫אז אם אני עושה לוג של האקספונט,
[01:55:07 - 01:55:08] ‫אז האקספונט נופל,
[01:55:08 - 01:55:11] ‫והדבר הזה פשוט יוצא.
[01:55:11 - 01:55:14] ‫אם הדבר הזה הוא קבוע, ‫הדבר הזה פשוט יוצא,
[01:55:14 - 01:55:16] ‫המרחק בין X
[01:55:16 - 01:55:21] ל-Mu שיצא לי יחזר בריבוע.
[01:55:25 - 01:55:30] ‫אז זאת מקסימום לייטליות לגאוסיאן ‫שאני עושה פרדיקציה ‫על התוחלת שלו,
[01:55:30 - 01:55:31] ‫זה פשוט כמו לעשות שגיאה ריבועית
[01:55:32 - 01:55:36] ‫בין מה שיצא לי ל-X האמיתי.
[01:55:37 - 01:55:39] ‫אז בעצם החלק הזה של ה-objective function,
[01:55:40 - 01:55:45] ‫אפשר לחשוב עליו בתור פשוט שגיאה ריבועית ‫בין הפרדיקציה של המודל שלי, ‫של האאוטפוט של הרשת שלי,
[01:55:46 - 01:55:47] ‫ל-X האמיתי.
[01:55:50 - 01:55:51] ‫זה המשמעות של לוג,
[01:55:52 - 01:56:00] ‫קולקטיב אפנסיה זה לוג תטא פי אקס של זה, ‫תטא הוא התוחלת של איזושהי התפלגות גאוסיאנית, ‫בעצם מה זה אומר?
[01:56:01 - 01:56:04] ‫נריץ רשת, האאוטפוט שלה, ‫אני משווה לתמונה, ‫ואעושה שם שגיאה ריבועית.
[01:56:06 - 01:56:06] ‫זה הכול.
[01:56:08 - 01:56:08] בסדר?
[01:56:10 - 01:56:11] ‫אבל זה רק חלק אחד מה-objective.
[01:56:12 - 01:56:16] ‫חלק אחר זה ה-Q של זה, זה ה-fe. ‫צריך לחשב גם את הנסגרת לפי-fe.
[01:56:19 - 01:56:20] ‫אז באמת, אם חושבים על זה ככה,
[01:56:21 - 01:56:23] ‫זו הסיבה שקוראים לזה auto-encoder,
[01:56:24 - 01:56:26] ‫יש פה עוד תרשים שמראים את זה קצת,
[01:56:27 - 01:56:30] ‫שבאיזה אפשר לחשוב על הדבר הזה בתור...
[01:56:31 - 01:56:33] כל המודל הזה נכנס תמונה,
[01:56:35 - 01:56:37] ‫בתוך המודל הזה יוצא מהצד השני תמונה,
[01:56:37 - 01:56:40] ‫ואנחנו משווים את התמונה שיוצאת ‫לאינפוט שהכנסנו.
[01:56:43 - 01:56:44] ‫אתם רואים את זה?
[01:56:45 - 01:56:46] ‫מהתרשים הזה, נכון?
[01:56:47 - 01:56:51] ‫נכנסת תמונה, ואז אמרנו ‫אנחנו עושים את האובייקטיב הזה, ‫אבל מה זה האובייקטיב הזה?
[01:56:51 - 01:56:55] ‫להפחיס את האובייקטיב הזה? ‫זה פשוט לעשות שגיאה ריבועית ‫בין מה שיצא לנו בסוף
[01:56:55 - 01:56:57] למה שיצא לנו בכניסה.
[01:56:59 - 01:57:01] ‫אם חושבים על זה במובן ‫של פשוט רשת כזאת,
[01:57:02 - 01:57:04] ‫אז זו רשת שאנחנו נותנים לה בתור אינפוט תמונה,
[01:57:05 - 01:57:06] ‫האוטפוט שלה זה גם תמונה.
[01:57:07 - 01:57:12] זה התוכלות של הגאוסיאן הזה ואנחנו עושים שגיאה ריבועית בין האוטפוט לאינפוט
[01:57:13 - 01:57:18] זה נקרא אוטונקודר רגיל, ההבדל הוא שאנחנו עושים
[01:57:19 - 01:57:22] יש באמצע גם, יש עוד מרכיבים לאובג'קטיב פאנקשן שלנו
[01:57:23 - 01:57:23] שזה הדבר הזה
[01:57:25 - 01:57:28] כל האופטימיזציה של פי פי
[01:57:29 - 01:57:35] אז אופטימיזציה של תטא היא מתנהגת כאילו זה אוטונקודר רגיל גם לא בדיוק רגיל כי יש פה באמצע משהו
[01:57:36 - 01:57:39] איזה רעש שנכנס גם, איזשהו חישוב שנעשה עם רעש
[01:57:39 - 01:57:42] יש קטע שאנחנו מחשבים תוכלות וסיגמה,
[01:57:43 - 01:57:45] אנחנו מוסיפים לדבר הזה איזשהו רעש
[01:57:46 - 01:57:48] יש פה חישוב שמקבל מטרווינט ורעש
[01:57:49 - 01:57:55] והאוטפוט זה תמונה ואנחנו עושים פשוט שגיאה ריבועית בין התמונה שיוצאת לתמונה המקורית
[01:57:56 - 01:57:58] ואז זה בעצם האחד שלנו, זה חישוב, אז זה האצילות, כן
[01:57:59 - 01:58:01] אז ככה זה נראה מבחינת החישוב
[01:58:02 - 01:58:03] ומה האוטפקטיב פאנקשן שלנו,
[01:58:03 - 01:58:07] אז יש לנו שני חלקים לאובייקטיב פאנקשן שלנו,
[01:58:08 - 01:58:10] תטא זה כל הפרמטרים של הרשת הזאת,
[01:58:11 - 01:58:13] פי זה כל הפרמטרים של הרשת הזאת, אוקיי?
[01:58:14 - 01:58:18] אז אנחנו עושים קודם כל את החלק הראשון הזה, אנחנו עושים
[01:58:21 - 01:58:24] גדי דיסיינט רגיל על כל מה שקורה כאן,
[01:58:25 - 01:58:27] גם על תטא וגם על פי, לפי השגיאה הריבועית הזאת.
[01:58:29 - 01:58:32] אבל יש פה עוד חלק שאנחנו צריכים לעשות לו אופטימיזציה
[01:58:32 - 01:58:36] וזה ה-KL Divergence בין ההתפלגות שיצאה כאן,
[01:58:36 - 01:58:40] ההתפלגות שיצאה להתפלגות של ה-Priar שלנו.
[01:58:41 - 01:58:43] אה, זה, סליחה, זה קשור לזה,
[01:58:45 - 01:58:57] זה ה-Label של הדבר הזה.
[01:58:59 - 01:59:01] אז יש פה עוד חלק שהוא ה-KL Divergence בין
[01:59:02 - 01:59:09] ה-Q שיצא כאן לאיזושהי נקודה X וה-Priar מראש היה לנו על ה-Z.
[01:59:11 - 01:59:24] אז אפשר לחשוב על זה בתור איזשהו אוטו-אנקודר עם רגולריזציה מוזרה קצת שעושה איזשהו K Divergence בין איזשהו משהו שיוצא באמצע של האוטו-אנקודר הזה
[01:59:24 - 01:59:25] לבין פרייר
[01:59:26 - 01:59:29] על פרייר שהחלטנו מראש,
[01:59:30 - 01:59:31] בדרך כלל הוא יהיה גאוסיין.
[01:59:33 - 01:59:40] גם כשה-Priar, כשהכל כאן גאוסיין אז אפשר גם לחשב את ה-KL הזה בצורה
[01:59:40 - 01:59:47] הומוליטית ולא צריך להשתמש בה דגימות מונטה-קרלו כדי לחשב את החלק הזה ואז עושים את המונטה-קרלו רק על החלק הזה
[01:59:49 - 01:59:53] ופה עושים את זה אנליטית ויש כאן את השקף רבה נראה לי,
[01:59:54 - 01:59:56] יש כאן את החישוב האנליטי שיוצא בין
[01:59:58 - 02:00:00] של ה-KL Divergence בין שני גאוסיינים.
[02:00:01 - 02:00:04] גאוסיין אחד יש לו פרמטרים וגאוסיין שני אין לו פרמטרים.
[02:00:04 - 02:00:08] וגאוסיין יש פרמטרים ועושים פרמטרים, זה יוצא פשוט החישוב כזה שתלוי בתוכנות וסיגמאות.
[02:00:12 - 02:00:14] נסתכל על זה אחר כך, אין טעם להיכנס לזה עכשיו.
[02:00:15 - 02:00:20] אבל בסופו של דבר מה שאני רוצה להסביר למה קוראים לזה בראשון אוטו-אנקודר?
[02:00:20 - 02:00:23] אפשר לחשוב על הדבר הזה בתור אוטו-אנקודר, שוב זה רשת
[02:00:23 - 02:00:24] שמכניס לאינפוט,
[02:00:25 - 02:00:28] שמכניס לתמונה, יוצאת תמונה, אני חושב שגיאה ריבועית,
[02:00:30 - 02:00:32] כמובן הזה זה בדיוק כמו אוטו-אנקודר רגיל.
[02:00:32 - 02:00:36] שני שינויים יש, אחד שבאמצע אני מכניס גם רעש,
[02:00:37 - 02:00:39] יש פה עוד איזה כניסה שהיא לא מצוירת כאן,
[02:00:39 - 02:00:41] שזרש, ושתיים,
[02:00:41 - 02:00:44] ה-objective function שלי זה לא רק השגיאה הריבועית הזאת,
[02:00:45 - 02:00:51] אלא גם ה-KL Divergence בין ההתפלגות שיוצאת כאן באמצע ל-Friar שהנחתי מראש.
[02:00:53 - 02:00:59] ומה שזה נותן זה שהמודל הזה, בניגוד לאוטו-אנקודר רגיל, זה מודל גנרטיבי, זאת אומרת אני יכול עכשיו,
[02:00:59 - 02:01:01] אחרי שסיימתי, אני יכול לדגום מהמודל הזה,
[02:01:02 - 02:01:03] תמונות חדשות.
[02:01:04 - 02:01:06] איך אני דוגם תמונות חדשות במודל הזה?
[02:01:07 - 02:01:09] תמונות חדשות במודל הזה.
[02:01:09 - 02:01:11] כן, ואני מגריל Z,
[02:01:12 - 02:01:15] איך אני מגריל Z? אני מחשב לפי ה-Prior,
[02:01:16 - 02:01:18] אני מתעלה לגמרי מכל החצי הזה של הרשת,
[02:01:20 - 02:01:21] אני פשוט מגריל Z מה-Prior שלי.
[02:01:22 - 02:01:24] ה-Prior שלי, לא כפי שהוא גאוסיין,
[02:01:25 - 02:01:28] 0i, אז אני מגריל נקודה מתוך ה-0i,
[02:01:29 - 02:01:31] ומכניס אותה לכאן פה,
[02:01:31 - 02:01:33] לא רואים את זה, אבל איפשהו כאן יש Z,
[02:01:34 - 02:01:37] זה נוכלות של הסיגמר של ה-Z, אבל איפשהו אחר כך יש
[02:01:38 - 02:01:44] את הכניסה של המודל הגנרטיבי, של P של X בהינתן Z, זה פשוט Z.
[02:01:45 - 02:01:50] כן, אז שם אני חותך את המודל, ומפה אני מכניס את ה-Z שדגמתי מה-Prior,
[02:01:50 - 02:01:53] וזה יוצר לי תמונה חדשה.
[02:01:54 - 02:01:56] והנדחה היא, ומה שדיברנו עליו קודם,
[02:01:56 - 02:01:59] שאם ה-Prior שלי היה
[02:02:01 - 02:02:03] איזה משהו כזה,
[02:02:03 - 02:02:05] מודל מושלם שעושה אופטימיזציה כמו שצריך,
[02:02:07 - 02:02:11] בעצם ירצף את ה-Prior הזה להרבה פוסטריורים קטנים שבאים מכל הנקודות,
[02:02:13 - 02:02:16] וכל ה-Prior הזה יהיה מכוסה,
[02:02:17 - 02:02:18] עם פוסטריורים,
[02:02:19 - 02:02:22] ואם אני אדגום נקודה מתוך המודל הזה,
[02:02:22 - 02:02:26] זה בעצם יהיה בתוך המרחב של קרוב לטריינינג סט שלי באיזשהו
[02:02:33 - 02:02:37] אפשר לחשוב על כל ה-VE, זה בתור משהו שלוקח התפלגות מאוד מסובכת
[02:02:38 - 02:02:39] של תמונות,
[02:02:40 - 02:02:44] ובעצם לומד להפוך אותה להתפלגות מאוד פשוטה שהיא גרסיונית,
[02:02:44 - 02:02:45] בעצם לומד מיפוי
[02:02:46 - 02:02:51] הלוך וחזור מהתפלגות מורכבת להתפלגות מאוד פשוטה של גרסיונית.
[02:02:53 - 02:02:55] ‫כן.
[02:02:55 - 02:02:58] ‫כן. ‫-פרקי התפלגות לא אוכלים להיות גדולים, נכון?
[02:02:59 - 02:03:01] ‫של החלוקים של האזרחים.
[02:03:03 - 02:03:07] ‫מה זאת אומרת לא אוכלים להיות גדולים? ‫-פרקים מדי גדול, אז יהיה לנו כאילו הרבה פעמים באמצע,
[02:03:07 - 02:03:10] ‫ואז בעצם הרבה תמונות לא גדולים.
[02:03:13 - 02:03:17] ‫-פרקי התוכלות לא אוכלים להיות גדולים? זאת אומרת מחוץ למעגל הזה אתה מתכוון?
[02:03:18 - 02:03:20] ‫או סוג של...
[02:03:20 - 02:03:24] ‫כן. ‫-לא, הכול צריך להיות בסביבת האפס. ‫-כן, כן.
[02:03:26 - 02:03:32] ‫אנחנו לא מארצים את המודל, אבל בגלל שהוא מנסה ‫למזל את ה-divergence הזה, אז כדאי לו
[02:03:33 - 02:03:43] ללמוד את זה. ‫זה דווקא משהו שקל ללמוד, ‫כי הסקאלה של הדאטה זה לא בעיה, ‫כי הוא יכול לכפר על זה, ‫איזה שיפור הוא מכפיל במספרים יותר גדולים.
[02:03:44 - 02:03:46] ‫זה יכול להגיע בדיוק לאותה תוצאה בסוף,
[02:03:47 - 02:03:52] ‫ונגרום לאיזשהו מימד כאן ‫להיות בתוך איזשהו כדור.
[02:03:53 - 02:03:53] ‫זאת אומרת.
[02:03:56 - 02:03:58] ‫זה לא אילוץ, זה לא מודל.
[02:04:06 - 02:04:09] ‫אוקיי, אז יש כאן כמה תוצאות. ‫זה מהמאמר המקורי מ-2015?
[02:04:12 - 02:04:13] אתם רואים פה?
[02:04:14 - 02:04:16] ‫זה דוגמא, זה ב...
[02:04:18 - 02:04:23] ‫לפני עשר שנים. ‫האמת שאני לא שפוך להגיד, ‫המאמר הזה בדיוק עכשיו זכה ב...
[02:04:24 - 02:04:28] ‫יש כל הכנסים הגדולים, ‫יש טסט אוף טיים בוורד,
[02:04:29 - 02:04:34] ‫שזה מאמר שהיה באותו כנס, ‫לפני עשר שנים,
[02:04:34 - 02:04:35] ‫וזכה על הכי הרבה,
[02:04:37 - 02:04:37] ‫הכי הרבה השפיע
[02:04:38 - 02:04:39] ‫בסואר האחרון.
[02:04:40 - 02:04:42] ‫השנה זה בדיוק עשר שנים,
[02:04:43 - 02:04:46] ‫המאמר הזה, והוא זכה באמת ‫באתר סטריפטריוריות.
[02:04:50 - 02:04:55] ‫ככה זה היה נראה לפני עשר שנים. ‫הם עשו מודל על אמניסט.
[02:04:56 - 02:05:01] ‫כל ריבוע כאן זה מודל אחר, ‫עם גודל אחר של Z.
[02:05:04 - 02:05:06] ‫יותר ויותר ממדים מ-Z.
[02:05:07 - 02:05:10] ‫הם פשוט דגמו מתוך המודל הזה, ‫כמו שאמרנו.
[02:05:10 - 02:05:13] ‫יצא להם פה כל מיני דברים. ‫חלק נראה כמו ספרות.
[02:05:13 - 02:05:14] ‫זה היה לגמרי.
[02:05:16 - 02:05:16] ‫אבל זה היה
[02:05:20 - 02:05:25] ‫תוצאה נחמדה, זה היה השנה לפני ‫פיקסל-CNNED ‫שלמדנו בשביל השנה.
[02:05:30 - 02:05:36] ‫פה הם אמרו משהו נחמד, ‫זה מה קורה, זה כש-Z הוא שני ממדים, ‫מה קורה כשמשנים את הממדים של זה?
[02:05:37 - 02:05:45] ‫כל דגימה, פה לא בדיוק דגמו ‫מתוך הפריור, אלא ממש שלטו באיזה Z, ‫לקחו Z
[02:05:46 - 02:05:53] ערכים מסוימים, ‫ולאט לאט שינו את אחד מהממדים, ‫ואז שינו את הממדים על השני, ‫וככה יצרו את הגריד הזה של התוצאות.
[02:05:54 - 02:05:57] ‫אז נראה שיש כאילו כל מיני אזורים ‫בתוך Z,
[02:05:58 - 02:06:03] ‫תחשבו על הריבוע הזה בתור משהו ‫של מעוגל הזה של ה-Z,
[02:06:04 - 02:06:06] ‫אז יש את האזור שאחראי על ה-60,
[02:06:06 - 02:06:08] ‫אחר כך זה הופך לאפס,
[02:06:09 - 02:06:11] ‫לארבע, תשע, שבע,
[02:06:13 - 02:06:14] ‫באזור הזה או אחד.
[02:06:14 - 02:06:19] ‫זה ממש כאילו, זה נתפס בגלל שזה חייב ‫למפות את ההתפלגות המורכבת ‫של ה-Z.
[02:06:20 - 02:06:21] ‫תמונות של ספרות,
[02:06:22 - 02:06:23] ‫התפלגות מאוד פשוטה,
[02:06:24 - 02:06:29] ‫אבל הדרך הכי ידע לעשות את זה, ‫זה שכל אזור בזד ‫הוא אחראי למשהו, לאיזושהי ספרה אחרת.
[02:06:30 - 02:06:33] ‫לאזורים דומים עם ספרות יחסית דומות.
[02:06:33 - 02:06:42] ‫אבל יש פה, נגיד, חורים באמת, ‫נגיד, אני לא יודע, ‫פה יש איזה משהו שהוא לא ספרה, ‫או משהו שבאמת הופיע בדאטה.
[02:06:43 - 02:06:47] ‫אז כנראה בתוך כל הדאטה ‫שהיה לפלספריו היה כאן אזור ‫שאף פעם לא הגיעו אליו.
[02:06:51 - 02:06:53] ‫שכן דגלו ממנו, יצא איזה משהו מוזר.
[02:06:56 - 02:07:02] ‫זה איזשהי, אחרי איזה שיפור של המודל, ‫אני לא זוכר בדיוק איזשהו שיפור, ‫אבל כבר הצליחו אחרי כמה שנים,
[02:07:02 - 02:07:13] ‫ואחרי שנתיים, אני חושב, ‫להגיע לתוצאות גם יפות ‫על תמונות יותר מורכבות, ‫איזה תמונות של פנים, ‫וגם פה הראו כל מיני דברים ‫מעניינים על ה-Latence,
[02:07:14 - 02:07:21] ‫אז כאן ממש אפשר לגרום ‫להכניס חיוך כאילו לתוך התמונה, ‫בלי לשנות את הזהות של הפנים.
[02:07:22 - 02:07:29] ‫כמו קודם, אין לי קודם פה בדיוק את הפרטים, ‫אבל כמו קודם, אפשר לחשוב ולהגיד ‫שיש איזה מימד בזד, או חלק מהמימדים בזד,
[02:07:29 - 02:07:31] ‫שאחראים על הזהות של הפנים,
[02:07:31 - 02:07:36] ‫במימדים אחרים שאחראים ‫על כל מיני דברים אחרים, כמו הזווית, ‫האם יש חיוך או אין חיוך,
[02:07:37 - 02:07:42] ‫אז אם משנים רק את מה שאחראי על החיוך, ‫אז זה יכול להכניס חיוך לתוך המודל.
[02:07:46 - 02:07:49] ‫מאז היו הרבה פיתוחים אז של המודל.
[02:07:50 - 02:07:51] ‫אחד מהפיתוחים,
[02:07:51 - 02:07:56] ‫שאני לא רוצה להיכנס לעומק, ‫על איך בדיוק הוא מיומש,
[02:07:56 - 02:07:58] ‫אבל רק שתבינו את הרעיון,
[02:07:58 - 02:08:04] ‫זה פשוט לעשות מודל היררכי של BA. ‫כן, אז כמו שהיה לנו קודם,
[02:08:05 - 02:08:06] ‫היה לי פה את ה...
[02:08:06 - 02:08:10] ‫זה היה המודל הגרפי שלי,
[02:08:10 - 02:08:11] ‫שזה יוצר את X,
[02:08:12 - 02:08:13] ‫ו-X הוא...
[02:08:16 - 02:08:19] X זה משהו שאני רואה, ‫וכל ה-Z זה משהו חבוי,
[02:08:20 - 02:08:23] ‫ואני יכול להניח, עדיין כן להניח, ‫איזשהו מבנה.
[02:08:23 - 02:08:26] ‫אני יכול להגדיל את זה פשוט ‫למימד מאוד גבוה,
[02:08:27 - 02:08:30] ‫אבל אולי יותר יעיל לחשוב ‫שיש איזשהו Z גדול,
[02:08:30 - 02:08:31] ‫1,
[02:08:32 - 02:08:33] ‫2,
[02:08:35 - 02:08:36] 3, ככה יש איזושהי היררכיה,
[02:08:37 - 02:08:42] ‫ואנם כל אחד מהם לעשות ‫באונד כזה וריאשונל,
[02:08:43 - 02:08:47] ‫ולחשב גם את כל ה-Qים ההפוכים.
[02:08:49 - 02:08:54] ‫עוד יש כל מיני דרכים לעשות את זה, ‫אפשר שכל אחד תלוי בקודם, ‫אפשר שכל אחד תלוי בכל מי שבא לפניו,
[02:08:55 - 02:09:09] ‫אז פה זה מה שהם עושים כאן, ‫יש להם כל מיני בלוקים כאלה, ‫יש רזידואל, אז יש Z1, Z2, Z3, ‫הוא תלוי בעצם בכיוון הזה, ‫כל אחד תלוי, אני חושב, ‫רק בקודם, ובכיוון ההפוך,
[02:09:10 - 02:09:14] ‫זה כן תלוי בכולם ביחד, ‫אם חושבים את ה-Q.
[02:09:15 - 02:09:17] ‫היו כל מיני ניסיונות לעשות,
[02:09:18 - 02:09:21] היה ל-BCE, ‫זה אחד ספציפי שנקרא N-BCE,
[02:09:22 - 02:09:27] ‫והוא השיג תוצאות די יפות, ‫אז יש פה מוזמנות שלו,
[02:09:28 - 02:09:39] ‫הוא גם עליהם ניסט ועליהם דאטה סט של פלפנים ‫שנתפרסל ל-A, ו-C for 10, זה תמונות ‫בגודל 32 או 32,
[02:09:41 - 02:09:44] ‫אבל לא הרבה פחות תמונות ‫מב-ImageNet,
[02:09:46 - 02:09:50] ‫וגם הצליחו להגיע לדאטה ‫עם רזולוציה הרבה יותר גבוהה.
[02:09:51 - 02:09:53] ‫-אז זה היה הכול דגימות מתוך המודל.
[02:09:55 - 02:09:58] ‫אז זה היה, אני זוכר באיזה שנה זה,
[02:09:59 - 02:10:00] אבל...
[02:10:03 - 02:10:12] ‫כן, היה איזה כמה שנים ‫שלא הצליחו עם V E LAS. V היה מאוד מבטיח בהתחלה, ‫ואז כמה שנים לא הצליחו להגיע איתו ‫לתוצאות ממש טובות,
[02:10:13 - 02:10:18] ‫ובינתיים מודלים אחרים ‫הגיעו לתוצאות הרבה יותר טובות, ‫כמו מודלים מאוד יותר ריגרסיב, ‫שראינו שבוע שעבר,
[02:10:18 - 02:10:23] ‫ואז אחרי איזה שנה גם לגאנז ‫התחילו להשיג תוצאות ממש טובות,
[02:10:23 - 02:10:28] ‫ואז אחרי כמה שנים הראו ‫שגם עם V E AES מצליחים, ‫אבל צריך יותר שכבות.
[02:10:30 - 02:10:35] ‫יש פה, זה רשק פחות, ‫היום יש פה את הטבלנטים ‫המספרים כמו שהם בשבוע שעבר.
[02:10:39 - 02:10:43] ‫סתם שתראו, לא כל כך חשוב ‫להתכנס כולה למספרים, ‫אבל זה גם, זה הלוג לייטליהוד,
[02:10:44 - 02:10:47] ‫בביטים ל-Dimension, זאת אומרת,
[02:10:47 - 02:10:49] ‫מחלקים במספר הפיקסלים.
[02:10:50 - 02:10:58] ‫חוץ מאמניסט, משום באמניסט, ‫הקונבנציה זה לא לעשות... ‫בגלל שהמאמרים הראשונים לא עשו את זה, ‫אז הם עושים את זה כפי בנאטס,
[02:10:59 - 02:11:01] ‫ולא מחלקים במספר הממדים,
[02:11:02 - 02:11:04] ‫אבל בקוד שיש לכם, ‫אז כן עושים את זה,
[02:11:05 - 02:11:06] ‫זה כן ביטס פרלימנט של זה.
[02:11:09 - 02:11:12] ‫אוקיי, אז בגלל זה המספרים פה ‫הרבה יותר גדולים כאן.
[02:11:13 - 02:11:17] אז זה ה-NVE, ‫זה המודל VE הזה עם כמה...
[02:11:18 - 02:11:20] כמה שכבות, אתם רואים, ‫ממש ראינו את זה פה ‫לכל מיני סוגים.
[02:11:21 - 02:11:23] ‫אוטו-רגרסיב מאוד, אז זה מה שראינו ‫שבוע שעבר.
[02:11:25 - 02:11:28] יש פה כמה, יש פיצל CNN פלוס פלוס, ‫עם כל מיני שיפורים,
[02:11:28 - 02:11:31] יש את פיצל RNN, יש את הגייטל פיצל CNN,
[02:11:32 - 02:11:36] פיצל סנייל, הוא נראה לי הכי טוב, ‫אני לא זוכר מה...
[02:11:38 - 02:11:41] כל מיני השיפורים ששאנו פה, ‫יש פה טרנספורמר שאפשר גם איתו לעשות,
[02:11:43 - 02:11:44] ‫שמה שדיברנו עליו שבוע שעבר.
[02:11:45 - 02:11:49] ‫פה זה מודלים שהם מבוססים על Flow,
[02:11:53 - 02:11:56] ‫Normalized and flows, שזה בשבוע הבא כנראה ‫נתחיל לדבר עליו,
[02:11:57 - 02:11:58] סוג אחר של מודלים.
[02:11:59 - 02:12:01] ‫כאן זה איזשהו שילוב של VE ו-flow,
[02:12:03 - 02:12:04] ‫פה זה VE זה מסוגים אחרים.
[02:12:07 - 02:12:14] ‫שתראו שיש כל מיני מודלים, ‫ואיך משווים ביניהם עם הלוגייטיות, ‫משהו שכדאי לשים לב, שבעצם
[02:12:15 - 02:12:21] ‫דווקא לא כתוב פה בצורה ברורה, ‫זה שעבור המודלים שמבוססים על VE, ‫בעצם אי אפשר לחשב
[02:12:21 - 02:12:22] ‫בלוגייטיות בצורה מדויקת.
[02:12:24 - 02:12:28] ‫בניגוד למודלים האלה, שאנחנו יודעים לחשב. פשוט, אם אתם תמונה, אנחנו חושבים
[02:12:29 - 02:12:32] ‫עבור כל הפיקסלים האלה, ‫זה חלק ממה שאתם צריכים לעשות ‫בתרגיל עכשיו.
[02:12:33 - 02:12:36] כאן אנחנו לא יכולים לחשב את זה, ‫אבל אנחנו יכולים או לחשב את החסם,
[02:12:37 - 02:12:39] ‫צריך לראות בדיוק במאמר מה יש כאן, או את החסם,
[02:12:40 - 02:12:42] ‫או הרבה פעמים בנוסף לחסם,
[02:12:42 - 02:12:44] ‫אפשר לעשות איזשהו,
[02:12:45 - 02:12:53] ‫לא בזמן האימון, ‫אבל בזמן אבל בזמן אבלואציה, ‫זה יכול להיות סביר ‫לעשות איזשהו מונטקרלו אסטימנט, ‫עם הרבה דגימות.
[02:12:56 - 02:12:57] ‫עכשיו התחלנו מזה שהדגימות,
[02:12:58 - 02:13:01] ‫לעשות את השערוך של האינטגרל ‫עם דגימות זה לא כל כך יעיל,
[02:13:02 - 02:13:07] ‫אבל אם הפונקציה שאנחנו דוגמים איתה ‫זה לא ה-prior, אלא זה ה-posterior,
[02:13:08 - 02:13:10] ‫אז הופכנו להיות קצת יותר יעיל. ‫ואחרי שיש לנו כבר פוסטריור,
[02:13:11 - 02:13:15] ‫הרבה פעמים עושים אבלואציה ‫עם הרבה דגימות מתוך הפוסטריור, ‫במקום דגימה אחת.
[02:13:19 - 02:13:26] ‫אוקיי, אז זהו, לגבי זה, ‫יש פה קצת פתרונות וחסרונות של VE's. ‫פתרונות זה שהמימוש של זה הוא,
[02:13:27 - 02:13:31] ‫דיברנו עכשיו הרבה על איך הגיעו לזה, ‫אבל המימוש בסופו של דבר ‫הוא די פשוט.
[02:13:32 - 02:13:33] זה נכון, אמרנו, ‫זה פשוט רוטו-אנקודר,
[02:13:34 - 02:13:37] ‫שאנחנו באמצע צריכים להכניס רעש ‫ולהוסיף עוד איזשהו איבר
[02:13:38 - 02:13:43] ‫באבג'קטיב פונקשן שלו, ‫את ה-K דייברג'נס הזה, ‫באבג'קטיב פונקשן.
[02:13:44 - 02:13:47] קל, בהינתן שיש לנו את המודל, ‫קל לנו לדגום ממנו,
[02:13:49 - 02:13:50] ‫אנחנו פשוט דוגמים גאוסיאן מ-Z,
[02:13:51 - 02:13:53] ‫ואז מריצים אותו בדרך הדקודר,
[02:13:53 - 02:13:58] ‫זה נקרא אנססטרל סמפלינג. ‫כשיש לנו מודל מכוון,
[02:13:59 - 02:14:04] ‫אז כשנדגום לפי הסדר של הגרף, ‫זה נקרא אנססטרל סמפלינג, ‫אז קל לנו לעשות את זה פה.
[02:14:07 - 02:14:18] ‫יש איזושהי משמעות שאפשר להעסיק, ‫ויש גם עוד כל מיני מודלים ‫שעובדים על זה, ‫שבאמת לגרום ל-Z להיות, ‫לתפוס כל מיני דברים מעניינים בדאטה.
[02:14:19 - 02:14:27] ‫ובעצם יש לנו המיפוי הזה לקוד Z, ‫זה משהו שנותן לנו, ‫מה שקראנו לו representation learning, ‫נותן לנו איזשהו ייצוג חדש לדאטה,
[02:14:27 - 02:14:31] ‫שהרבה פעמים אנחנו יכולים להשתמש בו,
[02:14:31 - 02:14:32] ‫ולהגיע לתוצאות יותר טובות,
[02:14:32 - 02:14:35] ‫באבקשה, למה שם.
[02:14:37 - 02:14:47] ‫אז קל לנו להגיע ל-Z מתוך Q, ‫סליחה, מתוך X, ‫לתתן תמונה חדשה, ‫אנחנו יכולים פשוט להריץ את האנקודר שלנו,
[02:14:48 - 02:14:52] ‫ואנחנו מקבלים את הייצוג של התמונה הזאתי ‫במרחב הזה של ה-Lated,
[02:14:52 - 02:14:53] ‫במרחב הזה של Z,
[02:14:55 - 02:15:03] ‫ואנחנו עדיין יכולים להשתמש בזה ‫כדי לא לחשב בדיוק את P של X, ‫אבל לקרב אותו, אני חושב, את החסם התחתון שלי.
[02:15:03 - 02:15:17] ‫החסרונות זה שכמו שאמרתי, קל לממש ‫את המודל הפשוט הזה עם הגאוסיאן ‫ולהגיע לתוצאות נחמדות ב-N-List, ‫אבל לקח די הרבה זמן ‫להגיע לתוצאות טובות ‫על תמונות אמיתיות
[02:15:18 - 02:15:24] ‫בפעולות יותר גדולות, ‫וזה מצריך מימוש קצת יותר מורכב ‫משני הגאוסיאנים האלה של P ו-Q.
[02:15:24 - 02:15:34] ‫החסרון הבא זה שאנחנו לא יכולים ‫באמת לחשב את B של X בצורה מדויקת,
[02:15:34 - 02:15:37] ‫אבל לקרב ולחסום, ‫אבל לא לחשב מדויקת.
[02:15:38 - 02:15:42] ‫ועוד בעיה זה שיש לנו את שני הפרמטרים, ‫שני הקומפוננטים האלה, P ו-Q,
[02:15:43 - 02:15:49] ‫לפעמים כשהביצועים לא כל כך טובים, ‫קשה לנו לדעת מי מהם שם, אם ה-Q שלנו לא היה מספיק טוב, ‫ה-P לא היה מספיק טוב,
[02:15:49 - 02:15:53] ‫אז גם כל מיני עבודות של מתקדמות יותר של B-E, ‫מנסות אחר כך
[02:15:54 - 02:15:55] שהפוסטריאור יהיה יותר מורכב,
[02:15:56 - 02:16:02] ‫לא רק באוסיאן, ‫אלא משהו הרבה יותר מורכב, ‫כי הם חושבים שזה המקור של הבעיה. ‫אם אחר כך יהיו מאמרים אחרים שאמרו בעצם,
[02:16:02 - 02:16:08] ‫המקור של הבעיה זה ‫שהליקודר הוא לא מספיק טוב, ‫ושההפך, ה-Prior,
[02:16:08 - 02:16:12] ‫לא ההפך, אבל בנוסף, ‫שה-Prior מספיק טוב, ‫שאי אפשר שה-Prior יהיה פיקסט,
[02:16:13 - 02:16:16] ‫בשביל זה שהפוסטריאור הוא מאוד חזק, ‫זה דווקא מחליש את ה-Prior.
[02:16:17 - 02:16:21] ‫קשה לעשות אנליזה מה המרכיבים ‫של הבעיות ברגע שיש בעיות,
[02:16:22 - 02:16:26] ‫ויש באמת כל מיני פיתוחים ‫שהפוסטריאור מגיע ממודל אחר,
[02:16:28 - 02:16:38] ‫שהפוסטריאור בעצמו הוא פיקסל CNN למשל, ‫או שהדיקודר הוא פיקסל CNN. ‫יש עוד כל מיני שילובים כאלה, ‫כי ה-VA בפני עצמו הפשוט, ‫הוא בדרך כלל לא מספיק
[02:16:39 - 02:16:40] ‫בין התוצאות האלה.
[02:16:42 - 02:16:44] זהו, זהו להיום.
[02:16:45 - 02:16:54] ‫יש לכם את התרגיל על פיקסל CNN. ‫בתרגיל הבא, אני חושב,
[02:16:55 - 02:17:05] ‫אני מבקש שזה יוצא בסדר, ‫אבל פשוט להשתמש באותו קוד ‫ולממש שם VAE ‫או לקחת VAE ‫לענות על כל מיני שאלות ‫שקשורות ל-VA.
[02:17:14 - 02:17:16] ‫תודה רבה, אדוני.