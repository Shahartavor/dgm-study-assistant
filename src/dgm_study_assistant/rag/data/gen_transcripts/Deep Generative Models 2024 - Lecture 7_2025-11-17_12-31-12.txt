[00:00:00 - 00:00:00] טוב,
[00:00:00 - 00:00:01] שלום לכולם,
[00:00:03 - 00:00:04] שלום לכולם,
[00:00:05 - 00:00:09] אז עברנו את החלק הראשון של הקורס,
[00:00:10 - 00:00:12] שלוש דקות לרמי ששרד,
[00:00:13 - 00:00:21] ועכשיו אנחנו נדבר יותר על נושאים שונים, וכל נושאים ממש מודל שאנחנו נסתכל עליו קצת יותר בפירוט,
[00:00:23 - 00:00:24] כולל מימוש.
[00:00:25 - 00:00:27] חלק נסתכל ממש על הפרטים בפרקידה,
[00:00:27 - 00:00:30] חלק אתם תעשו בתרגילים.
[00:00:32 - 00:00:37] והיום אנחנו נתחיל עם הגישה שנקראת auto-regressive models,
[00:00:39 - 00:00:41] לדבר יותר ספציפית על מודל שנקרא פיקס וCNN,
[00:00:42 - 00:00:47] וזה יהיה מודל שאתם תעבדו עליו בבת בית.
[00:00:48 - 00:00:52] אני לא תסתכלו ממש לממש אותו, כי אנחנו נתחיל מימוש כבר קיים שלו,
[00:00:53 - 00:00:58] שצטרפו לעשות עליו כל מיני תוספות ובדיקות,
[00:00:59 - 00:01:04] וזה יהיה גם הבסיס שאחר כך אנחנו נממש מודלים אחרים לתוך הכול.
[00:01:06 - 00:01:08] לא סיימתי לארגן את כל התרגילים,
[00:01:08 - 00:01:11] זה כבר עדיין לא ברור לי איך נראה עד הסוף,
[00:01:12 - 00:01:13] אבל זאת הכוונה,
[00:01:13 - 00:01:16] יכול להיות שנראה שיש כמה דברים שיותר מסובכים,
[00:01:16 - 00:01:18] בכל מסובכים זה משנה,
[00:01:19 - 00:01:20] אבל זאת הכוונה.
[00:01:20 - 00:01:25] אז היום כבר בסוף השיעור נתחיל להסתכל קצת על הקוד עצמו של הדברים האלה.
[00:01:27 - 00:01:29] קיבלתם כבר חזרה נכון תרגיל אחד,
[00:01:30 - 00:01:33] ויש לכם תרגיל שלישי עכשיו שאתם יכולים להגיש,
[00:01:34 - 00:01:35] על השבוע בגדול,
[00:01:37 - 00:01:42] והתרגיל הבא יהיה כבר תרגיל שקשור למימוש של מה שאנחנו נראה היום.
[00:01:50 - 00:01:53] שאלות על התרגילים, או כל הסטייטופ של הקורס?
[00:01:54 - 00:01:55] יש לכם את זה עם חלק חשבים?
[00:01:59 - 00:02:00] אוקיי, אז היום התוכנית,
[00:02:02 - 00:02:02] זה
[00:02:04 - 00:02:08] תתחיל שוב מחזרה בעצם להחזיר אתכם לתמונה גדולה של מה אנחנו רוצים לעשות.
[00:02:10 - 00:02:17] יש לכם כבר קצת יותר הבנה של חלק מהעקרונות היותר תיאורטיים,
[00:02:17 - 00:02:20] אבל אולי זה יהיה לכם יותר קר לראות קצת את התמונה הגדולה.
[00:02:22 - 00:02:29] ואז אנחנו נדבר על הגישה שאנחנו מדברים עליה היום, לא את ה-rugressive model. נראה אותה בהתחלה בלי מודלים עמוקים,
[00:02:29 - 00:02:30] בלי רשתות עמוקות
[00:02:31 - 00:02:32] רג'יסטיק רגרשן פשוט,
[00:02:33 - 00:02:37] אחר כך נעסוק לבחורת למה זה אומר לממש את זה עם הרשתות הדוגות,
[00:02:39 - 00:02:40] ואז נראה כמה דוגמאות,
[00:02:41 - 00:02:45] נעבור מהר לשלושת הדוגמאות האלה שהם מודלים קודמים,
[00:02:45 - 00:02:49] ונתעכב קצת יותר על המודל עבור פיקסל-CNN.
[00:02:49 - 00:02:53] המודלים האלה הם פיקסל-CNN מ-2015 אני חושב,
[00:02:54 - 00:02:56] אלא כמה שנים לפני.
[00:02:59 - 00:03:01] כולם פה עשו את הקורס ב-deep learning?
[00:03:02 - 00:03:06] דיברנו בשיעור הראשון אבל זה היה בזום ולא כל כך תבנתי מה מצארכם.
[00:03:07 - 00:03:08] מי עשה את הקורס ב-deep learning?
[00:03:12 - 00:03:13] זה הרוב הגדול.
[00:03:14 - 00:03:17] מישהו עושה עכשיו? אין עכשיו בעצם קורס בדיוק.
[00:03:19 - 00:03:21] מישהו עשה איזשהו החלטה בקורס?
[00:03:24 - 00:03:31] בסדר, טוב אני מקווה שכולם, לא שמנו את זה בתור דרישת דם רשמית,
[00:03:31 - 00:03:33] לא יודע אם זה היה נכון או לא נכון,
[00:03:34 - 00:03:36] הכוונה היא אבל שאתם,
[00:03:37 - 00:03:41] ההנחה היא שאתם תסתדרו עם לממש את הדברים האלה,
[00:03:41 - 00:03:43] אבל נעשה היום את זה חזרה קצרה כזאת בשביל מה זה אומר.
[00:03:45 - 00:03:48] אוקיי, אז אנחנו בתזכורת.
[00:03:48 - 00:03:53] מה זה מודלים גנרטיביים? אמרנו שזה מודלים שאנחנו,
[00:03:55 - 00:03:59] שהoutput שלהם הוא במימד גבוה, זה דבר ראשון,
[00:03:59 - 00:04:05] כמו תמונות, כמו אודיו, כמו טקסט וגם הם הסתברותיים.
[00:04:05 - 00:04:08] זה בגדול שתי הדרישות שלנו שאנחנו אומרים שהמודל הוא מודל גנרטיבי.
[00:04:09 - 00:04:13] הסתברותיים, זאת אומרת שהמודל שלנו בעצם ממדל את ההסתברות
[00:04:14 - 00:04:16] שלהם באיזשהו אופן,
[00:04:16 - 00:04:18] ועל ידי זה אנחנו יכולים לעשות כל מיני דברים,
[00:04:19 - 00:04:20] בין השאר גם לייצר
[00:04:21 - 00:04:23] דגימות מהמודל ההסתברות הזה.
[00:04:25 - 00:04:29] ומודלים שונים הם נמצאים במקומות שונים, יש כאלה שקל לדגום,
[00:04:30 - 00:04:34] אבל קשה נגיד לבדוק מה ההסתברות של דוגמה חדשה,
[00:04:35 - 00:04:37] ויש מודלים אחרים שדווקא
[00:04:38 - 00:04:41] בסדר, אמרתי את זה עכשיו. זה קשה לדגום,
[00:04:41 - 00:04:45] אבל קל לבדוק את ההסתברות של נקודה חדשה.
[00:04:47 - 00:04:47] יש
[00:04:48 - 00:04:52] מודלים שונים, יש להם בעצם משמעות אחרת למילה הזאת,
[00:04:53 - 00:04:55] אורבנמליסטיק, מה זאת אומרת שהמודלים זה דוגמאותי.
[00:04:56 - 00:04:57] אז היום אנחנו נראה דוגמה אחת,
[00:04:57 - 00:05:00] ונמשיך במשוך לדוגמאות.
[00:05:07 - 00:05:10] אוקיי, אז מה אנחנו יכולים לעשות עם המודלים האלה, או מה אנחנו רוצים לעשות איתם?
[00:05:11 - 00:05:12] אז אנחנו רוצים, לפעמים אנחנו,
[00:05:13 - 00:05:17] בראש יש לנו איזושהי משימה כמו פלסיפיקציה,
[00:05:17 - 00:05:19] ואנחנו משתמשים בגישה גנרטיבית כדי לפתור אותה.
[00:05:20 - 00:05:27] למשל, אנחנו לומדים את כל המודל ההסתברותי של תמונות של חופולים לעומת מודל ההסתברותי של תמונות של כלבים, כדי שאחר כך שיהיה לנו תמונה חדשה,
[00:05:28 - 00:05:31] פשוט נבדוק תחת איזה מודל יש להסתברות יותר גבוהה
[00:05:32 - 00:05:36] וזה הקלסיפיקציה שלנו, דרך שנעשה קלסיפיקציה
[00:05:37 - 00:05:38] ותמיד זאת המטרה
[00:05:39 - 00:05:41] מסרה אחרת זה לייצר דאטה
[00:05:41 - 00:05:45] למשל בתמונות, הרבה פעמים זה מדברה ממש אמנותית, אנחנו רוצים לייצר כל מיני
[00:05:47 - 00:05:49] תמונות של כל מיני דברים יפים ומעניינים
[00:05:50 - 00:05:52] אני לא חייב להיות אמנותי, זה יכול להיות גם
[00:05:52 - 00:05:56] אם אנחנו רוצים לייצר דוגמאות חדשות כדי שנוכל לאמן איזשהו מודל אחר
[00:05:57 - 00:06:00] או כדי שנבין איך הדאטה הזה בנוי
[00:06:01 - 00:06:01] וכל מיני דברים כאלה
[00:06:02 - 00:06:07] זו מטרה שנייה, ממש לייצר דאטה. המטרה השלישית, קראנו לה presentation learning
[00:06:08 - 00:06:16] זה בעצם להבין משהו על המבנה של הדאטה ולייצג אותו בדרך אחרת, במקום לייצג תמונה בתור אוסף של פיקסלים
[00:06:17 - 00:06:23] לייצג אותו בתור איזשהו אוסף של פקטורים או פיצ'רים שהם חבויים,
[00:06:23 - 00:06:26] הם לא ממש רואים אותם מהטיקסלים אלא
[00:06:26 - 00:06:29] אבל שאם יש לנו מודל גנרטיבי טוב של התמונות
[00:06:30 - 00:06:33] אפשר מתוכו להסיט מה המבנה
[00:06:34 - 00:06:39] אז אנחנו היום, זה יהיה דוגמה למודל שהוא לא כל כך טוב בשביל המשימה הזאת
[00:06:40 - 00:06:45] אבל שבוע הבא אנחנו נדבר על מודל עם משתנים חבועים שהם טובים דווקא יותר במשימה הזאת
[00:06:48 - 00:06:50] זו דוגמה שאמרנו, אוקיי, אם יש לנו מודל שיש לו הסתברות,
[00:06:51 - 00:06:54] אנחנו גם יודעים להגיד אם דברים הם בהסתברות גבוהה או נמוכה,
[00:06:54 - 00:06:57] אם הפרדיקציה שלנו אנחנו בטוחים לגביה
[00:06:57 - 00:07:01] אם נריץ הרבה פרדיקציות כל פעם זה יהיה אותו דבר או שכל פעם זה יהיה משהו אחר
[00:07:01 - 00:07:08] זה דברים שיכולים לעזור לנו מאוד לא רק לעשות פרדיקציה אלא גם לעשות איזשהם מערכת קבלת החלטות על סמך הפרדיקציה הזאת
[00:07:09 - 00:07:13] אנחנו יודעים שאנחנו מאוד בטוחים במשהו, זה יעזור לנו לקבל החלטה
[00:07:13 - 00:07:14] לגבי זה
[00:07:15 - 00:07:24] ומשהו שהוא קצת יותר כללי ומכיל קצת את הכל או מין גישה לעשות את כל הדברים האלה זה פורמליסטיק אינפרנס זה בעצם כל משימה ברגע שהמודל שלנו הוא מודל
[00:07:24 - 00:07:29] הסתברותי אנחנו יכולים להגדיר כל מיני משימות על ידי איזושהי בעיית
[00:07:29 - 00:07:32] אוריבליסטיק אינפרנס זאת אומרת יש לנו משתנה שאנחנו צריכים
[00:07:33 - 00:07:34] להבין מה ההסתברות שלו
[00:07:35 - 00:07:40] משתנה זה למשל יכול להגיד האם יש לנו קלאס של קלב או קלאס של חתום
[00:07:41 - 00:07:49] או האם עד כמה התמונה הזאת של הפנים היא של מישהו שהוא במצב רוח טוב או לא טוב
[00:07:50 - 00:07:53] כל מיני דברים שאנחנו מוצאים או מה הפיקסל
[00:07:54 - 00:07:57] מהחצי התחתון של התמונה והיא נותנת החצי העליון של התמונה
[00:07:58 - 00:08:01] כל מיני דברים כאלה אנחנו יכולים להגדיר בתור יש לנו משתנים שאנחנו לא יודעים
[00:08:01 - 00:08:04] לא יודעים אותם ואנחנו צריכים לחשב את ההסתברות אליהם
[00:08:07 - 00:08:11] וזה בעצם בעיה של הסקה הסתברותית אוריבליסטיק אינפרנס
[00:08:11 - 00:08:13] ואז אם יש לנו איזושהי דרך
[00:08:13 - 00:08:15] כללית לתור כל מיני בעיות
[00:08:16 - 00:08:20] של הסקה הסתברותית כזאת אנחנו יכולים לפתור הרבה בעיות שונות
[00:08:21 - 00:08:23] אפילו בעיות שכשאימנו את המודל שלנו לא ידענו
[00:08:23 - 00:08:31] שזה מה שהם מנהלים את זה, זה בעצם זה הדבר שהכי אולי מראה את הכוח של הגישה הזאת,
[00:08:32 - 00:08:32] גישה גנטית.
[00:08:35 - 00:08:36] אוקיי,
[00:08:36 - 00:08:44] איך אנחנו לומדים את המודל הזה, אז גם תזכורת למשהו שראינו פה כמה פעמים,
[00:08:44 - 00:08:47] אנחנו מניחים שיש לנו התפלגות שמייצרת את הדאטה,
[00:08:47 - 00:08:48] קוראים לה P-Data,
[00:08:49 - 00:08:51] אנחנו מניחים שיש לנו מודל
[00:08:52 - 00:08:53] עם איזושהי התפלגות
[00:08:54 - 00:08:57] איזה שהם פרמטרים לא ידועים, אנחנו קוראים למודל הזה P-Data,
[00:08:58 - 00:09:00] אנחנו רוצים ש-P-Data יהיה קרוב לפי דאטה.
[00:09:03 - 00:09:06] אם הדאטה שלנו היה בדו-מימד היינו יכולים לצייר את
[00:09:07 - 00:09:08] הדרך הזה,
[00:09:08 - 00:09:13] שאזורים גבוהים זה אזורים עם הסתברות גבוהה, אזורים מנופים זה אזורים עם הסתברות נמוכה.
[00:09:15 - 00:09:15] אוקיי,
[00:09:17 - 00:09:22] מה המרכיבים שאנחנו צריכים כדי לאמן מודל
[00:09:22 - 00:09:23] נרטיבי או החלטות שצריך לעשות.
[00:09:24 - 00:09:26] אז קודם כל אנחנו צריכים דאטה כדי לעבוד איתו,
[00:09:27 - 00:09:30] והדאטה הזה גם צריך להיות
[00:09:31 - 00:09:32] מייצג טוב את הדאטה.
[00:09:34 - 00:09:37] צריכים להציג דוגמאות שמייצגות את המרחב הזה שאנחנו רוצים
[00:09:38 - 00:09:40] עליו ללמוד מודל ההתערבותי,
[00:09:41 - 00:09:44] וגם אנחנו צריכים איזשהו ייצוג טוב שלו,
[00:09:44 - 00:09:45] אני מדבר על זה קצת היום.
[00:09:46 - 00:09:47] מודל,
[00:09:47 - 00:09:49] אנחנו צריכים להחליט מה המודל ההתערבותי שלנו,
[00:09:49 - 00:09:52] מה הפרמטריזציה שלו,
[00:09:53 - 00:09:56] ואיזה סוג של מודל זה, אז כבר ראינו דוגמה של גאוסיאנים ותערובת גאוסיאנים,
[00:09:56 - 00:10:20] ודיברנו על
[00:10:22 - 00:10:29] הרבה פעמים בסופו של דבר הרשתות נוירונים זה פרמטריזציה לגאוסיאנים או תערובת גאוסיאנים
[00:10:30 - 00:10:32] או לברנולי, כפי שנראה היום.
[00:10:32 - 00:10:36] אחר כך יש את השאלה של איך אנחנו מאמנים, זאת אומרת מה הobjective function שלנו,
[00:10:37 - 00:10:38] שלרוב זה יהיה מקסימום מולייקליות,
[00:10:39 - 00:10:40] דיברנו על זה גם כבר,
[00:10:41 - 00:10:42] וגם זה מה שנעשה היום.
[00:10:44 - 00:10:47] בסוף הקורס אנחנו נדבר קצת על שיטה אחרת שנקראת Score Matching,
[00:10:48 - 00:10:49] גם אפשר לחשוב עליה בתור
[00:10:50 - 00:11:02] קירוב של maximum likelihood, או איזושהי דרך לשערך את maximum likelihood והמרכיב הרביעי זה איך אנחנו עושים את האופטימיזציה של ה-objective function.
[00:11:04 - 00:11:06] יש מקרים כמו היום שזה קל,
[00:11:07 - 00:11:09] לא צריכים להסתבך עם כל מיני מערכים כאלה,
[00:11:10 - 00:11:14] ויש מקרים שזה בעיקר כשיש לנו משתנים חבויים
[00:11:15 - 00:11:16] שאנחנו צריכים לעשות דברים יותר מתוחכמים,
[00:11:17 - 00:11:19] וזה קצת התוארה שדיברנו בשני שיעורים האחרונים,
[00:11:20 - 00:11:20] שזה ה-Various
[00:11:20 - 00:11:22] Influence ו-MCMC
[00:11:23 - 00:11:25] היום לא נדבר על זה, ובשבוע הבא נחזור
[00:11:27 - 00:11:28] לשיטות האלה.
[00:11:30 - 00:11:32] ובגדול הבעיה, הסיבה שזה קשה זה שפשוט
[00:11:33 - 00:11:36] לקחנו את הכול בצורה ישירה, זה שיש יותר מדי פרמטרים,
[00:11:38 - 00:11:40] ויש לנו את קללת המימד, נכון?
[00:11:40 - 00:11:46] המספר הפרמטרים גדל בצורה אקספוננציאלית, אם הם רוצים תשתף הסתברות לכל אחת מהאפשרויות
[00:11:47 - 00:11:49] של תמונה בגודל
[00:11:50 - 00:11:50] N-Pixels
[00:11:52 - 00:11:55] אנחנו צריכים להכניס כל מיני הנחות למבנה
[00:11:55 - 00:12:00] אנחנו נדבר קצת על המבנה, אבל דיברנו על conditional independence
[00:12:01 - 00:12:04] בתור סוג של המערכות שמורידות לנו את מספר הפרמטרים
[00:12:09 - 00:12:09] אוקיי
[00:12:10 - 00:12:12] אז בואו נתחיל לדבר קצת על הדאטה, אז
[00:12:13 - 00:12:16] אנחנו שתי דוגמאות, אני חושב שאנחנו נעבוד איתן
[00:12:18 - 00:12:19] כנראה עד סוף הקורס,
[00:12:20 - 00:12:22] זה דאטה סטרנט שנקרא אמניסט
[00:12:23 - 00:12:24] זה דאטה סט של,
[00:12:25 - 00:12:28] אני מניח שאתם מכירים אותו כבר, דאטה סט של תמונות
[00:12:30 - 00:12:31] בקור לבן
[00:12:32 - 00:12:34] של ספרות מ-0 עד 9
[00:12:37 - 00:12:40] ודאטה הסט השני זה דאטה של תמונות טבעיות
[00:12:42 - 00:12:43] שנקרא ImageNet
[00:12:44 - 00:12:46] ואנחנו נעבוד עם גרסה מתנת שלו
[00:12:47 - 00:12:48] Imagement 32
[00:12:49 - 00:12:52] הוא מקשיב עליו רק 32 או 32 אחרת
[00:12:54 - 00:12:57] ממנה החישוב של כל דבר שנעשה יהיה קצת גדול
[00:12:59 - 00:13:02] יש כמה דרכים לייצג תמונות
[00:13:03 - 00:13:06] אז שתי הדרכים הסטנדרטיות זה דרך דיסקרטית
[00:13:07 - 00:13:10] ככה זה ממש בעצם במחשב,
[00:13:11 - 00:13:13] בדרך כלל ככה נשמעות התמונות
[00:13:14 - 00:13:18] בתור כל פיקסל, בעצם אינטג'ר מספר
[00:13:20 - 00:13:27] מספר שלם, ציובי, עם 8 ביטס, זאת אומרת זה מספר בין 0 ל-255
[00:13:29 - 00:13:30] דברים שונים, יש לנו
[00:13:31 - 00:13:31] 256
[00:13:36 - 00:13:37] ערכים שונים
[00:13:38 - 00:13:39] לכל פיקסל
[00:13:40 - 00:13:43] ושיטה אחרת לייצג תמונות זה שכל פיקסל הוא ערך רציף
[00:13:44 - 00:13:46] ואז בדרך כלל מנרמלים את זה בין 0 ל-1
[00:13:50 - 00:13:52] אם אם יש לנו תמונת צבעונית אז בעצם כל פיקסל הוא
[00:13:55 - 00:13:56] שלושה ערכים כאלה
[00:13:56 - 00:14:02] זה יכול להיות גם תמונת צבעונית עם ייצוג דיסקטי ותמונת צבעונית עם ייצוג רציף
[00:14:04 - 00:14:07] בסדר, היום אנחנו נדבר על הייצוג הדיסקטי
[00:14:08 - 00:14:10] מודלים מאוד אינגרסיביים
[00:14:11 - 00:14:14] פיתחו אותם בהתחלה עם גישה כזאת, זה היה יותר קל
[00:14:15 - 00:14:17] אבל כל מה שאנחנו רואים היום גם אפשר לעשות
[00:14:18 - 00:14:20] בייצוג רציף
[00:14:27 - 00:14:32] אז מודלים, בשביל הייצוג הדיסקרטי בעצם המודלים שדיברנו עליהם
[00:14:33 - 00:14:37] אנחנו ממשיכים לדבר עליהם היום, זה מודל ברנולי ומודל קטגורי
[00:14:38 - 00:14:41] ברנולי זה פשוט אומר שבמקומה שלנו יש רק
[00:14:41 - 00:14:43] לכל פיקסל יש רק שני ערכים
[00:14:44 - 00:14:45] אז אנחנו,
[00:14:46 - 00:14:47] יש לנו פרמטר אחד בעצם,
[00:14:47 - 00:14:48] שאומרים לנו מה ההסתברות
[00:14:49 - 00:14:51] שהפיקסל יהיה לבן
[00:14:52 - 00:14:55] והאחד פחות ההסתברות זה ההסתברות שהוא יהיה שחור
[00:14:56 - 00:15:00] זה קטגורי, זה שיש לנו, נגיד אם אנחנו לוקחים באמת
[00:15:00 - 00:15:02] 256 ערכים לכל פיקסל
[00:15:03 - 00:15:06] ואנחנו צריכים 255 מספרים כאלה שהם כולם
[00:15:08 - 00:15:08] בין 0 ל-1
[00:15:09 - 00:15:10] כשהסכום שלהם שווה 1
[00:15:12 - 00:15:13] הסכום שלהם קטן מ-1,
[00:15:13 - 00:15:16] כי אז יש את הערך האחרון שהוא משלים ל-1
[00:15:17 - 00:15:19] אני צריך 255 מספרים כדי
[00:15:23 - 00:15:36] למדל את ההסתברות הקטגורית של כל ההסתברות הקטגורית של קטגורי זה העיתוק שאנחנו נדבר עליו היום, ואם אנחנו בשבוע הבא אני חושב שכבר נעבור לייצוג רציף
[00:15:36 - 00:15:39] אז דיברנו כבר על גאוסיאן, זה יכול להיות ייצוג רציף
[00:15:40 - 00:15:42] למרות שהוא ייצוג לא כל כך טוב כי אם הערכים שלנו
[00:15:43 - 00:15:47] הם בין 0 ל-1 ולא יכולים להיות קטנים מ-0 ולא יכולים להיות גדולים מ-1
[00:15:47 - 00:15:53] אז אנחנו גם לא יכולים בעצם על ידי גאוסיאן למדל את אותו דבר הראשון, גאוסיאן יש פה
[00:15:54 - 00:15:55] יכול לקבל את כל האחרון
[00:15:56 - 00:16:02] אבל עדיין הרבה פעמים מקרבים את זה על ידי איזשהו גאוסיאן או איזושהי תערובת של גאוסיאן כמו שראינו
[00:16:12 - 00:16:14] עד עכשיו בעצם מה שאנחנו ראינו,
[00:16:15 - 00:16:17] לא כל כך נכנסנו ממש למודלים של תמונות,
[00:16:17 - 00:16:24] אבל בכל הדוגמאות שדיברנו בכיתה כשאמרנו שאנחנו עושים מידול ברנולי או קטגורי אז באמת אמרנו
[00:16:25 - 00:16:28] שקבענו שאנחנו עושים פרמטרידציה מלאה של ההתפלגות הזאת
[00:16:29 - 00:16:31] זאת אומרת אנחנו פשוט שומרים את הפרמטר
[00:16:32 - 00:16:34] אם זה ברנולי אנחנו שומרים את הפרמטר פיד
[00:16:34 - 00:16:39] אם זה קטגורי אז יש לנו פרמטר שאומר מה ההסתברות של כל אחד מהערכים האפשריים
[00:16:40 - 00:16:43] אם זה גאוסיאן אז יש לנו את התוחלת פשוט מספר שאומר מה התוחלת
[00:16:44 - 00:16:46] ועוד איזה מספר שאומר מה הבעיה,
[00:16:46 - 00:16:50] ‫אם זה רב-לימדי, אז זה כמה מספרים ‫לווקטור במטריצה.
[00:16:51 - 00:16:59] ‫אבל זאת הכוונה שאנחנו... ‫זה ייצוג, זה ייצוג, זה פרמטריזציה פשוטה. ‫זה פשוט הפרמטריזציה המלאה של ה... של הבעיה.
[00:17:00 - 00:17:06] ‫היום אנחנו נדבר על פרמטריזציה ‫קצת יותר שעושה כל מיני הנחות,
[00:17:06 - 00:17:09] ‫אלא היא מבוססת על רשתות כמות.
[00:17:09 - 00:17:14] ‫ככה, תכף נסביר קצת יותר מה זה, ‫תכף נסביר קצת יותר מזמן.
[00:17:17 - 00:17:20] ‫טוב.
[00:17:26 - 00:17:33] ‫אוקיי, מה הבעיה עם פרמטריזציה מלאה? ‫זה מה שאמרנו, פרמטריזציה מלאה, ‫אנחנו די מהר,
[00:17:34 - 00:17:36] ‫מספר הפרמטרים גדל.
[00:17:37 - 00:17:40] ‫וגם אם אנחנו עושים בייצוג רציף, ‫גם בייצוג בדיד,
[00:17:41 - 00:17:42] ‫ראינו למשל שבתמונה,
[00:17:44 - 00:17:46] ‫אם אנחנו, יש לנו n פיקסלים,
[00:17:46 - 00:17:49] ‫נכון, אנחנו נצטרך שתיים, ‫נגיד שזו תמונה אפילו בינארית,
[00:17:50 - 00:17:53] ‫לא עם 256 ערכים בכל פיקסל, ‫אין שני ערכים בכל פיקסל.
[00:17:54 - 00:17:59] ‫אחרי זה יש לנו n פיקסלים, ‫אנחנו נצטרך שתיים בפזקת n ערכים ‫כדי
[00:18:02 - 00:18:07] לתת איזושהי הזדברות קטגורית ‫בכל אחת מהאפשרויות.
[00:18:07 - 00:18:08] ‫יש שתיים בפזקת n אפשרויות.
[00:18:11 - 00:18:14] ‫אז אנחנו צריכים, דיברנו על כל מיני,
[00:18:14 - 00:18:18] ‫כל מיני הנחות שאנחנו נעשו בדרך.
[00:18:19 - 00:18:20] ‫השימוש
[00:18:23 - 00:18:26] ‫בעל השרשרת של ההסתברות, נכון? ‫אז כל הסתברות כזאת היא
[00:18:27 - 00:18:30] של n פיקסלים יכולה לפרק ‫באותו
[00:18:30 - 00:18:33] ‫התפלגות, שרשרת של התפלגויות מותנות כאלה,
[00:18:34 - 00:18:37] ‫שכל אחד מותנה על כל אלה שלפניו,
[00:18:37 - 00:18:38] ‫שיינתן איזשהו סידור,
[00:18:39 - 00:18:41] ‫וכל סידור הוא סידור חוקי.
[00:18:42 - 00:18:43] זה תמיד נכון.
[00:18:44 - 00:18:48] ‫אבל זה עדיין, ראינו, ‫זה עדיין לא חוסך לנו את הפרמטרים, נכון? ‫כי פה,
[00:18:50 - 00:18:50] מה קורה כאן?
[00:18:52 - 00:18:55] ‫כאן יש לנו הרבה פחות פרמטרים. ‫בשביל הפרמטר הראשון יש לנו רק פרמטר אחד.
[00:18:55 - 00:18:58] ‫בשביל הפרמטר השני יש לנו,
[00:18:59 - 00:19:03] ‫כל ערך של x1 יש לנו פרמטר אחד ל-x2.
[00:19:04 - 00:19:05] ‫נדבר עכשיו לדוגמה ביננית.
[00:19:07 - 00:19:13] זה ברור? אמיתי? כן? אז אם x1 היה שווה 0, ‫אני צריך להגיד מה ההסתברות של x2 שווה 1.
[00:19:14 - 00:19:18] ‫אז יכול להיות שההסתברות של x2 ‫היא משהו אחר.
[00:19:19 - 00:19:20] ‫זו הכוונה, שזה מותנה.
[00:19:21 - 00:19:27] ‫אז אני צריך עבור כל ערך של x1 ‫לתת את ההסתברות של x2, ‫שבו נקרא שתרנו לי זה רק מספר 1.
[00:19:28 - 00:19:29] ‫אבל האחרון פה,
[00:19:30 - 00:19:31] ‫אז הוא מותנה על כל הקודמים,
[00:19:32 - 00:19:33] ‫על n-1 קודמים,
[00:19:34 - 00:19:36] ‫אז עבור כל קומבינציה שונה של כל הקודמים,
[00:19:37 - 00:19:39] ‫אני צריך לתת מספר עבור הפרמטר האחרון.
[00:19:40 - 00:19:47] ‫אז הטבלה שתארת את הדבר הזה, ‫הגודל שלה יהיה n-1 על 1.
[00:19:48 - 00:19:53] סליחה, כל האפשרויות של n מינוס 1 ‫זה 2 בחזקת n-1 על 1.
[00:19:54 - 00:20:00] ‫אז לא חסכתי את האקספוננציאליות ‫של הבעיה בפרמטרים,
[00:20:00 - 00:20:06] ‫אבל זה הגיוני, אם זה תמיד נכון, ‫אז אני לא עושה פה שום הנחה, ‫לא יכול להיות שהורדתי את מספר הפרמטרים שאני רוצה.
[00:20:06 - 00:20:11] ‫אוקיי, אז איך אפשר בכל זאת ‫להשתמש בדבר הזה כדי
[00:20:13 - 00:20:16] ‫לבנות מודל סביר?
[00:20:17 - 00:20:18] ‫אז בעצם דיברנו על שתי אפשרויות.
[00:20:20 - 00:20:22] ‫האפשרות פה משמאל,
[00:20:24 - 00:20:31] ‫זו האפשרות שאנחנו עושים, ‫אנחנו מניחים שיש conditional independence, ‫איטלוט מותנת,
[00:20:32 - 00:20:36] ‫על המשתנים שאנחנו רואים, ‫שבמקרה של תמונה זה פשוט הפיקסל,
[00:20:36 - 00:20:43] ‫בעצם אנחנו אומרים, ‫הההסתברות של כל התמונה שלנו, ‫של כל הפיקסלים,
[00:20:44 - 00:20:45] ‫היא שווה לאיזושהי מכפלה,
[00:20:46 - 00:20:49] ‫גם קודם זו הייתה מכפלה, נכון? ‫אבל גם הייתה מכפלה כזאת,
[00:20:51 - 00:20:54] ‫אבל כאן המכפלה הזאת, ‫כל xi, כל פיקסל,
[00:20:54 - 00:20:56] ‫הוא לא תלוי בכל הקודמים שלו,
[00:20:57 - 00:20:59] ‫אלא הוא תלוי רק בנגיד k קודמים,
[00:21:00 - 00:21:04] ‫כל הxים מ-i מינוס k עד i מינוס 1.
[00:21:04 - 00:21:13] ‫אז זה בעצם אומר שאני מניח ‫אי תלות מסוימת של פיקסל, ‫בהינתן הk הפיקסלים הקודמים שלו, ‫לפי איזשהו סידור,
[00:21:14 - 00:21:15] ‫הוא לא תלוי בשאר הפיקסלים.
[00:21:18 - 00:21:21] ‫אוקיי, אז זו אופציה אחת ‫שאנחנו יכולים לעשות,
[00:21:22 - 00:21:28] ‫וזה חושף לנו הרבה פרמטרים, ‫כי עכשיו כמה פרמטרים יהיה לנו.
[00:21:28 - 00:21:31] ‫במקום שאני צריך לבדוק ‫את כל האפשרויות ל-n
[00:21:32 - 00:21:34] מינוס 1 איברים, זה רק k,
[00:21:34 - 00:21:34] ‫אני חוסם את זה.
[00:21:35 - 00:21:41] ‫כן, אז עדיין יש לי משהו אקספוננציאלי, ‫אבל אני יודע, זה יכול להיות בגודל קטן, ‫כן, יכול להיות עם עשרה פיקסלים,
[00:21:42 - 00:21:45] ‫זה יהיה לי רק 2 פסקת 10 אפשרויות ‫לכל קבלת.
[00:21:47 - 00:21:49] ‫כן, אז זה 2 פסקת 10, כפול,
[00:21:50 - 00:21:53] ‫במקרה זה b שלי 2, פיקסל, פיקסל, בינארי, ‫זה המימד של כל פיקסל,
[00:21:54 - 00:21:55] ‫כפול n פיקסלים.
[00:21:57 - 00:21:58] ‫מה זה b?
[00:21:59 - 00:22:03] בי זה 2 או 256, זה כמה ערכים הפיקסל יכול לקבל.
[00:22:04 - 00:22:05] ‫זה המימד, כאילו, של ה...
[00:22:06 - 00:22:08] ‫של מופער הקבע.
[00:22:12 - 00:22:18] ‫אוקיי, אז הורדנו פה, בעצם ההורדה כאן ‫היא בזה שהחזקה היא ב-k ולא ב-n מינוס 1.
[00:22:21 - 00:22:25] ‫אבל אם אני רוצה שכל פיקסל יעלה תביא ‫בעשרה פיקסלים האחרונים, ‫אז גם בפיקסל האחרון,
[00:22:26 - 00:22:27] ‫שהיו מיליון פיקסלים לפניו,
[00:22:28 - 00:22:30] ‫אבל צריך לסתכל על כל מיליון האפשרויות,
[00:22:32 - 00:22:33] ‫אלא רק 2 פסקת 10 אפשרויות.
[00:22:34 - 00:22:39] ‫אוקיי, אז זו גישה אחת, ‫שאנחנו נדבר עליה קצת יותר בהרחבה היום.
[00:22:42 - 00:22:45] ‫קודם כול, מה הבעיה בגישה הזאתי? ‫ככה, כמו שהיא כתובה עכשיו,
[00:22:46 - 00:22:52] ‫זה שאנחנו בעצם מעבדים כאן הרבה מידע. ‫לפעמים אנחנו כן רוצים שהפיקסל יהיה תלוי ‫בכל מה שקרה לפני זה.
[00:22:53 - 00:22:55] ‫אבל אנחנו נראה שיש דרכים, בעצם,
[00:22:56 - 00:23:02] ‫יש דרך לעשות את זה, שזה כן יהיה תלוי ‫בכל הפיקסלים הקודמים, ‫אבל עם מספר יותר קטן של פרמרטיה, ‫לכך אנחנו נדבר עליו היום.
[00:23:02 - 00:23:12] ‫מה בעיינים של הגרסורה? ‫לא, אתה יכול להחליט, ‫זה מה שאנחנו נעשה היום, ‫שאנחנו מסדרים את הפיקסלים ככה, ‫הפיקסלים הסדר על זה,
[00:23:13 - 00:23:14] ‫זאת אומרת שאנחנו מגיעים לפיקסל הזה,
[00:23:15 - 00:23:17] ‫כל אלה זה הפיקסלים שהיו לפניו.
[00:23:18 - 00:23:20] ‫נכון, ואז אם אתה רוצה להיות ‫אנושא יותר לוקאבי,
[00:23:21 - 00:23:27] ‫לא עושה פיקסלים בעבודה הכי נמוכה. ‫נכון, בסדר, נכון. ‫אז בסדר הזה נגיד הפיקסל הזה,
[00:23:27 - 00:23:30] ‫אם הוא תלוי בעשרה הקודמים, ‫אז דווקא הוא תלוי בעשרה האלה.
[00:23:31 - 00:23:38] ‫-לא קשורים להבטאה, כן, נכון. ‫אז הסידור הזה מכניס ‫כל מיני דברים מודרים. ‫כן, אנחנו נרצה להתגבר על הדבר הזה,
[00:23:39 - 00:23:45] ‫וגם אולי שבאמת כל טקסט ‫יהיה תלוי בסביבה הקרובה שלו יותר ‫מאשר בדברים הקרובים,
[00:23:45 - 00:23:49] ‫וגם אבל שיהיה תלוי בכמה שיותר, ‫שאולי אפילו איכשהו ‫יהיה תלוי בהכול.
[00:23:50 - 00:23:52] ‫אז אנחנו נראה ש...
[00:23:53 - 00:23:56] ‫שתפתי את זה כאילו זה הדרך היחידה, ‫שזה זה שתי הדרכים היחידות, ‫אבל יש בעצם,
[00:23:56 - 00:24:01] ‫אפשר אפשר לשאול את זה בתור דרך שלישית ‫או איזושהי אדפטציה של הדרך הזאת ‫שתכף נראה אותך.
[00:24:03 - 00:24:05] ‫בדרך אחרת, שאנחנו לא נדבר עליה היום,
[00:24:07 - 00:24:11] ‫דיברנו עליה בעיקר בשבועיים-שלושה האחרונים, ‫ואנחנו נחזור אליה בהמשך,
[00:24:11 - 00:24:13] ‫זה שאנחנו,
[00:24:13 - 00:24:19] ‫הפיקלויות האלה שאנחנו עושים, ‫זה בהינתן משתנה פבוי חדש, ‫אנחנו נמצאים משתנה חדש,
[00:24:20 - 00:24:20] ‫אנחנו לא רואים אותו,
[00:24:21 - 00:24:24] ‫במקום שכל פיקסל, בהינתן השכנים שלו, ‫לא תלוי בשאר,
[00:24:24 - 00:24:29] ‫כל פיקסל, בהינתן איזה משהו מומצא חדש, ‫הוא בלתי תלוי בכל השאר.
[00:24:30 - 00:24:31] ‫זה מה שכתוב כאן,
[00:24:32 - 00:24:35] ‫כל פיקסל הוא רק פונקציה ‫של איזשהו משתנה פבוי,
[00:24:37 - 00:24:37] ‫שלא בא איתו.
[00:24:40 - 00:24:48] ‫עכשיו זה גם דרך להוריד ‫מספר הפרמטרים, ‫כי במקום ה-D בחזקת K, ‫שזה בעצם מספר האפשרויות שיש לי כאן,
[00:24:49 - 00:24:52] ‫אז זה יהיה מספר האפשרויות שיש לזה ז. ‫אז אם ז הוא גם נזקרטי,
[00:24:52 - 00:24:56] ‫אז מספר האפשרויות של Z, ‫נזקרטי כאן
[00:24:58 - 00:25:00] ‫המותפחית ל-H הוא Z,
[00:25:01 - 00:25:05] ‫אז מספר האפשרויות של Z כפול,
[00:25:06 - 00:25:10] ‫זה אותו דבר שאני חודש, ‫כמה פרמטרים אני צריך
[00:25:11 - 00:25:14] ‫בשביל למדד לדבר מולי, ‫את הקטגורית של פיקסל 1,
[00:25:15 - 00:25:16] ‫כפול מספר החוץ מ-Z.
[00:25:18 - 00:25:20] ‫אוקיי, אז האפשרות הזאת אנחנו לא נדבר היום,
[00:25:21 - 00:25:24] ‫ונחזור אליה בהקשר. ‫אנחנו נתמקד באפשרות הזאת,
[00:25:25 - 00:25:27] ‫עם איזושהי אדפטציה ‫שכן מאפשרת להיות תלוי
[00:25:28 - 00:25:39] בכל השאר, ‫להיות תלוי יותר בקרובים ‫וקצת בכל השאר, ‫באיזושהי דרך שלא בלי פרמטריזציה מלאה, ‫שנסתכלת על כל האפשרויות של הפרמטריזציה,
[00:25:40 - 00:25:41] ‫וזה יהיה יותר נגיד.
[00:25:42 - 00:25:44] ‫עדיין לא נמצאת מספר אקספוניציה ‫על איזושהי פרמטריזציה.
[00:25:45 - 00:25:54] ‫אוקיי, איך אנחנו מאמנים? ‫אז זהו עדיין, אנחנו עדיין בשלב החזרה,
[00:25:56 - 00:25:58] ‫אנחנו עושים עדיין תזכורת קצת.
[00:25:59 - 00:26:06] ‫אז איך אנחנו מאמנים את המגלים האלה? ‫אז אמרנו, אנחנו רוצים למצוא את ה-Teta ‫ככה ש-P-Teta יהיה כמה שיותר כך פי-דאטה,
[00:26:07 - 00:26:10] ‫ודרך סטנדרטי לעשות עם זה זה ‫מקסימום לייקליות.
[00:26:11 - 00:26:23] ‫נכון, היה לנו את הדרשים הזה, ‫שאנחנו מחפשים מתוך כל המודלים האפשריים ‫במשפחה שהגדרנו את ה-Beta ‫ככה שה-P-Teta יהיה כמה שיותר קרוב ל-P-Data.
[00:26:24 - 00:26:28] ‫אין לנו באמת גישה לפי-דאטה, ‫יש לנו עוד גישה לדגימות מתוך P-Data.
[00:26:30 - 00:26:33] ‫אבל בעצם כשמחפשים מקסימום לייקליות, ‫זה קצת אפילו לעשות מינימום,
[00:26:34 - 00:26:38] ‫להקהיל דייברג'נס ‫בין ה-P-Data ל-P-Teta.
[00:26:41 - 00:26:46] ‫ראינו פה את הפיתוח הזה שמראה ‫שלעשות מקסימום לייקליות, ‫זה כמו לעשות מינימום בין ה-P-Data ל-P-Data.
[00:26:49 - 00:26:53] ‫כי ה-Data שווה ל-Likelihood,
[00:26:53 - 00:26:54] ‫או ל-Mינוס ה-Likelihood,
[00:26:54 - 00:26:55] ‫בעוד איזה שהוא קבוע,
[00:26:56 - 00:26:58] ‫קבוע זהו האנטרופיה של ה-Data.
[00:27:02 - 00:27:02] אתם זוכרים את זה?
[00:27:03 - 00:27:06] כן, בעצם הדבר הזה, ‫למה זה יש פה קירוב? ‫כי במקור לנושא את האינטגרל,
[00:27:07 - 00:27:09] ‫אנחנו לא יכולים לראות את כל P-Data.
[00:27:10 - 00:27:15] ‫אנחנו מסתכלים על דגימות מתוך P-Data. ‫זו שיטת מונטה-קרווה, ‫שגם דיברנו עליה בהקשרים אחרים.
[00:27:16 - 00:27:21] ‫אנחנו עושים שערוך של האינטגרל הזה ‫על ידי דגימות של ה-training set שלנו,
[00:27:22 - 00:27:25] ‫אז יש לנו פה את הממוצע הזה, ‫במקום האינטגרל,
[00:27:26 - 00:27:28] ‫על לוג של P-Data,
[00:27:29 - 00:27:34] ‫זה בעצם ההסתברות תחת המודל שלנו, ‫או לוג ההסתברות תחת המודל שלנו,
[00:27:35 - 00:27:36] ‫של הדוגמה ה-I.
[00:27:36 - 00:27:42] ‫אז זה בעצם ה-objective function שלנו, ‫שאנחנו נרצה למזער.
[00:27:44 - 00:27:47] ‫אנחנו נרצה למקסם את לוג ההסתברות ‫או למזער את מינוס לוג ההסתברות,
[00:27:49 - 00:27:50] ‫על כל הדוגמאות שאנחנו רואים.
[00:27:54 - 00:27:56] ‫עוד משמעות שיש לדבר הזה,
[00:27:56 - 00:28:00] ‫זה קצת עוזר להבין ‫מה המספרים שיוצאים,
[00:28:01 - 00:28:02] ‫שמאמנים את הדברים האלה,
[00:28:03 - 00:28:04] ‫שעושים להם אבאלואציה.
[00:28:07 - 00:28:10] ‫זה בעיקר את הכיף כשהמודל הוא דיסקרטי.
[00:28:12 - 00:28:13] ‫כשהדאטה הוא דיסקרטי,
[00:28:13 - 00:28:15] ‫אנחנו ממנים את הפיקסלים ‫בצורה דיסקרטית.
[00:28:18 - 00:28:30] ‫אז המשמעות של הלוג-לייקליות ‫זה בעצם מספר הביטים ‫שצריך, מספר הביטים האידיאלי
[00:28:30 - 00:28:36] ‫שצריך, כדי לשדר את התמונה הזאת ‫למישהו אחר.
[00:28:37 - 00:28:40] ‫אז אם מישהו אחר מכיר את המודל שלנו, ‫נגיד למדנו איזשהו מודל,
[00:28:42 - 00:28:43] ‫ולמישהו אחר מכיר אותו,
[00:28:44 - 00:28:48] ‫אז בכמה ביטים אנחנו נצטרך ‫כדי לשדר את המודל הזה.
[00:28:48 - 00:28:50] ‫אם היה לנו את המודל המושלם, את P-Data,
[00:28:50 - 00:28:53] ‫זאת אומרת, ה-L-Divergence היה 0, ‫והיהיה נשאר לנו רק,
[00:28:54 - 00:28:55] ‫כפי שכתוב כאן ה-Cons,
[00:28:55 - 00:28:58] ‫שזה אינטגרל של P-Data, לא P-Data,
[00:28:59 - 00:29:00] ‫לאנטרופיה, יש לי דעה,
[00:29:01 - 00:29:02] ‫אז זה בעצם
[00:29:05 - 00:29:08] ‫הקידוד האידיאלי של תמונות,
[00:29:09 - 00:29:13] ‫הוא מסתמך על ההסתברות האמיתית ‫שממנה
[00:29:14 - 00:29:15] התמונות נצרו,
[00:29:17 - 00:29:22] ‫והעורך של הקידוד הזה שווה לאנטרופיה ‫של ההסתברות הזאת.
[00:29:22 - 00:29:24] ‫שוק הנסה תורת האינפורמציה?
[00:29:25 - 00:29:26] ‫אוקיי,
[00:29:27 - 00:29:30] אולי אתם מבינים פחות או יותר ‫מה אני מתכוון.
[00:29:33 - 00:29:34] ‫ברגע שהמודל שלנו הוא לא בדיוק,
[00:29:36 - 00:29:38] ‫לא בדיוק האנטרופיה, ‫הוא לא בדיוק המודל האמיתי,
[00:29:39 - 00:29:43] ‫אז מה שנשאר לנו כאן זה לא אנטרופיה, ‫זה אנטרופיה פלוס ה-KL Divergence הזה,
[00:29:45 - 00:29:49] ‫וה-KL Divergence הוא בעצם אומר לנו ‫כמה ביטים נוספים אנחנו צריכים,
[00:29:50 - 00:29:53] ‫אנחנו משלמים על זה שהמודל שלנו ‫הוא לא מושלם.
[00:29:53 - 00:30:00] ‫בגלל זה גם קוראים לזה קורס אנטרופיה. ‫זה אנטרופיה בין המודל שלנו למודל האמיתי.
[00:30:01 - 00:30:03] ‫זה אומר כמה ביטים אנחנו נצטרך
[00:30:03 - 00:30:09] ‫למודל דאטה שהגיעה מהתפלגות P-Data ‫על ידי קוד שנבנה על ידי P-Teta.
[00:30:11 - 00:30:17] אוקיי, אז אני אומר את זה ‫כדי שכשאנחנו נראה עוד מעט ‫כל מיני מספרים בסוף השיעור ‫על כל מיני מודלים,
[00:30:18 - 00:30:19] ‫מה הלוג-לייקיות שהם משיגים,
[00:30:20 - 00:30:24] ‫אז זה קל, לפעמים קצת קשה להבין ‫את המספרים האלה,
[00:30:25 - 00:30:30] ‫אבל כשחושבים על זה ככה, ‫זה קצת עוזר להבין ‫מה המשמעות של זה.
[00:30:31 - 00:30:35] ‫כמה ביטים צריך כדי לשדר את כל תמונה.
[00:30:36 - 00:30:40] ‫או אם בממוצע כמה תמונות, ‫כמה ביטים בממוצע ‫אם נצטרך כדי לשדר תמונות.
[00:30:41 - 00:30:50] ‫אוקיי, הדבר הזה נכון ‫רק לייצוג דיסקרטי, לייצוג רציף,
[00:30:51 - 00:30:53] ‫המשמעות של איטרופיה,
[00:30:54 - 00:30:55] דייברדנס,
[00:30:56 - 00:31:00] ‫פורמציה בלית כל זה, ‫הוא לא בדיוק אותו דבר, ‫והכל דיסקרטי יחסי,
[00:31:01 - 00:31:04] ‫כבר אין לזה כל כך משמעות ישירה כמו זאת,
[00:31:05 - 00:31:08] ‫אבל כשמספרים עדיין אפשר להשוות ‫בין מודלים שונים,
[00:31:08 - 00:31:16] ‫והמשמעות של זה, ‫ואפשר עכשיו לבדוק ‫כמה המודל קרוב למודל האמיתי שאתם מצטרפים בו.
[00:31:16 - 00:31:19] ‫היום אני לא מדבר על דאטה רציף, ‫אבל כשנגיע לזה אולי
[00:31:20 - 00:31:23] ‫קצת ננסה להבין ‫מה המשמעות של המסברים שיש.
[00:31:25 - 00:31:25] ‫טוב,
[00:31:25 - 00:31:29] ‫אז נחזור רגע לשתי אפשרויות שיש לנו. ‫אז אנחנו נראה היום,
[00:31:29 - 00:31:33] ‫רוצים לתת כאן את התמונות האלה ‫שדיברנו עליהן.
[00:31:39 - 00:31:45] ‫כבר דיברנו בעיקר על הדבר הזה ‫בשגרונות האחרונים, ‫היום נדבר על מה שקורה כאן.
[00:31:46 - 00:31:48] ‫אז בעצם לעשות,
[00:31:48 - 00:31:55] ‫היתרון של המודל הזה זה שאנחנו יכולים ‫לחשב את הלייפטיות שלו בצורה ישירה, ‫וקל לעשות אופטימיזציה.
[00:31:56 - 00:32:01] ‫כי בעצם יש לנו גישה ‫לכל מה שמחושב כאן, אוקיי?
[00:32:02 - 00:32:07] ‫המודל הזה הוא של XI, ‫בהינתן באיזשהם X'ים אחרים, ‫אנחנו רואים את כל ה-X'ים האלה, ‫בהינתן דוגמה שיש לנו,
[00:32:08 - 00:32:08] ‫אנחנו רואים את הכול.
[00:32:09 - 00:32:10] ‫אנחנו יכולים לחשב את P,
[00:32:11 - 00:32:15] ‫אנחנו יכולים לחשב את כל אחד מה-P'ים ‫האלה במכפלה הזאת, ‫ולהכפיל את כולם.
[00:32:16 - 00:32:18] ‫זה נותן לנו את ההסתברות,
[00:32:20 - 00:32:23] ‫הלייקליות של הדאטה שאנחנו רואים,
[00:32:24 - 00:32:26] של המודל, ‫על פני הדאטה שאנחנו רואים,
[00:32:27 - 00:32:31] ‫לנסות לזה load, לספום על כל הנדונות ‫שיש לנו ולעשות איזו אופטימיזציה.
[00:32:32 - 00:32:36] ‫כי אז אופטימיזציה אולי לא בטוח שהיא קלה, ‫אנחנו כתבואים במודל עצמו,
[00:32:36 - 00:32:37] ‫אבל לפחות לחשב את זה,
[00:32:38 - 00:32:41] ‫יש לנו את כל האינפורמציה ‫שאנחנו צריכים כדי לחשב את זה בצורה ישירה.
[00:32:42 - 00:32:50] ‫לעומת הדבר הזה, שזה בדיוק הדברים ‫שדיברנו בשבועות האחרונים, זה מסובך, ‫אנחנו צריכים לעשות כל מקום ‫איזשהו פוסטיריו על ה-Z,
[00:32:51 - 00:32:54] ‫אנחנו לא רואים את זה, ‫אנחנו לא יכולים לחשב את זה ‫בצורה ישירה, ‫כי אין לנו את זה.
[00:32:55 - 00:32:56] ‫גם אם יש לנו את ה...
[00:32:57 - 00:33:00] ‫יש לנו איזשהו מודל, ‫אנחנו רוצים לדעת ‫מה ההסתברות שנותן לנו לדאטה,
[00:33:01 - 00:33:02] ‫אין לנו דרך ישירה לעשות את זה.
[00:33:03 - 00:33:05] צריכים בדרך כלל לחשב בשביל פוסטריאור על Z
[00:33:06 - 00:33:08] ופוסטריאור על זה לשארך את
[00:33:09 - 00:33:12] ההסתברות של הדף, זה קצת מסובך, או על ידי דגימות
[00:33:13 - 00:33:16] או על ידי וריאשן אינפורנט ובמקרים מסוימים
[00:33:17 - 00:33:21] אנחנו יכולים לעשות את וריאשן האינפורנטס עד הסוף, אבל אפשר להיכנס ב-in
[00:33:22 - 00:33:25] וזה קצת יותר מסובך
[00:33:26 - 00:33:31] הדוגמאות של הגישה הזאת שנראה היום זה מיני פיקסל רנן ופיקסל קנן,
[00:33:32 - 00:33:33] אז אנחנו נדבר עליהם היום,
[00:33:33 - 00:33:37] הם כולם בעצם מפרקים את הדאטה לייצוג הזה,
[00:33:38 - 00:33:39] המודל בעצם עונה רק ככה
[00:33:40 - 00:33:43] וזה יהיה שאר אחוז, המודלים שנראים ככה,
[00:33:44 - 00:33:45] GMM כבר ראינו,
[00:33:46 - 00:33:47] כבר יש לנו מנקודרס,
[00:33:49 - 00:33:50] נורמלייזינג פלואוד, דיפיוז'ן מודלס,
[00:33:51 - 00:33:54] הם כולם אפשר לחשוב עליהם בתור latency כזה אחד
[00:33:55 - 00:33:59] וכל הפיקסלים מביאים את הפונקציה של ה-Latence,
[00:33:59 - 00:34:01] לפעמים יש לנו הירארציה של latency,
[00:34:02 - 00:34:08] שמודל ב-Latence ולפעמים זה לא בדיוק הדרך הראשונית
[00:34:08 - 00:34:12] לחשוב על המודל אבל כל המודלים האלה אפשר גם לתאר אותם בתור
[00:34:13 - 00:34:14] מודל שהוא כתוב כך.
[00:34:16 - 00:34:17] עוד אחד שלא קטעתי כאן זה GAN,
[00:34:18 - 00:34:23] זה גם מודל שאפשר לקטוא אותו ככה,
[00:34:24 - 00:34:26] Generative Unversarial Networks,
[00:34:27 - 00:34:28] כנראה לא נספיק לעשות
[00:34:31 - 00:34:32] קצת
[00:34:33 - 00:34:35] אבל הוא משתמש לפרק קטע לי גדולה גם במודלמות.
[00:34:37 - 00:34:38] אז כל זה זה יהיה בהמשך הקורס,
[00:34:39 - 00:34:45] והיום אנחנו בצד הזה שהכל קל, אוקיי? אין לנו משתנים חבועים, אנחנו רואים את כל הדאטה,
[00:34:45 - 00:34:49] והמטרה שלנו זה למצוא את המודל שמבצעים אותו לרדות.
[00:34:49 - 00:35:07] אוקיי, אז בעצם אמרתי שאנחנו עושים כאן את ההנחה שאנחנו עושים, זה שכל פיקסל תלוי ב-K הפיקסלים הקודמים וזו לא הנחה כל כך טובה,
[00:35:08 - 00:35:11] אז עכשיו נראה שבעצם אפשר לעשות איזשהו trade-off כזה בין
[00:35:12 - 00:35:13] כמה אנחנו
[00:35:15 - 00:35:17] מצמצמים את ההיסטוריה הזאת שאנחנו תלויים בה,
[00:35:17 - 00:35:19] ואיך אנחנו עושים את הפרמטריזציה.
[00:35:20 - 00:35:21] אז
[00:35:23 - 00:35:25] הרעיון הוא שאם אנחנו לא עושים פרמטריזציה מלאה,
[00:35:25 - 00:35:26] זאת אומרת אם אנחנו לא אומרים,
[00:35:27 - 00:35:28] עוברים על כל האפשרויות,
[00:35:29 - 00:35:30] מקור הבעיה שלנו הייתה שכאן,
[00:35:36 - 00:35:36] גם פה,
[00:35:37 - 00:35:38] היינו צריכים לבדוק את כל האפשרויות,
[00:35:39 - 00:35:42] מה אם אנחנו רוצים לראות טבלה של הדבר הזה, זה היה לנו 2 בחזקת n-1
[00:35:44 - 00:35:48] כל המלחמת הטבלה של 2 בחזקת n-1 אפשרויות,
[00:35:50 - 00:35:51] ופה אם זה בינארי זה היה רק
[00:35:52 - 00:35:54] על הדרך כלל, אבל בגלל בינארי זה היה
[00:35:56 - 00:36:04] 2 בחזקת n-5 חמישה מספרים על 2 בחזקת n-1 אפשרויות לטבלה הזאת היו יותר מדי שורות.
[00:36:06 - 00:36:07] אז אולי אנחנו יכולים,
[00:36:08 - 00:36:13] הרעיון כאן הוא שאולי אנחנו עדיין רוצים להיות תלויים בכל המשתנים האלה,
[00:36:14 - 00:36:16] אבל לא לתת ממש שורה לכל משתנה,
[00:36:16 - 00:36:18] אוקיי? לא להקדיש לכל משתנה שורה,
[00:36:18 - 00:36:22] אלא לעשות איזושהי פרמטריזציה משותפת כזאת של דעתו.
[00:36:23 - 00:36:27] אוקיי? אם אנחנו עושים פרמטריזציה לא מלאה, לדבר הזה קוראים פרמטריזציה מלאה.
[00:36:28 - 00:36:28] זה פשוט נותן,
[00:36:28 - 00:36:29] מסתכל על כל האפשרויות,
[00:36:30 - 00:36:33] כל אפשרות נותן את ההסתברות שנובעת מזה.
[00:36:33 - 00:36:38] אנחנו נגיד, נרצה שההסתברות שלנו תהיה איזושהי פונקציה של כל ה-x'ים האלה,
[00:36:39 - 00:36:40] ולא
[00:36:42 - 00:36:45] שורה שונה לכל אחד מה-x'ים.
[00:36:45 - 00:36:46] אוקיי?
[00:36:46 - 00:36:48] לפונקציה הזאת תהיה פרמטרים משותפים.
[00:36:48 - 00:36:51] ככה נהיה פחות פרמטר מ-2 עד כת אין ה-2.
[00:36:52 - 00:36:53] וזה יהיה הרעיון.
[00:36:54 - 00:36:58] עדיין אנחנו כנראה נרצה שלפיקסלים שהם איכשהו קרובים ל-x1, יהיה קצת יותר,
[00:36:59 - 00:37:00] יהיה להם יותר משקל.
[00:37:01 - 00:37:04] איכשהו הם ישביעו יותר על הפונקציה הזאתי
[00:37:05 - 00:37:06] מאשר הפיקסלים האלה.
[00:37:09 - 00:37:10] אז זה הרעיון של מה שנעשה היום.
[00:37:16 - 00:37:22] אוקיי, ודוגמה לאיך אפשר לעשות את זה זה עם Logistic Regression למשל.
[00:37:22 - 00:37:24] הנה אנחנו מדברים על הבעיה הבינארית. אנחנו רוצים
[00:37:26 - 00:37:27] להגיד מה ההסתברות
[00:37:33 - 00:37:34] של הפיקסל I
[00:37:38 - 00:37:41] להינתן כל הפיקסלים שהם קטנים מה-I,
[00:37:41 - 00:37:42] אני אשתמש בסביבות הזה
[00:37:43 - 00:37:46] ‫כל הפיקסים שבאים לפניו ואיש בסביבות.
[00:37:47 - 00:37:49] אז במקום להחזיק את כל הטבלה הזאתי,
[00:37:50 - 00:37:55] כל אחת מהאפשרויות של ה-x קטן מ-I תהיה לי שורה אחרת, אז במקום זה
[00:37:56 - 00:37:58] אני אשתמש בפונקציה הזאתי,
[00:37:59 - 00:38:02] אם זה ברנולי אז אני רק צריך איזושהי הסתברות שהיא
[00:38:03 - 00:38:04] מספר בין 0 ל-1
[00:38:05 - 00:38:09] הסיגמוד הזה הלוג'יסטיק כזה, זו פונקציה שפשוט מנרמלת לי
[00:38:10 - 00:38:12] איזשהו מספר ככה שהוא יהיה תמיד בין 0 ל-1.
[00:38:12 - 00:38:12] ‫אחד?
[00:38:15 - 00:38:21] ‫אז את מה שאני חושב שם, זה 1 באולימפי בפסקת
[00:38:21 - 00:38:23] ‫מינוס תטא,
[00:38:24 - 00:38:27] ‫הפול כל ה-x'ים הקטנים מיין.
[00:38:33 - 00:38:34] ‫השאלה פנימית בין
[00:38:34 - 00:38:35] הפרמטרים שלי
[00:38:36 - 00:38:37] ‫לכל ה-x'ים שקטנים מיין.
[00:38:39 - 00:38:42] ‫אוקיי, אז תשבו את מספר הפרמטרים שיש פה.
[00:38:43 - 00:38:45] ‫מספר הפרמטרים בתגלה הזאת שהייתה לנו קודם,
[00:38:46 - 00:38:48] ‫בעצם נתתי את הדבר הקודם אחד.
[00:38:49 - 00:38:51] ‫אני לא מניח שיש 250 שיש הרבה פעולה,
[00:38:51 - 00:38:53] ‫אז זה נראה שהוא בינארי.
[00:38:53 - 00:38:54] ‫אבל כאן יש,
[00:38:56 - 00:38:58] ‫כן, אני אגיד שזה קטן מ-I,
[00:39:00 - 00:39:02] ‫כמה פייקסטים יש שם קטנים מ-I,
[00:39:03 - 00:39:05] ‫זה 2, 0 וכמה פסקת.
[00:39:06 - 00:39:07] ‫אז מה יהיה,
[00:39:09 - 00:39:09] מה יהיה,
[00:39:09 - 00:39:10] ‫אה...
[00:39:11 - 00:39:18] ‫טוב, שתיים בפסקת I. ‫-I פחות 1. ‫שתיים עוד קטן I פחות 1.
[00:39:20 - 00:39:24] ‫אז כמה פרמטרים יש לי כאן בייצוג הזה, ‫וכמה פרמטרים בשביל הייצוג הזה?
[00:39:26 - 00:39:35] ‫I פחות 1. ‫בו יש פסוק I פחות 1, נכון? ‫כי זה מכפלה פנימית. בדרך כלל אנחנו מניחים כאן ‫שיש עוד גם ביאס כזה.
[00:39:36 - 00:39:37] ‫יש לי בשביל זה I.
[00:39:40 - 00:39:40] ‫אוקיי?
[00:39:41 - 00:39:41] ‫עוד כמה יש כאן?
[00:39:43 - 00:39:59] ‫אני לא יכול ממש להגיד, ‫אוקיי, אם הפיקסלים האלה היו,
[00:40:00 - 00:40:03] ‫הפיקסלים החוזמים היו ככה וככה וככה, ‫אז הם לגמרי משנה לי את ההסתברות.
[00:40:04 - 00:40:07] ‫אני לא יכול למדל את ההסתברות בצורה מלאה, אוקיי? ‫זה לא פרמטריזציה מלאה.
[00:40:07 - 00:40:12] ‫אני מתבדל אותה על ידי ‫איזושהי הנחה כזאת, היא ליניארית, ‫יש לי תמונות ליניארית ‫במה שהיה קודם,
[00:40:13 - 00:40:17] ‫ואז אני מפיל עליה את הסיגוויד הזה, ‫שעושה לי את המספר הזה בין 0 ל-1,
[00:40:18 - 00:40:19] ‫וזהו, זו הנחה שהיא מגבילה.
[00:40:20 - 00:40:27] ‫אז במקום להגביל את מספר הפיקסלים ‫בהיסטוריה שאני תלוי בהם, ‫אני מגביל את איך שאני תלוי בפיקסלים.
[00:40:28 - 00:40:30] ‫אני מרשה לעצמי להסתכל על כל הפיקסלים,
[00:40:30 - 00:40:37] ‫אבל לא לעשות כל מה שנמצא ‫עם כל הפיקסלים, אלא פרמטריזציה.
[00:40:39 - 00:40:47] ‫זה בעצם הכוונה שלי ב-Trade-off ‫בין conditional independence ‫לפרמטריזציה.
[00:40:52 - 00:40:55] ‫הדוגמה שאנחנו בעצם נתחיל ‫להסתכל עליה הרבה זה M-List,
[00:40:56 - 00:40:56] ‫כמו שאמרתי,
[00:40:57 - 00:41:06] ‫ואנחנו הרבה פעמים נסתכל על דאטה ‫שהוא אפילו עוד יותר פשוט, ‫זה נקרא בינארייד אמניסט,
[00:41:07 - 00:41:10] ‫שהפיקסלים האלה הם או 0 או 1 פשוט, ‫או שוחר לבן.
[00:41:11 - 00:41:17] ‫אמניסט המלא זה 255, 256 ערכים לכל פיקסים.
[00:41:23 - 00:41:24] ‫אז הדאטה הזה בנוי ככה, יש לו,
[00:41:25 - 00:41:27] ‫כל תמונה היא בגודל 28 על 28,
[00:41:28 - 00:41:29] ‫והיא שחור לבן, אז יש לה רק
[00:41:31 - 00:41:32] ‫ערוץ אחד.
[00:41:33 - 00:41:38] ‫זאת אומרת שסך הכול יש 784 פיקסלים, ‫סך הכול 784 ערכים בכלל.
[00:41:41 - 00:41:49] ‫במקרה בינאריים יכולים להיות ‫רק 0 או 1. אוקיי, אז המטרה שלנו היא למצוא ‫איזושהי פונקציית התפלגות כזאת, PX,
[00:41:51 - 00:41:52] ‫שהיא בעצם,
[00:41:52 - 00:42:07] ‫זה נותנת לנו הסתברות ‫על כל אחד מהערכים האפשריים ‫של הווקטור הזה בגודל 784. ‫ואנחנו רוצים שזה יהיה מודל טוב, ‫זאת אומרת, רוצים לדגום מהמודל הזה ‫שזה ייראה כמו ספרה.
[00:42:16 - 00:42:21] ‫אוקיי, אז בואו נסתכל על מודל כזה. ‫אז אמרנו שהדבר הזה תמיד נכון, אוקיי?
[00:42:21 - 00:42:24] ‫זה שוב פעם כלל השרשרת, ‫בלי שעשיתי הנחות, אוקיי?
[00:42:24 - 00:42:25] ‫באותו האחרון כאן,
[00:42:26 - 00:42:27] ‫הוא תלוי בכל הקודמים.
[00:42:32 - 00:42:35] ‫אז הפיקסל ה-784 זה הפיקסל האחרון, ‫הוא תלוי בכל הקודמים.
[00:42:36 - 00:42:39] ‫אז לא עשיתי פה הנחות על אי-תלות,
[00:42:41 - 00:42:45] ‫אז זה מה שאמרנו קודם, אוקיי? ‫אם עכשיו בעצם כל התפלגות כזאתי
[00:42:46 - 00:42:51] ‫היא תהיה לוג'יסטיק כזאתי, כמו מה שכתוב כאן,
[00:42:51 - 00:42:53] ‫פשוט עם איזשהו תטא.
[00:42:54 - 00:42:56] ‫לכל איי יהיה איזשהו תטא של פרמטרים, ‫תטא איי,
[00:42:57 - 00:42:58] ‫שהוא אומר,
[00:42:59 - 00:43:01] ‫כל פעם ההיסטוריה תהיה בגודל אחר.
[00:43:02 - 00:43:06] ‫זה בטיסל אני נמצא, ‫כל פעם יהיה איי מינוס 1 בטיסל ‫בהיסטוריה,
[00:43:06 - 00:43:08] ‫אז יהיה לי תטא בגודל אחר.
[00:43:10 - 00:43:20] ‫וזה יהיה הפרמטרים שלי, ‫כל אחד מהפקטורים האלה, ‫במקום שהוא יסקר את כל האפשרויות, ‫אני אופסוך אותו לפונקציות הלוג'יסטיק כזאת.
[00:43:20 - 00:43:28] ‫אז מה כתוב כאן? כתוב כאן x2 ‫זו פונקציה של x1, ‫ובינתן x1, ‫זה יהיה בעצם האינפוט של הוויסטיק הזה,
[00:43:28 - 00:43:30] ‫והפרמטרים שלו, ‫לכן נקודת פסיק,
[00:43:31 - 00:43:32] ‫זה יהיה תטא.
[00:43:33 - 00:43:37] ‫לאחרון גם הוא יהיה תלוי ‫בצפה יותר גדול,
[00:43:38 - 00:43:40] ‫וזה יהיה התטא שלו, ‫זה יהיה וקטור יותר גדול גם.
[00:43:42 - 00:43:44] ‫-tata זה יהיה בגודל של הדבר הזה,
[00:43:45 - 00:43:48] ‫פלוס 1, וזה גם שיהיה תבר חופשי.
[00:43:50 - 00:43:56] ‫אוקיי, אז כן,
[00:43:56 - 00:44:00] זה מה שכתבנו כאן, ‫כתבתי כאן את האחד הזה ‫כדי שיהיה את האיבר החופשי פה.
[00:44:02 - 00:44:06] ‫הסיגמה הזה זה דרך שכותבים את ה... ‫קוראים לזה סיגמוייד או לוג'יט,
[00:44:06 - 00:44:12] ‫זה פשוט ה-1 חלקי 1 בעוד E בחזבאת מינוס מה שיש פה.
[00:44:20 - 00:44:24] ‫זה מה שראינו קודם, ‫וזה מספר הפרמטרים שיוצאים פה, ‫כי לראשון יש
[00:44:26 - 00:44:30] רק פרמטר אחד, ‫כי הוא הפרמטר החופשי, ‫הוא לא מתאמן בכלום, ‫לשני שני פרמטרים,
[00:44:31 - 00:44:34] ‫כן הלאה עד לאחרון, ‫שראינו שיש פה N פרמטרים.
[00:44:35 - 00:44:38] ‫מסך הכול זה עצב דרך N בר ריבוע כשתיים פרמטרים.
[00:44:39 - 00:44:43] ‫זה מה שחסכנו מ-2 בחזקת N פרמטרים.
[00:44:47 - 00:44:49] ‫אוקיי, זה משהו שכבר אפשר למדל.
[00:44:50 - 00:44:59] ‫בעצם, מה זה אומר, המידול הזה? ‫שכל פקטור כזה הוא...
[00:45:00 - 00:45:01] ‫אחרי שאנחנו עושים את החישוב הזה,
[00:45:03 - 00:45:09] ‫התוצאה של החישוב הזה, ‫זה הפרמטר ברנולי של ה-TXL-I,
[00:45:10 - 00:45:12] ‫זאת אומרת, זו ההסתברות של ה-TXL-I שלווה את פאנט.
[00:45:13 - 00:45:16] ‫אוקיי,
[00:45:17 - 00:45:20] ‫ואפשר לחשוב על זה בשתי דרכים.
[00:45:21 - 00:45:25] ‫אחת, שזה מה שאמרתי עכשיו, ‫שזה ההסתברות שפיקסל ה-I שלווה את אחד, שהוא גדולה,
[00:45:27 - 00:45:30] ‫או שזה בעצמו כבר הפרדיקציה שלי ‫לפיקסל ה-I.
[00:45:31 - 00:45:32] זאת אומרת, אני יכול להגיד ש...
[00:45:33 - 00:45:35] ‫למרות שהפיקסל ה-I הוא,
[00:45:36 - 00:45:42] ‫אני יודע שבדעת הוא 0 או 1, ‫אולי הפרדיקציה שלי היא שהוא 0.6. ‫אני רוצה יותר מערכת עכשיו על זה ככה.
[00:45:42 - 00:45:48] ‫אז הדיקציה שלי לעצמה היא איזשהו מספר ‫בין 0.1, למרות שמראש ‫הפיקסל ה-I הוא 0.1.
[00:46:01 - 00:46:03] ‫כן, לפעמים זה נוח,
[00:46:04 - 00:46:08] כל מיני דברים, ‫נוח לעשות את החישוב של הלוז על גבי זה לפעמים.
[00:46:08 - 00:46:10] ‫היום אנחנו לא כל כך ניכנס לזה באמת, אבל
[00:46:12 - 00:46:15] ‫זה עוזר לפעמים,
[00:46:15 - 00:46:23] ‫ממדלים בסופו של דבר, ‫למרות שהדאטה הגיעה בתור בינארי, ‫עדיין ממדלים אותו כאילו שהוא רציף בעצם, ‫בין 0 ל-1,
[00:46:24 - 00:46:29] ‫ואז הפרדיקציה עושים בעולם הרציף, ‫והלא הסוסים בעולם הרציף, ‫למרות שכל ה-Labלים מגיעים רק ב-0.5.
[00:46:30 - 00:46:30] ‫זאת הכוונה.
[00:46:34 - 00:46:36] ‫נראה דוגמאות שלי, מה ההבדל?
[00:46:36 - 00:46:37] ‫אם מסתכלים על תמונה,
[00:46:39 - 00:46:41] ‫את האיפרדיקציה הזאת, ‫לעומת כנראה אינטרפדיקציה הזאת.
[00:46:43 - 00:46:53] ‫אוקיי, אז המודל כזה מיימש אותו ‫עם סיגמויד, ויש לו סיגמויד ולוג'יסטיק,
[00:46:54 - 00:46:56] ‫יש למודל אותו דבר, בעצם סיגמויד,
[00:46:57 - 00:46:59] ‫סיגמויד זה כל פונקציה ‫שיש לה פשוט ס.
[00:47:01 - 00:47:04] ‫זו הדוגמה הכי שימושית של סיגמויד,
[00:47:08 - 00:47:12] ‫אז השם שנתון על המודל הזה ‫זה פול אינטרס סיגמויד,
[00:47:12 - 00:47:12] ‫בליף
[00:47:15 - 00:47:17] ‫בליף נטוורק זה שם אחר לביילי נטוורקס.
[00:47:18 - 00:47:23] ‫באחת הרשת, יש לנו מודל של משתנים מכירים ‫עם
[00:47:27 - 00:47:30] חצים מקוונים שהם דאגים לגבי מעגלים.
[00:47:35 - 00:47:39] ‫אוקיי, אז נגיד שזה הרשת שלנו, ‫אז איך אנחנו עושים אבלואציה?
[00:47:39 - 00:47:40] ‫מה זאת אומרת אבלואציה?
[00:47:41 - 00:47:43] ‫זה אומר שאם יש לנו...
[00:47:43 - 00:47:51] ‫קודם כול, זה דרך גרפית ‫להראות את המודל הזה, כן? ‫בהינתן נגיד שיש לנו ארבעה פיקסלים, ‫אז הפיקסל הראשון,
[00:47:51 - 00:47:54] ‫אנחנו עושים לו פרדיקציה, ‫הוא לא תלוי בשום דבר אחר.
[00:47:55 - 00:47:58] ‫הפרדיקציה של הפיקסל השני ‫תהיה תלויה בראשון,
[00:47:59 - 00:48:02] ‫השלישי תהיה תלויה גם בראשון ובשני, ‫והרביעי בשלושת הראשונים.
[00:48:06 - 00:48:09] ‫זו דרך לעשות להראות את הגרף של ה...
[00:48:10 - 00:48:10] ‫שוב, בזה?
[00:48:13 - 00:48:15] ‫ומה אנחנו יכולים לעשות עם זה? ‫אז הנה אנחנו רוצים לעשות אבלואציה.
[00:48:16 - 00:48:19] ‫אבלואציה זה בהינתן ש... ‫תגיד שלמדנו כבר את המודל,
[00:48:20 - 00:48:21] ‫היא נותנת תמונה חדשה,
[00:48:22 - 00:48:23] ‫מה ההסתברות שלה תחת המודל שלה.
[00:48:24 - 00:48:25] ‫איך עושים את זה?
[00:48:30 - 00:48:31] ‫אם אפשר להגיד?
[00:48:32 - 00:48:32] ‫אם נגיד,
[00:48:33 - 00:48:34] ‫את הרעייה הראשונים באנגלציות?
[00:48:36 - 00:48:38] ‫אז בשביל לחשב? ‫כן, אפשר ל...
[00:48:39 - 00:48:40] יש לנו את כל הדאטה.
[00:48:40 - 00:48:43] ‫אנחנו מקבלים את הדאטה, ‫מקבלים את ארבעת אלף, אוקיי?
[00:48:44 - 00:48:45] ‫אז אנחנו צריכים לחשב,
[00:48:46 - 00:48:47] ‫נגיד, בשביל הראשון,
[00:48:48 - 00:48:50] ‫הראשון אין לנו... ‫הראשון הוא נראה כך.
[00:48:51 - 00:48:51] ‫נראה...
[00:49:00 - 00:49:01] כפי שווית אחד,
[00:49:04 - 00:49:05] ‫זה פירוט שהוא שווה אחד,
[00:49:06 - 00:49:07] ‫זה שווה ל...
[00:49:10 - 00:49:10] ‫אם נכון.
[00:49:11 - 00:49:13] ‫אז רק ה-טאטא אפס שלו.
[00:49:14 - 00:49:14] ‫טאטא אפס של אחד.
[00:49:17 - 00:49:18] ‫פשוט מספר,
[00:49:19 - 00:49:21] ‫זה הסתברות שהוא שווה אחד, ‫הסתברות שהוא שווה אפס.
[00:49:22 - 00:49:24] ‫אנחנו בודקים אם הוא אחד,
[00:49:26 - 00:49:28] ‫אז זה יהיה זה, זה יהיה המספר הזה,
[00:49:29 - 00:49:31] ‫אם הוא שווה אפס, זה יהיה אחד פחות זה.
[00:49:32 - 00:49:32] ‫זה יהיה הפקטור הראשון.
[00:49:34 - 00:49:39] ‫כי שזה כפוך זה, אם פירוט אחד שהיא חלקת לאפס,
[00:49:40 - 00:49:41] זה פשוט אחד פה חוץ.
[00:49:44 - 00:49:46] כן, אפשר לכתוב את זה בקצור, זה פי x1
[00:49:48 - 00:49:49] יהיה שווה ל...
[00:50:10 - 00:50:13] 1-1-1-1-1-1-1-1-1-1-1.
[00:50:21 - 00:50:22] זה סתם, אנחנו רוצים לממש את זה בקוד.
[00:50:24 - 00:50:25] תמיד רק אחד מהם יגלום,
[00:50:26 - 00:50:29] x1 הוא שווה או 1 הוא 0. או שזה יהיה,
[00:50:30 - 00:50:32] או שזה יפעל או שזה יפעל.
[00:50:34 - 00:50:36] זו ההסתברות של x1. עכשיו, מה אם x2?
[00:50:37 - 00:50:41] x2, אני צריך לחשב אותו קודם כל. אז x2,
[00:50:42 - 00:50:44] אני צריך לחשב אותו, אני צריך להסתכל על x1 גם.
[00:50:46 - 00:50:47] זה בסדר, כי x1 נתון לי גם.
[00:50:48 - 00:50:49] נתונים לכל ה-xים שאני חושב פה.
[00:50:50 - 00:50:52] זה יהיה בדיוק אותו חישוב כמו קודם.
[00:50:55 - 00:50:58] אבל הדבר הזה עכשיו יהיה תלוי גם ב-x1.
[00:50:59 - 00:51:03] 1-1-1-1-1-1-1-1 פרמטר
[00:51:07 - 00:51:12] כמו x1 ועוד z2,0.
[00:51:17 - 00:51:19] והדיפיק שלי תלוי גם ב-x1,
[00:51:19 - 00:51:21] לא רק תלוי גם ב-x1,
[00:51:21 - 00:51:22] זה תלוי ב-x1.
[00:51:24 - 00:51:25] ועוד 1-1-1,
[00:51:27 - 00:51:28] במקרה של x2 עד ה-0,
[00:51:30 - 00:51:32] כפול 1-1-1.
[00:51:37 - 00:51:53] זה x2 בהינתן x1.
[00:51:55 - 00:51:58] x3 הוא בהינתן x1 ו-x2.
[00:52:01 - 00:52:03] תוכלי שוב, רק שכאן מופיע לי עכשיו x1 ו-x2.
[00:52:04 - 00:52:08] אוקיי, ויש לי את כל הפרמטרים האלה. תגיד שכבר למדתי את המודל הזה.
[00:52:09 - 00:52:13] יש לי את כל הפרמטרים האלה, אז עכשיו כשיש לי תמונה חדשה אני רוצה להגיד מה ההסתברות שלה.
[00:52:14 - 00:52:17] אני פשוט עושה את כל הפישובים האלה ומחפיש אותם.
[00:52:22 - 00:52:22] אוקיי?
[00:52:23 - 00:52:23] ברור?
[00:52:27 - 00:52:29] זה מאוד קל לעשות את זה, עכשיו עוד בצורה ישירה.
[00:52:31 - 00:52:32] בדבר אחת, עכשיו עוד בזה במקביל.
[00:52:34 - 00:52:46] אתם לא נראים מרוצים.
[00:52:49 - 00:52:49] ברור?
[00:52:51 - 00:52:54] זה אבלואציה, וגם לזה אנחנו משתמשים בשביל לעשות האימון. זה מה ש...
[00:52:55 - 00:52:56] המשמעות ש...
[00:52:57 - 00:53:00] זה אחת מהיתרונות של המודל הזה, שאפשר לחשב את זה בצורה יעילה.
[00:53:02 - 00:53:02] וכשנאמן את זה,
[00:53:02 - 00:53:06] הרי איך אנחנו מאמנים, אנחנו עושים מקסימום לייקטיות וחושבים את הלייקטיות וגוזרים,
[00:53:06 - 00:53:07] אנחנו עושים גדיאן דיסנט.
[00:53:08 - 00:53:14] זה, קודם כל נעשה את החישוב הזה ונגזור את זה כדי למצוא את הערכים היותר טובים לתת לכל הפרמטרים שלנו.
[00:53:17 - 00:53:21] אוקיי, אז זה לגבי אבלואציה, שזה קשור גם לאימון.
[00:53:21 - 00:53:24] מה אנחנו רוצים לדגום מתוך המודל הזה?
[00:53:33 - 00:53:35] נכון.
[00:53:35 - 00:53:37] בתמונה ההתחלה מ-X1 ל-X1.
[00:53:38 - 00:53:41] נכון. זה צריך לעשות 1-1, אי אפשר לעשות בממביל.
[00:53:42 - 00:53:46] אנחנו פשוט נדגום קודם את X1. טוב, יש לנו את ההסתברות של X1,
[00:53:46 - 00:53:49] את ההתפלגות של X1, כי X1 לא תלוי בשום דבר אחר.
[00:53:50 - 00:53:52] אז יש לנו את ההתפלגות הזאת, התפלגות דרמונולית פשוט.
[00:53:54 - 00:53:58] אוקיי, זה מספר, הדבר הזה זה מספר בין 0 ל-1, ואנחנו דוגמים את המספר הזה, אוקיי? אנחנו עושים,
[00:53:59 - 00:54:00] נראה לי שאתם יכולים לתבוע קוד, אז תדעו
[00:54:03 - 00:54:06] אנחנו דוגמים פשוט מ...
[00:54:07 - 00:54:12] נאמפאי זה רנדום צ'וייס, אני חושב, נותנים לו מספר או רנדום
[00:54:14 - 00:54:19] פשוט רנד גדול מהמספר שייצא כאן,
[00:54:20 - 00:54:23] ואז אם זה רנד אינט גדול מהמספר הזה
[00:54:27 - 00:54:31] ואז זה נותן לי 1 אם זה גדול מהמספר הזה, ו-0 אם זה קטן מהמספר הזה
[00:54:33 - 00:54:35] אם זה הסתברות של 1, זה צריך עוד רנד קטן מהמספר,
[00:54:36 - 00:54:36] אוקיי? לא משנה.
[00:54:37 - 00:54:37] אבל יש דרך פשוט,
[00:54:38 - 00:54:40] זה מתפר בנולי שאומר מה ההסתברות שהערך הזה שווה 1,
[00:54:41 - 00:54:43] אוקיי? אני פשוט דוגם מההסתברות הזאת,
[00:54:43 - 00:54:48] יצא לי 1, יצא 1, יצא 0, יצא 0. וזה מה שאני שם ב-x1.
[00:54:49 - 00:54:51] ועכשיו אני רוצה לחשב את x2,
[00:54:51 - 00:54:57] שב-x2 הוא צריך לדעת את x1 לא צריך לדעת שום דבר אחר, אבל יש לי כבר דבימה מ-x1,
[00:54:58 - 00:55:00] אז אני מכניס את x1 למודל הזה,
[00:55:00 - 00:55:01] עוד פעם, יצא לי מספר,
[00:55:02 - 00:55:03] ההסתברות של x2 שווה 1.
[00:55:04 - 00:55:06] אני דוגם מההסתברות הזאת,
[00:55:06 - 00:55:08] יצא 1, יצא 1, יצא 0, יצא 0,
[00:55:09 - 00:55:10] זה מה שאני שם ב-x2.
[00:55:10 - 00:55:13] עכשיו אני משתמש ב-x1 ו-x2 כדי לדגום את x3.
[00:55:14 - 00:55:16] אוקיי? ככה הלאה עד שאני דוגם את כל ה...
[00:55:18 - 00:55:19] אוקיי? אז זה גם משהו מאוד פשוט,
[00:55:20 - 00:55:22] החיסרון זה שאי אפשר לעשות את זה במקביל.
[00:55:24 - 00:55:29] אוקיי? אז אם יש לי תמונה גדולה של מיליון פיקסלים, אז צריך שתחזור לפעולה הזאת
[00:55:30 - 00:55:30] מיליון פעמים.
[00:55:32 - 00:55:39] אוקיי. זה ברור דגימה ממודל אוטו-רגרסיבי.
[00:55:40 - 00:55:43] אתה רואה פה שנייה על דלית האימון פה?
[00:55:43 - 00:55:46] אמרת שבסוף אתה מפקיע את אותו בעיה הזאת.
[00:55:46 - 00:55:47] כן, אז פי x.
[00:55:53 - 00:55:53] בעצם,
[00:55:54 - 00:55:55] בתהלית האימון אני רוצה ממש
[00:55:56 - 00:55:59] לומר אבל בגדול אני רוצה לעשות חד פקריאנט,
[00:55:59 - 00:56:01] לוג e
[00:56:02 - 00:56:03] תטא
[00:56:04 - 00:56:04] ל-xi.
[00:56:05 - 00:56:06] כן, כל הדוגמאות שלי.
[00:56:07 - 00:56:09] את זה אני רוצה לגזור לפי תטא.
[00:56:10 - 00:56:12] נכון? ולעשות גדול מסיימת על תטא.
[00:56:14 - 00:56:15] ואיך אני מחשב כל אחד כזה?
[00:56:16 - 00:56:18] אז לוג של p של ההסתברות
[00:56:19 - 00:56:22] זה ה-p של ההסתברות כאן, נכון?
[00:56:22 - 00:56:24] פי להסתברות
[00:56:24 - 00:56:26] זה המכפלה של כל הדברים האלה.
[00:56:27 - 00:56:28] עד i, כן?
[00:56:28 - 00:56:30] לא, i פקאנדמר זה מספר הדוגמה.
[00:56:32 - 00:56:37] אוקיי, יש לי j, יש לי j דוגמאות.
[00:56:39 - 00:56:39] כל אחד מהם
[00:56:42 - 00:56:43] זה דיקפטר של 1 פקריאנט.
[00:56:45 - 00:56:47] אין גדול מספר הדוגמאות.
[00:56:51 - 00:56:52] כל דוגמה,
[00:56:53 - 00:56:55] אם אתה תעבור על כל הפיקסלים ולעשות את זה,
[00:56:56 - 00:57:00] בגלל שזה לוג, במקום מכפלה זה ספורם, אוקיי? של לוג
[00:57:02 - 00:57:05] כמו שכתוב כאן שזה p ל-x
[00:57:08 - 00:57:10] i again, נדלי i.
[00:57:10 - 00:57:12] דוגמא ה-j, הפיקסל ה-i.
[00:57:14 - 00:57:14] בהינתן
[00:57:16 - 00:57:22] הדוגמא ה-j קום שקטן מ-i והתטא של i.
[00:57:27 - 00:57:28] זה הדבר הזה.
[00:57:28 - 00:57:32] זה ההסתברות של תמונה ספורטית.
[00:57:34 - 00:57:36] התחום של כל הדברים זה לוג ההסתברות של תמונה ספורטית.
[00:57:55 - 00:57:57] כן, אבל את זה אני צריך לגדור פשוט.
[00:57:58 - 00:57:59] אז אני יכול לעשות את כל הדברים האלה במקביל.
[00:58:00 - 00:58:01] יש לי את כל, תמיד יש לי את כל ה-xים.
[00:58:03 - 00:58:05] כל איטרציה, יש לי את התטאים ש...
[00:58:06 - 00:58:07] את ה-xים הקודמים,
[00:58:08 - 00:58:09] את ה-xים שאני רוצה לבדוק.
[00:58:10 - 00:58:10] אני פשוט
[00:58:11 - 00:58:13] תוכח תמונות מאיזשהו מיני-batch,
[00:58:13 - 00:58:14] מספר על כל הפיקסלים,
[00:58:15 - 00:58:16] מחשב את זה,
[00:58:16 - 00:58:18] דובר בתטא ומעביר את זה ככה.
[00:58:23 - 00:58:27] הנקודה היא שכל החישוב כזה אני יכול לעשות בבת אחת במקביל בצורה יעילה.
[00:58:28 - 00:58:34] אוקיי, זה סתם דוגמאות של איזשהו מודל שעושה את זה באמת עם סיגמוידס כאלה.
[00:58:35 - 00:58:40] זה בלי רשתות עמוקות, אוקיי? זה רק סיגמוידס כאלה.
[00:58:41 - 00:58:49] זה בעצם פונקציות פיניאריות עם פונקציות סיגמוידס על גבי הפונקציה ליניארית.
[00:58:49 - 00:58:52] פונקציה שלו בעצם תלוי בצורה ליניארית בפיקסלים הקודמים.
[00:58:53 - 00:59:00] זה דאטה סט שנקרא קלטק מעל לאחד סילואטס וכל מיני אמצעיות כאלה של אובייקטים.
[00:59:02 - 00:59:03] יוצר משהו
[00:59:03 - 00:59:04] לא הכי טוב,
[00:59:06 - 00:59:06] משהו יצא.
[00:59:10 - 00:59:13] שמאל זה הדאטה המאבנתי וימין זה הדגימות מתוך המודלים.
[00:59:16 - 00:59:18] אוקיי, אז מה הבעיה? למה זה לא יוצא ממש טוב?
[00:59:19 - 00:59:25] הבעיה שיש לנו פה קשוט ליניארית אוקיי? ואנחנו עכשיו רוצים
[00:59:28 - 00:59:31] שפיקסים. זה באמת יהיה תלוי בפיקסים הקודמים אבל בצורה קצת יותר מורכבת.
[00:59:31 - 00:59:34] מצד אחד אנחנו לא רוצים את הפרמטריזציה המלאה
[00:59:35 - 00:59:39] של זה שאנחנו רואים את כל האפשרויות של מה שהיה קודם מה תהיה ההסתגרות.
[00:59:39 - 00:59:42] אנחנו גם רוצים שזו תהיה איזושהי פונקציה שמסתכלת על האינפוט ואומרת
[00:59:43 - 00:59:43] מה ההסתגרות
[00:59:44 - 00:59:47] אבל שזה לא יהיה פונקציה ליניארית כזאת פשוטה אלא משהו קצת יותר מורך
[00:59:47 - 00:59:53] ‫אנחנו נעשה הפסקה של עשר דקות, ‫ואז נדבר עליך, בבקשה, מסובכת, ‫ואני מרשת את כל הדקות.
[01:00:17 - 01:00:31] ‫אם זה לא היה מספר פרמטרים, ‫אז זה לא היה מזה להסתובב, ‫היה קשר.
[01:00:32 - 01:00:34] ‫אנחנו התחלנו מזה שאנחנו יכולים ‫למצוא בפקר הזה של שרית.
[01:00:36 - 01:00:38] ‫כפי שלי, ‫שאנחנו יכולים לכתוב אותו ‫בתור פי של איפסקה.
[01:00:40 - 01:00:44] ‫כמו שאתה בוחר, ‫בפקר השופטרת, זה אומר שיש לנו ‫איזשהו דאג בלי מעגלים.
[01:00:45 - 01:00:58] ‫אם יש לך מעגל, זה אומר ש... ‫תחשוב על הדרך שאתה דוגם, ‫לא על הדרך לדגום. ‫-על להכפיא את הזה ‫בשביל להגיע להסתובבות שצריך לעשות. ‫-כן, תחשוב על הדרך אינטואיטיבית ‫שלא היית יכול לדגום.
[01:00:59 - 01:01:05] ‫אם היה בפיקסל הראשון ‫היהיה קול בשני או בשנייה, ‫אבל שנייה יותר בראשון, ‫אולי אולי הייתם מבטיחים. ‫אבל פחות לא בדרך פקודה.
[01:01:14 - 01:01:25] ‫-כן, תודה רבה.
[01:01:44 - 01:01:46] ‫-כן, תודה רבה.
[01:02:14 - 01:02:15] ‫-כן, תודה רבה.
[01:02:44 - 01:02:45] ‫-כן, תודה רבה.
[01:03:14 - 01:03:16] ‫-כן, תודה רבה.
[01:03:44 - 01:03:45] ‫-כן, תודה רבה.
[01:04:14 - 01:04:15] ‫-כן, תודה רבה.
[01:04:44 - 01:04:45] ‫-כן, תודה רבה.
[01:05:14 - 01:05:15] ‫-כן, תודה רבה.
[01:05:44 - 01:05:45] ‫-כן, תודה רבה.
[01:06:14 - 01:06:15] ‫-כן, תודה רבה.
[01:06:44 - 01:06:45] ‫-כן, תודה רבה.
[01:07:14 - 01:07:17] ‫יש הצעה אחת חודש שלוש.
[01:07:18 - 01:07:18] ‫-כן.
[01:07:21 - 01:07:22] ‫אני לא ממש פתוח.
[01:07:22 - 01:07:23] ‫כן.
[01:07:44 - 01:07:44] ‫-כן, תודה רבה. ‫-כן, תודה רבה. ‫-כן, תודה רבה. ‫-כן, תודה רבה. ‫-בקצת הלאומיתו להגיע
[01:08:14 - 01:08:24] ‫אתה יכול להירשם גם למיילים שלנו.
[01:08:44 - 01:09:14] ‫תודה רבה.
[01:09:14 - 01:09:16] ‫-תודה רבה.
[01:09:44 - 01:09:52] ‫הם ממלאים, חברת הכנסות שלהם,
[01:09:53 - 01:09:54] ‫להביא אותי לכמה תשובות.
[01:09:55 - 01:09:55] ‫כן.
[01:09:57 - 01:09:57] ‫זה,
[01:09:57 - 01:09:59] ‫בלעדות שלהם מאז רובי ‫הם שאולכם בבית ספקים.
[01:10:00 - 01:10:02] ‫אז גם בחלק משלו...
[01:10:02 - 01:10:05] ‫אני מודה על אחד.
[01:10:06 - 01:10:08] ‫הם מביאים את התנדות שלהם,
[01:10:09 - 01:10:11] ‫מהמחלקות האלה, ולכן,
[01:10:11 - 01:10:15] אונפולטריאליום של אונפולטריאליום כדי לרכז בו דברים
[01:10:41 - 01:10:49] ‫כן,
[01:10:49 - 01:10:51] במספירות איך בדיוק,
[01:10:51 - 01:10:55] ‫זה לא יכול להיות שנספיק היום, ‫אבל יש נראה את התפקידות לסלן.
[01:11:02 - 01:11:03] ‫הוא קימצאנו
[01:11:11 - 01:11:19] ‫כאילו, באמת, סתם נוסעים להפחיד את העמקה.
[01:11:41 - 01:11:47] ‫תודה רבה.
[01:12:11 - 01:12:24] ‫לא, בואו נתחיל. נראו עשר דקות כבר, נכון?
[01:12:25 - 01:12:26] ‫-כן, סליחה. ‫-בואו נתחיל.
[01:12:27 - 01:12:28] ‫עכשיו זה גם שוב
[01:12:29 - 01:12:30] ‫אמור להיות תזכורת לרובכם.
[01:12:31 - 01:12:36] ‫אז בעצם אנחנו אמרנו, אנחנו לא רוצים ‫שהמודל שלנו יהיה רק לינארי כזה.
[01:12:37 - 01:12:45] ‫יש את התלות בין ה-Xים הקודמים ‫להסתברות של ה-X החדש, ‫שהיא תהיה פשוט פינארית.
[01:12:45 - 01:12:47] ‫זה קצת פשוט מדי, אנחנו רוצים לסבך את זה קצת.
[01:12:49 - 01:12:52] ‫אנחנו נשתמש ברשתות נוירונים כדי לעשות את זה.
[01:12:54 - 01:12:57] ‫אוקיי, אז תזכורת רגע. ‫אז בעצם Deep Learning,
[01:12:58 - 01:13:02] ‫השם שנתנו לשימוש ברשתות נוירונים ‫עם הרבה שכבות,
[01:13:03 - 01:13:05] ‫זה פותח בעיקר בשביל כסיפיקציה.
[01:13:05 - 01:13:08] ‫אז בעצם נותן איזשהו אילפוט, ‫זה עובר דרך הרבה שכבות,
[01:13:09 - 01:13:11] ‫נותן בסוף איזושהי אדיקציה לקלאס.
[01:13:14 - 01:13:18] ‫אבל לרוב, איך שזה ממומש, ‫זה על בסיס משהו די דומה ל-Gסיק רגרשן.
[01:13:19 - 01:13:20] ‫אז אם יש לוגיסטיק רגרשן,
[01:13:22 - 01:13:24] ‫שאנחנו מאמנים את זה,
[01:13:24 - 01:13:26] ‫כבר את זה אנחנו לא יודעים לפתור ‫בצורה המלאיתיתו.
[01:13:27 - 01:13:28] ‫אנחנו מאמנים את זה עם גרדיאנט דסנט.
[01:13:35 - 01:13:43] ‫אם אנחנו היינו רוצים לעשות אותו דבר, ‫רק שפה תהיה תלות שהיא לא ליניאלית,
[01:13:44 - 01:13:46] ‫תהיא תלות שהיא קצת יותר מורכבת.
[01:13:50 - 01:13:52] ‫אז זה הרעיון ב-Deep learning.
[01:13:54 - 01:13:58] ‫בעצם איך שזה ממומש, ‫יש לנו כל מיני סוגי ארכיטקטורות,
[01:13:59 - 01:13:59] ‫זה לדוגמה
[01:14:00 - 01:14:06] ‫קונגנט לארכיטקטורה שלנו ‫בסוף כלל קונבולוציות, ‫אני מדבר יותר קצת גם היום,
[01:14:07 - 01:14:13] ‫אבל בעיקרון יש לנו בדרך כלל ‫כל מיני שכבות, ‫כל שכבה מסתכלת על ההרכוש של השכבה הקודמת,
[01:14:14 - 01:14:16] ‫עושה איזשהו חישוב ליניארי,
[01:14:16 - 01:14:18] ‫פלוס משהו לא ליניארי,
[01:14:18 - 01:14:20] ‫שזה יותר הכי פשוט ל-value,
[01:14:20 - 01:14:21] ‫זה משהו ש...
[01:14:22 - 01:14:29] זה פונקציה שחוברת ככה, זאת אומרת, ‫עד איזשהו ערך מסוים היא מאפסת.
[01:14:30 - 01:14:35] ‫עם הכניסה לערך שקטן ‫מאיזשהו פשוט ומאפסת את זה, ‫אחרת היא פשוט מעבירה אותו.
[01:14:37 - 01:14:39] ‫זה הדבר היחיד שהוא לא ליניארי.
[01:14:40 - 01:14:42] ‫חוץ מהשכבה האחרונה,
[01:14:42 - 01:14:53] ‫שהיא בדרך כלל גם, ‫בקלסיפיקציה זה SoftMarks, אוקיי? ‫-SופטMarks, אפשר לחשוב על זה ‫בתור הכללה של לוג'יסטיק רגשי.
[01:14:54 - 01:14:54] ‫-לוג'יסטיק רגשי.
[01:14:55 - 01:14:58] אם היינו רק שני קלאסים, ‫אז בעצם אפשר להגיד שהאאוטפוט שלנו יהיה
[01:15:00 - 01:15:03] ‫פשוט יהיה כתוב כאן במקום טטה כפול X, ‫טטה כפול
[01:15:05 - 01:15:07] ‫השכבה האחרונה, ‫הארטפוט של השכבה האחרונה ברשת,
[01:15:08 - 01:15:14] ‫וזה יהיה, אנחנו נפעיל על זה את הסיגמויד ‫כדי שזה יהיה מספר בין 0 ל-1, ‫וזה יהיה ההסתברות
[01:15:15 - 01:15:15] של כלבים,
[01:15:16 - 01:15:18] ‫ואחד פחות זה, ‫זה יהיה ההסתברות של חתולים.
[01:15:19 - 01:15:22] ‫אם יש לנו יותר משתי קבוצות,
[01:15:23 - 01:15:24] ‫אז יש את ההכללה הזאת P,
[01:15:25 - 01:15:26] ‫שקוראים לזה SoftMarks בדרך כלל,
[01:15:27 - 01:15:28] ‫יש את זה פשוט החישוב הזה,
[01:15:28 - 01:15:31] ‫XI והאאוטפוט בשכבה האחרונה,
[01:15:32 - 01:15:32] ‫של הדוגמה I,
[01:15:34 - 01:15:36] ‫אנחנו מכפילים את זה ‫באיזשהו משהו לינארי,
[01:15:38 - 01:15:40] ‫ושים את האקספוננט של זה.
[01:15:41 - 01:15:46] ‫יש פרמטר כזה, יש פרמטר אחר ‫לכל אחד מה-K קלאסיס,
[01:15:47 - 01:15:48] ‫ל-M ליסט יש לנו,
[01:15:49 - 01:15:50] ‫אם עושים את הסיפיקציה ל-M ליסט,
[01:15:51 - 01:15:55] ‫יש לנו עשר התפקות שונות,
[01:15:56 - 01:16:00] ‫היו לנו בשכבה האחרונה ‫10 קבוצות שונות של תטא,
[01:16:00 - 01:16:04] ‫כל אחד מהם מכפיל ‫בארטפוט של השכבה לפני האחרונה,
[01:16:05 - 01:16:07] ‫אז נקח את האקספוננט של זה,
[01:16:07 - 01:16:08] ‫כדי שזה יהיה מספר חיובי,
[01:16:09 - 01:16:12] ‫ונחלק בסכום כל האקספוננטים ‫של כל ה...
[01:16:12 - 01:16:14] כל התטות האחרות, של הקלאסים האחרים,
[01:16:15 - 01:16:16] ‫ככה שזה יהיה מספר
[01:16:16 - 01:16:21] ‫שהוא יהיה חיובי בין 0 ל-1, ‫שסכום כל המספרים האלה יהיה שווה 1.
[01:16:21 - 01:16:25] בעצם השכבה האחרונה של הרשת הזאת ‫תהיה הסתברות,
[01:16:26 - 01:16:27] קטגורית,
[01:16:27 - 01:16:30] על כל הקטגוריות שיכולות להיות,
[01:16:30 - 01:16:31] ‫כל הקלאסים שיכולים להיות.
[01:16:36 - 01:16:37] הלוס,
[01:16:39 - 01:16:41] כמו זה נראה ככה, ‫זה בדיוק בעצם מה שכתבנו פה,
[01:16:42 - 01:16:46] ‫או מקרה יותר כללי שזה Soft Max, עם K קלאסים,
[01:16:47 - 01:16:48] אוקיי, זה סכום על כל ה...
[01:16:49 - 01:16:49] סלחו לי,
[01:16:53 - 01:16:54] ‫ההשתלם כאן אין שני מערכים אחרים,
[01:16:55 - 01:16:59] ‫אז כאן I זה הדוגמה ו-P זה המחלקה,
[01:17:00 - 01:17:12] ‫אם המחלקה שלנו זה אותו דבר, ‫אנחנו יכולים להגיד שהערכים שהפיקסל מקבלים יכול לקבל, ‫זה בעצם המחלקה שיכול להיות, אם זה פיקסל בינארי, ‫או 0 או 1,
[01:17:13 - 01:17:15] ‫אז אנחנו נגיד יהיה לנו פה שני קלאסים,
[01:17:16 - 01:17:21] ‫P יהיה לנו שני ערכים, או 0 או 1, ‫אם זה יותר, אם זה נגיד 256 ערכים,
[01:17:21 - 01:17:22] ‫אז יהיה לנו...
[01:17:23 - 01:17:26] ‫זה הופך להיות כמו בעיית קלאסיפיקציה ‫עם 256 קלאסים.
[01:17:29 - 01:17:36] ‫המקסימום לייקיות של האלגוריתם הזה, ‫זה בדיוק אותו דבר כמו לעשות ‫מקסימום לייקיות בבעיית קלאסיפיקציה.
[01:17:37 - 01:17:38] ‫זה מה שכתוב כאן.
[01:17:38 - 01:17:39] ‫אנחנו עוברים על כל הדוגמאות,
[01:17:40 - 01:17:43] ‫אנחנו עוברים על כל האפשרויות,
[01:17:44 - 01:17:45] ‫רק אחד מהם דולק,
[01:17:46 - 01:17:51] ‫נכון, הפיקסל המסוים, ‫אז רק הדוגמה ה-K שלו דולקת,
[01:17:51 - 01:17:54] ‫רק הקלאסי-K שלו דולקת, ‫יש לו ערך מסוים,
[01:17:55 - 01:17:59] ‫וזה הערך שאנחנו מכניסים לחישוב.
[01:18:02 - 01:18:03] ‫לא שזה הדבר כזה.
[01:18:05 - 01:18:06] ‫בסדר, ודרך זה אנחנו עושים
[01:18:10 - 01:18:14] ‫גוזרים את זה ומחסימים את זה.
[01:18:17 - 01:18:18] ‫זה ככה גם עושים בקלאסיפיקציה,
[01:18:19 - 01:18:20] ‫אנחנו עושים את זה לא על...
[01:18:21 - 01:18:25] ‫כל הדאטה-סט שלנו בזאת אחת. ‫אז אם אנחנו רוצים לאמן מודל על M.N.יסט,
[01:18:26 - 01:18:27] ‫לא מאמנים על...
[01:18:28 - 01:18:32] ‫לא כל פעם לוקחים את כל הנקודות של M.N.יסט, ‫את כל התמונות של M.N.יסט,
[01:18:33 - 01:18:41] ‫ומעדכנים, עושים עדכון אחד של התטא ‫על סמך כל התמונות, ‫אלא עובדים סטוקסטי גרדיטנסי. ‫אותו פעם לוקחים איזשהו סאבסט של תמונות
[01:18:42 - 01:18:44] ‫ועובדים איתו.
[01:18:44 - 01:18:45] זה פותר לנו כמה בעיות,
[01:18:46 - 01:18:47] ‫וגם יותר יעיל.
[01:18:48 - 01:18:52] ‫אנחנו צריכים לוקח את סטייל על הכול ‫כדי לעשות רק חישוב קטן.
[01:18:53 - 01:18:56] ‫זה גם קצת מכניס לנו רעש ‫לפעמים הוא עוזב
[01:18:57 - 01:18:58] ‫לצאת מכל מיני מינימומים
[01:18:59 - 01:19:02] ‫לוקאליים כאלה שקודמיות לנו.
[01:19:05 - 01:19:08] טוב, זה, מי שלא זוכר את הדברים האלה, ‫צריך לעשות חזרה לדיט-לרנינג.
[01:19:09 - 01:19:11] איך שזה ממומש,
[01:19:12 - 01:19:14] זה על ידי ספריות שעושות פוד-טודיף,
[01:19:15 - 01:19:18] ‫כמו טנסור-פלואו, פייטרוץ' וג'אקס.
[01:19:20 - 01:19:22] ‫והרעיון הוא שבעצם כל שכבה,
[01:19:22 - 01:19:29] כל הדרך שאנחנו בונים ‫את הרשתות האלה היא מודולרית.
[01:19:30 - 01:19:34] זאת אומרת, ‫אנחנו בונים כל שכבה כזאת ‫בצורה כזאת, ‫שאנחנו יודעים לחשב את החישוב שהיא עושה,
[01:19:35 - 01:19:36] ‫בהינתן האינפוט של השכבה הקודמת,
[01:19:37 - 01:19:41] ‫אבל גם לגזור, להעביר את הנגזרת אחורה, ‫מה שנקרא ב-propagation.
[01:19:41 - 01:19:44] זה הכול ממומש כבר בצורה אוטומטית ‫בכל שכבה,
[01:19:44 - 01:19:47] ‫כל מה שאנחנו צריכים לעשות, ‫להכניס את השכבות האלה בסדר הנכון,
[01:19:48 - 01:19:50] ‫והגזירה תהיה אוטומטית.
[01:19:52 - 01:19:53] יש כאן דוגמה למימוש כזה.
[01:19:54 - 01:19:57] אז לא תצטרכו אף פעם בעצם ‫לראות קוד כזה.
[01:19:58 - 01:19:59] פעם היה צריך כזה,
[01:19:59 - 01:20:03] סבירות קשה וסמיכה, היה צריך לכתוב את הקוד הזה, ‫עכשיו הכול כבר נמצא בתוך הספריות.
[01:20:04 - 01:20:07] זה למשל שכבה ליניארית עם סיגמויד,
[01:20:08 - 01:20:09] ‫אז יש כאן את החישוב הפורוורד,
[01:20:11 - 01:20:11] אוקיי?
[01:20:12 - 01:20:13] ‫אנחנו צריכים להכפיל את ה-X ו-W,
[01:20:14 - 01:20:20] ‫ואז לעשות את ה-A פאנקס, ‫זה בעצם הסיגמויד הזה,
[01:20:21 - 01:20:21] ‫אוקיי?
[01:20:22 - 01:20:25] ויש פה חישוב את הנגזרת של זה, ‫מה יוצאי הנגזרת,
[01:20:26 - 01:20:28] ‫ובעצם יש דרך לשרשרת כל הדברים האלה, ‫גם
[01:20:29 - 01:20:32] בחישוב קדימה וגם בחישוב של הנגזרת של זה.
[01:20:34 - 01:20:36] אוקיי, אז זה גם מה שאנחנו נראה ‫שאנחנו נשתמש.
[01:20:37 - 01:20:40] אז עכשיו בוא נראה דוגמה ראשונה, ‫שזה בעצם כמו ה...
[01:20:40 - 01:20:52] ‫כמו ה-FVSBI שראינו קודם, ‫שזה ה-Full-Evisable סיגמויד בליב נטוורק, ‫רק עם שכבה אחת של ראשית ניורונים, ‫שיש לה שכבה אחת חבויה אחרת.
[01:20:53 - 01:20:59] ‫הידן לייר פה זה לא שונה ‫מלייטנד וייבל שראינו קודם, כן? זה שכבה חבויה ‫של רשת ניורונים.
[01:21:00 - 01:21:01] יש איזושהי קונבנציה שלא כולם
[01:21:04 - 01:21:06] ‫מתמשים בה, אבל בדרך כלל כן, ‫כשאומרים הידן,
[01:21:07 - 01:21:12] ‫אז הכוונה לשכבות בתוך הרשתות, ‫בתוך גישוב.
[01:21:12 - 01:21:13] ‫כשאומרים לי latent,
[01:21:14 - 01:21:18] ‫אז מתכוונים למשתנה סטוכסטי חבוי, ‫שעושים עליו את ה...
[01:21:21 - 01:21:24] ‫דיברנו בשיעורים הקודמים, ‫שאנחנו לא נחזור לדבר עליהם ‫בשבועות הבאים,
[01:21:24 - 01:21:28] ‫בדבר יש לנו inference וכוונים כאלה.
[01:21:29 - 01:21:33] אוקיי, אז פה זה לא כזה דבר, ‫זה פשוט שכבה במקום שאנחנו נחשב,
[01:21:34 - 01:21:36] ‫איך שהוא כאן, במקום שהוא יהיה לינארי,
[01:21:37 - 01:21:40] ‫תטא כפול X. ‫אז קודם אנחנו עושים רישוב אחד ‫שמחשב את H,
[01:21:41 - 01:21:49] ‫שזה יהיה תטא כפול X. ‫אז משהו אחר שמחשב את הארטמוט הזה, ‫שזה יהיה תטא אחר כפול ה-H שיצא.
[01:21:50 - 01:21:56] ‫אנחנו מפחים את כל ה-X, ‫זה ה-X הראשון שלנו, ‫שזה 784 פיקסלים.
[01:21:57 - 01:22:00] ‫עכשיו אנחנו מחשבים את ה-H הראשון. ‫ה-H הראשון הוא לא פונקציה של כלום.
[01:22:02 - 01:22:02] ‫אוקיי?
[01:22:02 - 01:22:05] ‫בשכבה הזאת היא לא מקבלת שום אינפוט, ‫יש לה איזשהו פתחול,
[01:22:06 - 01:22:07] ‫זה משהו.
[01:22:08 - 01:22:10] ‫עכשיו יש לנו את X0, השיערוך של
[01:22:18 - 01:22:21] ‫מה המשמעות של ה-0 הזה כאן, ‫אבל הפיקסל הראשון של ה-X הזה,
[01:22:23 - 01:22:32] ‫הוא יהיה פשוט פונקציה של H1. ‫פה בעצם לא השתנה כלום, ‫רק שבמקום שיהיה לנו משתנה חופשי אחד,
[01:22:33 - 01:22:36] ‫אומר לנו מה ההצטברות, ‫יש לנו פה איזשהו אישור מסובך.
[01:22:37 - 01:22:40] ‫זה קצת מיותר, אבל זה מי שיוצא ‫מהחישוב הזה, זה לא משנה.
[01:22:42 - 01:22:45] ‫הרבה מספרים שבסופו של דבר ‫נותנים לנו מספר אחד, ‫שהוא תמיד יהיה יותר מספר,
[01:22:46 - 01:22:50] ‫זה אומר מה ההסתברות ‫שהפיקסל הראשון שווה 1. פחות מעניין.
[01:22:51 - 01:22:55] ‫מה שמעניין זה מה שקורה אחר כך, אוקיי? ‫הפיקסל השני, כשאנחנו רוצים לחשוב ‫את הפיקסל השני,
[01:22:55 - 01:22:57] ‫אז הוא תלוי ב-Hiden השני,
[01:22:58 - 01:23:01] ‫וה-Hiden השני הוא פונקציה של הפיקסל הראשון,
[01:23:01 - 01:23:02] ‫אז הוא כבר פונקציה,
[01:23:03 - 01:23:06] ‫שיוצא שיעור פונקציה לא לינארית ‫של הפיקסל הראשון.
[01:23:08 - 01:23:09] ‫אוקיי? אתם רואים למה זה לא לינארי?
[01:23:10 - 01:23:16] ‫יש כאן חישוב לינארי, פלוס בתוך כל יוניט כזה ‫יש איזה משהו לא לינארי,
[01:23:17 - 01:23:17] ‫נגיד רלייו,
[01:23:18 - 01:23:23] ‫ואז יש לנו חישוב לינארי ‫של התוצאה שלו. ‫זה כבר יהיה משהו שהוא לא לינארי ‫בפיקסל הפיקסל שלו.
[01:23:24 - 01:23:26] ‫זה יכול לתפוס אולי דברים ‫קצת יותר מעניינים.
[01:23:26 - 01:23:28] ‫ואם נסתכל על הפיקסל האחרון,
[01:23:29 - 01:23:32] ‫אז הוא מסתכל על ההידן האחרון,
[01:23:33 - 01:23:35] ‫שהוא פונקציה של כל הפיקסלים הקודמים,
[01:23:36 - 01:23:39] ‫אז זו יהיה פונקציה לא לינארית ‫של כל הפיקסלים הקודמים.
[01:23:42 - 01:23:42] בסדר?
[01:23:43 - 01:23:44] כן.
[01:23:47 - 01:23:51] אז זה למשל החלטה שאפשר לעשות, ‫שזה משהו שנקרא נייל,
[01:23:52 - 01:23:55] ‫מה הגודל של שכבת הביניים הזאתי.
[01:23:55 - 01:24:01] ‫בעצם זה שהגודל של שכבת ביניים ‫הזאתי לא 500, אלא היא 500 כפול
[01:24:04 - 01:24:05] 780 דארץ.
[01:24:07 - 01:24:10] ‫זה מודל שנראה שאתה מצייר אותו.
[01:24:20 - 01:24:22] ‫אז יש לנו כאן את ה-X וה-input כאילו,
[01:24:24 - 01:24:25] ‫אז כאן יש לנו את ה-A.
[01:24:25 - 01:24:25] H
[01:24:28 - 01:24:31] ‫מאוד גדול, כן, ‫הוא יהיה פי 500 כפול גדול ב-H.
[01:24:32 - 01:24:33] ‫אז יש לנו את הפרדיקציות.
[01:24:35 - 01:24:38] יש לנו פה כל מיני הוראותים כאלה.
[01:24:39 - 01:24:44] צריכים אבל להישמע להוראות האלה, ‫שכל קבוצה של H
[01:24:45 - 01:24:47] ‫שהיא קובעת את ה-X הבא,
[01:24:48 - 01:24:50] ‫היא רק תלויה ב-Xים הקודמים.
[01:24:50 - 01:24:58] ‫אוקיי, אז המוגל הזה נקרא ‫נאייד, Neural אוטו-רגרסיבי דנסיטי אסטימשי,
[01:24:58 - 01:24:59] ‫זה מ-2014.
[01:25:00 - 01:25:06] ‫אז אתה מחליט להוסיף עוד מ-500 ל-500 כפול גדולים.
[01:25:07 - 01:25:11] ‫אז יש כמה החלטות שיש לעשות. ‫אפשר להרחיב את זה, ‫אפשר גם שזה יהיה יותר עמוק משתי שכבות.
[01:25:12 - 01:25:13] ‫בעצם הגדלנו את זה עכשיו ל...
[01:25:14 - 01:25:16] ‫אפשר לקרוא לשתי שכבות בשכבה האחת מילינארית,
[01:25:17 - 01:25:18] ‫כאילו עשינו את הצעד הבא,
[01:25:18 - 01:25:22] ‫שיש לנו שני דברים לינאריים ‫ומשהו לינארי אחד באמצע.
[01:25:22 - 01:25:25] ‫אפשר שהדבר הזה יהיה יותר רחב,
[01:25:25 - 01:25:29] ‫ואופציה אחת. ‫אופציה שנייה, זה שיהיה לנו כמה שכבות כאלה.
[01:25:32 - 01:25:35] ‫אין משהו שאומר מה כדאי לעשות.
[01:25:38 - 01:25:47] ‫אין משהו במקסימום לייקליות ‫בעצם בין כמה אופציות.
[01:25:48 - 01:25:49] ‫ולהראות מה עובד הכי טוב.
[01:25:50 - 01:25:55] ‫קצת קשה לעשות גרדיאנט דיסנד דרך זה, ‫כי זו החלטה דיסקרטית כזאתי, ‫האם אני מוסיף או לא מוסיף,
[01:25:56 - 01:26:01] ‫אז אין לך גרדיאנט לדבר הזה. ‫אבל אתה יכול פשוט לבדוק כמה דברים ‫ולראות מה יצא הכי טוב.
[01:26:03 - 01:26:08] ‫זה כמו לעשות מקסימום לייקליות בצד, ‫אבל בצורה חיפוש טיפש כזה, לא חיפוש טיפש כזה.
[01:26:12 - 01:26:15] ‫אוקיי, אז זה תוצאות של נייד, ‫של המודל הזה.
[01:26:17 - 01:26:21] ‫שם הוא רואה טוב, אצלי נתן עושה יותר טוב, ‫אבל יש כאן הבדל ‫בין מה שיש לשמאל ומימין.
[01:26:22 - 01:26:25] ‫כל זה זה דגימות, אוקיי? ‫שוב, למדנו את המודל,
[01:26:25 - 01:26:30] ‫ואז דגמנו. איך דגמנו? ‫כמו שראינו קודם, אנחנו מעבירים...
[01:26:34 - 01:26:38] ‫אנחנו עושים את החישוב הזה, ‫זה נותן לנו את ההסתברות של x0.
[01:26:39 - 01:26:41] ‫אנחנו דוגמים מההסתברות הזאת.
[01:26:42 - 01:26:43] ‫יצא לנו 0 או 1,
[01:26:44 - 01:26:46] ‫אנחנו שמים את זה כאן, אוקיי?
[01:26:47 - 01:26:51] ‫אז אנחנו מפעילים את h2, דוגמים,
[01:26:51 - 01:26:55] סליחה, מחשבים את h2, מחשבים את x2,
[01:26:55 - 01:26:59] ‫אוקיי? דוגמים ממנו, יצא לנו 0 או 1. ‫אנחנו נותנים פה,
[01:27:00 - 01:27:03] ‫בדיוק את החישוב שעשינו קודם, ‫אבל שעכשיו כל חישוב הוא לא חישוב ליניארי,
[01:27:03 - 01:27:06] ‫אלא יש לו שתי שכבות, שתי רמות של חישוב.
[01:27:08 - 01:27:10] ‫וכך אנחנו, עד שאנחנו מגיעים ‫לכל ההסתברויות.
[01:27:11 - 01:27:14] ‫זה נותן לנו תמונה אחת. ‫אז אפשר להתחיל עוד פעם, מההתחלה,
[01:27:15 - 01:27:19] ‫איפשהו יוצא לנו בהסתברויות ‫שאנחנו דוגמים משהו אחר,
[01:27:19 - 01:27:21] ‫ואז כל מה שיהיה מתחת יהיה שונה.
[01:27:22 - 01:27:28] ‫יכול להיות שהפיצול הראשון יצא לנו עכשיו 0 ‫במקום 1 בפעם הראשונה, ‫אז עכשיו כל החישוב הזה יהיה שונה.
[01:27:29 - 01:27:31] ‫כל התמונה שתצא לנו תראה אחרת.
[01:27:32 - 01:27:38] ‫אז זו דוגמה ש... אני לא יודע כמה דוגמאות יש פה בערך. 100 קיימים חזרו ‫על התהליך הזה, ‫בכל פעם יצא משהו אחר.
[01:27:39 - 01:27:42] ‫אז מה שרואים משמאל זה הממד ‫בדיוק כמו שתיארתי,
[01:27:42 - 01:27:50] ‫אז מה שרואים מימין זה ש... ‫עשו את התהליך כמו שתיארתי, ‫אבל מה שציירו כאן זה לא את ה-0 או 1 שיצא, ‫אלא את ההסתברות שיצאה.
[01:27:53 - 01:27:55] ‫זה יותר חלק קצת.
[01:27:56 - 01:28:00] ‫למשל פה יש בשלוש הזה, ‫יש כל מיני חורים שיצא 0,
[01:28:01 - 01:28:03] ‫למרות שההסתברות אולי יותר גדולה, ‫שיהיה 1,
[01:28:05 - 01:28:08] ‫אבל גם אם זה הסתברות 90%, 90% שזה יוצא 1,
[01:28:09 - 01:28:10] ‫לפעם ל-10 יוצא 0.
[01:28:11 - 01:28:20] ‫זה יוצא כאן 0. יש בו איזה חור כזה שחור, ‫אמרתי שסתים על זה כאן וקצת יותר חלק, ‫אז זה ערך קצת יותר קצת פחות לבן, ‫כי זה 90% ולא 99%,
[01:28:21 - 01:28:26] ‫אבל זה עדיין ימר על הלבן. ‫זה יותר נוח להסתכל ‫על המונח החלקה הזאת.
[01:28:29 - 01:28:30] ‫אוקיי, אז זה התוצאות.
[01:28:32 - 01:28:35] ‫זה משהו חשוב, זה מתוך המאמר הזה שלהם, ‫של מייל מ-2011.
[01:28:40 - 01:28:46] ‫אוקיי, אז נקודה למחשבה שהיא... ‫אני חושב שנוח וכדאי לחשוב ככה ‫על מודלים כאלה,
[01:28:47 - 01:28:49] ‫גם אולי יתקשר לנו קצת למודלים ‫שנראה בהמשך.
[01:28:50 - 01:28:52] ‫אפשר לחשוב על מודלים כאלה ‫בתור auto-encoders.
[01:28:54 - 01:28:55] ‫אתם יודעים מה זה auto-encoders?
[01:28:56 - 01:28:58] ‫בקורס לדיפ-לרנינג דיברתם על auto-encoder?
[01:29:00 - 01:29:02] ‫-auto-encoder זה פשוט מודל שמקבל,
[01:29:03 - 01:29:06] ‫שהinput שלו והoutput והlabel,
[01:29:06 - 01:29:10] ‫שאיתו אנחנו עושים את ה...
[01:29:11 - 01:29:16] ‫מחשבים את הלוס, אותו דבר. ‫בעצם המטרה שלו זה לעשות auto-encoder,
[01:29:16 - 01:29:18] ‫זאת אומרת לעשות פרדיקציה לעצמו.
[01:29:19 - 01:29:26] ‫אז אפשר לחשוב על auto-regressive models ‫בתור auto-encoders,
[01:29:27 - 01:29:29] ‫אבל שהoutput הוא מוזז.
[01:29:30 - 01:29:32] ‫בעצם x-time לא יכול להסתכל...
[01:29:32 - 01:29:37] ‫אחד שיש פה כל מיני אילוצים ‫על החיבורים,
[01:29:39 - 01:29:41] ‫זאת אומרת ש-x2 לא יכול להיות תלוי בעצמו,
[01:29:42 - 01:29:44] ‫הוא רק יכול להיות תלוי ‫בכל מי שבא לפניו.
[01:29:45 - 01:29:47] ‫אם אנחנו יכולים לסדר ככה את החצים,
[01:29:47 - 01:29:48] ‫אז זה חוקי.
[01:29:49 - 01:29:51] ‫עוד דרך לחשוב על זה,
[01:29:52 - 01:29:53] ‫בעצם אותו דבר,
[01:29:55 - 01:30:00] זה שהoutput הוא בעצם איזשהו
[01:30:03 - 01:30:09] ‫איזשהו שיפט של האינפוט. ‫שוב, אסור שיהיו חצים שעוברים ‫משמאל לימין,
[01:30:10 - 01:30:17] ‫אבל החץ של עצום, זאת אומרת, x2, ‫אני משווה אותו לאיזושהי הזוזה. ‫זאת אומרת, גם ככה זה מצוייר פה.
[01:30:23 - 01:30:26] ‫הכול כאן מוזז באחד מזה, זה לא תלוי בכלום.
[01:30:32 - 01:30:36] ‫טוב, לא משנה. ‫אז הפיקסל הראשון הוא לא תלוי בכלום.
[01:30:37 - 01:30:40] ‫הפיקסל השני תלוי רק בפיקסל הראשון,
[01:30:40 - 01:30:45] ‫הפיקסל השלישי תלוי בכל מה שבא לפני הפיקסל השלישי,
[01:30:46 - 01:30:46] ‫וככה הלאה.
[01:30:48 - 01:30:50] ‫דרך שהרבה פעמים ‫מממשים דברים כאלה,
[01:30:51 - 01:30:53] ‫זה שמכניסים את האינפוט,
[01:30:54 - 01:30:55] ‫מסתכלים על האוטפוט, ‫אבל באיזושהי הזזה.
[01:30:58 - 01:31:01] תזכרו את זה, ואולי כשנדבר על זה, ‫כשנדבר על קוד, אז...
[01:31:02 - 01:31:04] ‫זה ילכה עם קצת יותר ברור מה אני מתכוון.
[01:31:09 - 01:31:15] ‫אוקיי, אז פה יש איזו גרסה יותר עמוקה ‫כזאת של נייד, שנקראת מייד,
[01:31:15 - 01:31:16] ‫והם בעצם
[01:31:19 - 01:31:22] ‫כן הסתכלו על זה בתור, ‫הם ממש אוטו-אנקודר כזה, ‫בתור רשת עמוקה,
[01:31:23 - 01:31:29] ‫שפשוט יש כל מיני אילוצים ‫על המיקום שמותר לשים פץ.
[01:31:30 - 01:31:31] אוקיי? כמו שאמרנו,
[01:31:31 - 01:31:35] ‫אני לא רוצה ש... ‫אסור לי ש-X3 יהיה תלוי ב-X3. ‫זה תלוי רק ב-X1 וב-X2.
[01:31:36 - 01:31:40] ‫אסור לי שיהיה לי כאן מסלול מ-X3 ‫לפרדיקציה של X3.
[01:31:43 - 01:31:46] ‫אז בשיטה הזאת, ‫הם קוראו לזה ‫מאסק אוטו-אנקודר.
[01:31:47 - 01:31:51] ‫הם פשוט חישבו כל מיני מפות כאלה ‫של מאסקים,
[01:31:52 - 01:31:53] ‫ומאסק זה פשוט
[01:31:54 - 01:31:57] פטריצות שיש להם, ‫בארכים בין 0 או 1. ‫אתהם מכפיל את זה
[01:32:00 - 01:32:01] ‫במשקולות שיש לכם.
[01:32:02 - 01:32:04] ‫ואז בעצם אתה מאפס את החיצים, ‫אתה כאילו מוחק חלק מהחיצים.
[01:32:06 - 01:32:11] ‫וככה הם מאלצים, צריך לבנות את המסכות האלה ‫בצורה חכמה, ככה שבאמת
[01:32:12 - 01:32:19] ‫לא יהיה מסלול מכל פיקסל בעצמו, ‫וגם לא יהיה מסלול מהפיקסלים האלה.
[01:32:19 - 01:32:19] ‫זאת אומרת,
[01:32:20 - 01:32:24] ‫מכל פיקסל בפרדיקציה יהיה מסלול ‫רק מכל הפיקסלים הקודמים.
[01:32:25 - 01:32:29] ‫יש הרבה דרכים שאפשר לסדר ככה את הרשת, ‫ככה שזה יקרה.
[01:32:29 - 01:32:32] ‫אבל צריך לגרום לזה שבאמת...
[01:32:33 - 01:32:40] ‫זה נעשה בקונפיגורציה הראשונית של הרשת, ‫או שיש איזה דרופ-אאוט ‫שקורית תוך כדי האימון?
[01:32:41 - 01:32:47] ‫זה נעשה, יש להם פה כמה שיטות, ‫כי אני חושב שהם מאמנים ביחד ‫עם כמה מסכות שונות,
[01:32:49 - 01:32:54] ‫אבל הרעיון הזה, בגדול אתה צריך לחשוב ‫על זה כאילו זה לפני זה. יש לך, זה בעצם איך שאתה בונה את הרשת,
[01:32:55 - 01:32:57] ‫אבל כדי שהדברים יהיו יותר יעילים, ‫בונים את הרשת המלאה,
[01:32:57 - 01:33:02] ‫עם כל השכבות המודולריות האלה ‫שראינו שעובדות מראש,
[01:33:03 - 01:33:05] ‫שפשוט בונים מסכות
[01:33:06 - 01:33:09] ‫שמכפילים אותן כל פעם שעושים את החישובים.
[01:33:10 - 01:33:14] ‫אני חושב שהיה להם גם איזה משהו ‫שכל פעם הם מסתכלים על מסכות אחרות,
[01:33:14 - 01:33:16] ‫אבל כל מסכה כזאת צריכה להיות חוקית.
[01:33:27 - 01:33:28] ‫אוקיי.
[01:33:32 - 01:33:34] ‫אז זה ברור, הרעיון פה?
[01:33:37 - 01:33:41] ‫זה לא ספור למה שהיה פה, נכון? ‫כי פה יש את הוועצים משותפים במקומות ה...
[01:33:44 - 01:33:49] ‫באיזה לא היה. ‫-כן, אז פה חלק מהוועצים משותפים, ‫הם לא חייבים להיות משותפים, ‫זה תראו איך אתה בונה את זה, אבל כן.
[01:33:50 - 01:33:54] זה, כן. מצד אחד, אתה יכול להביא לך יותר
[01:33:55 - 01:33:57] ‫יותר משקולות ויותר עמוק.
[01:33:58 - 01:34:00] ‫מצד שני, אתה גם קצת משתף בין המשקולות.
[01:34:00 - 01:34:05] ‫אבל אני חושב שאתה בונה את המאסק. ‫אתה יכול לבנות מאסק ‫שזה יהיה בדיוק שקול למה שהיה קודם.
[01:34:06 - 01:34:10] ‫זאת אומרת, יש כאן הבדל ‫שיש פה יותר שכבות, אבל אם היה רק שכבה אחת, ‫היית יכול לבנות את המאסק
[01:34:11 - 01:34:12] ‫שיגרום לזה להיות
[01:34:15 - 01:34:23] בדיוק ככה. זה בעצם יש פה מאסק, אוקיי? ‫אז זה מחקת ואת הכול, זה מחקת את כל מה שבורקת וזה, ‫מחוספת כאילו שם.
[01:34:24 - 01:34:29] ‫אז היית יכול לבנות בדיוק את המאסק הזה, ‫אבל בעצם אפשר לבנות מאסקים יותר כלליים ‫שעדיין ישמרו את האילוץ הזה,
[01:34:30 - 01:34:33] ‫שכל פיקסל תלוי רק בפיקסלים הקודמים.
[01:34:38 - 01:34:45] ‫אוקיי, עוד, אז זה נקרא Mage. ‫עוד יש כמה מודלים שמסתמכים על Repרנט,
[01:34:45 - 01:34:46] ‫נראה לי על כל זה.
[01:34:46 - 01:34:47] מכירים את זה?
[01:34:48 - 01:34:49] שמעתם את זה בשתי מוחות?
[01:34:50 - 01:34:52] ‫אוקיי, אז פה הרעיון זה ‫שאנחנו שומרים איזשהו מצב
[01:34:53 - 01:34:56] ‫שהוא תלוי כל פעם ‫בכל ההיסטוריה שהייתה עד עכשיו.
[01:34:59 - 01:34:59] ‫אז
[01:35:01 - 01:35:03] יש כמה דרכים לממש את זה, ‫זו דרך פשוטה,
[01:35:03 - 01:35:06] ‫לסטי-אנג זו דרך מאוד פופולרית,
[01:35:07 - 01:35:08] ‫ובעצם כל,
[01:35:08 - 01:35:09] אנחנו שומעים משהו שנקרא...
[01:35:16 - 01:35:21] ‫ההידן זה בעצם ה-State שלנו, אוקיי? ‫אז ההידן הוא בהתחלה ‫הוא לא תלוי בכלום, ‫יש לו איזשהו זכוי.
[01:35:22 - 01:35:23] ‫אז כל פעם אנחנו מעדכנים את ההידן,
[01:35:25 - 01:35:27] ‫ה-State שלנו, על ידי שני דברים, ‫על ידי
[01:35:27 - 01:35:28] ההידן הקודם,
[01:35:29 - 01:35:30] ‫הערכים שהיה לנו קודם,
[01:35:31 - 01:35:32] ‫בפיקסל הקודם,
[01:35:33 - 01:35:35] ‫והפיקסל הנוכחי שאנחנו רואים.
[01:35:36 - 01:35:37] ‫שני הדברים האלה משפיעים
[01:35:38 - 01:35:39] ‫על ה-State שלנו,
[01:35:40 - 01:35:42] ‫והארקוד שלנו תלוי רק בסטייט הנוכחי.
[01:35:43 - 01:35:50] ‫אז ככה אנחנו מקדמים כל פעם. ‫בפיקסל הראשון הוא לא תלוי בכלום, ‫הואי רק תלוי למה שכתוב כאן, ‫באתחול שלנו של ה-State.
[01:35:51 - 01:35:58] ‫הפיקסל השני הוא יהיה תלוי ‫בעדכון של הירוקים כאן, אוקיי? ‫הירוקים יתעדכנו לפי מה שהיה קודם,
[01:35:58 - 01:36:00] ‫וגם לפי הפיקסל הראשון,
[01:36:01 - 01:36:02] ‫אוקיי? ‫זה יהיה הפיקסל השני.
[01:36:05 - 01:36:10] הפיקסל השלישי הוא, עוד פעם, ‫ה-State יתעדכן, מה שהיה כאן, אז
[01:36:11 - 01:36:15] ‫הפיקסל הראשון איכשהו ישפיע ‫על מה שקורה כאן, בגלל זה,
[01:36:16 - 01:36:17] ‫אבל גם הפיקסל השני ישפיע.
[01:36:17 - 01:36:20] אז הפיקסל השלישי יתלוי עכשיו גם בראשון וגם בשני
[01:36:21 - 01:36:23] ככה עד האחרון שנהיה תלוי בכולם
[01:36:23 - 01:36:27] זו דרך אחרת שוב פעם לממש את הרעיון הזה שכל פיקסל לא יצטרכה על הפיקסל
[01:36:31 - 01:36:34] אנחנו נראה עוד מעט, יש גם דוגמה של
[01:36:36 - 01:36:38] שימוש בזה בתמונות
[01:36:39 - 01:36:43] היתרונות בגישה הזאת זה שזה בעצם
[01:36:45 - 01:36:47] קצת יותר כללי, זה לא מאלץ אותנו ממש לבנות
[01:36:47 - 01:36:53] משהו לתמונות בגודל מסוים, ועכשיו אנחנו יכולים להמשיך לבנות את התמונות כמה שאנחנו רוצים
[01:36:54 - 01:36:56] אין פה משהו שאומר מה הגודל של התמונה
[01:36:59 - 01:37:02] ובאופן כללי פגישה כזאת
[01:37:06 - 01:37:11] ריקרנס זה משהו שאפשר לממש איתו כל פונקציה, יש כל מיני משפטים על זה שזה
[01:37:13 - 01:37:17] כמו רשתות עמוקות באופן כללי זה נותן לנו דרך לממש
[01:37:18 - 01:37:20] כל פונקציה שאפשר לחשב אפשר לממש ככה
[01:37:21 - 01:37:23] אבל החסרונות זה שזה קודם כל זה עדיין
[01:37:24 - 01:37:28] למרות שזה קצת יותר כללי זה עדיין דורש איזשהו סדר שאנחנו מניחים מראש על הפיקסלים
[01:37:30 - 01:37:36] והבעיה העיקרית זה שזה מאוד איטי, זאת אומרת אנחנו לא יכולים לעשות את מה שעשינו קודם בצורה מגבילית, לחשב את כל ה...
[01:37:36 - 01:37:39] אפילו אם אנחנו מקבלים תמונה אנחנו לא יכולים לחשב את הלייקיות שלה
[01:37:40 - 01:37:42] בצורה מגבילה על כל הפיקסלים
[01:37:43 - 01:37:46] כי בעצם בשביל הפיקסל האחרון אני צריך לדעת מה ה-State שלו שהוא תלוי
[01:37:46 - 01:37:48] בכל המעברים שאני עושה
[01:37:49 - 01:37:52] אז הפיקסל האחרון יש לו באמת חישוב שהוא יכול להיות מאוד מאוד מורכב
[01:37:52 - 01:37:57] אבל הוא גם תלוי בכל מה שקרה בדרך, אבל אני חייב לעשות את כל החישובים האלה בצורה סדרתית,
[01:37:58 - 01:37:59] אחד תחת השנייה
[01:38:00 - 01:38:06] אז האימון של זה הרבה יותר איטי וגם כשעושים backpropagation בעצם צריך לעבור דרך כל התהליך הזה עד הסוף
[01:38:06 - 01:38:09] ויש כל מיני בעיות של רדיאנטים
[01:38:10 - 01:38:13] הם עולמים או מתפוצצים או כמו דברים כאלה
[01:38:16 - 01:38:19] יכול להיות יותר חזק אבל יותר קשה קצת לאימון
[01:38:22 - 01:38:22] אז זה דוגמאות,
[01:38:23 - 01:38:28] פה זה לא דגימות מ-0 אלא דגימות מתוך חצי תמונה
[01:38:29 - 01:38:32] אז בעצם כל הפיקסלים האלה כבר היו נתונים,
[01:38:32 - 01:38:33] חצי מהתמונה הזאת
[01:38:34 - 01:38:36] מתוך תמונה אמיתית
[01:38:36 - 01:38:40] ואז פשוט דגמו השלמות אפשריות של התמונה הזאת
[01:38:41 - 01:38:45] אתם רואים שזה מתופסת מה שבדרך כלל יש בתמונות, שהתמונות הן חלקות,
[01:38:45 - 01:38:48] הצבא נשמע הרבה על כל דבר אם יש פה מיני תופעות או זה חוזר על עצמו,
[01:38:49 - 01:38:50] פה יש פה מיני תופעות של גלים,
[01:38:50 - 01:38:52] כאן יש את הטקסטורה של הדשא,
[01:38:52 - 01:38:53] זה כמובן קצת קטנות אז קשה לראות
[01:38:54 - 01:39:00] וגם הוא מבין שאם יש איזשהו עד שממשיך או שמבטחים אז כנראה הוא ימשיך
[01:39:03 - 01:39:04] אני יודע שיש דברים סימטריות,
[01:39:06 - 01:39:07] אז כמובן משלים את ה...
[01:39:09 - 01:39:10] ברוב הדגימות משלים את ה...
[01:39:11 - 01:39:14] זה הגאונטרובי, אבל גם פה יש הרבה דוגמאות
[01:39:14 - 01:39:17] שהוא פחות או יותר משלים את ה...
[01:39:18 - 01:39:19] את המעגל הזה
[01:39:20 - 01:39:21] לא תמיד
[01:39:22 - 01:39:28] רואים שהוא למד משהו על מבנה של דברים בעולם, וכל זה רק מתוך
[01:39:29 - 01:39:32] העיקרון הזה שכל פיקסל תלוי בכל הפיקסלים הקודמים
[01:39:33 - 01:39:34] עם איזשהו חישוב
[01:39:35 - 01:39:41] לדוגמה הזאת, חישוב שבנוי על איזשהו ריקרנס, אבל תכף נהיה דוגמה שלא חייב להיות כבר
[01:39:42 - 01:39:47] כן, אז הסיבה שזה עובד יותר טוב ממה שהיה קודם, זה פשוט שהחישוב הוא יותר מורכב, יש יותר פרמטרים
[01:39:48 - 01:39:52] אנחנו עדיין, זה לא בפרמטריזציה מלאה שאנחנו מסתכלים על כל האפשרויות
[01:39:53 - 01:40:02] אבל זה הרבה יותר פרמטרים מסתם לעשות מכפלה ליניארית של כל הפיקסלים הקודמים באיזה שהם פרמטריזציה, יש עוד כמה שכבות של חישוב כזה
[01:40:06 - 01:40:09] אוקיי אז מה שרציתי שנסתכל עליו קצת יותר לעומק,
[01:40:09 - 01:40:11] יש לנו בערך ארבעים דקות לזה,
[01:40:12 - 01:40:15] זה מודל שנקרא פיקסל CNN,
[01:40:16 - 01:40:18] CNN זה convolutional Neural Networks,
[01:40:18 - 01:40:21] זה באמת מבוסס על הרעיון של קונבולוציה
[01:40:23 - 01:40:29] הרעיון זה שקונבולוציה זה משהו שהוכח שהוא מאוד שימושי בוויז'ן באופן גדלי,
[01:40:30 - 01:40:34] ובעצם השאלה אם אנחנו יכולים להשתמש בזה לא רק כדי לעשות
[01:40:34 - 01:40:39] פלסיפיקציה נגיד לתמונות אלא גם למודלים גנרטיביים של תמונות
[01:40:39 - 01:40:43] שני הדברים שהיינו רוצים לתפוס
[01:40:44 - 01:40:46] בקונבולוציות זה אחד
[01:40:48 - 01:41:02] זה ההיררכיה הזאת שתלויה בלוקאליות, זאת אומרת שאנחנו מוסבים הרבה קונבולוציות אחת על השנייה, מה שזה עושה בעצם זה שיש תלות מאוד חזקה באזור מסוים לסביבה הקרובה,
[01:41:02 - 01:41:07] ותלות פחות חזקה על הסביבה היותר קרובה, ולאט לאט זה נחלש,
[01:41:07 - 01:41:13] אבל עדיין אפשר לגנות ככה ממש היררכיה שתלויה בכל התמונה,
[01:41:14 - 01:41:16] יש איזושהי תלות גובלית ויותר ויותר תלות לוקאלית,
[01:41:16 - 01:41:17] אוקיי? זה דרך,
[01:41:19 - 01:41:27] יש הרבה דרכים לעשות את זה אבל קונבולוציות זה במשך השנים זה הדרך שהכי טובה כנראה שהם מצאו לעבוד עם תמונות
[01:41:28 - 01:41:29] בדרך שתופסת את
[01:41:33 - 01:41:34] המבנה הזה שיש לתמונות, אוקיי?
[01:41:34 - 01:41:39] ‫שבאופן לוקאלי הפיקסלים מאוד קשורים אחד לשני, ‫אבל גם יש איזשהו קשר גלובלי בין
[01:41:40 - 01:41:41] ‫אזורים שונים של התמונות.
[01:41:41 - 01:41:45] ‫אז זה מה שקראתי כאן ההיררכיה ‫שמבוססת על לוקאליות.
[01:41:47 - 01:41:49] ‫והתכונה השנייה זה הדרך שבה המשקולות
[01:41:51 - 01:42:00] ‫משותפים בין פיקסלים שונים, אוקיי? אז גם ב-RNN וגם ברשת שהיינו קודם, כמו שאמרו פה, ‫אז היה איזשהו שיתוף,
[01:42:01 - 01:42:03] ‫כאן זה דרך מאוד ספציפית לשתף
[01:42:04 - 01:42:05] ‫בין משקולות.
[01:42:06 - 01:42:09] ‫זאת אומרת, ממש אותו חישוב ‫נעשה לכל אזור בתמונה
[01:42:10 - 01:42:12] ‫בצורה מנגלית.
[01:42:13 - 01:42:14] זה הרעיון של קומבולוציות.
[01:42:18 - 01:42:18] צריכים חזרה,
[01:42:19 - 01:42:23] צריכים חזרה רק מהירה, ‫מה זה בדרך כלל אומר קומבולוציה? ‫אז קומבולוציה זה,
[01:42:25 - 01:42:25] נתניהו את זה ככה,
[01:42:26 - 01:42:27] זה בדרך כלל עם כוכבית כזאת,
[01:42:28 - 01:42:31] ‫ובעצם יש לנו שתי פונקציות, F ו-G,
[01:42:32 - 01:42:36] ‫בדרך כלל אחת מהן זה הסיגנל שלנו, ‫התמונה נגיד, ‫והשנייה זה, אנחנו קוראים לזה הפילטר.
[01:42:37 - 01:42:39] ‫מה שאנחנו מפעילים על התמונה,
[01:42:40 - 01:42:48] ‫זה המשקולות שלנו. ‫אמרתי שיש שיתוף של המשקולות, ‫אז המשקולות זה יהיה בעצם איזשהו פילטר ‫שאנחנו מפעילים על התמונה.
[01:42:50 - 01:42:50] ‫הדרך שאנחנו מפעילים את זה,
[01:42:51 - 01:42:55] ‫אנחנו מכפילים כל הסביבה ‫של איזשהו פיקסל מסוים,
[01:42:55 - 01:42:59] ‫אז זה החישוב שאנחנו עושים ‫לפיקסל במקום מסוים,
[01:43:00 - 01:43:01] ‫אז כל הסביבה של הפיקסל הזה,
[01:43:01 - 01:43:05] ‫אנחנו מכפילים במשקולות האלה ‫של הפילטר, לסוכנין.
[01:43:06 - 01:43:09] ‫אוקיי, יש כאן כל מיני ביזוליזציות ‫בחד מימד.
[01:43:10 - 01:43:11] ‫אז אם אני לוקח את
[01:43:15 - 01:43:19] ‫זה הפילטר, 1, 0 ומינוס 1, ‫ואני מכפיל את זה בסיגנל הזה.
[01:43:20 - 01:43:21] ‫אז כאן אני מבין ב-1 פחות זה,
[01:43:22 - 01:43:30] ‫0 פחות זה ומינוס 1 פחות זה, ‫אז אני מתכוון בין 2. ‫-3 ושוב, עוד פעם אני מכפיל ‫שלושת המספרים האלה ב-1, 0, מינוס 1, בסוכן,
[01:43:31 - 01:43:32] ‫זה נותן לי את הערכים שיש לזה.
[01:43:33 - 01:43:35] ‫זה החישוב, זה הפילטר.
[01:43:36 - 01:43:41] ‫זה החישוב, זה מה שזה אומר, ‫להפעיל את הקונבולוציה של זה ‫עם הסיגנל הזה.
[01:43:42 - 01:43:43] זה ה-RT.
[01:43:45 - 01:43:53] ‫כן, אז זה תלוי איך אתה מגדיר את זה. ‫כן, פה זה לא כתוב, הפוך.
[01:43:55 - 01:43:57] ‫כן, אתה לא יודע איך אתה קורא ל-F עם F או אחר,
[01:43:58 - 01:43:58] ‫אתה צודק.
[01:43:59 - 01:44:02] ‫הדרך כבר נכונה לעשות ‫להגדיר את זה ויש הופכים אחדים.
[01:44:05 - 01:44:08] ‫יש אצלנו גם ככה, הכול נלמד, ‫אז זה לא משנה, כאילו,
[01:44:08 - 01:44:13] ‫אם אתה מימשת את זה ככה או ככה, ‫זה לא כל כך משנה.
[01:44:14 - 01:44:15] ‫סתם,
[01:44:16 - 01:44:18] פחות חשוב לי מענייננו, ‫אבל לידע כללי,
[01:44:19 - 01:44:24] ‫קונבולוציות משתמשים בזה ‫אפילו לא ברשתות נלמדות, ‫אלא זה ממש בעיבוד אותות,
[01:44:25 - 01:44:26] ‫אבל אפשר להשתמש בזה ‫לעשות כל מיני דברים.
[01:44:27 - 01:44:28] ‫אז למשל, להחליק
[01:44:28 - 01:44:30] ‫יש לנו איזשהו סיגנל,
[01:44:30 - 01:44:34] ‫אם אנחנו עושים לו קונבולוציה ‫עם משהו שנראה ככה,
[01:44:35 - 01:44:39] ‫או אפילו עם משהו שהוא פשוט שטוח,
[01:44:40 - 01:44:41] ‫אמרתי שזה נראה פרקטורסיאני כזה,
[01:44:42 - 01:44:49] ‫ובעצם מה שזה אומר ‫שכל פיקסל יהיה ממוצע של כל הסביבה ‫של הפיקסל המקורי.
[01:44:50 - 01:44:52] ‫נגיד שהיה לנו פיקסל כזה, ‫שהיה לנו כל מיני חורים,
[01:44:53 - 01:44:54] ‫קפיצות כאלה,
[01:44:54 - 01:44:57] ‫אנחנו נתחיל, נעשה קונבולוציה של זה עם זה,
[01:44:57 - 01:45:00] ‫אנחנו נקבל איזושהי סיגנל קצת יותר חלק ‫למה שקונקס.
[01:45:01 - 01:45:04] ‫אם משתמשים בזה רוקאי בשביל להחליק, ‫אז יש ממש פילטרים,
[01:45:04 - 01:45:07] ‫כל מיני סוגים של פילטרים ‫שאפשר להשתמש בהם להחליק,
[01:45:08 - 01:45:14] ‫אבל אפשר גם הפוך, ממש להשתמש בזה ל-edge detection כזה, ‫לזהות מתי יש קפיצות,
[01:45:15 - 01:45:18] ‫מתי יש שינוי גדול. ‫אז אם אנחנו עושים קונבולוציה ‫עם פילטר כזה,
[01:45:19 - 01:45:21] ‫יש לו מינוס אחד ואחד,
[01:45:22 - 01:45:26] ‫ובעצם כל פעם שיש ערכים זהים,
[01:45:27 - 01:45:27] ‫ליד השני,
[01:45:28 - 01:45:28] ‫אז הם יתבטלו.
[01:45:30 - 01:45:33] ‫אחד מהם אנחנו נכפיל במינוס אחד, ‫ואת השכן שלו נכפיל באחד,
[01:45:33 - 01:45:35] ‫הם יתבטלו. ‫אבל אם יש ערכים שונים,
[01:45:36 - 01:45:38] ‫אז הם כבר לא יתבטלו, ‫ואז אנחנו נזהה שהייתה כאן ‫איזושהי קפיצה.
[01:45:40 - 01:45:46] ‫אז פה יש לנו סיגנל של תיאור. ‫אז פה סיגנל אנחנו קודם, ‫יש פה קפיצה כאן, קפיצה כאן, כאן וכאן, ‫יש ארבע קפיצות,
[01:45:46 - 01:45:51] ואנחנו ממש מגדלים ערכים 0 בכל המקומות, חוץ מ-4 הנקודות האלה שהייתה קפיצה,
[01:45:52 - 01:45:55] שזו התוצאה של הקונבולוציות.
[01:45:58 - 01:46:02] אוקיי, בדו-מימד גם אפשר להפעיל, אז בדו-מימד אז העיקרון הוא אותו עיקרון,
[01:46:03 - 01:46:07] רק שעכשיו הפילטר שלנו מוגדר בתור משהו,
[01:46:07 - 01:46:10] זה הפילטר, לא ראינו אותנו, הוא פילטר 3 על 3 לדוגמה הזאת,
[01:46:11 - 01:46:13] הוא מוגדר בתור משהו דו-מימדי,
[01:46:14 - 01:46:15] קודם זו הייתה פונקציה חד-עדית,
[01:46:15 - 01:46:18] עכשיו זה יהיה משהו דו-מימדי, שאנחנו מפעילים אותו על תמונה
[01:46:19 - 01:46:19] דו-מימדית
[01:46:20 - 01:46:23] והארטפוט הוא גם תמונה, אוקיי? אז הארטפוט, הפיקסל הזה,
[01:46:23 - 01:46:28] הוא יהיה פשוט הקומבינציה הלינארית של כל הפיקסלים בסביבה של הפיקסל כאן במקור.
[01:46:28 - 01:46:32] יש איזו שאלה מה אנחנו עושים אם אנחנו יוצאים החוצה,
[01:46:32 - 01:46:37] אבל אם נתעלה מזה רגע, אז נגיד הפיקסל הזה, הוא יהיה פשוט הסביבה של 3 על 3,
[01:46:38 - 01:46:39] זה הגודל של הפילטר שבחרנו,
[01:46:42 - 01:46:43] שאנחנו נעשה
[01:46:45 - 01:46:48] הפעילה הלינארית של 3 על 3 עם הערכים בתמונה,
[01:46:49 - 01:46:51] יסכום את הכל, וזה יהיה הערך של הפיקסלים.
[01:46:52 - 01:46:53] אוקיי? אז זו המשמעות של קונבולוציה
[01:46:54 - 01:46:54] קדום-ממדית,
[01:46:55 - 01:46:59] וגם אותם דברים שיש בכלל במד אפשר לעשות פה, מוחליט, למצוא אדג'ים,
[01:47:01 - 01:47:02] כל מיני דברים כאלה.
[01:47:02 - 01:47:09] מה עושים עם פיקסלים כשהם חורגים? אז יש כל מיני שיטות, לפעמים מניחים שזה אפס פשוט, לפעמים מניחים שזה
[01:47:10 - 01:47:10] פיקסל
[01:47:12 - 01:47:14] אותו ערך כמו הפיקסל הכי קרוב,
[01:47:15 - 01:47:18] לפעמים מניחים שזה מציק לי וחוזר אחורה בקור,
[01:47:19 - 01:47:21] ובאפליקציה יש כל מיני
[01:47:23 - 01:47:24] יש כל מיני יתרונות לכל אחת מההנחות האלה.
[01:47:27 - 01:47:28] איך זה מומש ברשתות?
[01:47:29 - 01:47:38] אז בעצם הקונבנציה זה שיש לנו תמונה, תמונה היא נגיד גודל 32-32-32-3 אם זה תמונה צבעונית,
[01:47:39 - 01:47:41] אנחנו מפעילים עליה איזשהו פילטר,
[01:47:41 - 01:47:44] אז נגיד הפילטר לגבימה הזאתי הוא 5x5,
[01:47:44 - 01:47:44] אבל גם לא יש
[01:47:46 - 01:47:47] שלושה ערוצים.
[01:47:48 - 01:47:52] אפשר לחשוב על זה בתור שלושה פילטרים שהם בגודל 5 ו-5.
[01:47:53 - 01:47:55] כל אחד מהם אנחנו מפעילים על שלושת ה...
[01:47:56 - 01:47:59] אנחנו מפעילים לא רק את הסביבה של 5 ו-5 של כל טיקסל, אלא גם
[01:48:00 - 01:48:02] כל שלושת הערוצים שיש שם.
[01:48:03 - 01:48:05] וזה נותן לנו בעצם שלושה,
[01:48:06 - 01:48:07] אני חושב שאמור להיות כתוברנו.
[01:48:14 - 01:48:28] אוקיי, סליחה, זה לא... כן, אז ה-5 ו-5 ו-3, אז בגלל שכאן יש שלושה ערוצים, אוקיי, אז אני מפעיל את ה-5... כל פיקסל, אני מפעיל אותו ב-5... בסביבה של 5 ו-5,
[01:48:29 - 01:48:32] ושלושה... יש לי ערך שונה לכל אחד מהצבעים.
[01:48:33 - 01:48:36] אני צריך שיהיו לי 5 פחול 5 כפול 3 מספרים
[01:48:38 - 01:48:38] בפילטר הזה,
[01:48:40 - 01:48:41] וזה נותן לי
[01:48:41 - 01:48:42] אה...
[01:48:43 - 01:48:44] זה נותן לי תמונה חדשה.
[01:48:46 - 01:48:50] אוקיי, שוב, יש כל מיני תנאים לאיך אני מתייחס ל-output,
[01:48:51 - 01:48:52] אז במקרה הזה,
[01:48:52 - 01:48:54] לדוגמה הזאתי זה ירד מ-32 ל-28,
[01:48:55 - 01:48:57] כי אני לא מרשה לפיקסל לחרוב מהחוצה.
[01:49:00 - 01:49:03] במקרים שלנו אנחנו נעשה שהכל יישאר באותו גודל, אנחנו כן מרשים פה,
[01:49:04 - 01:49:10] ואני יכול לשים הרבה פילטרים כאלה, כמו שיש לי ברשתות הרבה ניורונים שונים,
[01:49:10 - 01:49:15] אז אני יכול, שיהיו לי הרבה פילטרים כאלה. כל אחד מהם ייצור לי תמונה חדשה,
[01:49:16 - 01:49:19] ועכשיו בעצם מה שזה אומר, שה-Layר השני שלי יהיה
[01:49:19 - 01:49:24] ל-Layר שמורכב, אם קודם היו לי שלושה ערוצים ועכשיו יש לי תמונה עם חמישה ערוצים נגיד,
[01:49:25 - 01:49:26] ועל זה אני יכול לעשות עוד פעם קונבולוציות.
[01:49:29 - 01:49:29] אה...
[01:49:30 - 01:49:30] אוקיי,
[01:49:31 - 01:49:33] זה היה תזכורת אותך ל-Layר שתות קונבולוציות.
[01:49:34 - 01:49:36] בסדר?
[01:49:38 - 01:49:39] אוקיי.
[01:49:39 - 01:49:43] אז מה-מה עושים בפיקסל CNN? אז בפיקסל CNN הכל אותו דבר,
[01:49:44 - 01:49:49] אנחנו רוצים להשתמש במה שעשינו קודם, זאת אומרת אנחנו רוצים למדל את הפיקסל ה-I
[01:49:50 - 01:49:52] שהוא יהיה רק פונקציה של כל הפיקסלים הקודמים,
[01:49:53 - 01:49:56] נכון? כי אנחנו רוצים שזה יהיה חוקי, שזה יהיה
[01:49:57 - 01:49:59] פירוק של כלל השרשרת חוקי,
[01:49:59 - 01:50:01] אנחנו רוצים לממש את זה על ידי קונבולוציות,
[01:50:01 - 01:50:03] זאת אומרת בעצם להשתמש ב...
[01:50:04 - 01:50:06] להשתמש ביתרונות שיש בקונבולוציות,
[01:50:06 - 01:50:11] ‫אחד, שזה דרך יעילה שמה, ‫לשתף משקולות בין פיצים שונים.
[01:50:12 - 01:50:13] ‫ושתיים, זה
[01:50:18 - 01:50:22] ‫ברגע שאנחנו עושים היררכיה כזאת ‫של הרבה שכבות של קונבולוציה,
[01:50:22 - 01:50:27] ‫זה באופן אוטומטי עושה לנו ‫את ההיררכיה הזאתי של דברים לוקאליים ‫מקבלים משקל יחסית גבוה,
[01:50:28 - 01:50:30] ‫אבל עדיין יש איזושהי השפעה ‫של כל התמונה.
[01:50:32 - 01:50:33] ‫תלוי כמה שכבות יש לנו,
[01:50:34 - 01:50:36] ‫פחות אזור יחסית גדול על כל פיקסל.
[01:50:37 - 01:50:40] ‫אנחנו רוצים לממש את הדבר ‫אחר זה עם קונבולוציות,
[01:50:41 - 01:50:43] ‫אבל לשמר את התכונה הזאת ‫שכל פיקסל רק
[01:50:44 - 01:50:46] תלוי בפיקסלים הקודמים. ‫אסור לנו שהפיקסל הזה יהיה תלוי
[01:50:47 - 01:50:48] ‫בפיקסלים שבאים אחר.
[01:50:49 - 01:50:52] ‫אז אם אני סתם מפעיל קונבולוציה, ‫זה מה שיקרה, נכון?
[01:50:54 - 01:50:55] ‫ואני נסתכל חזרה על מה
[01:50:58 - 01:50:58] ‫הנגיף הזה,
[01:50:59 - 01:51:02] ‫אז כל פיקסל כאן, ‫תלוי בכל הסביבה שלו,
[01:51:02 - 01:51:04] ‫אוקיי? וזה תלוי גם בפיקסלים שאחר כך.
[01:51:05 - 01:51:07] ‫ועוד יותר, אם יש היררכיה כזאת,
[01:51:07 - 01:51:09] ‫זה אומר שהפיקסל הבא, ‫הסביבה בעצם גדלה.
[01:51:11 - 01:51:15] ‫הפיקסל הזה תלוי בפיקסל השכן, ‫שהוא בעצמו היה כבר תלוי ‫בפיקסל השכן שלו,
[01:51:15 - 01:51:18] ‫אז כל שכבה שאנחנו הולכים וגדלים, ‫זה בעצם היה תלוי ביותר ויותר פיקסל.
[01:51:20 - 01:51:21] ‫אז אני רוצה שזה יקרה,
[01:51:21 - 01:51:23] ‫אבל רק לכיוון אחד,
[01:51:24 - 01:51:25] לא לעתיד.
[01:51:25 - 01:51:30] אם נסדר את הפיקסלים לפי סדר, הוא רק לפיקסלים בעבר ולא לפיקסלים בעתיד
[01:51:34 - 01:51:46] אה, גם כאן לא אמרתי את זה, אבל זה בעצם השיתוף של המשקולות, זה בגמרי שאומרים כאן, כל פיקסל כאן הוא אותה פונקציה של הפיקסלים השכנים שלו, הפיקסלים השכנים שלו הם שונים,
[01:51:46 - 01:51:49] אבל אני מכפיל אותם באותן משקולות, באותו פילטר
[01:51:49 - 01:51:52] זאת הכוונה של השיתוף של המשקולות
[01:51:52 - 01:51:57] גם משקולות שהוא יעיל ואנחנו רוצים להשתמש בו
[01:51:57 - 01:52:04] עכשיו רק בתוך הדבר הזה. בעצם אנחנו רוצים לפתח קונבולוציות שרק,
[01:52:05 - 01:52:06] רשתות קונבולוציות,
[01:52:06 - 01:52:10] stacks כאלה של קונבולוציות שרק בסופו של דבר זה שכל פיקסל רק תלוי
[01:52:11 - 01:52:12] בפיקסלים הקודמים.
[01:52:13 - 01:52:15] קוראים לזה גם לפעמים causal convolutions.
[01:52:16 - 01:52:18] אם חושבים על הדבר הזה בתור זמן,
[01:52:18 - 01:52:20] למרות שזה לא באמת זמן בפיקסלים,
[01:52:21 - 01:52:26] אבל הרבה פעמים עדיין מדברים על זה במונחים של זמן, ואז זה בעצם אפשר להגיד קונבולוציה סיבתית.
[01:52:29 - 01:52:31] אוקיי, יש גם את העניין של צבע,
[01:52:31 - 01:52:33] איך עושים את זה בצבע, אז
[01:52:34 - 01:52:39] בעצם יש פה דוגמה לאיך אפשר לעשות את זה, אז קונטקסט זה בעצם כל הפיקסלים הקודמים.
[01:52:40 - 01:52:42] עכשיו יש לי את הפיקסל הנוכחי, יש לי את ה-R, G וה-B שלו,
[01:52:43 - 01:52:46] אז אחת מהדרכים לממש את זה, זה לעשות stack כזה,
[01:52:47 - 01:52:55] שאני עושה קונבולוציה אבל אני עושה איזשהו masking על הקונבולוציה בצורה כזאתי שהשכבה הראשונה
[01:53:00 - 01:53:05] נסתכל על ה... יש לי את כל הפיקסלים הקודמים ואת הפיקסל הנוכחי והחישוב שאני עושה זה ככה
[01:53:06 - 01:53:10] ה-Red בפיקסל הנוכחי הוא תלוי בקונטקסט רק בפיקסלים הקודמים,
[01:53:11 - 01:53:13] ה-G מותר לו להיות תלוי ב-Red
[01:53:14 - 01:53:16] של הפיקסל הנוכחי ובכל הקונטקסט
[01:53:17 - 01:53:24] וה-B מותר לו להיות תלוי ב-G וה-R של הפיקסל הנוכחי ובכל הפיקסלים הקודמים
[01:53:26 - 01:53:28] ה-B של הפיקסל הנוכחי לא נכנס לשום דבר
[01:53:29 - 01:53:30] רק הוא ייכנס לפיקסל הבא,
[01:53:31 - 01:53:32] הוא ייכנס לקונטקסט של הפיקסל הבא
[01:53:33 - 01:53:37] עכשיו ברגע שעשיתי את זה אז בכל השכבות שבאות מעל
[01:53:38 - 01:53:45] אני כבר לא, אני יכול, הוא כן יכול להוסיף כאן את המשקולת הזאת מהפיקסל לעצמו
[01:53:47 - 01:53:50] ה-G הזה הוא כבר לא תלוי ב-G הזה
[01:53:50 - 01:53:53] ואני יכול עכשיו פרטיין להיות תלוי בו, כי הוא כבר לא יהיה תלוי בו
[01:53:54 - 01:54:00] יש כל מיני דרכים לעשות את המאסקינג האלה, אבל זו השיטה שאנחנו נתבוסס עליה עכשיו
[01:54:02 - 01:54:03] זה ברור המיסוך הזה?
[01:54:05 - 01:54:06] בעצם יוצא שעכשיו הפיקסל הזה
[01:54:07 - 01:54:09] הוא לא תלוי בעצמו, ה-R לא תלוי ב-R
[01:54:10 - 01:54:11] זאת אומרת אין פה שום ערך
[01:54:12 - 01:54:15] שראה את הערך, את עצמו, כאילו באינטקסט
[01:54:15 - 01:54:18] הערך הזה ה-R הוא רק ראה את הקונטקסט בתלויין
[01:54:19 - 01:54:22] הערך הזה ראה את ה-R של הפיקסל המסוגלית ואת כל התלויין
[01:54:23 - 01:54:26] ו-B ראה את G, R וכל הקונטקסטים בתלויין
[01:54:28 - 01:54:32] אז אנחנו מקיימים את התנאים של כלל השרשרת
[01:54:34 - 01:54:37] יש לנו איזשהו סידור, סידרנו את כל הפיקסלים וכל פיקסל מסוגל לפי RGB
[01:54:38 - 01:54:41] זה כל הסידור של המשתנים שלנו,
[01:54:41 - 01:54:45] וכל הפירוק הזה של כלל השרשרת נקטעה
[01:54:51 - 01:54:53] אוקיי, אז אם אנחנו מסתכלים על זה בתור קונבולוציות,
[01:54:54 - 01:54:57] אז בעצם יש ככה גם עניין ה-RGV,
[01:54:57 - 01:54:58] אני חושב שמסתכלים עליו רק על ערך אחד
[01:54:59 - 01:55:02] אז אנחנו צריכים להכפיל את המשקולות של הקונבולוציה
[01:55:05 - 01:55:05] בפסיכה הזאתי
[01:55:07 - 01:55:08] מה זה אומר?
[01:55:08 - 01:55:15] זה אומר שאנחנו מניחים שזה כבר אחרי ההיפוך, אנחנו כבר הופכים לזה, אז בעצם הפיקסל הזה
[01:55:16 - 01:55:18] אנחנו נכפיל אותו בפילטר הזה,
[01:55:19 - 01:55:20] בפילטר שהוא 5x5,
[01:55:21 - 01:55:23] אבל כל הערכים האלה,
[01:55:24 - 01:55:25] כל המשקולות שפה,
[01:55:26 - 01:55:28] זה להם איזשהו ערך,
[01:55:29 - 01:55:33] כל המשקולות של זה, הפיקסל עצמו וכל הפיקסלים שבאים אחריו בסידור,
[01:55:34 - 01:55:35] אנחנו תמיד נכפיל אותם ב-0,
[01:55:35 - 01:55:40] אוקיי, אז כל קונבולוציה שאנחנו נפיל, כשאנחנו נפיל, לפני שאנחנו מפעילים את הקונבולוציה,
[01:55:41 - 01:55:44] אנחנו נכפיל את הפילטרים במסכה הזאתי.
[01:55:45 - 01:55:46] למה יש פה פילטר דקה,
[01:55:47 - 01:55:47] זה לא משנה שזה
[01:55:49 - 01:55:51] פשוט שזה ממש קורה יותר טובה?
[01:55:52 - 01:55:55] אפשר להגיד שזה בעצם יותר קטן ואז הכול מוזז.
[01:55:56 - 01:55:58] כן, אז כשממשים את זה באמת, זה אפשר לעשות את זה כבר.
[01:55:59 - 01:56:01] כשמציגים את זה יותר קל, לחשוב על זה קל.
[01:56:05 - 01:56:09] כאילו מספר שורות אפשר להגיד שזה מספר מודליקט.
[01:56:12 - 01:56:16] זה לא תזהל, זה מובן מה אנחנו עושים בעצם, אנחנו מפיינים פונקציות
[01:56:19 - 01:56:23] פטישתות קונבולוציה רביעויות, רק כשאנחנו עכשיו נכפיל את המאסט הזה,
[01:56:25 - 01:56:30] אז זה יהיה המאסט שאנחנו נכפיל בעצם בלייר הראשון,
[01:56:30 - 01:56:32] בלייר השני את תוספת זה שכאן יש אחד.
[01:56:35 - 01:56:37] מותר לנו כבר להסתכל על עצמנו.
[01:56:40 - 01:56:40] כן.
[01:56:49 - 01:56:52] אתה יכול, אבל אתה מפספס קצת את אלה.
[01:56:53 - 01:56:54] זאת אומרת, אז יש לך כל פיקסל.
[01:56:57 - 01:56:59] אז תכף אנחנו נראה שגם זה לא כל כך טוב,
[01:56:59 - 01:57:04] אבל מה שאתה אומר זה עוד פחות במובן הזה שאם יש לנו
[01:57:05 - 01:57:09] תמונה מה שיוצא זה שהפיקסל הזה יהיה תלוי רק
[01:57:10 - 01:57:12] בפיקסלים האלה.
[01:57:15 - 01:57:17] בפיקסלים שמעליו ומשמאלו.
[01:57:18 - 01:57:22] אנחנו רוצים, זה לא בדיוק סידור שכאילו קצת מפספס
[01:57:26 - 01:57:30] כמה פיקסלים. לא סידרת את זה בצורה כזאת שאתה מסתכל על כל הפיקסלים שהיו לפניך.
[01:57:35 - 01:57:40] פעם הייתה ישר קופצת, או בפיקסל הזה אתה מחספס את ה...
[01:57:40 - 01:57:45] היית יכול כאילו להגיד שהסידור שלך הוא אלכסונים כאלה, אבל גם אתה מחספס את זה בכל פעם.
[01:57:48 - 01:57:53] אתה רוצה כאילו, מה הדבר הכי יעיל? אתה משתמש בכל מה שאתה יכול עד עכשיו בצורה שעדיין הוא קונקי.
[01:57:56 - 01:58:00] אוקיי, אבל גם פה יש איזושהי בעיה שקודם קוראים לזה בליינד ספוט.
[01:58:01 - 01:58:03] בעצם יוצא שאתה
[01:58:04 - 01:58:07] לא משתמש בכל האלכסון השמאלי הזה.
[01:58:07 - 01:58:08] זאת אומרת, הפיקסל הזה,
[01:58:09 - 01:58:10] אחרי שאתה מפעיל כמה פעמים
[01:58:12 - 01:58:13] את הקונבולוציה הזאת,
[01:58:14 - 01:58:17] היית רוצה באופן כללי, כשאתה מפעיל כמה פעמים קונבולוציה,
[01:58:18 - 01:58:22] אז מה שקורה, נגיד שכל הפיקסלים באותו גודל,
[01:58:22 - 01:58:26] אז אחרי פעם אחת בפיקסל הזה הוא כבר תלוי בסביבה הזאת.
[01:58:27 - 01:58:28] אחרי שהפעת פעמיים,
[01:58:28 - 01:58:35] ‫אז הפיקסל הזה תלוי בפיקסל שליד, ‫שהוא כבר גם תלוי בסביבה הזאתי שלו,
[01:58:35 - 01:58:38] ‫זאת אומרת, יש תלוי בסביבה כזאתי. ‫אחרי שלוש פעמים,
[01:58:39 - 01:58:46] ‫השכמה שלישית הייתה כבר תלוי, ‫קוראים לזה פילד אוף יו. ‫הפילד אוף יו שלך, האזור שאתה משתמש בו ‫כדי לחשב את הערך כאן,
[01:58:46 - 01:58:49] ‫הולך וגדל, כמו שאתה שם ‫עוד שכבות של קונבולוציה.
[01:58:50 - 01:58:52] ‫עכשיו, כשאנחנו משתמשים ‫במאסקים האלה שלנו,
[01:58:53 - 01:58:53] ‫היינו רוצים
[01:58:57 - 01:58:58] שזה גם יגדל,
[01:58:58 - 01:59:01] ‫אבל ישמר את הסדר, ‫זאת אומרת שהפיקסל הראשון
[01:59:02 - 01:59:03] ‫יהיה תלוי
[01:59:06 - 01:59:07] ‫בכזה דבר,
[01:59:08 - 01:59:09] ‫בשכבה הראשונה,
[01:59:10 - 01:59:14] ‫השכבה השנייה תהיה תלויה ‫בכזה דבר,
[01:59:19 - 01:59:21] ‫בשכבה השלישית בכזה דבר,
[01:59:23 - 01:59:26] ‫הופ, ולא, לא, לא, ‫אז היא תמיד תהיה
[01:59:28 - 01:59:32] ‫שאף פעם לא ניגע במה שיש אחר כך, ‫אבל שכן יתרחב לכל מה שהיה לפני,
[01:59:33 - 01:59:34] ‫כל האפשרויות שהיו לפני.
[01:59:35 - 01:59:37] ‫אבל זה לא המצב, אם אנחנו משתמשים ‫במאסקים שהיו לנו קודם,
[01:59:38 - 01:59:42] ‫מה שקוראות זה שיש לנו ‫מין בליינספוט כזה אלחסוני ‫שהולך לגביה.
[01:59:43 - 01:59:44] ‫אבל האזור הזה,
[01:59:44 - 01:59:50] אני לא יודע למה, ‫כי בעצם הפיקסל שהיה פה,
[01:59:51 - 01:59:53] ‫הוא גם לא יסתכל על השכן הזה מימינו.
[01:59:53 - 01:59:56] ‫אף אחד מהפיקסלים ‫לא יסתכלו על השכנים מימינם,
[01:59:57 - 02:00:00] ‫אז בעצם גדלים רק בצורה אלחסונית טובה.
[02:00:01 - 02:00:04] ‫אבל האדור הזה אנחנו מאבדים על זה.
[02:00:05 - 02:00:06] ‫אז יש להם פה דברי,
[02:00:07 - 02:00:08] ‫במאמר
[02:00:10 - 02:00:17] ‫לפצל את הקונבולוציות ‫לשתי סוגי קונבולוציות, ‫אחת ורטיקל ואחת וריזונטל,
[02:00:18 - 02:00:19] ‫שבעצם כן משנה לזה.
[02:00:19 - 02:00:21] ‫אנחנו נכנסים פה קצת ‫לפרטים טכניים של המימוש.
[02:00:25 - 02:00:26] ‫איך עושים את זה?
[02:00:26 - 02:00:29] ‫אנחנו נסתכל גם על הקוד,
[02:00:30 - 02:00:31] ‫אני מסכים לראות את זה בדיוק,
[02:00:32 - 02:00:38] ‫אבל הרעיון הוא כזה, בעצם מפצלים את זה ‫לפילטר ורטיקלי,
[02:00:39 - 02:00:41] ‫זה נראה ככה, mask בעצם ורטיקלי,
[02:00:42 - 02:00:44] ‫למאסק אוריזונטלי,
[02:00:45 - 02:00:47] ‫שהוא רק מסתכל על,
[02:00:47 - 02:00:53] ‫לא רק מסט וריזונטלי, זה קונבולוציה אוריזונטלית, ‫שרק מסתכלת על השורה הנוחותית.
[02:00:54 - 02:00:57] ‫אוקיי, ועוד פעם נעשה הפרדה ‫בין השכבה הראשונה והשכבות האחרות.
[02:00:57 - 02:01:00] ‫בשכבה הראשונה אנחנו מסתכלים ‫על כל השורות שמעלינו,
[02:01:01 - 02:01:03] ‫לא כולל השורה שלנו, שזו שורה אמצעית,
[02:01:04 - 02:01:09] ‫וכל הפיקסלים שהם משמאלנו, ‫זה לא חייב להיות חמש וחמש,
[02:01:10 - 02:01:12] ‫כל הפיקסלים שהם משמאלנו ‫בשורה הנוחותית.
[02:01:12 - 02:01:14] ‫זאת אומרת, אם נסכום את זה ואת זה,
[02:01:15 - 02:01:19] ‫קונבולוציה זה פעולה ליניארית, ‫ואני יכול לעשות אותה פעם אחת ככה ‫אבל פעם שנייה, ‫ואז נסכום את זה.
[02:01:20 - 02:01:24] ‫אם נסכום את זה וזה, ‫נקבל קונבולוציה בעצם שהיא שקולה ‫למה שהיה לנו קודם.
[02:01:27 - 02:01:30] ‫בשכבות האחרות,
[02:01:31 - 02:01:33] ‫אז אני מסתכל על כל השורות,
[02:01:35 - 02:01:41] ‫סליחה, על כל השורות מעליי, ‫כולל השורה שלי. ‫הההבדל הוא שכאן אני מכליל ‫גם בשורה שלי את מה שבא מקדימה,
[02:01:43 - 02:01:45] ‫אבל אני לא מפעיל את זה ‫בשכבה הראשונה, אז זה כפי,
[02:01:46 - 02:01:48] ‫ואת מה שהיה לנו קודם.
[02:01:50 - 02:01:56] ‫על השורה שלי, את כל השארה עם שמאל, ‫כולל הערך שלי,
[02:01:57 - 02:01:57] ‫של הפיקסל העל.
[02:01:58 - 02:02:04] ‫ואז תכף נראה איך מפעילים את זה. ‫אז בעצם מפעילים, בשכבה הראשונה ‫מפעילים את זה ומוסיפים את זה,
[02:02:05 - 02:02:07] ‫ואז בשכבות הבאות
[02:02:07 - 02:02:10] ‫בעצם שומרים איזשהו סטאק ‫שהוא רק של ה-Verticals,
[02:02:10 - 02:02:12] ‫שמפעילים את זה אחד על השני,
[02:02:12 - 02:02:15] ‫ואת זה כל פעם מוסיפים פעם אחת על התוצאה.
[02:02:15 - 02:02:20] ‫זה מאפשר בעצם לדבר הזה ‫לגדול בדיוק כמו שרצים.
[02:02:22 - 02:02:25] ‫בפעם הראשונה זה נשאר אותו דבר, ‫אבל השכבה הראשונה נשאר אותו דבר.
[02:02:26 - 02:02:31] ‫השכבה השנייה והלאה, ‫הפיקסל הזה מסתכל על כל הדברים שאתה.
[02:02:32 - 02:02:34] ‫אבל זה יהיה קצת ממיניה.
[02:02:35 - 02:02:37] ‫בפעם הבאה זה יהיה עוד פעם ימינה.
[02:02:37 - 02:02:39] ‫עוד פעם ימינה עד שזה יגיע בסוף והכול.
[02:02:40 - 02:02:47] ‫זה עדיין יוצא חוקי וזה יוצא יעיל, ‫ויוצא שזה מכיל את כל הקונטקסט ‫שרצינו להכיל.
[02:02:52 - 02:02:57] ‫אוקיי, אז בואו נסתכל קצת על תוצאות, ‫ואז יהיה לנו איזה עשר דקות ‫להסתכל קצת על הקוד.
[02:03:00 - 02:03:05] ‫אז אוקיי, זו הייתה הפעם הראשונה בערך ‫שהציגו תוצאות של
[02:03:08 - 02:03:09] גנרציה של תמונות.
[02:03:09 - 02:03:15] ‫הם לא אמיסטיקה, ‫אז אמיסטריים קודם היה גנרציה, ‫כבר היה תוצאות די טובות.
[02:03:18 - 02:03:19] ‫היו תוצאות שניים?
[02:03:19 - 02:03:22] ‫כן, כן, גנרציה של תוצאות טובות.
[02:03:23 - 02:03:32] ‫אז פה גם הייתה תוצאות טובות באמליסט, ‫אבל הדבר הראשונה שהתחילו להראות תוצאות ב-Image-Net זה אלפיים ותופע עשרה, חוני ועשרה.
[02:03:33 - 02:03:37] ‫אותו זמן התחילו גם לפתח מודל אחר שמתרגן,
[02:03:37 - 02:03:39] ‫שגם התחיל להראות תוצאות טובות.
[02:03:40 - 02:03:43] ‫בערך ב-2017 כבר היו תוצאות ממש טובות,
[02:03:46 - 02:03:48] ‫בעיקר בהתחלה זה היה יותר על פנים,
[02:03:50 - 02:03:57] ‫ובסביבות 2014-2020 התחילו להיות תוצאות ‫ממש טובות על אימג'-Net מלא.
[02:03:58 - 02:03:59] ‫אני לא יודע אם אתם רואים, אני אספרו אחר כך,
[02:04:00 - 02:04:05] ‫אבל זה נראה כזה יחסית, ‫כאילו הסטטיסטיקה של הצבעים, ‫והכול נראה טוב, יש אג'ים, ‫יש כל מיני טקסטורות,
[02:04:06 - 02:04:09] ‫ואין פה בדיוק דברים ‫שאפשר להבין מהם.
[02:04:13 - 02:04:14] ‫זה משהו שאני אמרתי, כמו
[02:04:14 - 02:04:18] ‫שמיים, למדבר, או איזה סלע, ‫אין פה שום דבר ברור.
[02:04:19 - 02:04:20] ‫אבל זה תופס,
[02:04:21 - 02:04:24] תופס משהו, בהחלט משהו ‫שלא הצליחו לעשות קודם,
[02:04:25 - 02:04:27] ‫אבל זה עדיין לא ממש מודל של תמונות,
[02:04:28 - 02:04:35] ‫נצליח לפתוס את כל המבנה של מה יש בתמונה, ‫בצורה שאדם
[02:04:36 - 02:04:38] ‫את הדברים הסמנטיים שיש בתחומים.
[02:04:43 - 02:04:47] ‫זה מתוך המאמר שלהם, ‫גם יש כאן השוואה של הלוג לייטליות,
[02:04:48 - 02:04:52] ‫שמה שיצא להם, ‫השוואה למודלים אחרים.
[02:04:53 - 02:04:55] ‫אז פה זה דוגמה, זה על אמיסט,
[02:04:57 - 02:04:57] ‫אוקיי?
[02:04:57 - 02:05:02] ‫על ידי מספרים שזה NLL, ‫זה נגטיב לוג לייטליות,
[02:05:03 - 02:05:04] ‫שזה מה שאמרנו קודם,
[02:05:04 - 02:05:10] ‫זה בעצם אורך הקידוד הזה, ‫בגלל שזה דבר דיסקרטי, ‫אז הדבר הזה אומר כמה ביטים צריך
[02:05:11 - 02:05:15] ‫כדי לשדר תמונה של NX בממוצע.
[02:05:18 - 02:05:27] ‫למרות שכאן אני רואה שכתוב ‫פשוט זה בנאצ, אוקיי? זה לא ביטים, אוקיי? ‫הלוג שהם עשו שם זה לוג בבסיס סעיג,
[02:05:27 - 02:05:31] ‫זה לא לוג בבסיס 2. ‫זה לא בדיוק ביטים, אבל זה בערך, אוקיי?
[02:05:32 - 02:05:36] ‫אז יש פה כמה מודלים שדיברנו עליהם.
[02:05:38 - 02:05:40] ‫אתם מכירים, יש NAID,
[02:05:41 - 02:05:45] ‫עוד כל מיני גרסאות של NAID, ‫שהם נסתכלו על סידורים שונים של הפיקסלים.
[02:05:46 - 02:05:59] ‫אז נגיד NAID צריך 88 פיקסלים בגרסה, וגילה 88 NUTS ‫כדי לשדר תמונת אמיסט בממוצע,
[02:05:59 - 02:06:04] ‫בגרס המשוחררות, 85-85, 84. ‫יש פה את מייד
[02:06:05 - 02:06:07] שראינו קודם, ‫שאתם יודעים מה עשינו האלה שעשינו.
[02:06:09 - 02:06:10] ‫זה יותר טוב,
[02:06:10 - 02:06:12] יותר טוב ממייד, ‫אבל לא יותר טוב מהשיכון שלו,
[02:06:13 - 02:06:13] ‫מ-86.
[02:06:15 - 02:06:15] יש פה מודל
[02:06:19 - 02:06:19] דרו,
[02:06:20 - 02:06:21] זה מודל שהוא,
[02:06:22 - 02:06:26] נדבר עליו בשבוע הבא, אולי לא עליו, ‫אבל זה מודל שמבוסס על VE,
[02:06:27 - 02:06:29] ‫כתוב כאן קטן שווה מ-80.
[02:06:30 - 02:06:30] ‫אם אתם זוכרים,
[02:06:31 - 02:06:36] ‫בדישה של וריאשיונל, ‫וויריאשיונל יותר קודר, ‫של וריאשיונל אינפרנס,
[02:06:37 - 02:06:39] ‫אנחנו לא יכולים לקשב אייקלון, ‫אנחנו יכולים לקשב רק חסם על הלייטי.
[02:06:40 - 02:06:44] ‫אנחנו יודעים שזה קטן מ-80, ‫אבל לא יודעים לקשב כמה זה בדיוק.
[02:06:47 - 02:06:49] ‫וזה מה שיוצא ב...
[02:06:49 - 02:06:51] ‫יש כאן את פיקסל-CNL,
[02:06:51 - 02:06:53] ‫עכשיו, זה 81.
[02:06:54 - 02:07:00] ‫ויש כאן, מה שאתם רואים כאן, ‫זה קוראים ל-pixel RNN,
[02:07:01 - 02:07:05] ‫זה בעצם פיקסל-CNN, ‫עם כל הצרקים האלה של קונבולוציות, ‫אבל בנוסף יש גם איזשהו סטייט,
[02:07:06 - 02:07:10] ‫שהוא ריקרנט כזה, ‫עם איזשהו LSTM שעובר,
[02:07:10 - 02:07:16] ‫אז בעצם זה אומר שהאימון יהיה יותר איטי, ‫כי הם צריכים לאמן את הכול בצורה סדרתית,
[02:07:17 - 02:07:17] ‫ולאו בצורה
[02:07:20 - 02:07:20] מקבילית.
[02:07:21 - 02:07:25] ‫אז הוא מציג תוצאות יותר טובות, ‫אבל זה הרבה יותר יקר לאימון.
[02:07:28 - 02:07:29] ‫כן, הספרים האלה ברורים.
[02:07:31 - 02:07:40] ‫לא אמרתי, בעצם ברגע שיש לנו ‫את כל הקונבולוציות האלה, ‫אנחנו יכולים פשוט לאמן ככה ‫את המודל שלנו בצורה, ‫כמו שעשינו קודם, ‫אנחנו מפעילים את הקונבולוציות,
[02:07:40 - 02:07:46] ‫יוצאים לנו בעצם ה-output ‫בכל פיקסל, ‫יוצאה לנו ההסתברות שזה,
[02:07:46 - 02:07:50] איזה בינארי, ‫זה הסתברות שזה יהיה אחד, ‫בכל אחד מהפיקסלים,
[02:07:51 - 02:07:54] ‫וזהו, אנחנו יכולים לדגום מזה פשוט, ‫אם אנחנו יכולים לייצר תמונה,
[02:07:55 - 02:07:58] ‫או לחשב את המכפלה של המספרים האלה ‫כדי לחשב את ה-iQ.
[02:08:00 - 02:08:05] ‫בדיוק כמו קודם, פשוט אנחנו ממנשים ‫את הכול עם קונבולוציות ‫שאנחנו מפעילים בבת אחת על הפועל,
[02:08:06 - 02:08:09] ‫עם כל המאסטים האלה, ‫זה היתרון ביעילות שלי.
[02:08:14 - 02:08:19] ‫אוקיי, ויש פה עוד דוגמאות, ‫אז כמו שאמרתי זה היה פחות או יותר,
[02:08:19 - 02:08:25] ‫המודלים הראשונים שהצליחו לייצר בדאטה, ‫אני חושב שזה היה ממש הראשון, ‫על אימג'נט,
[02:08:26 - 02:08:29] ‫אז הם באימג'נט לא ישבו לאף אחד ‫כמו הראשונים, אז זה היה...
[02:08:30 - 02:08:32] ‫פה הם עשו כל זה כאין ביטים,
[02:08:34 - 02:08:36] ‫וזה חלקי ה-dimensiones, חלקי מספר הפיקסלים.
[02:08:37 - 02:08:39] ‫זה מספר יותר קטן ממי שהכולם.
[02:08:40 - 02:08:48] ‫זה כאילו יותר ביטים כדי לשדר תמונת אימג'נט ‫למפי תמונת M-List.
[02:08:50 - 02:08:57] ‫ההבדל הזה שכאן זה גם ביטים ‫וזה גם חלקי מספר הפיקפונים, ‫שזה אלף עשרות ואלף עשרות ואלף עשרות פה,
[02:08:58 - 02:09:00] ‫ואני נמצא כאן, ומתי הלכים
[02:09:03 - 02:09:04] ‫אבל זה מסוים יותר גדולה.
[02:09:05 - 02:09:06] ‫אז צריכים כאן
[02:09:08 - 02:09:11] ‫בדרוג גודל של שלוש וחצי ארבע ‫ביטים לוק איקסל.
[02:09:14 - 02:09:14] ופה
[02:09:16 - 02:09:19] זה CfAR-PEN, אתם מכירים את הספר הזה?
[02:09:20 - 02:09:24] ‫זה גם דאטה-סט שהוא גם גודל 32-32, ‫אבל הרבה יותר קטן מאימג'נט.
[02:09:26 - 02:09:30] ‫זה קצת בעייתי, ‫כי יש פה מספר חצי קטן של תמונות, ‫אז די מהר עושים שם אוברפיטינג.
[02:09:32 - 02:09:36] ‫אז הם הראו פה רגועות של המפועל ‫בשווי לכל מיני חברים אחרים.
[02:09:37 - 02:09:39] ‫יש פה משהו שמבוסס על GMM's,
[02:09:40 - 02:09:43] ‫אולי גם עם איזה מרשת עמוקה כזאת.
[02:09:44 - 02:09:44] ‫יש פה...
[02:09:46 - 02:09:49] ‫נייס, אנחנו נדבר על זה קצת, ‫זה משהו שמבוסס על
[02:09:50 - 02:09:52] ‫Normalizing Slows.
[02:09:53 - 02:09:56] ‫יש פה גם איזה גרסה ראשונית ‫טוב של Diffusion, ‫אני לא יודע מה זה בדיוק.
[02:10:07 - 02:10:11] ‫כן, גם פה וגם פה אתם רואים ‫יש את המספרים של הטסט ‫ואת המספרים של הטועים.
[02:10:13 - 02:10:19] זה לא בעייתיות על התמונות שהיו באימון.
[02:10:20 - 02:10:23] ‫אבל אולי תדבר על תמונות שהיו בטסט.
[02:10:24 - 02:10:28] ‫יכול להיות מצב שאנחנו ממש נהיה ‫רואים אודרפיקינג.
[02:10:29 - 02:10:30] ‫אם המודל ממש לומד רק
[02:10:31 - 02:10:36] דברים שהם בטריינג, ובלי יכולת להכליל, ‫אז יהיה לו לייט ליוד מאוד טוב
[02:10:37 - 02:10:40] על הטריינינג, ‫אבל הלייט ליוד יהיה גרוע על הטסט.
[02:10:41 - 02:10:43] ‫הנגטיב והלאט ליוד יהיה הרבה יותר גבוה על הטסט.
[02:10:45 - 02:10:47] ‫רואים שפה הוא קצת יותר גבוה, ‫אבל זה לא כל כך גבוה.
[02:10:47 - 02:10:50] ‫אין פה אולי, גם פה ולא בטח שאלה.
[02:10:52 - 02:10:53] ‫אוקיי,
[02:10:54 - 02:10:55] אז יכול להיות שתעשו,
[02:10:55 - 02:10:57] ‫אני אראה עכשיו את הקוד,
[02:10:57 - 02:10:59] ‫נסתכל עליו כמה דקות,
[02:10:59 - 02:11:01] ‫ויכול להיות שתצטרכו,
[02:11:03 - 02:11:05] ‫חלק מהתרגיל אולי זה לממש את ה...
[02:11:06 - 02:11:10] ‫לעשות את החישובים האלה, של כל מיני מרסאות שונות של המודל,
[02:11:10 - 02:11:10] ‫כן,
[02:11:13 - 02:11:16] אוקיי, יש כאן איזה סיכום, אולי לפני שאני אדבר על זה,
[02:11:16 - 02:11:18] ‫סיכום על מודלים של אוטו-רגרסיב,
[02:11:19 - 02:11:19] ‫אז
[02:11:20 - 02:11:22] מה טוב בהם? קל מאוד לדגום מהם.
[02:11:23 - 02:11:26] ‫האלגוריתם מאוד פשוט, ‫אתם פשוט דוגמים פיקסל-פיקסל.
[02:11:27 - 02:11:29] ‫החיסרון זה שזה יכול לקחת את זה ‫הרבה זמן.
[02:11:31 - 02:11:34] יתרון שני זה שקל לחשב את ההסתברות שלהם,
[02:11:36 - 02:11:36] ‫אוקיי, וזה מה ש...
[02:11:38 - 02:11:39] ‫שזה פשוט המכפלה של הדברים האלה,
[02:11:40 - 02:11:40] אוקיי?
[02:11:41 - 02:11:44] ‫ובאופן אידיאלי, אם אפשר לעשות את הדברים האלה במקביל,
[02:11:45 - 02:11:49] ‫אז זה עוד יותר קל, ‫אפשר לעשות את זה בצורה יעילה, ‫וזה מאוד עוזר לאימון.
[02:11:49 - 02:11:53] ‫כי באימון בעצם אנחנו צריכים ‫לחשב את הדבר הזה הרבה פעמים, ‫על הרבה דוגמאות,
[02:11:54 - 02:11:55] ‫כל פעם גם לגזור את זה.
[02:11:55 - 02:11:58] ‫אם אפשר לעשות את זה בצורה יעילה, ‫אז זה מאוד עוזר לאימון.
[02:11:59 - 02:12:04] ‫מתי אי אפשר לעשות את זה בצורה יעילה ‫כשיש לנו איזשהו RNN, איזשהו LSTN, ‫שצריך להתעדכן כל פעם
[02:12:05 - 02:12:06] בין הפיקסלים.
[02:12:06 - 02:12:17] אנחנו דיברנו על דוגמאות דיסקרטיות, זאת אומרת שארטקוד היה בסדר התפלגות ברנולי או קטגורית, אבל אפשר לעשות את אותו דבר על דאטה שהוא רציף
[02:12:18 - 02:12:29] למשל אפשר שהארטקוד יהיה גאוסיאן שהתוחלת שלו היא אי פונקציה של כל האיקסים הקודמים גם יכול להיות איזושהי רשת עמוקה שנותנת לנו את התוחלת
[02:12:29 - 02:12:34] והקוואריאנס זה גם איזושהי רשת עמוקה שנותנת לנו את הקוואריאנס
[02:12:34 - 02:12:37] זה יכול להיות גאוסיאנים, אולי תערובת של גאוסיאנים
[02:12:37 - 02:12:43] שכל התוחלות וכל הקוואריאנסים הם אאוטפוטים של איזושהי רשת עמוקה שנותנת לנו.
[02:12:44 - 02:12:47] אז כל מה שאמרנו עכשיו הוא לא באמת חייב להיות רק על דיסקרטית,
[02:12:47 - 02:12:52] חייב להיות דוגמא דיסקרטית כי זה יותר פשוט להבין וזה גם איך שפיתחו את המודלים האלה מראש.
[02:12:53 - 02:12:55] יש אני חושב מודל שנקרא פיקסל cnn פלוס
[02:12:56 - 02:12:57] שהוא פחות או יותר מומש
[02:12:58 - 02:12:59] פיקסל cnn פלוס פלוס
[02:13:01 - 02:13:03] חיסרון גדול של המודלים האלה
[02:13:04 - 02:13:07] מעבר לזה שזה יכול לקחת זמן לדגום מהם
[02:13:08 - 02:13:11] ושהם לא כל כך חוזרים לנו לאחת מהמשימות שרצינו
[02:13:11 - 02:13:14] של מודלים גנרטיביים זה מה שקראנו representation learning
[02:13:14 - 02:13:23] כי אין לנו בתוך כל התהליך הזה איזשהו ייצוג שהוא מועמד טבעי להיות הייצוג של התמונה, זה משהו שתוכנס לתמונה בצורה גלובלית
[02:13:24 - 02:13:28] ובעצם אנחנו סידרנו את הפיקסלים האלה סתם באיזושהי צורה יחסית שריבותית
[02:13:28 - 02:13:33] כל פיקסל פלוס קודמי ויש לנו כל מיני ערכים שאנחנו מחשבים באמצע אבל אין
[02:13:33 - 02:13:38] איזה משהו שחושבים שאולי הוא תופס משהו יותר עמוק על התמונה
[02:13:38 - 02:13:39] בצורה גלובלית
[02:13:40 - 02:13:44] יכול להיות שאיזשהו ערך או טיקסל האחרון שתלוי בכולם
[02:13:45 - 02:13:46] אין איך שהוא תופס דברים כאלה אבל
[02:13:47 - 02:13:51] אין ממש מועמד טבעי למודלים אחרים יש לנו בדרך כלל יותר
[02:13:53 - 02:13:54] תקווה
[02:13:56 - 02:13:59] לצפות שדבר כזה יתפוס באמת משהו שהוא מעניין
[02:13:59 - 02:14:03] אוקיי שאלות לפני שאני עובר על ה...
[02:14:05 - 02:14:05] קוד?
[02:14:08 - 02:14:13] אין לנו הרבה זמן אני רק אראה כזה שתהיה לכם טעימה של איך זה נראה ואז אני אפרסם את זה
[02:14:17 - 02:14:22] כן אז זה כל לאב כזה שהוא ממש טוטוריאל אתם רואים?
[02:14:29 - 02:14:59] אוקיי, תודה רבה, תודה רבה, תודה רבה,
[02:14:59 - 02:15:04] אוקיי זה ממש טוטוריאל מסביר נחמד גם כל דברים שאמרנו עליהם אנחנו רוצים לבנות את הדבר הזה ויש פה כל מיני
[02:15:11 - 02:15:16] ויזואליזציות נחמדות, אוקיי אז קודם כל זה בנוי על דורץ' על פייטורץ'
[02:15:30 - 02:15:36] והדוגמה שיש פה זה אמניסט,
[02:15:37 - 02:15:38] אוקיי אז מוריד את הדאטה סט של אמניסט
[02:15:41 - 02:15:43] אוקיי מראה דוגמאות זה הדאטה סט עצמו
[02:15:52 - 02:15:54] עכשיו נדבר על כל העניין הזה של המאסקינג
[02:15:55 - 02:15:56] אוקיי מה שדיברנו בדיוק עכשיו
[02:15:57 - 02:15:58] אז יש פה את המימוש של ה...
[02:16:00 - 02:16:04] של השימוש במאסקינג,
[02:16:05 - 02:16:08] כלומר לעשות קונבולוציה ועם המאסקינג, אז יש כאן את הקרנל עצמו
[02:16:10 - 02:16:16] קרנל זה בעצם הפילטר שאנחנו מפעילים עליו, הם מפעילים את אותה קונבולוציה, אז יש כאן את ההגדרה של הגודל שלו
[02:16:17 - 02:16:20] זה כל מיני דברים שיש כבר בפייטורץ'
[02:16:20 - 02:16:25] של בונה שכבה של קונבולוציה עם גודל קרנל שאנחנו רוצים
[02:16:26 - 02:16:29] מה אנחנו רוצים שיקרה מחוץ לתמונה
[02:16:30 - 02:16:33] ובנוסף אנחנו מגדירים את המאסק
[02:16:34 - 02:16:38] באופן איזה שהוא משתנה, למרות שהוא לא משתנה מלמד, אבל הוא משתנה של המודל
[02:16:39 - 02:16:47] וכשאנחנו מפעילים את ה-forward אז אנחנו לא סתם עושים את הקונבולוציה, אנחנו קודם מכפילים את ה-weights של הקונבולוציה, שזה הפילטר הזה,
[02:16:48 - 02:16:49] מוכשלים את זה במאסק
[02:16:50 - 02:16:52] ואם המאסק שלנו יהיה חוקי אז זה בעצם
[02:16:55 - 02:16:57] יגרום למודל שלנו להיות סיבתי כזה,
[02:16:58 - 02:16:59] שכל פיקסל הוא רק קוראים לו פיקסל אם הוא תזמין
[02:17:01 - 02:17:05] ועכשיו יש כאן כמה מיני מימושים של מאסקים שונים, בדיוק עם מה שדיברנו עליו
[02:17:06 - 02:17:11] אז יש פה את ההפרדה הזאת בין ורטיקל לאוריזונטל
[02:17:12 - 02:17:13] אז בוורטיקל
[02:17:14 - 02:17:26] יש כאן תמיד שתי אופציות בשביל השכבה הראשונה והשכבות האחרות, בשכבה הראשונה אנחנו גם רוצים לעשות מעס גם לאפס את השורה האמצעית
[02:17:27 - 02:17:27] ואחר כך לא
[02:17:28 - 02:17:28] יש פה
[02:17:29 - 02:17:29] יש פה פלאג כזה
[02:17:30 - 02:17:33] אבל תמיד אנחנו רוצים לאפס את כל השורות
[02:17:34 - 02:17:36] שהם אחרי חצי מהקרנל סייט
[02:17:38 - 02:17:39] זה מה שכתוב כאן, אנחנו
[02:17:39 - 02:17:42] מאפסים את זה, אנחנו מאפסים את זה באחדות ואז מאפסים
[02:17:45 - 02:17:48] והקרנל של האוריזונטל הוא רק בגודל אחד,
[02:17:48 - 02:17:50] הוא רק שורה אחת
[02:17:51 - 02:17:53] וגם אותו דבר, חצי מהשורה הזאת אנחנו מאפסים
[02:17:55 - 02:17:55] כולל האמצע
[02:17:58 - 02:18:03] בהתחלה, תמיד את כל מה שבאחרי האמצע וכולל האמצע גם באיתרציה הרשמית
[02:18:05 - 02:18:07] ויש פה איזה דרך לעשות ויזואליזציה למה קורה
[02:18:08 - 02:18:12] על ידי זה שהם פשוט מחשבים את ה... בגלל שאפשר לחשב את הגרדיאנט בצורה אוטומטית
[02:18:13 - 02:18:20] אז אפשר פשוט לדעת האם פיקסל אחד באיזה פיקסל הוא תלוי על ידי חישוב גרדיאנט, פשוט מחשבים מה הגרדיאנט של כל הפיקסלים האחרים
[02:18:20 - 02:18:22] כפונקציה של פיקסל אחד
[02:18:23 - 02:18:27] ואז אם יש איזשהו גרדיאנט זה אומר שהפיקסל הזה תלוי,
[02:18:27 - 02:18:29] החישוב שלו תלוי בפיקסלים האחרים
[02:18:30 - 02:18:35] איפה יש את כל הויזואליזציה של חישוב הדרך לעשות ויזואליזציה על הגרדיאנטים האלה
[02:18:36 - 02:18:37] לא ניכנס לפה
[02:18:39 - 02:18:42] ואז זה אומר, אוקיי, אם אני מפעיל פעם אחת את האוריזונטל
[02:18:42 - 02:18:46] בגודל, במוקדם גודל שלוש על שלוש
[02:18:46 - 02:18:49] ובעצם זה אומר שהאוריזונטל יש לו רק פיקסל אחד משמאל,
[02:18:50 - 02:18:50] תסתכל עליו
[02:18:51 - 02:18:52] אז הוא רק תלוי בפיקסל אחד משמאל
[02:18:53 - 02:18:57] הוורטיקל תלוי בשלושת הפיקסלים שמעליו וזה אומר שיש גרדיאנט
[02:18:57 - 02:19:01] כל שאר הפיקסלים בגרדיאנט הוא 0, הם לא משפיעים על הפיקסל האדום
[02:19:01 - 02:19:03] רק שלושת אלה הם משפיעים על האדום
[02:19:05 - 02:19:07] ועכשיו כשמחברים את שניהם,
[02:19:07 - 02:19:09] זה בדיוק מה שרצינו בשביל כלום
[02:19:13 - 02:19:15] עכשיו יש כמה שכבות שמפעילים
[02:19:16 - 02:19:19] אז אחרי שמפעילים שתי שכבות, זה בעצם התלות
[02:19:20 - 02:19:25] פה זה גם הערך נורא של הגרדיאנט, כאן זה דרך בינארי קרובי, האם הוא שונה מ-0
[02:19:25 - 02:19:30] אז זה המאסק שיוצא, מבחינת שכבלת שלישית, הוא גדל
[02:19:31 - 02:19:32] ערבית, הוא גדל עוד יותר,
[02:19:33 - 02:19:34] חמישית, הוא גדל עוד יותר.
[02:19:40 - 02:19:50] אוקיי, יש פה עוד כל מיני דברים שלא נכנסנו, יש עוד כל מיני טריקים במאמר שלא נכנסנו אליהם, אחד זה איזשהו גייטינג כזה בתוך הקונבולוציות, משהו שדומה קצת ל-STM,
[02:19:51 - 02:19:51] תוך קונבולוציות,
[02:19:52 - 02:19:53] רגע חשוב.
[02:19:54 - 02:19:54] רגע.
[02:19:56 - 02:20:05] המטרה כאן זה שבעצם זה הכל יהיה יותר יעיל, שבפחות פרמטרים יהיה דרך להסתכל בצורה יעילה על יותר פיקסלים בקונפקסט.
[02:20:06 - 02:20:09] זו שיטה אחת, יש עוד שיטה שנקראת DILATION,
[02:20:10 - 02:20:12] שזה מעניין את פה,
[02:20:13 - 02:20:22] שבעצם גם בונים קונבולוציות, בונים פילטרים שהם לא מסתכלים על השכבה הקרובה, אלא מדלגים כל פעם על פיקסל אחד, זו היררכיה כזאת שקופצת יותר מהר.
[02:20:22 - 02:20:25] אבל עדיין, מפעילים את כל המאסקים באותה צורה.
[02:20:26 - 02:20:31] אז זה הטריקים האלה, ופה יש את המודל עצמו,
[02:20:33 - 02:20:34] אני לא יודע אם אני יכול להריץ את זה עכשיו.
[02:20:41 - 02:20:42] זה מספיק לרוץ,
[02:20:43 - 02:20:47] אז הוא פשוט בנוי את ה-VSTAC וה-HSTAC,
[02:20:47 - 02:20:49] זה ה-Layיר הזה של ה-Stacking,
[02:20:49 - 02:20:52] כמו שאמרנו, אה, לא הראתי לכם,
[02:20:52 - 02:20:54] זה לא ראוי איפה זה,
[02:20:56 - 02:21:16] אתם תצטרכו לראות את זה לבד, יש את הרעיון הזה של איך ה-Virtical וה-Horizontal מחוברים,
[02:21:17 - 02:21:23] אפשר לראות את זה בוויזואליזציות שהם עשו כאן, בעצם
[02:21:39 - 02:21:42] עושים כאן כמה שכבות של קונבולוציות, זה הוויזואליזציות שראינו קודם,
[02:21:43 - 02:21:46] אז בעצם מפעילים, יש משהו שקוראים לו vertical image,
[02:21:47 - 02:21:50] שהוא פשוט stack של ה-Vertical Involution, זה אחד על השנייה,
[02:21:51 - 02:21:57] ואז בסופו של דבר מה שאנחנו מראים זה שאנחנו מפעילים את ה-Horizontal Involution על ה-Vertical הנוכחי,
[02:21:57 - 02:22:01] יש לנו stack של Varticles שהולך וגדל, וכל כך אנחנו מפעילים הוריזונטל
[02:22:02 - 02:22:02] אחד רק,
[02:22:03 - 02:22:06] אצלו לנו להפעיל את ה-Horizontal כמה פעמים ולעשות stack-איף שלו,
[02:22:07 - 02:22:11] זה הדרך היעילה שלהם לעשות את זה, בסופו של דבר ההסבר פה הוא מאוד מאוד
[02:22:17 - 02:22:28] אז יש פה דוגמא, זה המודל עצמו עם כל הדברים בתוכו כבר,
[02:22:29 - 02:22:30] ופונקציה שנקראת פלט-לייטליהוד,
[02:22:31 - 02:22:35] פונקציה שנקראת Sample שמייצרת את הדגימות
[02:22:39 - 02:22:42] וגם את כל האופטימיזציות שצריכים בשביל האימון
[02:22:47 - 02:22:54] וזה מה שיוצא ממנו בסופו של דבר
[02:22:55 - 02:23:03] הפיקסל האמצעי בתמונה, למה הוא תלוי, זה דרך לדאוג שביקורנות לעשות את הדבר הזה לכל פיקסל, לראות שכל פיקסל לא תלוי בפיקסלים האחרים
[02:23:05 - 02:23:10] ועד לשתקן את האימון, פה עכשיו הוא יטען אני חושב משהו שהוא כבר קיים
[02:23:16 - 02:23:17] נראה לי עד עד
[02:23:47 - 02:23:52] זה לא מספיק,
[02:23:52 - 02:23:53] זה כבר הזמן.
[02:24:00 - 02:24:06] שיחקתי עם זה, זה לא עובד עכשיו, אבל בעיקרון הוא, אם לא משנים כלום, הוא אמור לטעון את המודל שהוא כבר אימן,
[02:24:07 - 02:24:10] ואז אפשר לדגום ממנו, לחשב איתו לייקליות של כונות חדשות,
[02:24:10 - 02:24:11] כל מיני דברים כאלה.
[02:24:11 - 02:24:13] אבל תרגיל איכשהו יהיה מבוסס על הקוד הזה,
[02:24:14 - 02:24:19] אני אשלח לכם את הקוד הזה עם עוד כל מיני דברים לממש עליו.
[02:24:21 - 02:24:26] ויכול להיות, אנחנו קצת בסמסטר מקוצר, אז יכול להיות שמה שנעשה זה נחבר את
[02:24:28 - 02:24:33] התרגיל הזה כבר עם המודל הבא, אולי גם בשבוע הבא כבר נדבר על DAES,
[02:24:34 - 02:24:37] אז אולי כבר התרגיל יהיה גם לממש DAES
[02:24:37 - 02:24:38] על אותו קוד
[02:24:39 - 02:24:40] ולהשוות ביניהם.
[02:24:44 - 02:24:45] טוב, וזהו להרוג.
[02:24:45 - 02:24:45] תודה רבה.