[00:00:00 - 00:00:00] ‫שלום.
[00:00:22 - 00:00:23] ‫-שלום.
[00:00:27 - 00:00:29] ‫מעניינים, את יכולה להיות ‫בתרגיל?
[00:00:30 - 00:00:33] עכשיו זה היה תרגיל של ה-VAE היום, אני חושב שזה היום רגיש, נכון?
[00:00:34 - 00:00:35] כולם הגישו?
[00:00:39 - 00:00:45] היו הרבה ששאלו אותי על עניין של הווריאנטס של ה-output
[00:00:46 - 00:00:51] הדבר הזה שאמרתי, ה-hyper-pounter זה שאמרתי שאפשר שערך טוב בשבילו הוא ארץ נקודה אחת
[00:00:52 - 00:00:57] אז הם כבר לא הבינו איפה להשתמש בו, הם אמרו שאמרו שאם משתמשים בו הוא פחות טוב
[00:00:58 - 00:01:03] אני חושב שמה שתזכרו שהיה לנו בתחילת השיעור שבה הפספסתם זה שהלוג
[00:01:04 - 00:01:07] של הסתברות גאוסיאנית
[00:01:08 - 00:01:12] אם אנחנו, הפרדיקציה שלנו זה התוחלת של התוחלת הגאוסיאנית
[00:01:12 - 00:01:15] אז לוג של הפרדיקציה הגאוסיאנית זה פשוט השגיאה הריבועית
[00:01:16 - 00:01:22] בין התוחלת הזאת שעשינו לפרדיקציה לערך האמיתי של התמונה
[00:01:23 - 00:01:24] נכון? אם יש לנו
[00:01:24 - 00:01:29] אינטרסיאנית נראית משהו חוזר, אז נגדלת p במינוס
[00:01:30 - 00:01:30] נורמה
[00:01:32 - 00:01:32] x
[00:01:32 - 00:01:33] פחות מיום
[00:01:35 - 00:01:36] מדוע
[00:01:37 - 00:01:45] אם יש לנו, נגיד שזה וקטור, אוקיי? אבל כשהמטריצה קובריאנס היא רק אלכסונית עם ערך קבוע
[00:01:47 - 00:01:48] בעצם מה שאנחנו מקבלים זה ערך שהוא קבוע
[00:01:50 - 00:01:51] אז זה וקטור,
[00:01:51 - 00:01:55] והנורמה של הוקטור הזה של ההתפלש בין x ל-mue
[00:01:57 - 00:01:59] ו-mue זה הפרדיקציה של הרשת שלנו
[00:02:00 - 00:02:01] זה ה-output שלה
[00:02:02 - 00:02:04] אז אם אנחנו לוקחים לוג של הדבר הזה
[00:02:05 - 00:02:09] נשאר לנו רק פה זה לא תלוי ב-x ולא ב-mue, זה לא משנה
[00:02:10 - 00:02:11] מה שנשאר לנו זה רק האיבר הזה
[00:02:12 - 00:02:13] אבל הרבה אני חושב,
[00:02:14 - 00:02:17] כל כך אנשים ששאלו אותי, חלק אמרו שהם עשו פה, השתמשו
[00:02:18 - 00:02:20] בפונקציה שנקראת mse
[00:02:22 - 00:02:31] שזה אותו דבר, מה זה בין mse לזה, ההבדל הוא ש-mse זה מין סקודר או זה פשוט מחלק בין מספר האיברים שיש פה
[00:02:32 - 00:02:33] שלנו 700 ומשהו פיקסלים
[00:02:36 - 00:02:37] וההסתברות באמת של התמונה זה פשוט סכום כל
[00:02:39 - 00:02:41] סכום כל השגיאות הריבועיות האלה
[00:02:42 - 00:02:48] אז כשיש את הסכום של כל השגיאות הריבועיות האלה ערך סביר לשים פה זה בערך 0.1
[00:02:48 - 00:02:51] זה בערך גם מה שאנחנו מצפים שיהיה,
[00:02:52 - 00:02:55] נגיד, אם הייתי עושה פרדיקציה לפיקסלים
[00:02:57 - 00:03:02] אז הייתי מצפה שבין 0 ל-1 זה לא כזה ישנה אם אני נותן פלוס או מינוס 0.1,
[00:03:03 - 00:03:03] לא אמור לשנות
[00:03:04 - 00:03:08] 1 אם הייתי שם פה 1 זה כנראה לא מודל כל כך טוב
[00:03:10 - 00:03:12] אז 0.1 זה משהו סביר, זה באמת נותן,
[00:03:13 - 00:03:14] אמור לתת תוצאות יחסית טובות
[00:03:15 - 00:03:16] מי שעשה פה סכום,
[00:03:17 - 00:03:23] אז בעצם יוצא שהחלוקה בדבר הזה היא לא נכונה,
[00:03:23 - 00:03:28] אז יוצא שהערך הכי טוב לשים פה יהיה ערך אחר,
[00:03:28 - 00:03:30] יוצא שאחד אולי הוא פה כן נעלם טוב.
[00:03:32 - 00:03:36] אני חושב שאם תעשו את החישוב זה יוצא בערך הבדל של פי ארפה, זה לא כזה,
[00:03:37 - 00:03:39] הסיגנל לא אמור להיות כזה רגיש לדבר הזה.
[00:03:40 - 00:03:43] באופן כללי כשעושים את הדבר הזה פלוס יש את ה-KL term,
[00:03:43 - 00:03:49] אז בעצם כל מה שאנחנו עושים פה זה משקול שונה של ה-KL term וה-Square-Loss של שם.
[00:03:51 - 00:03:54] באמת יש ערכים שבהם זה יהיה כן קצת יותר טובות, זה ערכים שיהיה פחות טובות,
[00:03:55 - 00:03:57] לא תמיד זה מסתדר גם ממה שאנחנו חושבים, לפעמים יש,
[00:03:57 - 00:04:03] גם האופטימיזציה היא בעיה קשה, לפעמים בשביל האופטימיזציה צריך לתת ערכים אחרים
[00:04:04 - 00:04:07] שהם לאו דווקא מה שהיינו רוצים שיהיו באמת ערכים בסופו של דבר.
[00:04:07 - 00:04:10] ויש הרבה תיאוריה גם סביב הדבר הזה, הרבה מחקר על איך לעשות את זה,
[00:04:11 - 00:04:12] לפעמים משנים את זה תוך כדי האופטימיזציה,
[00:04:14 - 00:04:16] כמו שעושים עם ה-Learning-Rate,
[00:04:17 - 00:04:21] יש פה כל מיני Scheduling כאלה לשנות את המשקולים השונים של האיברים האלה.
[00:04:22 - 00:04:25] אז בעד אחד אמר לך שמתאים לסכום או לממוצע הזה?
[00:04:26 - 00:04:28] 0.1 אמור להתאים לסכום.
[00:04:30 - 00:04:33] שזה אמור בדרך כלל לעשות ממוצע.
[00:04:34 - 00:04:38] כשאתה עושה ממוצע את מחלקת בעצם ב-700.
[00:04:39 - 00:04:41] ב-2 כפול 0.1 בריבוע הזה
[00:04:44 - 00:04:49] אולי מישהו שאל אותי, עשיתי את החישוב והצעה לי דרך דל של פי ארבע כזה.
[00:04:50 - 00:04:54] אני לא זוכר לאיזה כיוון, אם זה היה 0.4 או 0.0
[00:04:57 - 00:04:59] אבל בערכים האלה זה לא אמור להיות באמת רגיש כל פעם.
[00:05:02 - 00:05:12] גם שוב, יכול להיות שדווקא משהו שלא נראה הגיוני ייתן תוצאות יותר טובות, כי יש כאן הרבה תהליכים מורכבים של האופטימיזציה עצמה, בהתחלה המודל הוא ממש גרוע,
[00:05:12 - 00:05:15] אז יכול להיות שדווקא טוב ולתת לסיגמה ממש גדולה.
[00:05:17 - 00:05:23] ויש גם הרבה מודלים לומדים את ה-output של הרשת זה לא רק המיועד, אלא גם הסיגמה.
[00:05:24 - 00:05:26] וגם לפעמים יש ערך לתת סיגמה שונה לכל פיקסל.
[00:05:27 - 00:05:29] יש פיקסל שהמודל יכול להיות בטוח לגביהם,
[00:05:30 - 00:05:33] למשל כל הפיקסלים במסגרת של אמליסט,
[00:05:34 - 00:05:39] המודל בדרך כלל יכול להיות מאוד בטוח שהם 0.0. רק באמצע יש איזושהי איוורגנות.
[00:05:40 - 00:05:42] אז לפעמים יש ערך ללמוד גם את הדבר הזה,
[00:05:42 - 00:05:43] ושזה יהיה שונה פר-פיקסל.
[00:05:44 - 00:05:45] אבל לא רציתי להיכנס לעניין הזה בתרגיל.
[00:05:47 - 00:05:47] טוב,
[00:05:48 - 00:05:49] אז זה היה לגבי התרגיל.
[00:05:50 - 00:05:51] אז מעט דיברנו על DAs,
[00:05:52 - 00:05:57] מאז דיברנו גם על normalizing flows, שזה היה התרגיל שהשבוע קבלו.
[00:05:58 - 00:06:03] והיום אנחנו נדבר על מודלים שהם בעצם לא מעניינים אותם עם לייק יידוד.
[00:06:04 - 00:06:07] זה כולל מודלים שנקראים גנץ,
[00:06:09 - 00:06:11] נדבר על כל מיני דברים שקשורים לדגימות,
[00:06:12 - 00:06:15] זה נדבר על מודלים שהם מבוססים על אנרגיה,
[00:06:16 - 00:06:20] וספציפית זה בעצם יהיה הקדמה למודלים שמבוססים על score,
[00:06:20 - 00:06:25] שכל הדבר הזה זה בעצם, תחשבו על זה בתור הקדמה לדיפיוז'ן מודל, זה הבסיס של דיפיוז'ן מודל,
[00:06:26 - 00:06:26] שנדבר עליהם
[00:06:30 - 00:06:32] בשני המפגשים שישארו בשבוע הבא,
[00:06:32 - 00:06:33] בשבוע השיעור האחרון.
[00:06:35 - 00:06:41] אוקיי, אז מה המתוכן לנו היום? אנחנו נדבר על העניין הזה של לייטיות לעומת דגימות.
[00:06:42 - 00:06:47] אם אנחנו רוצים מודלים שיש להם לייטיות טוב או מודלים שנותנים לדגימות טובות,
[00:06:47 - 00:06:49] אבל השקילות הן באותו דבר.
[00:06:50 - 00:06:56] זה סוג של הקדמה למה שעושים בגן, זה To Sample-Test.
[00:06:57 - 00:07:00] נדבר על מודל זה בגן,
[00:07:00 - 00:07:05] אחר כך נדבר על איך אנחנו יכולים לעשות אבלואציה לדגימה,
[00:07:06 - 00:07:09] לדגימות או למודל שמייצר דגימות.
[00:07:09 - 00:07:14] ואז נדבר על הנושאים האלה של אנרגי בייסט מודל וסגור בסטמנס.
[00:07:17 - 00:07:18] אז תזכורת,
[00:07:18 - 00:07:23] אנחנו לומדים מודלים גנרטיביים, אנחנו רוצים מודל שמייצר לנו דאטה
[00:07:25 - 00:07:32] במימד גבוה בצורה הסתברותית, אוקיי? אז בעצם לא רק חשוב לנו בדרך כלל שהוא ייצר דאטה, אלא שהוא יהיה מודל הסתברותי
[00:07:33 - 00:07:34] של הדאטה.
[00:07:35 - 00:07:38] ואנחנו מניחים שיש איזשהו פי דאטה,
[00:07:38 - 00:07:40] איזושהי הסתברות שמייצרת את הדאטה האמיתי,
[00:07:40 - 00:07:45] אנחנו רוצים להגיע לאיזשהו פי דאטה שהוא כמה שיותר קרוב לפי דאטה.
[00:07:46 - 00:07:50] והמטרה של כל הדבר הזה זה שיש לנו כמה מטרות אפשריות,
[00:07:50 - 00:07:53] או לייצר דגימות בתוך המודל הזה, לייצר דאטה,
[00:07:55 - 00:08:03] או להשתמש בזה כדי ללמוד איזשהו מבנה, אז למשל ב-DAE ראינו את זה שהייצוג של ה-Latence של ה-DAE אולי הוא תופס איזה משהו
[00:08:04 - 00:08:06] בצורה יותר טובה מהפיקסלים של התמונה.
[00:08:08 - 00:08:10] ולתפוס את האי הוודאות שיש לנו,
[00:08:11 - 00:08:14] פחות דיברנו על הנושא הזה,
[00:08:14 - 00:08:25] אבל זה דווקא מתקשר למה שעכשיו אמרנו על הסיגמה הזאתי, אם למשל היינו עושים מודל שנותן סיגמה שונה לכל פיקסל, אז היינו יכולים גם להגיד איזה פיקסלים אנחנו בטוחים, או איזה פיקסלים אנחנו פחות בטוחים, ולעשות כל מיני
[00:08:26 - 00:08:28] מערכת קבלת הפלטות שתלויה בדברים האלה.
[00:08:29 - 00:08:32] ובאופן כללי אנחנו רוצים לעשות כל מיני,
[00:08:33 - 00:08:36] לפתור כל מיני בעיות שונות שאפשר לפרמל אותן בתור
[00:08:39 - 00:08:41] רובייליסטיק אינפרנס, עסקה הסתברותית.
[00:08:42 - 00:08:48] היום אנחנו בעצם ניתן יותר רגש על הדבר הזה,
[00:08:49 - 00:08:50] על ייצור דאטה.
[00:08:52 - 00:08:57] בעצם גאנד זה המודל הראשון שנתן תוצאות ממש טובות
[00:09:00 - 00:09:01] במשימה הזאתי, המשימה הראשונה.
[00:09:02 - 00:09:05] עד אז בעצם כשהיו מדברים על ג'נרטיב מודלס,
[00:09:06 - 00:09:09] הכוונה הייתה יותר דברים לסוג הזה וזה,
[00:09:10 - 00:09:10] שלושת אלף,
[00:09:12 - 00:09:14] להשתמש במודלים האלה כדי לעשות כל מיני חישובים, לפתור כל מיני בעיות,
[00:09:16 - 00:09:18] ופחות היה דגש על לייצר ממש דאטה.
[00:09:20 - 00:09:25] וגם זה היו תוצאות ממש טובות למשימה הראשונה הזאתי, שממש זה עשה
[00:09:26 - 00:09:30] שיפט לכל הפוקוס של המושג הזה, אפילו ג'נרטיב מודלס,
[00:09:31 - 00:09:35] ועד היום כשמדברים על ג'נרטיב מודלס ועכשיו ג'נרטיב AI,
[00:09:35 - 00:09:39] הכוונה היא בעיקר לייצור דאטה, למודלים שיודעים לייצר דאטה.
[00:09:39 - 00:09:41] פחות מדברים על היכולות ההסתברותיות
[00:09:42 - 00:09:46] והיכולות לעשות כל מיני העסקה הסתברותית עם המודלים,
[00:09:46 - 00:09:50] שזה היה באופן מודלס תמיד הפוקוס של התחום הזה.
[00:09:52 - 00:09:57] עם דיפיוז'ן מודל אולי אפשר להתחיל לדבר חזרה לאזורים האלה.
[00:09:59 - 00:10:00] אוקיי,
[00:10:01 - 00:10:01] אז
[00:10:02 - 00:10:10] אין סוג של מודלים ראינו עד עכשיו, אז ראינו אוטו-רגרסיב מודלס, נכון ששם פירקנו את
[00:10:12 - 00:10:15] המודל שלנו להרבה שלבים כאלה.
[00:10:17 - 00:10:24] גם normalizing flows בעצם אפשר לכתוב אותם באיזושהי דרך כזאת, אולי לא תלוי ב-X הקודם, אלא באיזושהי שרשרת של
[00:10:27 - 00:10:28] משתנים,
[00:10:28 - 00:10:32] שאנחנו יודעים לחשב באופן מדויק את ההסתברות שלהם,
[00:10:33 - 00:10:37] והכל פה הוא מדויק, אנחנו יכולים לעשות את האימון בצורה יעילה בגלל זה,
[00:10:38 - 00:10:43] אבל הבעיה בעיקר במודלים אוטו-רגרסיב וגם ב-flow שהם מאוד עמוקים,
[00:10:43 - 00:10:45] זה שהדגימה היא מאוד איטית.
[00:10:46 - 00:10:48] אנחנו צריכים לעשות הרבה תהליכים,
[00:10:48 - 00:10:51] זו אחת מהבעיות לפחות של המודלים האלה.
[00:10:52 - 00:10:54] מודל אחר שדיברנו עליו זה VAE,
[00:10:54 - 00:11:00] ויש לנו אותו קודר שהוא מאשר לנו לשלב בעצם
[00:11:00 - 00:11:06] שני מודלים שהם אולי כל אחד מהם יחסית פשוט, אבל השילוב ביניהם בעצם נותן לנו מודל שהוא סך הכול יותר עשיר.
[00:11:09 - 00:11:13] בגלל שאחד מהמשתנים עכשיו הוא חבוי ואנחנו לא יודעים לחשב בצורה יעילה את
[00:11:15 - 00:11:15] ההתפלגות עליו,
[00:11:16 - 00:11:20] אז אנחנו צריכים להשתמש בשיטות יחסית יותר מורכמות,
[00:11:21 - 00:11:25] ורישונל אינפרנס ואנחנו עושים אמורטייז ורישונל אינפרנס ואנחנו מאמנים
[00:11:25 - 00:11:30] רשת לעשות בעצמה את הוירישונל אינפרנס בצורה יעילה וזה מפלגתם את התרגיל.
[00:11:34 - 00:11:37] הדגימה מהמודל הזה היא דווקא מאוד מהירה,
[00:11:38 - 00:11:39] נכון?
[00:11:39 - 00:11:43] הסתכלתם את זה בתרגיל, פשוט דוגמים מהמודל הזה ובהינתן לזה דוגמים מהמודל הזה.
[00:11:44 - 00:11:45] זה שני שלבים.
[00:11:46 - 00:11:48] אבל בעיה, אני לא יודע אם שמתם לב,
[00:11:49 - 00:11:52] בהם ניסיונל קצת פחות רואים את זה, שהדגימות הן בדרך כלל
[00:11:53 - 00:11:55] לא מאוד חדות.
[00:11:57 - 00:12:01] לרוב הדגימות יראו לכם, גם בתמונות רגילות, יראו
[00:12:03 - 00:12:04] פלושפשות קצת.
[00:12:06 - 00:12:14] הראיתי, כשעשינו את השיעור הלוואי, אני ראיתי לכם דוגמאות של מודלים קצת יותר מתקדמים, שיש להם הרבה שלבים, היררכיה כזאת של לייטנס, ואז
[00:12:14 - 00:12:17] זה בדרך כלל משפר את התוצאות של הדגימות.
[00:12:19 - 00:12:22] אבל זה שוב פעם מכניס אותנו לזה שהדגימה היא קצת יותר איטית.
[00:12:22 - 00:12:23] באופן כללי
[00:12:27 - 00:12:31] בתהליך דגימה כזה קצר של פיט פורוורד אחד כזה ורשת,
[00:12:32 - 00:12:37] לא מצליחים עדיין להגיע לתוצאות של גימות שהן מאוד חדות.
[00:12:40 - 00:12:41] בשיטות האלה, כן?
[00:12:41 - 00:12:45] היום אנחנו נראה בגאנס שבעצם אפשר להגיע לתוצאות האלה.
[00:12:45 - 00:12:46] אז
[00:12:47 - 00:12:48] מה הסיבה שהדבר הזה קורה?
[00:12:49 - 00:13:03] אז אחת מהסיבות זה שבעצם אנחנו עושים אופטימיזציה ללייטליהוד והלייטליהוד הוא לאו דווקא מתיישר בצורה מושלמת עם האיכות של הדגימות.
[00:13:03 - 00:13:05] אז בואו נדבר על זה ברגע.
[00:13:07 - 00:13:12] כמובן שאם המודל שלנו הוא מושלם, זאת אומרת הגענו ל-KL Divergence 0, בדיוק
[00:13:12 - 00:13:14] הצלחנו למצוא את המודל,
[00:13:15 - 00:13:17] עשינו מקסימום לייטליהוד והגענו בדיוק למודל של הדאטה.
[00:13:18 - 00:13:25] בדיוק לדאטה שלנו מצאנו את הפרמטרים ככה שהם יוכלו ממש כמו המודל של הדאטה.
[00:13:26 - 00:13:30] אז לא תהיה בעיה, הכל יהיה אופטימלי אבל מה קורה כשאנחנו לא נמצאים בדיוק פה אופטימלי,
[00:13:31 - 00:13:32] נמצאים באיזשהו מרחק.
[00:13:33 - 00:13:35] אז מה אנחנו מפסידים במרחק הזה?
[00:13:36 - 00:13:41] אז בדרך כלל מה שקורה בלייטליהוד זה שהוא יותר מושפע מכל מיני
[00:13:42 - 00:13:51] כל מיני דברים שהם לוקאליים בתמונה ופחות מדברים שהם גלובליים.
[00:13:52 - 00:13:57] אז בעצם אחד מהדברים שהם הכי בולטים, אני אמרתי את זה כבר כמה פעמים, בתמונות אחת מהתכונות
[00:13:57 - 00:14:01] הכי בולטות, הסיבה שאני אומר הכי בולטות זה בגלל שהלייטליהוד תופס את זה הכי חזק, זה החלקות.
[00:14:02 - 00:14:05] זה שפיקסל, אם יש לי פיקסל אחד, הפיקסל שלידו רוב סיכויים שיהיו באותו צבע.
[00:14:06 - 00:14:10] המודל שתופס את ההסתברות הזאת בצורה מדויקת
[00:14:11 - 00:14:14] זה ייתן לו הרבה קרדיט מבחינת הלייטליהוד
[00:14:15 - 00:14:21] לעומת מודל שמצליח להבין את המבנה של פנים באופן כללי שיש שתי עיניים בפנית במטרה.
[00:14:22 - 00:14:26] זה קצת פחות חשוב מבחינת הביטים של הלייטליהוד.
[00:14:29 - 00:14:37] אחת מהדרכים להראות את זה אולי בצורה קצת יותר פורמלית זה אפילו עם מודל כזה פשוט. נגיד שהמודל שלי שלמדתי,
[00:14:38 - 00:14:39] הוא נראה ככה, הוא
[00:14:40 - 00:14:43] איזשהו מיקסצ'ר של שני מודלים.
[00:14:44 - 00:14:51] תזכור שאם יש לי מיקסצ'ר של שני מודלים שהמשקולות שלהם שוות אחד סך הכל אז זה גם מודל הסתברותי תקני.
[00:14:53 - 00:14:55] אז נגיד שהמודל שלי הוא לא משנה איך עמדתי אותו, הגעתי למצב כזה
[00:14:56 - 00:15:01] שהוא שילוב של שני מודלים, אחד זה המודל האמיתי של הדאטה, זה המודל המושלם,
[00:15:02 - 00:15:04] ושתיים זה המודל שהוא סתם רעש.
[00:15:06 - 00:15:09] והמשקולות הן כאלה שרק אחוז מהזמן אני בעד.
[00:15:10 - 00:15:11] פי דאטה,
[00:15:11 - 00:15:13] 99% מהזמן אני ברעש,
[00:15:15 - 00:15:16] המודל שלי הוא מודל של רעש.
[00:15:18 - 00:15:27] אז קוראים לך מודל כזה של תערובת של מודלים, אפשר לחשוב עליו בתור מודל עם latent variables שקובע אם אני הולך לפה או אם אני הולך לפה בדגימה,
[00:15:27 - 00:15:28] שאני מייצר דגימה,
[00:15:29 - 00:15:32] והמשקולות האלה אומרים ש-99% מהזמן אני מייצר דגימה כזאת,
[00:15:33 - 00:15:35] ואחוז אחד מהזמן אני מייצר דגימה כזאת.
[00:15:35 - 00:15:41] אז מה יהיה הלוג-לייקליות של המודל הזה?
[00:15:42 - 00:15:43] אז הלוג-לייקליות של המודל הזה
[00:15:45 - 00:15:47] יהיה שווה ל...
[00:15:48 - 00:15:49] אפשר לכתוב ככה לואו של הסכום הזה,
[00:15:50 - 00:15:50] ועכשיו
[00:15:54 - 00:15:58] הצפיפות של ההסתברות של מודל היא מספר שהוא תמיד חיובי, אז אם אני אוריד את כל הדבר הזה,
[00:15:59 - 00:16:01] אני בטוח שאני מוריד קצת מהאיכות של
[00:16:02 - 00:16:03] המגדל שלי.
[00:16:05 - 00:16:11] אז כן, אז הורדתי את האיכות של המודל שלי, אז ההסתברות של המודל, הלוג-הסתברות של המודל שלי היא גדולה מהדבר הזה,
[00:16:12 - 00:16:13] שרק מסתכל על האיבר הראשון,
[00:16:14 - 00:16:19] והדבר הזה, כשמתוך לוג אני יכול להפריד אותו לסכום של שני דברים, נכון, זה לוג של הדאטה,
[00:16:19 - 00:16:20] זה המודל המושלם,
[00:16:21 - 00:16:32] פחות לוג של 100. זה פחות לוג של 100. עכשיו זה מספר שהוא לא כזה גדול,
[00:16:32 - 00:16:39] במיוחד כאשר הדאטה שלי הוא מספר הפיקסלים שיש לי, למשל, בתמונה היא מאוד גדולה.
[00:16:39 - 00:16:41] אז אם יש לי נגיד מיליון פיקסלים בתמונה,
[00:16:41 - 00:16:42] כל תמונה,
[00:16:43 - 00:16:46] כל פיקסל יכול לקבל נגיד 256 ערכים,
[00:16:47 - 00:16:53] אז אם אני לא יודע שום דבר על הפיקסל, אז יש לי שם שמונה ביטים בעצם של מידע,
[00:16:53 - 00:16:56] אז הערך של הדבר הזה הוא בעצם שמונה, הכי גרוע,
[00:16:56 - 00:17:00] ואני יכול לשפר את זה, אבל הכל נסחם, אוקיי? זה שמונה ועוד שמונה ועוד שמונה מיליון פעמים,
[00:17:01 - 00:17:11] וזה נשאר פה תמיד קבוע, זה איזה משהו גלובלי כזה שנשאר פשוט של 100. האפקט של זה הוא מאוד קטן, יחסית לכל מיני אפקטים שהם לוקאליים,
[00:17:12 - 00:17:17] מדברים על ההסתברות של כל אחד מפיקסל, דברים שחוזרים על עצמם הרבה בתמונה,
[00:17:17 - 00:17:20] יהיו הרבה יותר משמעותיים ללייטיות מאשר
[00:17:20 - 00:17:21] כל מיני דברים גלובליים.
[00:17:24 - 00:17:28] הרבה פעמים הדברים הגלובליים זה מה שיותר חשוב לנו כשאנחנו אומרים שדגימה היא נראית טוב,
[00:17:29 - 00:17:33] ואנחנו צריכים לראות שיש כבר פנים, שני עיניים,
[00:17:34 - 00:17:36] יותר חשוב לנו מאשר
[00:17:37 - 00:17:39] בדיוק איך הצבע של העין נראה
[00:17:41 - 00:17:44] במעבר בין הצבעים בתוך הפנים.
[00:17:46 - 00:17:46] אוקיי,
[00:17:47 - 00:17:52] אז זה רק להמחשה שלייטיות טוב לא אומר שהדגימה תהיה טובה,
[00:17:52 - 00:17:56] גם הפוך זה נכון, אם יש לנו מודל שמייצר דגימות טובות זה לאו דווקא אומר שהלייטיות שלו תהיה טוב.
[00:17:57 - 00:17:59] הדוגמה מאוד
[00:18:01 - 00:18:04] הטריליאלית של הדבר הזה זה מודל שעושה פשוט אוברפיטינג לטריינינג.
[00:18:05 - 00:18:07] זה מודל שפשוט זוכר את כל הטריינינג כשהוא ראה,
[00:18:08 - 00:18:15] ואז בהסתברות שאחד חלקי M, שזה מספר הדגימות בטריינינג, מייצר דוגמה אחת מתוך הטריינינג סט,
[00:18:16 - 00:18:18] אז יהיה לו דגימות שנראות ממש טוב,
[00:18:19 - 00:18:20] זה יהיה דגימות ממש מהטריינינג סט,
[00:18:21 - 00:18:25] אבל הלייטיות שלהם היא מאוד גרוע, כי כשנמדוד את הלייטיות על test sets,
[00:18:27 - 00:18:29] הוא ייתן שם, ההסתברות תהיה אפס בעצם,
[00:18:29 - 00:18:36] כי המודל הזה בעצם מודל שהוא למד את דלתאות מסביב לדגימות שהוא ראה בטריינינג סט,
[00:18:36 - 00:18:37] מספיק שיש הבדל קטן,
[00:18:38 - 00:18:39] ההסתברות ישר תהיה רגע לאפס.
[00:18:43 - 00:18:43] אוקיי,
[00:18:44 - 00:18:54] אז הרעיון בעצם בגנץ זה, אוקיי, אם חשוב לנו לייצר דגימות טובות, בואו ננסה לעשות ממש אופטימיזציה לדבר הזה באופן ישיר,
[00:18:54 - 00:18:55] ולא דרך לייטיות.
[00:18:57 - 00:18:58] אוקיי, זה הרעיון.
[00:19:01 - 00:19:02] אוקיי, אז אנחנו מקבלים
[00:19:07 - 00:19:16] שתי דגימות, ואנחנו רוצים בעצם איפשהו למצוא איזשהו מדד לאיכות של הדגימות שנוכל לעשות לו אופטימיזציה.
[00:19:17 - 00:19:23] איזושהי דרך לעשות אופטימיזציה, לגרום לדגימות, לא יודע אם אתם רואים שזה דגימות טובות, זה ממש הדגימות מהדאטה אני חושב,
[00:19:23 - 00:19:27] לא יודע אם זה פה נמצא ממודל טוב או שזה הדאטה עצמו,
[00:19:28 - 00:19:32] וזה דגימות של איזשהו מודל שהוא סביר אבל לא משהו,
[00:19:33 - 00:19:37] היינו רוצים איכשהו לעשות אופטימיזציה ככה שהדגימות של זה יהיו יותר דומות לדגימות,
[00:19:39 - 00:19:42] ואני לא זאת אומרת, למשל הדגימות שקיבלנו בתריימות בסציה,
[00:19:43 - 00:19:45] בלי לעבור דרך הלייטליבט.
[00:19:48 - 00:19:52] אוקיי, אז זו בעיה שכבר היא קשורה מאוד לבעיה סטטיסטית קלאסית,
[00:19:52 - 00:19:54] שנקראת two simple tests,
[00:19:55 - 00:19:57] שבהינתן שני סטים של דגימות
[00:19:58 - 00:20:11] s1 וs2 אנחנו רוצים בעצם לדעת האם הדגימה הזאת הגיעה מהתפלגות p, הדגימה הזאת הגיעה מהתפלגות q,
[00:20:11 - 00:20:13] אנחנו רוצים לדעת האם p שווה ל-q,
[00:20:14 - 00:20:17] בעצם האם הדגימות האלה הגיעו מאותה התפלגות כמו הדגימות האלה.
[00:20:19 - 00:20:21] זה נקרא two simple tests,
[00:20:21 - 00:20:33] וזה משהו שהוא לא בדיוק מדד, הוא משהו בינארי, אם אנחנו יכולים להגיד כן או לא, האם בדרך כלל זה מפורמל בתור hypothesis testing כזה, זו שיטה בסטטיסטיקה,
[00:20:34 - 00:20:36] יש לנו ה-null hypothesis, ההיפותזת
[00:20:37 - 00:20:39] התפקודת העברית, ההיפותזת הבסטיס,
[00:20:39 - 00:20:42] שאומרת לנו ש-p שווה באמת ל-q,
[00:20:42 - 00:20:46] וההיפותזה השנייה שאומרת h1 כאן, שאומרת ש-p לא שווה ל-q,
[00:20:47 - 00:20:49] והדרך שעושים את זה, זה פשוט
[00:20:51 - 00:20:53] בונים כל מיני סטטיסטיקות,
[00:20:55 - 00:20:57] ואז משווים בין הסטטיסטיקות האלה,
[00:20:58 - 00:21:04] ואם זה עובר על איזשהו סף, אם ההשוואה נראית שהדברים דומים, אז אנחנו אומרים שזה שווה,
[00:21:05 - 00:21:06] אחרת אנחנו אומרים שזה שונה.
[00:21:07 - 00:21:17] אוקיי? אז נגיד סוג הסטטיסטיקות זה יכול להיות התוחלת של הדגימות או ה-variants או כל מיני סטטיסטיקות אחרות שאפשר לחשוב עליהן.
[00:21:22 - 00:21:32] אוקיי? וזה באמת לא עובר דרך מודל שמחשב את ה-like-יות של הדגימות שלנו תחת איזושהי התפלגות.
[00:21:32 - 00:21:38] זה משהו קצת יותר ישיר שעושה לנו השוואה בין הדגימות עצמן.
[00:21:41 - 00:21:49] אוקיי, מה הבעיה עם זה? לבנות את הסטטיסטיקות האלה זה מאוד קשה, אוקיי? אז אם אני מסתכל נגיד כאן, אז יש לנו שני גאוסיונים,
[00:21:49 - 00:22:03] אם בתוחלת ואותה שונות אבל תוחלת שונה למשל, אז אם אני אקח את התוחלת של דגימות שהגיעו מהירוק ותוחלת של דגימות שהגיעו מהאדום, אז כנראה שאני אזייש שהם הגיעו מהתפלגויות שונות
[00:22:04 - 00:22:05] ולא תהיה בעיה.
[00:22:07 - 00:22:14] כאן אולי זה יותר קשה, נכון? כי אם אני אקח את התוחלת של האדום ותוחלת של הירוק זה יהיה אותו דבר, אבל אם אני אסתכל על השונות אולי אני אבין שהאדום
[00:22:15 - 00:22:18] שונה והוא יותר מפוזר מהירוק.
[00:22:18 - 00:22:23] פה יש לנו שתי התפלגויות שהתוחלת והשנות שלהן היא אותו דבר, אבל עדיין ההתפלגויות שונות.
[00:22:24 - 00:22:29] אז הסטטיסטיקות האלה לא היו מספיקות כדי להבין שההתפלגויות האלה הן שונות,
[00:22:30 - 00:22:32] ונצטרך לחשוב על סטטיסטיקה יותר מתוחכמת.
[00:22:32 - 00:22:36] וזה רק בממד אחד שאפשר להראות שצריך סטטיסטיקות,
[00:22:37 - 00:22:42] בהרבה מצבים צריך סטטיסטיקות יותר מתוחכמות.
[00:22:42 - 00:22:44] תחשבו מה קורה בממדים גבוהים,
[00:22:45 - 00:22:46] השיטה הזאת היא באופן ישיר,
[00:22:46 - 00:22:49] היא לא נראית מבטיחה.
[00:22:53 - 00:22:59] אז הרעיון פה זה בעצם לא לחשוב מראש על סטטיסטיקות כאלה שיבדילו
[00:23:00 - 00:23:04] בין שני הסטים של הדגימות,
[00:23:04 - 00:23:07] אלא ללמוד משהו שמפריד בין שני הסטים של הדגימות.
[00:23:07 - 00:23:16] אז במקום שיהיה לנו סט של סטטיסטיקות כאלה, יהיה לנו איזשהו מודל נלמד שמנסה למצוא את ההבדלים בין
[00:23:17 - 00:23:21] הדגימות השונות, ואם הוא מצליח למצוא הבדלים אז סימן שהדגימות לא הגיעו מאותה התקבלות.
[00:23:22 - 00:23:23] אם הוא לא מצליח למצוא הבדלים
[00:23:23 - 00:23:25] אז סימן שהם כן הגיעו מאותה התקבלות.
[00:23:29 - 00:23:31] אוקיי, אז איך זה עובד?
[00:23:32 - 00:23:33] אם אנחנו לומדים,
[00:23:34 - 00:23:43] השיטה שאנחנו פשוט לומדים זה קלאסיפייר, אוקיי? בעצם אנחנו, יש לנו איקסים של הדגימות שלנו שהגיעו מאיזשהו קלאס אפס,
[00:23:44 - 00:23:46] נגיד שזה הדאטה
[00:23:47 - 00:23:53] שהמודל שלנו מייצר, ו-X שהגיע מ-Y1 שזה דאטה אמיתית
[00:23:54 - 00:23:59] תחשבו במצב הזה שיש לנו שני מודלים שמייצרים דאטה, או שני התפלגויות שמייצרו דאטה,
[00:24:00 - 00:24:02] אחת נקרא לאחץ ואחת נקרא לאחד
[00:24:02 - 00:24:06] ויש לנו את ה-X מתוך הראשון ואת ה-X מתוך השני
[00:24:07 - 00:24:09] אם אנחנו לומדים קלאסיפייר
[00:24:10 - 00:24:11] אז הקלאסיפייר בעצם,
[00:24:12 - 00:24:14] אם אנחנו עושים את ה... למשל למדלים את הקלאסיפייר עם
[00:24:16 - 00:24:18] לוגיסטיק רגשן או קוס אנטרופי,
[00:24:19 - 00:24:30] אז הוא בעצם מוצא לנו את ההסתברות ההפוכה, נכון? ש-Y שווה 1 בהינתן... מה ההסתברות שבהינתן תמונה? מה ההסתברות שה-Label שלה
[00:24:30 - 00:24:31] הוא 1,
[00:24:32 - 00:24:32] אוקיי? אז אם ה-Label
[00:24:34 - 00:24:39] זה לא כלבים או חתולים אלא זה הגיע מהדגימה הזאת, היא הגיעה מהדגימה הזאת
[00:24:41 - 00:24:44] של דרך אגב יכולת גם כלבים וחתולים, דגימה אחת של כלבים וחתולים,
[00:24:45 - 00:24:48] אז זה בעצם מה שהקלאסיפייר
[00:24:49 - 00:24:55] ממודד, נכון? קלאסיפייר אופטימלי בעצם ייתן לנו את ההסתברות הזאת.
[00:24:57 - 00:24:59] אפשר לראות לפי חוק בייס שאנחנו יכולים בעצם,
[00:24:59 - 00:25:02] מהמידע, מהארקות שהקלאסיפייר מוציא,
[00:25:03 - 00:25:09] אז בעצם אפשר להוציא את הסיגנל הזה שאומר מה היחס
[00:25:10 - 00:25:12] בין ההתפלגויות ההפוכות, אוקיי?
[00:25:13 - 00:25:18] יש לי X, אני מעביר אותו לתוך הקלאסיפייר שלי, אם הקלאסיפייר שלי הוא כבר אומן
[00:25:18 - 00:25:21] על הדאטה והגיע לאופטימום שהוא יכול היה להגיע,
[00:25:22 - 00:25:27] אז בעצם ה-output של הקלאסיפייר הזה זה בדיוק יהיה היחס,
[00:25:27 - 00:25:34] או הלוג של היחס בין ההסתברויות ההפוכות, זאת אומרת מה ההסתברות שהתמונה הזאת הגיעה מהדגימה הראשונה,
[00:25:35 - 00:25:38] לעומת ההסתברות שהתמונה הזאת הגיעה מהדגימה השנייה.
[00:25:38 - 00:25:46] אוקיי? או שהקלאס הזה מייצר, מה ההסתברות שהקלאס הזה ייצר לי את הדוגמה ההיא הזאת,
[00:25:47 - 00:25:49] לעומת מה ההסתברות שקלאס אחד ייצר לי את הדוגמה הזאת.
[00:25:50 - 00:26:00] זה החישוב, אם אתם צוחקים בוג'יסטיק רגרשן ושינלנט זה בדיוק החישוב שאנחנו מכניסים את הדבר הזה לתוך
[00:26:02 - 00:26:06] המכנה כאן, וזה בעצם, הפונקציה הזאת זה סיגמוד,
[00:26:07 - 00:26:08] אוקיי? סיגמוד.
[00:26:08 - 00:26:12] של משהו זה 1 חלקי 1 ועוד האקספוננט שלו.
[00:26:14 - 00:26:14] בסדר?
[00:26:15 - 00:26:18] אז בעצם האאוטפוט של הקלאסיפייר
[00:26:19 - 00:26:23] הוא יהיה פשוט היחס בין ההסתברות.
[00:26:25 - 00:26:27] אם המספר הזה הוא יהיה חיובי גדול, זאת אומרת שאנחנו,
[00:26:28 - 00:26:32] ההסתברות שהוא הגיעה מ-0 היא הרבה יותר גבוהה מההסתברות שהוא הגיעה מ-1,
[00:26:32 - 00:26:34] אוקיי? אם הוא יהיה חיובי קטן זה יהיה הפוך.
[00:26:34 - 00:26:37] אם הוא יהיה שלילי גדול אז זה יהיה הפוך.
[00:26:38 - 00:26:46] הדבר הזה הוא, זה לא איזה משהו חדש, כן? זה משהו שמשתמשים בו הרבה לעשות כל מיני
[00:26:47 - 00:26:51] בדיקות האם יש לנו דוגמה שהיא חריגה או כל מיני דברים כאלה, כן?
[00:26:53 - 00:26:55] העניין הזה שהקלאסיפייר תופס את היחס הזה, זה
[00:26:57 - 00:27:00] לא איזה תוצאה חדשה שהמציאו עם גן או זה משהו.
[00:27:01 - 00:27:03] אבל בעצם זה השימוש שאנחנו נעשה,
[00:27:04 - 00:27:06] אז בואו נגדיר רגע מה זה גן,
[00:27:06 - 00:27:09] אז גם זה קודם כל זה קיצור של Generative Adversarial Network.
[00:27:11 - 00:27:12] והרעיון הוא שיש לנו בעצם שני מודלים.
[00:27:13 - 00:27:15] מודל אחד שהוא מייצר את הדאטה
[00:27:16 - 00:27:21] בצורה ישירה, אוקיי? בצורה מהירה, יש לנו זה שמגיע שוב,
[00:27:21 - 00:27:24] זה רעש נגיד לגרסיאני,
[00:27:24 - 00:27:25] אוקיי?
[00:27:27 - 00:27:31] לא יודע אם זה נכון לקרוא, לפעמים קוראים לזה רעש, אבל יותר נכון לקרוא לזה ה-Latent Variable שלנו,
[00:27:32 - 00:27:35] אוקיי? זה ה-Latent Variable שאנחנו מניחים שיש עליו איזה פריור פשוט,
[00:27:36 - 00:27:39] כמו ב-VAE למשל, שיש לו גאוסיאנר איזוטרופי,
[00:27:41 - 00:27:47] וממנו אנחנו, יש לנו איזשהו מודל עם פרמטרים תטא שמייצר את X,
[00:27:49 - 00:27:49] אוקיי?
[00:27:50 - 00:27:53] אז זה ה-Generator, זה המודל שמייצר את הדאטה,
[00:27:53 - 00:27:55] ויש לנו מודל אחר שנקרא Discriminator,
[00:27:56 - 00:27:57] שהוא בהינתן X
[00:27:58 - 00:28:00] מנסה לחזות את ה-Label שלו,
[00:28:00 - 00:28:02] איזשהו Label Bינארי,
[00:28:03 - 00:28:04] Y שאומר 0-1,
[00:28:04 - 00:28:06] שאומר לנו האם הדאטה הגיעה
[00:28:07 - 00:28:10] מה-Training Set האמיתי, או האם הוא הגיע מהמודל.
[00:28:12 - 00:28:16] בסדר, אז ה-Y פה זה לא יהיה Labeling של איזשהו
[00:28:17 - 00:28:22] מה יש בתוך התמונה, אלא זה בדיוק כמו שדיברנו קודם, זה האם ב-To-Sample test,
[00:28:23 - 00:28:25] האם זה הגיע מה-Sample הראשון או מה-Sample השני,
[00:28:25 - 00:28:31] שבמקרה הזה ה-Sample הראשון יהיה ה-Training Set וה-Sample השני יהיה דגימות שהגיעו מהמודל הזה.
[00:28:35 - 00:28:41] בדרך כלל אנחנו מאמנים את הג'נרטור.
[00:28:45 - 00:28:50] זה אמרנו, זה מודל שהוא פשוט, יש לנו איזה רשת שעושה מיפוי מ-Z ל-X-T,
[00:28:51 - 00:28:54] אוקיי, המודל הזה הוא בניגוד ל-VaE,
[00:28:56 - 00:29:03] למרות שאיך שאתם אימשתם את זה, זה לא קצת מחליש את הטענה הזאת, אבל ב-VaE באופן כללי אנחנו חושבים על ה-Output של הרשת,
[00:29:03 - 00:29:05] בתור התפלגות על X-ים.
[00:29:06 - 00:29:09] הרשת מוציאה לנו את ההתפלגות על ה-X-ים,
[00:29:09 - 00:29:10] וכך אנחנו גם מאמנים את זה.
[00:29:11 - 00:29:14] פה אנחנו חושבים על ה-Output של הרשת בתור ה-X-ים עצמם.
[00:29:16 - 00:29:21] כשאתם מימשתם את ה-VaE, אז זה היה קצת דומה לזה, כי ה-Output שלכם היה התוחלת של ההתפלגות,
[00:29:22 - 00:29:27] והתייחסתם לתוחלת הזאת בתור ה-Data עצמו, בסופו של דבר כשהצעתם דגימות,
[00:29:28 - 00:29:31] אבל לפחות איך שאימנתם את זה, אימנתם את זה כאילו זה מייצר התפלגות
[00:29:32 - 00:29:33] גאוסיאנית על ה-Data,
[00:29:34 - 00:29:37] וזה מה שניסיתם למקסם.
[00:29:37 - 00:29:40] פה אנחנו פשוט לא מתייחסים לזה בתור איזה משהו הסתברותי,
[00:29:41 - 00:29:50] אלא פשוט זה מיפוי שבנתנזל מייצר לי דגימה אחת, X. וקוראים לדבר הזה לפעמים, לגנץ קוראים לזה לפעמים implicit
[00:29:54 - 00:29:54] probability.
[00:29:55 - 00:29:58] זה כאילו לא משווה איזושהי התפלגות בצורה ישירה,
[00:29:59 - 00:30:01] אבל צריך לתפוס איכשהו את ההתפלגות של ה-Data,
[00:30:02 - 00:30:03] כדי לייצר דגימות טובות,
[00:30:04 - 00:30:08] אבל הוא לא עושה את זה בצורה מפורשת ישירה כמו מודלים אחרים.
[00:30:12 - 00:30:17] ואנחנו תכף נראה איך הוא עושה את זה, אבל הוא בעצם ימזער איזשהו 2-sample-test כזה,
[00:30:18 - 00:30:25] זאת אומרת הוא ינסה לקרב את התשובה של ה-2-sample-test לתשובה הזאתי, של פידת ה-CN שלה, ל-PT.
[00:30:26 - 00:30:29] הוא ינסה לגרום לתוצאה של ה-objective הזה להיות
[00:30:30 - 00:30:32] בכיוון שאומר ש-PT שווה לפי דף.
[00:30:34 - 00:30:36] למרות זאת ה-discriminator אז הוא
[00:30:38 - 00:30:46] classifier שהוא מאומן ככה שהוא מבדיל בין דגימות שהן אמיתיות, שהן דגימות שהגיעו מה-training set
[00:30:47 - 00:30:49] זה יהיה אחד מה-label שלו,
[00:30:49 - 00:30:52] ו-label אחר זה יהיה דגימות שהגיעו מהמודל,
[00:30:52 - 00:30:53] מה-generator,
[00:30:53 - 00:30:55] לפעמים קוראים לזה real ופייק.
[00:30:59 - 00:31:03] והוא בעצם עובד על אותו objective,
[00:31:04 - 00:31:08] רק שהוא הולך לכיוון של לגרום להבדיל בין
[00:31:09 - 00:31:11] הדגימות האלה,
[00:31:11 - 00:31:13] שפי תטא לא שווה לפי דאטה.
[00:31:15 - 00:31:24] המטרה שלו זה לדעת לחזות את ה-why האמיתי, לדעת לחזות אם הדגימה הזאת הגיעה מה-training data שלנו או שהגיעה מהמודל עצמו.
[00:31:25 - 00:31:36] ולפעמים ממש מפרמלים את זה בתור משחק הזה של מינימאקס, שיש פה בעצם תחרות בין הדיסקרימינטור לג'נרטור.
[00:31:37 - 00:31:40] יש גם הרבה מחקר על הכיוון הזה של הדבר הזה.
[00:31:42 - 00:31:47] בואו נסתכל רגע יותר בפירוט על האובג'קטיב, אז הדיסקרימינטור
[00:31:48 - 00:31:51] זה האובג'קטיב שלו שהוא מנסה למקסם אותו.
[00:31:51 - 00:32:00] אוקיי, אז קודם כל יש לנו כאן שני סאצ'ים של פרמטרים, יש לנו את הפרמטרים של G והפרמטרים של D. G זה הג'נרטור, D זה הדיסקרימינטור.
[00:32:01 - 00:32:05] וזה האובג'קטיב פונקשן של הדיסקרימינטור,
[00:32:05 - 00:32:09] שזה פשוט כמו קורס אנטרופי או לוג'יסטיק רגשן.
[00:32:10 - 00:32:13] הוא רוצה למקסם את ההסתברות
[00:32:14 - 00:32:17] של איקסים שהגיעו מפי דאטה,
[00:32:17 - 00:32:20] זאת אומרת הוא רוצה לתת להם ערך גבוה.
[00:32:21 - 00:32:27] זה יהיה הלוג הסתברות שהוא חושב ש-Y שווה דאטה ויינתן X,
[00:32:28 - 00:32:30] והוא רוצה למקסם את האחד פחות הדבר הזה,
[00:32:31 - 00:32:34] הוא רוצה שהמספר הזה יהיה קטן בעצם.
[00:32:35 - 00:32:39] הוא רוצה לתת ציון נמוך ל-X'ים שהגיעו מהג'נרטור.
[00:32:40 - 00:32:42] זה לא, יש פה איזה תוכנות, אחת זה
[00:32:43 - 00:32:49] כל הנקודות שמגיעות מפי דאטה בעצם נכנסות לדבר הזה, וכל הנקודות שהגיעו מהג'נרטור נכנסות לדבר הזה.
[00:32:51 - 00:33:03] אוקיי, אז זה בעצם קוסט סנטרופי של המקרה של שניים, זה בדיוק כתוב אבג'קטיב פאנקציין של לוג'יסטיק רגרשן.
[00:33:11 - 00:33:16] וזה בעצם מה שאמרנו קודם, שבעצם אם הוא עושה את העבודה שלו בצורה מושלמת,
[00:33:18 - 00:33:18] אז הארטפוד שלו,
[00:33:19 - 00:33:21] לפעמים הארטפוד שלו זה אימלוג,
[00:33:22 - 00:33:23] אבל הארטפוד שלו,
[00:33:24 - 00:33:25] כשחושבים עליו בתור הסתברות,
[00:33:26 - 00:33:34] זה יהיה פשוט הדבר הזה יפה, זה ההסתברות של הדאטה, חלקי ההסתברות של הדאטה ועוד
[00:33:36 - 00:33:39] ההסתברות שהדוגמה הזאת הגיעה מהג'נרטור.
[00:33:43 - 00:33:45] מה שכתוב כאן זה בהינתן תמונת הנקודה X,
[00:33:46 - 00:33:48] מה ההסתברות שהגיעה לפי של דאטה?
[00:33:48 - 00:33:52] זה כתוב ככה זה מרצה כמו
[00:34:10 - 00:34:12] אוקיי, אז אנחנו צריכים להשתמש בדבר הזה
[00:34:13 - 00:34:18] כדי להראות מה הג'נרטור לומד,
[00:34:18 - 00:34:23] אז האובג'קטי של הג'נרטור הוא בדיוק אותו אובג'קטיב, רק שפה כתוב עכשיו מינימום במקסימום.
[00:34:26 - 00:34:35] אוקיי, אבל שימו לב שהג'נרטור לא משתתף באיבר הזה, כן? זה משתתף רק פה, כי הוא מייצר את הדגימות שנכנסות לפה.
[00:34:36 - 00:34:38] אפשר היה לחשוב שהג'נרטור האובג'קטיב היום שלו הוא רק זה,
[00:34:38 - 00:34:40] כאילו הוא נגזרת
[00:34:40 - 00:34:41] של האיבר הזה תהיה אפס.
[00:34:45 - 00:34:46] שאלות על זה?
[00:34:49 - 00:34:51] אפס זריר באחד זאת, אוקיי?
[00:34:52 - 00:34:53] נראה לי שהפוך פה.
[00:34:56 - 00:34:59] אז תמיד הוא מבלבל בלוג'יסטי קרגשן,
[00:34:59 - 00:35:02] כי השאלה אם מגדירים את זה היא מינוס או לא.
[00:35:10 - 00:35:11] אני חושב שזה כתוב כאן, אחד זה,
[00:35:12 - 00:35:14] זאת אומרת, הדבר הזה צריך להיות גבוה.
[00:35:18 - 00:35:22] כשהאיקס מגיע מהדאטה.
[00:35:24 - 00:35:27] בסופו של דאטה זה אחד, אחד זה דאטה.
[00:35:34 - 00:35:35] אוקיי, אז זה הג'נרטור.
[00:35:37 - 00:35:45] עכשיו, מה שאפשר להראות על הג'נרטור זה שהוא במקרה שהדיסקרימינטור הוא אומן באמת והוא אופטימלי,
[00:35:45 - 00:35:47] הוא יודע להבדיל בצורה אופטימלית בין דאטה שהגיעה
[00:35:49 - 00:35:52] מהטרנינג סט לעומת דאטה שהגיעה מהג'נרטור.
[00:35:53 - 00:35:54] אפשר לעשות את הפיתוח הזה.
[00:35:55 - 00:35:58] זה ה-objective function של הג'נרטור.
[00:35:58 - 00:36:03] כשהצרתי פה את מה שהדיסקרימינטור לומד בהנחה שהוא אופטימלי.
[00:36:04 - 00:36:05] עכשיו,
[00:36:05 - 00:36:09] סתם איזה טריק כאן שעושים, מחלקים בשתיים את המכנה,
[00:36:10 - 00:36:13] זה מוציא לנו, יש פה פעמיים, זה מוציא לנו לוג ארבע החוצה.
[00:36:14 - 00:36:15] למה מחלקים פה בשתיים?
[00:36:23 - 00:36:26] אחר כך זה הופך להיות פעיל דייגוירג'נס בין שתי התפלגויות.
[00:36:28 - 00:36:30] למה צריך לחלק כמו שתיים?
[00:36:32 - 00:36:35] כי סתם סכום של שתי התפלגויות זה לא התפלגות, אבל אם אני מחלק בשתיים,
[00:36:36 - 00:36:38] זה כמו מיקשר של שתי התפלגויות.
[00:36:38 - 00:36:42] אז עכשיו בעצם כשחילקתי את זה בשתיים אני יכול להסתכל על הדבר הזה,
[00:36:42 - 00:36:47] זה כמובן לא משפיע באמת על האופטימיזציה של Objective Function, אז כל הדבר הזה לא משנה.
[00:36:48 - 00:36:52] אז אפשר לחשוב שאני עושה אופטימיזציה לדבר הזה, זה כאילו שאני עושה אופטימיזציה לדבר הזה,
[00:36:53 - 00:37:04] שזה סכום של שני KL divergenzes בין ה-P data להתפלגות המיקסצ'ר הזה של שניהם, של ה-P data וה-P generator,
[00:37:04 - 00:37:08] וה-KL ההפוך בין ה-P generator לאותו המיקסצ'ר.
[00:37:10 - 00:37:11] הדרך הזאת לעשות את ה-KL בין
[00:37:12 - 00:37:23] בין שתי התפלגויות, כל פעם בין אחת מההתפלגויות למיקסר של שני ההתפלגויות, זה נקרא Yense and Shannon Divergence,
[00:37:24 - 00:37:28] זה גם סוג של Divergence
[00:37:28 - 00:37:29] בין שתי התפלגויות.
[00:37:32 - 00:37:35] אז אפשר לחשוב על ה-Generator גם בתור סוג של
[00:37:39 - 00:37:42] ממזער בכל זאת איזשהו
[00:37:43 - 00:37:48] Divergence בין ההתפלגויות, מה שתמיד אמרנו שאנחנו רוצים שהמודל הגנרטיבי שלנו יעשה, נכון?
[00:37:49 - 00:37:53] אז גם פה בהנחה שה-Discriminator הוא אופטימלי, או שכל תהליך האופטימיזציה הזה עובד,
[00:37:54 - 00:38:00] אז כשהכל מתכנס אפשר להגיד שה-Generator שלנו גם עשה איזושהי אופטימיזציה
[00:38:00 - 00:38:01] והתקרב
[00:38:02 - 00:38:03] ל-P של הData,
[00:38:03 - 00:38:04] להתפלגות האמיתית של הData.
[00:38:05 - 00:38:11] למרות שבשום שלב אנחנו לא מתעסקים ב-likelihood של המודל הזה ואפילו לא מחשבים אותו,
[00:38:11 - 00:38:13] ואפילו המודל שלנו,
[00:38:13 - 00:38:14] ה-Output שלו הוא לא התפלגות,
[00:38:14 - 00:38:16] אין שם שום דבר שהוא קשור להתפלגות
[00:38:18 - 00:38:20] במודל עצמו, הוא פשוט מייצר דגימות.
[00:38:24 - 00:38:27] כמה תכונות של ה-Divergence הזה,
[00:38:28 - 00:38:29] אז קוראים לזה לפעמים גם סימטרי כאל Divergence,
[00:38:30 - 00:38:31] כי זה הופך את זה לסימטרי.
[00:38:36 - 00:38:38] ותכונה שהיא משמעותית,
[00:38:39 - 00:38:41] זה שה-Divergence הזה הוא בניגוד לכאל Divergence
[00:38:41 - 00:38:45] הוא mode-seeking. זאת אומרת, כשדיברנו קצת על הכיוונים של ה-KL Divergence,
[00:38:46 - 00:38:54] אז פה גם יש את התופעה הזאת שזה mode-seeking בניגוד ל-KL Divergence רגיל, אז מה זה אומר?
[00:38:54 - 00:38:58] אם הדאטה שלנו נראה ככה, למשל שהוא דאט אדום לימדי,
[00:38:58 - 00:39:03] יש לו שני אזורים עם הסתברות גבוהה, יש לו שני modes,
[00:39:05 - 00:39:09] אבל נגיד שהמודל שלנו הוא יחסית מוגבל, הוא יכול רק להיות מודל עם mode אחד,
[00:39:09 - 00:39:10] עם גרסיאן או משהו כזה.
[00:39:11 - 00:39:14] אז אם אנחנו פשוט ממזעירים KL Divergence, אז זה לא מה שהראינו,
[00:39:15 - 00:39:19] ה-KL Divergence יהיה, קוראים לזה לפעמים mode covering.
[00:39:19 - 00:39:22] הוא מנסה לכסות את כל ה-modes כמה שאפשר,
[00:39:22 - 00:39:30] כי הוא משלם מחיר מאוד גבוה אם יש בדאטה אזור עם התפלגות גבוהה,
[00:39:32 - 00:39:36] שב-KL, שבהתפלגות שמצאנו יש התפלגות 0,
[00:39:37 - 00:39:38] כי ה-0 שם מופיע במכנה.
[00:39:39 - 00:39:45] ואז זה מחיר מאוד גבוה ל-KL Divergence, אז עודיף לו לכסות כמה שיותר את הכול,
[00:39:46 - 00:39:47] ואז אנחנו מקבלים,
[00:39:48 - 00:39:51] העונש של הדבר הזה זה שאנחנו מקבלים הסתברות יחסית גבוהה
[00:39:51 - 00:39:58] באזורים שהם הדאטה, בעצם אין שם דאטה, ההסתברות של הדאטה היא מאוד נמוכה.
[00:40:01 - 00:40:05] דרך אגב, זה מסביר למה ב-VaE, למשל,
[00:40:05 - 00:40:08] הרבה פעמים אנחנו מקבלים דאטה שהוא יחסית מטושטש.
[00:40:09 - 00:40:11] כי אם אין לנו יכולת,
[00:40:12 - 00:40:19] לדאטה אמיתי יש לנו הרבה יותר משני מודלס, תחשבו על כל פיקסל יש כמה אפשרויות שיכולות להיות שם, כל אפשרות זה מוד.
[00:40:20 - 00:40:24] אני חושב שנתנו את הדוגמה הזאת עם ה-7, נכון? עם ה-7 שיכול להיות שיש שם קו, יכול להיות שאין שם קו.
[00:40:25 - 00:40:29] המודל איכשהו צריך לתפוס את כל הדברים האלה, הוא צריך לעשות, אם המודל הוא לא מספיק חזק,
[00:40:29 - 00:40:33] אז הוא צריך לעשות איזשהו covering כזה, לכסות את כל האפשרויות,
[00:40:34 - 00:40:36] ואז רוב הסיכוי שהוא ימצא איזה משהו באמצע.
[00:40:36 - 00:40:41] אז נגיד האמצע בין פיקסל שהוא ממש שחור לפיקסל שהוא ממש לבן,
[00:40:42 - 00:40:44] זה משהו באמצע, ואז הכל יראה יחסית מטושטש.
[00:40:45 - 00:40:51] זו אחת מהסיבות שהרבה פעמים מודלים כשהם עושים מעצמנו ל-rightlihood הם יוצאים יותר מטושטשים.
[00:40:53 - 00:40:58] אז זה קהל רגיל, אוקיי? אבל הקהל הזה קוראים לו mode-seeking. למה קוראים לו mode-seeking?
[00:40:59 - 00:41:03] כי בעצם אין לו את הדבר הזה, כי במכנה אף פעם לא מופיע לו אחד מההסתברויות,
[00:41:03 - 00:41:04] זה תמיד הסכום שלהם.
[00:41:05 - 00:41:11] אז אף פעם לא קורה שכשאחד מההסתברויות האלה אפס אתה תשלם מחיר, כי זה אומר שבמכנה לא יהיה לך אפס.
[00:41:13 - 00:41:15] אז בעצם מה שקורה זה שעדיף
[00:41:16 - 00:41:19] עדיף ללכת, עדיף לתפוס אחד מהמודים יותר טוב.
[00:41:19 - 00:41:24] במקום לעשות ממוצע כזה שעובר על הרבה אזורים עם הסתברות אפס,
[00:41:26 - 00:41:33] הדברג'נס הזה מעודד את המודל לקחת את המוד הגדול מביניהם ולהתמקד בו.
[00:41:34 - 00:41:44] אז בעצם מה שאנחנו נקבל אם אנחנו נדגום דברים כאלה, הדברים יהיו חדים אבל חלק מהאפשרויות לא יהיו לנו בתוך המודל.
[00:41:45 - 00:42:01] עכשיו אפשר לדמיין בדוגמה הפשוטה הזאת של השבע שהמודל הזה יחליט בגלל שרוב השבעים שהוא ראה היו בלי הקו אז הוא יחליט אף פעם לא לעשות קו בשבע ופשוט להתמקד רק בשבעים בלי קו ולעשות אותם בצורה טובה בצורה חדה.
[00:42:01 - 00:42:07] אוקיי, אז זה לגבי הדגימות, אז איך זה נראה בסופו של דבר האלגוריתם?
[00:42:09 - 00:42:12] אז זה נראה ככה, אנחנו דוגמים דאטה מתוך
[00:42:13 - 00:42:14] ה-Rainיצייט שלנו, זה רגיל,
[00:42:15 - 00:42:15] זה מיני-Batch,
[00:42:16 - 00:42:22] אנחנו דוגמים רעש מתוך הפריור בעצם של ה-Z שלנו,
[00:42:23 - 00:42:25] זה שוב זה ה-Late-Temperial הזה,
[00:42:26 - 00:42:29] ואז אנחנו מחשבים את ה-objective function
[00:42:31 - 00:42:33] בשביל הג'נרטור
[00:42:35 - 00:42:37] שהוא נראה ככה, אז בעצם אנחנו צריכים
[00:42:38 - 00:42:40] פה אנחנו לא מסתכלים בכלל על
[00:42:41 - 00:42:45] הדגימות, השלב הזה הוא לא רלוונטי לדבר הזה, נכון? כי אנחנו רק,
[00:42:45 - 00:42:57] יש לנו את M-Z'ים שייצרנו מתוכם אנחנו מעבירים אותו תחת המודל מייצרים M-X'ים שהם פייק בעצם ונותנים אותו לדיסקרימינטור
[00:42:57 - 00:43:02] ובעצם אנחנו עכשיו עושים gradient descent על תטא
[00:43:04 - 00:43:13] שזה הפרמטרים של הג'נרטור שלנו, אנחנו נשנה קצת את הפרמטרים של הג'נרטור ככה שהוא יגרום ל-D לתת ציון יותר,
[00:43:13 - 00:43:18] לא כתוב כאן באיזה כיוון עושים את זה, אבל הציון צריך להיות יותר לכיוון של ריב,
[00:43:19 - 00:43:23] gradient descent, אז אנחנו מורידים את הדבר הזה,
[00:43:24 - 00:43:27] אנחנו רוצים שהדבר הזה ייראה פחות,
[00:43:27 - 00:43:29] סך הכל ייראה פחות חיובי,
[00:43:30 - 00:43:31] שהוא יחשוב
[00:43:33 - 00:43:38] שהאיבר הזה יהיה פחות שלילי, זאת אומרת שהוא ייתן לזה ציון יותר גבוה,
[00:43:38 - 00:43:41] שהוא חושב בעצם שהדגימות האלה הן מתאמת,
[00:43:43 - 00:43:46] אז זה הכיוון של שלב אחד,
[00:43:46 - 00:43:49] והשלב השני זה לעשות gradient descent
[00:43:50 - 00:43:53] על אותו objective, אבל עכשיו יש פה את שני ה-terms,
[00:43:54 - 00:43:57] את שני האיברים האלה, אז האיבר הזה זה בדיוק אותו איבר כמו פה,
[00:43:58 - 00:43:59] שמקבל בדיוק את אותן דגימות,
[00:44:00 - 00:44:02] ורק עכשיו הוא עושה את האופטימיזציה ההפוכה לדבר הזה,
[00:44:04 - 00:44:08] ובנוסף הוא גם מקבל את הדאטה האמיתי תוך המיני-batch,
[00:44:08 - 00:44:12] ואותם הוא מנסה למקסם,
[00:44:13 - 00:44:16] אז פה אנחנו יודעים שזה דוגמאות אמיתיות ושליליות,
[00:44:16 - 00:44:19] וכמו בסופרווייז לרנינג רגיל אנחנו מאמנים את הקלאסיפה.
[00:44:21 - 00:44:23] זה מין משחק כזה בין שני ה...
[00:44:23 - 00:44:24] שני המודלים האלה.
[00:44:29 - 00:44:31] כן, בסך הכל אפשר לכתוב את זה ככה,
[00:44:31 - 00:44:33] ומינימום אינטטה ומקסימום אינטפי,
[00:44:34 - 00:44:38] ויש הרבה אומנות איך לעשות את הדבר הזה בצורה יעילה,
[00:44:38 - 00:44:47] בצורה שעובדת, זה אני חושב תרשים מהמאמר הראשון המקורי של איזה המחשה באחד מה... מה קורה,
[00:44:48 - 00:44:50] לא בטוח שהמחשה הזאת כזאת תופסת את מה שבאמת קורה,
[00:44:50 - 00:44:55] אבל סתם כשתבינו את התרשים אז אני אגיד האדום זה הדאטה האמיתי למשל,
[00:44:56 - 00:45:00] הירוק זה באיזשהו שלב באימון מה שהג'נרטור מייצר,
[00:45:01 - 00:45:08] וזה גם דוגמה לאיך הוא מייצר את זה, הוא בעצם ממפה את כל מדחר ה-Z'ים בעיקר לאיזשהו אזור מסוים, ב-X'ים,
[00:45:09 - 00:45:10] זה האזור הירוק,
[00:45:10 - 00:45:14] והקלאסיפה הוא סתם בהתחלה לא כזה טוב, וזה נותן לציונים כאלה.
[00:45:16 - 00:45:18] אנחנו מאמנים את הקלאסיפה הירו, זה נהיה קצת יותר טוב,
[00:45:18 - 00:45:22] הוא יודע שהאזור הזה נוטה להיות אמיתי והאזור הזה נוטה להיות
[00:45:23 - 00:45:24] פייק,
[00:45:25 - 00:45:28] ואז כשאנחנו מאמנים את הג'נרטור אז הג'נרטור טיפה מתקרב
[00:45:29 - 00:45:45] לאזור האמיתי כי הוא רוצה למקסם את ה... הוא רוצה שהדיסטרימינטור על הדגימות הירוקות יהיו יותר גבוהות, בעצם ייתנו ציון יותר גבוה, אז הוא מזיז את הדגימות שלו קצת למקום שבו הדיסטרימינטור יחשוב שהוא מקבל ציון גבוה,
[00:45:46 - 00:45:51] ובהתכנסות כשהכול עובד טוב אז שתי ההתפלגויות היו שוות, ואז עשרים מיני טוב אין המושג
[00:45:53 - 00:45:54] מה זה נכון ומה לא נכון.
[00:45:55 - 00:45:57] כן, בדרך כלל לא מגיעים באמת לשלב כזה,
[00:45:59 - 00:46:01] אבל זה הרעיון באופן עקרוני.
[00:46:04 - 00:46:06] וזה באמת הצליח מאוד,
[00:46:06 - 00:46:12] המאמר ביקוריום ב-2014, אני לא יודע אם שמתם לב, אבל כמעט כל המודלים שדיברנו עליהם התחילו ב-2014,
[00:46:12 - 00:46:23] זה היה שנה שבארץ שנתיים אחרי הפריצה הגדולה של Deep Learning באופן כללי, אז הם מימשו הרבה רעיונות
[00:46:24 - 00:46:27] של מודלים גנרטיביים והסתברותיים.
[00:46:27 - 00:46:34] ב-2014, אני חושב שכל המודלים שדיברנו עליהם, גם PixelCNN, גם VAE,
[00:46:35 - 00:46:39] וגם Flows, הכל התחיל שם, וגם Gantz.
[00:46:39 - 00:46:49] די מהר, אבל בניגוד למודלים האחרים, כבר ב-2015 זה היה ברור שהדגימות שהם מייצרים הם הרבה יותר טובות
[00:46:50 - 00:46:53] מהדגימות האחרות, וכל שנה זה הלך להשתפר.
[00:46:56 - 00:46:57] היה באיזשהו שלב אתר כזה,
[00:46:59 - 00:47:03] פה שממש מייצר מלא אנשים.
[00:47:04 - 00:47:11] זה נקרא This is not a person, or This person does not exist.
[00:47:12 - 00:47:14] פשוט מייצר מלא תמונות של אנשים.
[00:47:16 - 00:47:19] במקור זה העמדה, אז אולי עכשיו כבר שינו דברים.
[00:47:20 - 00:47:22] אבל זה באמת היה המודל הראשון
[00:47:24 - 00:47:26] שייצר
[00:47:28 - 00:47:30] דגימות שממש נראות טוב.
[00:47:31 - 00:47:35] אני לא יודע אם אתם זוכרים, בשנים האלה התחילו להתפרסם, ובאמת כל
[00:47:40 - 00:47:43] התחום הזה תפס כאילו הרבה כותרות, בזה שבעצם אפשר כבר לייצר תמונות
[00:47:44 - 00:47:45] שנראות
[00:47:46 - 00:47:48] במעיקר של פנים, זה נדחים ופנים,
[00:47:49 - 00:47:50] שנראות כבר ממש אמיתיות,
[00:47:51 - 00:47:52] ושאי אפשר להבדיל ביניהן
[00:47:53 - 00:47:53] לתמונות אמיתיות.
[00:47:55 - 00:47:59] עדיין, אבל זה מודל שהוא מאוד בעייתי, והוא נחשב
[00:48:01 - 00:48:05] מאוד קשה לאימון, בגלל העניין הזה שיש לנו שני
[00:48:09 - 00:48:12] קומפוננציות שבעצם גובות אחת עם השנייה.
[00:48:14 - 00:48:19] יש פה הרבה דברים שהם לא יציבים, נדבר רגע על שני עניינים שם.
[00:48:20 - 00:48:23] עדיין אפשר להוריד קוד שעובד,
[00:48:23 - 00:48:26] וכנראה תצליחו,
[00:48:27 - 00:48:34] אבל אם יש לכם איזה דאטה חדש, או משהו שהוא נראה קצת אחר, יכול להיות שזה דורש יותר עבודה ממודלים אחרים.
[00:48:36 - 00:48:38] אז בואו נדבר על שתי בעיות
[00:48:41 - 00:48:42] שנחשבות,
[00:48:42 - 00:48:48] בעצם נדבר על הבעיות האלה של היציבות של האופטימיזציה, ומה שנקרא mode collapse,
[00:48:49 - 00:48:53] אחר כך נדבר על איך אנחנו עושים בכלל אבלואציה להדגימות שיוצאות לנו בסופו של דבר.
[00:48:54 - 00:49:01] אז הרבה פעמים מה שרואים באופטימיזציה, שיש איזשהו סוג של אוסילציה כזאת, הכל
[00:49:02 - 00:49:05] ה-discriminator נהיה טוב, ואז זה חוזר להיות גרוע, נהיה טוב, זה חוזר להיות גרוע, ואותו דבר הג'נרטור.
[00:49:07 - 00:49:10] ואין לנו איזה משהו שהולך ומשתפר כמו לייקליות, שאנחנו יכולים לדעת
[00:49:11 - 00:49:13] מה מודל משתפר ומשתפר, ולתת איזשהו סף אולי
[00:49:15 - 00:49:16] ששם אנחנו רוצים לעצור.
[00:49:19 - 00:49:21] וזה משהו שקשור, מה שנקרא mode colab,
[00:49:21 - 00:49:30] זה קשור לעניין הזה גם של ה-Mode Seeking שאמרנו, אז באמת רואים את הדבר הזה, רואים שיש הרבה דברים שחוזרים על עצמם הרבה פעמים ב...בדגימות.
[00:49:31 - 00:49:33] אז פה, שוב, אני לא יודע אם אתם רואים טוב,
[00:49:33 - 00:49:34] זה דאטה שאומן על ה...
[00:49:35 - 00:49:38] הדאטה ספט כזה של חדרי מלון, אני חושב,
[00:49:40 - 00:49:43] ויש איזה סוג של מיטה כזאת, נורא, של מיטה,
[00:49:43 - 00:49:45] שפשוט חוזרת על עצמה במלא דגימות.
[00:49:45 - 00:49:48] ויש איזשהו mode שממש המודל הזה אוהב,
[00:49:48 - 00:49:49] אני אעצר אותו הרבה.
[00:49:52 - 00:49:58] אז ממש עשו הרבה מחקר על העניין הזה, אז זה אחת מהדרכים להראות את זה,
[00:49:58 - 00:50:01] שראו את זה בדאטה מאוד פשוט, שוב, זה דאטה שהוא דו-ממדי,
[00:50:02 - 00:50:02] שוב,
[00:50:03 - 00:50:06] גרשן מיקסשר כזה, שיש בו איזה עשר קומפוננטות,
[00:50:06 - 00:50:08] כל אחת במקום אחר,
[00:50:09 - 00:50:15] ואז ממש אפשר לראות שבזמן האימון יש מין מרדף כזה בין ה-discriminator לג'נרטור.
[00:50:15 - 00:50:20] אז בהתחלה ג'נרטור מייצר רק דגימה אחת באזור מסוים,
[00:50:20 - 00:50:24] עד שה-discriminator יודע שכל מה שמייצר משם הוא פייק.
[00:50:25 - 00:50:28] אז הג'נרטור פשוט שוכח מהאזור הזה ופשוט עובר לאזור אחר.
[00:50:29 - 00:50:44] ככה הוא ממש עושה חיפוש כזה במלוכה והם רודפים אחד אחרי השני והם לא מגיעים למצב שזה מתייצבים. זאת אומרת, אם זה מה שקורה זה קשור שני דברים, גם לעניין הזה של ה-mode-seeking של ה-objective function,
[00:50:45 - 00:50:49] וגם לתהליך האופטימיזציה הזה שהוא מורכב יותר,
[00:50:49 - 00:50:54] כי הוא מתחיל שני כוחות שהם מתחרים אחד בשני.
[00:50:58 - 00:51:01] אוקיי, אז לסיכום של גאנס,
[00:51:02 - 00:51:04] בעצם עברנו על זה קצת מהר על גאנס,
[00:51:06 - 00:51:08] לפני כמה שנים זה באמת היה המודל הכי,
[00:51:09 - 00:51:13] הפך להיות מודל סינונימי לג'נרטיב מודלס, וזה נהיה,
[00:51:13 - 00:51:15] הרבה אנשים במקום להגיד ג'נרטיב מודלס היו אומרים פשוט גאנס,
[00:51:16 - 00:51:18] זה היה מודל שהיה מייצר הרבה דגימות,
[00:51:19 - 00:51:20] הוא היה מאוד מוצלח,
[00:51:21 - 00:51:22] ובאמת הוא
[00:51:26 - 00:51:28] שינה את כל התפיסה של מה זה ג'נרטיב מודל,
[00:51:28 - 00:51:33] אנשים עכשיו בדרך כלל חושבים על מודלים שמייצרים דגימות, ולא מודלים שמאפשרים לעשות
[00:51:34 - 00:51:34] כל מיני
[00:51:38 - 00:51:40] פעולות, משימות שונות שקשורות להסתברות,
[00:51:42 - 00:51:45] אבל זה היתרונות שלו באמת, הוא מייצר פשוט דגימות ממש טובות,
[00:51:46 - 00:51:51] הוא פשוט במובן הזה שאנחנו לא צריכים באמת לבנות הסתברות, אנחנו רק צריכים לבנות איזשהו ג'נרטור,
[00:51:52 - 00:51:53] משהו שמייצר דגימות
[00:51:53 - 00:51:54] ושהוא גזיר,
[00:51:55 - 00:51:58] וזה מאפשר לנו להפעיל את השיטה הזאת עליו,
[00:52:00 - 00:52:03] אבל הוא בסופו של דבר קשה לאימון,
[00:52:03 - 00:52:05] יש לו את העניין הזה של ה-mode collapse,
[00:52:07 - 00:52:10] ויש פה, תדחו אם תסתכל אם מישהו כן רוצה,
[00:52:11 - 00:52:11] יש פה
[00:52:12 - 00:52:12] דבר
[00:52:13 - 00:52:16] עם הרבה טריקים,
[00:52:17 - 00:52:18] לך לאמן את זה,
[00:52:18 - 00:52:20] אנשים עדיין משתמשים בזה,
[00:52:21 - 00:52:28] אבל גם עכשיו רוב הדגימות שאתם רואים, רוב הסאמפלים שאתם רואים הם יותר כבר מ-difusion models שזה יהיה נושא
[00:52:29 - 00:52:29] הבא שלהם.
[00:52:31 - 00:52:32] גם בניגוד לנורמליות,
[00:52:33 - 00:52:36] אז יש לו ייצור קטן של הדאטה,
[00:52:37 - 00:52:39] יכול לעשות את זה על הייתו, נכון, נכון,
[00:52:39 - 00:52:40] זה גם יתרון.
[00:52:40 - 00:52:45] אבל מצד שני אין לך באופן ישיר דרך לחזור מהתמונה למיפוי.
[00:52:46 - 00:52:48] נגיד דברים כמו representation learning,
[00:52:48 - 00:52:50] אין לך דרך ישירה לעשות את זה.
[00:52:52 - 00:53:01] שוב, בגלל שהמודל הזה הוא כזה טוב, אז פיתחו מעל זה עוד כל מיני דברים, למשל, קודם אתה מאמן את זה, ואחר כך אתה מאמן איזה מודל שמנסה להפוך למשל את הגן.
[00:53:03 - 00:53:04] היה,
[00:53:04 - 00:53:07] אני זוכר כמה שנים שכל המאמרים וכל הכנסים היו רק על גן.
[00:53:08 - 00:53:09] זה ממש היה התפוצצות.
[00:53:11 - 00:53:17] אבל זה קצת דרך שעכשיו כאילו הדבר החם החדש זה diffusion models. כנראה שעוד כמה שנים יהיה משהו חדש.
[00:53:18 - 00:53:19] אבל
[00:53:22 - 00:53:26] באופן ישיר המודל הזה הוא בעצם לא מודל הסתברותי, אז יש בו הרבה דברים שלא,
[00:53:29 - 00:53:31] פשוט לא קיימים בו בצורה ישירה,
[00:53:32 - 00:53:35] וצריך בעצם לפתח על גבי הדבר הזה.
[00:53:35 - 00:53:38] חשבו על זה סוג של מכונה שמייצרת דגימות,
[00:53:38 - 00:53:45] ואחר כך צריך לעשות מעל זה כל מיני דברים כדי לחלץ מזה את המידע שאתה צריך בשביל לפתור כל מיני בעיות.
[00:53:50 - 00:53:51] אוקיי.
[00:53:54 - 00:53:56] אני רוצה לעבור על הנושא הזה, ואז נעשה הפסקה.
[00:53:56 - 00:54:01] הנושא הזה הוא על איך אנחנו עושים את זה ועל what's your לדגימות,
[00:54:01 - 00:54:07] אוקיי? אז נגיד שאימנו בצורה מוצלחת גן, לא רק גן, אבל נגיד שגם אימנו מודל אחר עם likelihood,
[00:54:08 - 00:54:15] או כל מודל, אנחנו רוצים לא לתת ציון על ה-likelihood של המודל, כמו שאמרנו, זה לא תמיד אותו דבר כמו האיכות של הדגימות,
[00:54:16 - 00:54:19] רוצים לתת ציון על האיכות של הדגימות של המודל.
[00:54:21 - 00:54:23] כן, נניח עכשיו שזה לא ה-training data, יש לזה שני מודלים,
[00:54:24 - 00:54:27] זה מייצר דגימות יפות, זה מייצר דגימות לא כל כך יפות,
[00:54:27 - 00:54:31] והייתי רוצה לתת משהו שאומר שהציון הזה יותר גבוה.
[00:54:32 - 00:54:35] כן, על מה הייתי רוצה לתת ציון?
[00:54:36 - 00:54:39] מה יגרום לי לתת ציון גבוה לדגימות
[00:54:39 - 00:54:40] למה ציון נמוך?
[00:54:42 - 00:54:43] יש לכם איזה
[00:54:44 - 00:54:45] מענות?
[00:54:46 - 00:54:49] אני לא יודע אם אתם רואים, אתם רואים שמשמאל זה יותר טוב מימין?
[00:54:50 - 00:54:51] למה?
[00:54:51 - 00:54:52] למה זה נראה לכם יותר טוב?
[00:54:55 - 00:54:58] נסועי להבין מה גורם לכם לחשוב שהדגימות האלה יותר טובות לדגימות האלה?
[00:55:00 - 00:55:00] יש אובייקטים
[00:55:02 - 00:55:05] שאתם מזהים איזה אפשר כל מיני אובייקטים שם שאתם יודעים
[00:55:05 - 00:55:10] שקיימים בעולם. זה פחות מוצשטש ונכון, זו שיטה טובה. יש הרבה מטריפות.
[00:55:11 - 00:55:17] נכון, אז זה באמת בעיה אבל זה לא משהו שהוא כזה מוגדר טוב, איך אנחנו מגדירים שדגימות הן טובות.
[00:55:19 - 00:55:20] זה לא ברור גם,
[00:55:20 - 00:55:25] הרבה פעמים אנחנו רוצים משהו אחר מהדגימות, זה לא בטוח, זה תמיד אנחנו רוצים בדיוק את אותו דבר, אבל
[00:55:27 - 00:55:32] באמת יש כמה דברים, אז אחת מהשיטות שמשתמשים לפעמים זה פשוט אנשים,
[00:55:33 - 00:55:33] נותנים את זה לאנשים,
[00:55:34 - 00:55:39] אבל זה הרבה פעמים יקר, זה יכול להיות גם ביאס,
[00:55:40 - 00:55:49] תלוי בין האנשים וכן אני לא בטוח שזה תופס בדיוק את מה שאנחנו רוצים.
[00:55:50 - 00:55:53] יש עוד עניין שלא אמרתם, זה שבעצם אנחנו,
[00:55:53 - 00:55:56] קשה לדעת אם האם יש פה הכללה כשאנחנו רק מסתכלים על דגימות,
[00:55:57 - 00:56:00] אז דגימות שייראו מושלם יכול להיות שהן פשוט דגימות מהטרנינג סט,
[00:56:01 - 00:56:02] הן לא דגימות חדשות,
[00:56:03 - 00:56:08] וככל שהדאטה יותר מורכב, הטרנינג סט יותר גדול, קשה לבדוק את הדברים האלה.
[00:56:14 - 00:56:22] זו בעיה שהיא קשה, אנחנו יכולים עם לייטליות לתפוס די בקלות האם עושים לה אוברפיטים או לא,
[00:56:23 - 00:56:27] אבל סתם כשאנחנו מסתכלים על דגימות, אפילו אם אנחנו, בני אדם מסתכלים על הדגימות,
[00:56:28 - 00:56:29] לנו קשה לדעת אם יש הכללה,
[00:56:30 - 00:56:32] ואפילו אם אנחנו רואים את הטרנינג סט ואת הדגימות,
[00:56:33 - 00:56:34] לפעמים זה יהיה לנו קשה לדעת
[00:56:35 - 00:56:36] אם יש הכללה או לא.
[00:56:40 - 00:56:46] אנחנו רוצים לדעת איזשהו מדד שהוא לא איכותי, אלא כמותי כזה, איזושהי מטריקה.
[00:56:47 - 00:56:52] נדבר על שתי מטריקות שהן פופולריות, אחת נקראת Inception Score,
[00:56:53 - 00:56:58] והשנייה נקראת פרשי Inception Distance FID בקיצור.
[00:56:59 - 00:57:01] שניהן די גרועות האמת,
[00:57:02 - 00:57:04] אבל הן הכי טובות שיש כרגע,
[00:57:04 - 00:57:07] זה הסטנדרט שכולם משתמשים בו.
[00:57:09 - 00:57:12] אבל זו בהחלט בעיה שהיא רחוקה מלהיות פתורה.
[00:57:13 - 00:57:14] אז מה זה Inception Score?
[00:57:16 - 00:57:23] אז ב-Inception Score אנחנו משתמשים באיזשהו קלאסיפייר שהוא אומן לפני זה,
[00:57:24 - 00:57:27] קלאסיפייר שהוא אומן לא להבדיל בין הדגימות
[00:57:27 - 00:57:31] True or fake אלא בין ממש איזה קלאס יש ב...
[00:57:32 - 00:57:32] בתמונות.
[00:57:33 - 00:57:34] אומן על תמונות אמיתיות,
[00:57:35 - 00:57:38] למשל על אימג'נט יש שם אלף קטגוריות,
[00:57:40 - 00:57:43] אז הוא אומן לעשות קלאסיפיקציה לקטגוריות באימג'נט.
[00:57:44 - 00:57:45] ועכשיו בהינתן...
[00:57:47 - 00:57:50] אוקיי, אז יש פה שתי הנחות. הנחה ראשונה זה
[00:57:57 - 00:58:01] שהדאטה שלנו בעצם הוא קשור לאיזשהו... הוא לא חייב להיות ממש באימג'נט,
[00:58:01 - 00:58:03] אבל לפחות הוא קשור פה,
[00:58:03 - 00:58:07] אז אם אנחנו נאמן דאטה שהוא מגיע מאיזשהו קלאספיקציה לגמרי,
[00:58:07 - 00:58:09] אז אולי ה-score הזה לא יהיה קשור.
[00:58:11 - 00:58:15] זה לא חייב להיות אימג'נט, לרוב הפרי-טרן קלאסיפייר הזה הוא מאימג'נט,
[00:58:16 - 00:58:19] אבל ההנחה היא שיש לנו בעצם
[00:58:25 - 00:58:28] שהדאטה שאיתו אנחנו מאמינים את המודל הגנרטיבי שלנו,
[00:58:28 - 00:58:31] הוא דאטה שיש לו קלאסים שונים,
[00:58:32 - 00:58:37] ושיש לנו מודל שהוא כבר אומן על הדאטה האמיתי הזה כדי לעשות פרדיקציה לקלאסים השונים.
[00:58:38 - 00:58:38] אוקיי, אז אם זה המצב,
[00:58:40 - 00:58:50] אז אנחנו רוצים בעצם שהדגימות שאנחנו מייצרים איכשהו ישקפו את ה-output של הקלאסיפייר הזה בצורה דומה.
[00:58:51 - 00:58:57] אוקיי, אז זה קשור למה שאמרתם כאן, שאתם רואים שבתמונה הזאת יש אובייקטים שאתם מזהים,
[00:58:58 - 00:59:04] זה בעצם הקלאסיפייר הזה, הוא בדיוק מזהה את האובייקטים האלה, נכון? אז אנחנו רוצים שהקלאסיפייר הזה יזהה את האובייקטים האלה
[00:59:05 - 00:59:06] בתמונות שאנחנו רואים,
[00:59:06 - 00:59:12] וספציפית יש כאן שני מדדים שמרכיבים את המטריקה הזאת,
[00:59:12 - 00:59:14] אחד נקראת prediction sharpness,
[00:59:14 - 00:59:16] והשנייה prediction diversity.
[00:59:18 - 00:59:22] אז prediction sharpness,
[00:59:23 - 00:59:23] נחושב ככה,
[00:59:24 - 00:59:25] זה הקלאסיפייר של ה-P,
[00:59:25 - 00:59:30] זה ההסתברות שנותנת הקלאס Y בהינתן X.
[00:59:31 - 00:59:32] אז הדבר הזה זה בעצם
[00:59:34 - 00:59:35] אנטרופיה,
[00:59:37 - 00:59:39] או n-put-a-mינוס, זה המינוס של האנטרופיה
[00:59:40 - 00:59:44] על הפרדיקציה, אוקיי? זה אומר שאנחנו רוצים שהקלאסיפייר יהיה בטוח במה שהוא רואה כאן,
[00:59:45 - 00:59:48] אוקיי? אנטרופיה גבוהה של ה-output של הקלאסיפייר,
[00:59:49 - 00:59:52] זה אומר שהוא בעצם ההסתברות יחסית שטוחה, הוא לא יודע מה הוא רואה,
[00:59:53 - 00:59:55] אנטרופיה נמוכה,
[00:59:56 - 01:00:00] זה בלי המינוס כאן, אז זה בעצם אומר שאנחנו רוצים שהוא יהיה מאוד,
[01:00:00 - 01:00:08] הכי טוב זה שהוא יגיד שזה כלב בהסתברות 1 וכל השאר בהסתברות 0. אוקיי? אז זה נקרא פדיקשן-שאפנס.
[01:00:12 - 01:00:21] ו-diversity זה הפוך, אבל כשאנחנו עושים ממוצע על כל הדוגמאות שאנחנו רואים, כן? אם הדאטה שלנו היה מייצר רק דגימות כאלה,
[01:00:22 - 01:00:24] אז בעצם תמיד ה-output היה מאוד חד,
[01:00:24 - 01:00:26] אבל הוא תמיד היה אותו חד, אותו output.
[01:00:27 - 01:00:31] וכאן גם ה-output עבור כל דוגמה היה מאוד חד של הקלאסיפייר שלנו,
[01:00:32 - 01:00:34] אבל סך הכל היה לנו את כל הקלאסים.
[01:00:35 - 01:00:39] אוקיי? אז תסתכלו אחר כך בנוסחה, אבל זה בדיוק מה שקורה כאן בנוסחה.
[01:00:40 - 01:00:47] אנחנו רוצים שעבור כל דוגמה תהיה אנטרופיה יחסית נמוכה, שההתפלגות תהיה חדה,
[01:00:47 - 01:00:49] אבל סך הכל כשאנחנו ממצים על כל הדאטה,
[01:00:49 - 01:00:53] שההתפלגות הקלאסים שאנחנו רואים תהיה מאוד שטוחה.
[01:00:55 - 01:01:00] אוקיי, סך הכל במכפלה, אבל בעצם בגלל שלוקחים פה את האקספוננט,
[01:01:00 - 01:01:04] אז זה כאילו סכום של האנטרופיות האלה עם המשקל החיובי והשלילי.
[01:01:05 - 01:01:07] אפשר היה לכתוב את זה גם בתור K-L Divergence,
[01:01:07 - 01:01:09] לא חושב שזה כזה עוזר אינטואיטיבית.
[01:01:13 - 01:01:17] אוקיי, נקרא Inception Score, מישהו יודע מה קורה ב-Inception Score?
[01:01:21 - 01:01:21] Inception?
[01:01:25 - 01:01:25] מה?
[01:01:26 - 01:01:33] זה לא קשור לסרט, זה פשוט, אולי זה עקיפין כן קשור לסרט, זה שם של מודל של קלאסיפייר שאיתו עשו את הדבר הזה בדרך כלל.
[01:01:33 - 01:01:36] עדיין, בדרך כלל משתמשים בקלאסיפייר נתון,
[01:01:36 - 01:01:37] שהוא מאן על אימג'נט,
[01:01:38 - 01:01:40] אז זה קלאסיפייר של גוגל, שנראה לי מ-2000,
[01:01:42 - 01:01:42] לא יודע,
[01:01:42 - 01:01:44] 16-17 שנקרא, קראו לו Inception,
[01:01:45 - 01:01:45] זה קלאסיפייר.
[01:01:47 - 01:01:49] יכול להיות שהם קראו את זה בגלל הסרט.
[01:01:51 - 01:01:54] אז ככה זה נקרא Inception Score, כי הוא פשוט מסתכל על ה-Output של הקלאסיפייר.
[01:01:54 - 01:01:57] אוקיי,
[01:01:58 - 01:02:00] אז זה מודל אחד,
[01:02:00 - 01:02:06] בעיה איתו זה שהוא בעצם לא כל כך מסתכל על הדאטה,
[01:02:06 - 01:02:09] רק דרך העקיפין של הקלאסיפייר יתאמן על הדאטה.
[01:02:11 - 01:02:17] אבל אם הטרנינג סט עכשיו הוא קצת שונה אז אנחנו לא מסתכלים על איך הדאטה האמיתי,
[01:02:17 - 01:02:20] אנחנו לא משווים את המודל שלנו לסמפלינג מהדאטה האמיתי בשום אופן.
[01:02:22 - 01:02:23] אז פרשי Inception Score,
[01:02:23 - 01:02:25] אני קצת מנסה לתקן את זה,
[01:02:26 - 01:02:27] והוא נחשב יותר,
[01:02:28 - 01:02:35] הרבה מאוד מאמרים פשוט מראים את שניהם אבל הוא נחשב קצת יותר עמיד, אבל כמו שאמרתי שני הדברים האלה, כל המדדים לא נחשבים כל כך טובים.
[01:02:38 - 01:02:40] אז המודל הזה איך שהוא עובד,
[01:02:42 - 01:02:46] מעניין, זה ככה אנחנו מייצרים, יש לנו נגיד את הדאטה,
[01:02:47 - 01:02:53] את שני הדגימות שלנו, הדגימות שיצרנו G והדגימות מה-Test Set שלנו.
[01:02:54 - 01:02:55] לא אומר Training Set, אלא אומר Test Set.
[01:02:57 - 01:03:03] עכשיו אנחנו מחשבים איזשהו פיצ'ר לכל הדגימות שלנו,
[01:03:04 - 01:03:06] G ופיצ'ר לכל הדגימות מה-Test Set,
[01:03:07 - 01:03:08] FG ו-FT,
[01:03:09 - 01:03:13] שהפיצ'ר הזה הוא מחושב על ידי למשל Inception,
[01:03:16 - 01:03:19] מודל כזה, מודל קלאסי פייר כמו Inception,
[01:03:19 - 01:03:21] אבל אנחנו לא רק מסתכלים על האוטקוד שלו,
[01:03:21 - 01:03:26] אנחנו מסתכלים למשל על איזשהו וקטור שהוא אחרי הוקטור האחד לפני האחרון בדרך כלל נתחיל,
[01:03:27 - 01:03:28] איזשהו וקטור קצת
[01:03:28 - 01:03:34] תופס יותר מידע על הדאטה, כאילו לא אחרי שצמצמנו את הכל לקלאסים שיש לנו,
[01:03:35 - 01:03:36] אלף קלאסים שלנו,
[01:03:36 - 01:03:39] אז יהיה איזה משהו שמכיר קצת יותר מידע על התמונה הזאת.
[01:03:40 - 01:03:42] אנחנו פשוט רוצים שהסטטיסטיקה של
[01:03:45 - 01:03:47] השכבה הזאת ברשת תהיה דומה
[01:03:47 - 01:03:50] עבור הדגימות שהצהרנו והדגימות
[01:03:53 - 01:03:53] האמיתיות.
[01:03:54 - 01:03:58] במקום להשוות את הסטטיסטיקה של הפיקסלים עצמם,
[01:03:58 - 01:03:59] שזה הרבה יותר קשה,
[01:04:00 - 01:04:05] אז פה זה לדעתי שהוא קצת יותר מצומצם ואמור להיות משהו שהוא יותר קרוב לסמנטיקה של התמונות,
[01:04:06 - 01:04:13] ותופס כל מיני תכונות גלובליות של התמונות שעוזרות לקלאסי פייר כדי לזהות איזה קלאס יש בתמונה.
[01:04:14 - 01:04:21] הספציפית הם פשוט עושים פיט לשני גאוסיאנים ירד ומדומים, זאת אומרת עושים
[01:04:22 - 01:04:25] ל-FG עושים פיט לגאוסיאנים, זאת אומרת
[01:04:25 - 01:04:28] הוא מציג את התוחלת שלו ואת ה-entreset co-Varions,
[01:04:28 - 01:04:32] ואותו דבר ל-FT, שזה הדאטה שהגיעה מהטרנינג סט,
[01:04:33 - 01:04:38] ויש כאן איזשהו מדד מרחק שנקרא פרשת דיסטנס בין שני הגאוסיאנים.
[01:04:40 - 01:04:43] וזה מדד מרחק, אז בכל זאת יותר קטן זה יותר
[01:04:44 - 01:04:47] טוב. וזה המדד אולי הכי סטנדרטי שיש היום,
[01:04:48 - 01:04:50] אבל שוב, המדד הזה הוא די גרוע.
[01:04:53 - 01:04:54] לא מצאו משהו יותר טוב כל כך,
[01:04:55 - 01:04:58] אבל הוא לא כל כך טוב,
[01:04:58 - 01:05:03] אבל עדיין כל המאמרים שתראו יהיה את ה-FID score,
[01:05:04 - 01:05:06] וגם אני,
[01:05:06 - 01:05:07] מניסיון שלי,
[01:05:07 - 01:05:07] כל מיני מודלים,
[01:05:08 - 01:05:11] אפשר ממש לראות שהדגימות נראות יותר טוב,
[01:05:11 - 01:05:13] אבל ה-FID שלהם יהיה יותר גרוע.
[01:05:14 - 01:05:17] אם משהו שם קצת משתבש, למשל,
[01:05:18 - 01:05:19] טיפה פחות חלק,
[01:05:20 - 01:05:30] אז זה הורס את ה-FID. כל מיני דברים כאלה, שהם מקבלים מחיר שונה יותר ממה שהיינו, משקל יותר גדול במחיר יותר ממה שהיינו רוצים.
[01:05:31 - 01:05:33] אבל זו באמת בעיה קשה, שקשה
[01:05:35 - 01:05:37] לכמת בצורה, בתור מספר אחד.
[01:05:38 - 01:05:40] ויש עוד כל מיני דברים שאתם יכולים לראות גם במאמרים,
[01:05:41 - 01:05:42] למשל יש precision ו-recall.
[01:05:43 - 01:05:47] אה, אחת מהקבלות ב-FID זה גם באינספציין score, זה שזה לא תופס בכלל overfitting.
[01:05:48 - 01:05:51] זאת אומרת, מודל שממש מייצר את הטריינינגס דאטה,
[01:05:51 - 01:05:55] הוא כנראה יקבל גם אינספציין score ממש טוב, וגם FID ממש טוב.
[01:05:56 - 01:05:58] נכון? כי הדאטה היא ממש דומה לטסט דאטה.
[01:06:00 - 01:06:01] אז פה יש עוד כל מיני מדדים,
[01:06:01 - 01:06:06] למשל, יש precision ו-recall, שהוא אמור לתפוס את ה...
[01:06:06 - 01:06:16] את ה... גם overfitting באיזושהי צורה, אבל זה קצת יותר, יש כל מיני דרכים לממש את זה.
[01:06:16 - 01:06:18] בגדול ה-precision ו-recall זה שאתם יכולים,
[01:06:19 - 01:06:20] נגיד יש לנו שתי התפלגויות,
[01:06:20 - 01:06:22] ההתפלגות האמיתית וההתפלגות שייצרנו,
[01:06:23 - 01:06:27] לוקחים דגימות מפה ובודקים מה ההסתברות שלהן תחת המודל השני,
[01:06:27 - 01:06:28] אז זה ה-precision,
[01:06:28 - 01:06:30] זאת אומרת שאנחנו, כל הדגימות שאנחנו מייצרים,
[01:06:31 - 01:06:34] יש להן הסתברות טובה תחת המודל,
[01:06:34 - 01:06:36] תחת הדאטה האמיתית,
[01:06:36 - 01:06:41] ו-recall זה הפוך, אוקיי? אנחנו לוקחים דאטה אמיתית ובודקים מה ההסתברות שלהן תחת המודל.
[01:06:43 - 01:06:52] איך מממשים את הדברים האלה זה לא ברור, אוקיי? אבל יכול להיות שיש לנו דרכים שהן יחסית יקרות, בגלל שזה משהו שאנחנו צריכים לעשות רק לא בזמן האימון, אלא פעם אחת בסוף,
[01:06:53 - 01:06:55] אנחנו יכולים לממש כל מיני שיטות יותר יקרות לממש את זה.
[01:06:56 - 01:06:57] כי תזכרו שאין לנו דרך ל...
[01:06:58 - 01:07:02] אם היה לנו את המודל הזה, הכחול, אז כבר פתרנו את הבעיה, זה היה מודל של הדאטה.
[01:07:02 - 01:07:02] שזה דאטה.
[01:07:03 - 01:07:07] בעולם אנחנו יכולים לעבור דוגמאות מסוימות, לעשות כל מיני חישובים שמקרבים לזה.
[01:07:09 - 01:07:12] אז גם זה אתם תראו לפעמים, Precision ו-recall זה כל מיני שיטות לחשב את הדברים האלה.
[01:07:13 - 01:07:14] יש כאן דוגמה ממאמר,
[01:07:16 - 01:07:20] בטבלה שהוא מפרסם, זה מאמר של latent diffusion models,
[01:07:21 - 01:07:23] זה המודל שמשמש Stable Refusion,
[01:07:25 - 01:07:30] והוא משווה לכל מיני גנים שונים על כל מיני דאטה-סט של צלב-A,
[01:07:31 - 01:07:32] FHQ, זה דאטה של
[01:07:34 - 01:07:36] שני דאטה-סט של פנים,
[01:07:38 - 01:07:40] זה דאטה של תמונות של נסיעות,
[01:07:41 - 01:07:44] זה דאטה של חדרי שינה, למה שראינו קודם.
[01:07:44 - 01:07:47] סתם, יש פה את ה-FID, את ה-Precision וה-recall.
[01:07:48 - 01:07:53] יש לי באותו מאמר, לא שמתי, יש גם עוד טבלות שמראות את ה-Inception score,
[01:07:53 - 01:07:56] בדרך כלל בטבלות האלה זה בקיצור IS.
[01:07:58 - 01:08:00] זה ממש מאמרים חדשים,
[01:08:00 - 01:08:03] אז זה המדדים שהם משתמשים בהם, למרות שהם רחוקים מלהיות מושלמים.
[01:08:06 - 01:08:10] אוקיי, ויש מקומות שבהם זה יקר מדי לחשב,
[01:08:11 - 01:08:14] אני לא זוכר בדיוק איך הם ממשו את Precision ו-Recall בכל מודל,
[01:08:14 - 01:08:17] אבל לפעמים זה יקר מדי לחשב ולא עושים את זה.
[01:08:20 - 01:08:26] טוב, אז בואו נצא להפסקה, ואז נדבר על Energy Based Model ו-Score Based Models.
[01:15:00 - 01:15:00] תודה רבה.
[01:23:00 - 01:23:30] תודה רבה.
[01:23:31 - 01:23:31] כן,
[01:23:32 - 01:23:34] רב מקשיב.
[01:23:47 - 01:23:48] יאללה, בואו נמשיך.
[01:23:49 - 01:23:53] אז הנושא שלנו עכשיו זה אנרגי בייסט מודלס.
[01:23:54 - 01:23:57] אז בעצם עד עכשיו דיברנו על מודלים,
[01:24:00 - 01:24:02] ההסתברותיים של הדאטה, נכון?
[01:24:03 - 01:24:09] ואחד מהמלצים אצלנו אנחנו צריכים, המודל שלנו הוא בעצם, אם אנחנו חושבים על המודל שלנו בתור כן מודל שהוא לא עוגן,
[01:24:09 - 01:24:16] לא שמייצר דאטה, אלא מודל שהוא בעצם מייצר את המודל ההסתברותי על הדאטה,
[01:24:17 - 01:24:20] זאת אומרת, הוא ייתן איזושהי דוגמה, הוא יודע להגיד מה ההסתברות שלה,
[01:24:21 - 01:24:27] אז מה שאנחנו צריכים שהמודל הזה יקיים זה שהערך הזה שהוא נותן הוא יהיה תמיד חיובי,
[01:24:28 - 01:24:36] ושהאינטגרל של המודל הזה על כל האינפוטים האפשריים שהוא יכול לקבל
[01:24:36 - 01:24:37] יהיה אחד,
[01:24:38 - 01:24:43] זה האילוץ שלנו על מודל הסתברותי, זו המשמעות שזה מודל הסתברותי.
[01:24:47 - 01:24:49] התנאי הראשון הזה הוא די קל
[01:24:52 - 01:24:57] לקיים והתנאי השני הוא העניין המשמעותי פה
[01:24:58 - 01:25:01] למה זה חשוב שמודל כזה יתקיים?
[01:25:03 - 01:25:10] בצורה אינטוליטיבית, מעבר לזה שאנחנו יכולים להגיד שזה מודל הסתברותי, אנחנו בעצם רוצים בזמן האימון
[01:25:11 - 01:25:14] לתת למודל הזה כל פעם להגדיל את המספר שהוא נותן
[01:25:15 - 01:25:16] על הדאטה שאנחנו רואים, נכון?
[01:25:17 - 01:25:23] ואם אנחנו לא נדאג שהדבר הזה תמיד יתנרמל לאחד,
[01:25:24 - 01:25:27] אז בעצם זה שאנחנו מגדילים את הערך על איזושהי
[01:25:27 - 01:25:30] נקודה לא אומר שאנחנו מגדילים את ההסתברות על הנקודה הזו,
[01:25:30 - 01:25:33] יכול להיות שאנחנו נגדיל את הערך לכל הנקודות,
[01:25:34 - 01:25:36] אם רק בגלל שיש לנו את התנאי הזה,
[01:25:36 - 01:25:42] אז בעצם להגדיל את ההסתברות על איזושהי נקודה אומר שבאופן יחסי אנחנו מגדילים את ההסתברות שלה,
[01:25:43 - 01:25:45] כי אנחנו יודעים שבשאר הנקודות
[01:25:46 - 01:25:49] זה ירד, בסכום של שאר הנקודות זה ירד,
[01:25:50 - 01:25:54] אז זה אינטואיטיבית, למה הדבר הזה, התכונה הזאת חשובה?
[01:25:55 - 01:25:56] כמו שאמרתי,
[01:25:56 - 01:25:58] ‫לגרום למודל לתת משהו חיובי תמיד,
[01:25:59 - 01:25:59] ‫זה לא בעיה.
[01:26:02 - 01:26:05] ‫נכון, אפשר ככה להתכונן ‫של מה שיוצר מישהו,
[01:26:07 - 01:26:10] ‫אבל התנאי השני הוא התנאי הקשה.
[01:26:16 - 01:26:17] ‫ובאמת,
[01:26:19 - 01:26:20] זהו...
[01:26:20 - 01:26:30] ‫כן, אז עוד לפני רגע, תכף נכתוב את זה ‫בצורה של אנרגי-בייסט מודל, ‫כבר דיברנו על זה בעצם, ‫על מה זה אנרגי-בייסט מודל,
[01:26:31 - 01:26:32] ‫החלק הראשון של הקורס,
[01:26:33 - 01:26:38] ‫אבל בעצם עכשיו אנחנו צריכים, ‫בינתיים שיש לנו איזשהו מודל ‫שגרמנו לו להיות תמיד חיובי,
[01:26:38 - 01:26:42] ‫אנחנו צריכים איכשהו תמיד לחלק אותו ‫באיזשהו מספר
[01:26:44 - 01:26:45] שהוא הסכום הזה,
[01:26:46 - 01:26:47] ‫כדי שבסך הכול הדבר הזה,
[01:26:48 - 01:26:49] ‫הסכום הזה, הכול, יהיה שווה אחד.
[01:26:50 - 01:26:57] ‫אז המודלים שאנחנו יודעים ‫לעבוד איתם בצורה ישירה, ‫שראינו חלק מהם, ‫עושים את זה בצורה...
[01:26:57 - 01:27:02] ‫בעצם מפרקים את המודל איזה... ‫מפרקים את ההסתברות
[01:27:02 - 01:27:05] ‫לביטוי שאנחנו יודעים עבורו ‫לחשב את הדבר הזה.
[01:27:07 - 01:27:16] ‫כשהמודל הוא אוטו-רגרסיב, ‫אז בעצם אנחנו מחלקים את זה ‫למכפלה של הרבה הסתברויות ‫של נימד אחד, ‫שעבור כל אחת מהן אנחנו יודעים ‫לחשב את הדבר.
[01:27:16 - 01:27:18] למשל, כל אחד מהן הוא גאוסיאן,
[01:27:19 - 01:27:23] הוא גאוסיאן או שאולי הוא עוזי הסתברות בדידה אז אנחנו יודעים פשוט לחשב את הסכום הזה
[01:27:25 - 01:27:29] גם דיברנו על הדוגמאות, למשל מילשר מודל, שזה סוג של late and variable מודל
[01:27:30 - 01:27:44] בגלל שאנחנו לוקחים שתי התפלגויות וסוכמים או יותר וסוכמים אותן בצורה שהיא הסכום של המשקולות הוא אחד אז גם אנחנו יודעים לחשב את מה שיוצא בסופו של דבר המקדם נאמון הזה
[01:27:45 - 01:27:48] בפלואו גם בעצם אנחנו יודעים לחשב את המקדם נאמון הזה
[01:27:48 - 01:27:53] וגם אפשר לחשוב עליו בתור latent variable רק שהאינטגרל הזה אנחנו יודעים לחשב אותו
[01:27:55 - 01:27:57] בעצם האינטגרל הזה הופך להיות
[01:27:57 - 01:28:03] תמיד לקבל ערך רק עבור latent אחד ויש ניקוי הפוך יחיד
[01:28:04 - 01:28:04] מכל x ו-z
[01:28:05 - 01:28:18] אז הדבר הזה שנקרא partition function הוא בעצם משהו מאוד בעייתי וחשוב ואחד מהמכשולות בעצם
[01:28:18 - 01:28:22] בדרך לממש את המודליים שלהם
[01:28:22 - 01:28:24] אוקיי אני עושה לי כאן
[01:28:25 - 01:28:30] כאן מופיע שבדרך כלל אם רוצים מכל הדבר הזה אנרג'י
[01:28:31 - 01:28:37] אז בדרך כלל זה מופיע בתור עם סימן שלילי
[01:28:38 - 01:28:40] אתם זוכרים שדיברנו על זה בשבעה מאות שנים?
[01:28:40 - 01:28:41] אז
[01:28:43 - 01:28:47] כשהמודל נראה ככה שבעצם הוא אקספוננט של איזושהי פונקציה
[01:28:48 - 01:28:50] עם איזשהו מקדם נרמול
[01:28:50 - 01:28:55] אז החלק הזה אנחנו קוראים לו האנרגיה או מינוס שזה נקרא אנרגיה
[01:28:56 - 01:29:01] וה-z זה פשוט המקדם הנרמול שלו, אוקיי? אז פגשנו את זה
[01:29:06 - 01:29:06] עכשיו
[01:29:10 - 01:29:11] עצר לי כאן
[01:29:15 - 01:29:16] את הדבר הנאותי, אוקיי
[01:29:17 - 01:29:19] הדבר המהותי זה ברגע שיש לי את המודל
[01:29:22 - 01:29:25] ברגע שיש לי את המודל ככה אולי אני יכול פשוט
[01:29:26 - 01:29:30] למדל רק את הדבר הזה ואחר כך לדאוג
[01:29:31 - 01:29:34] לפרטישן פונקצ'ינג, אוקיי? לא מראש לבנות את המודל
[01:29:34 - 01:29:37] בצורה כזאת שאני יודע לנרמל אותו
[01:29:38 - 01:29:47] אלא פשוט ללמוד את המודל הזה וכל פעם לדאוג לדבר הזה אחר כך, אוקיי? אז אנחנו נראה דוגמאות למתי אפשר לעשות את זה
[01:29:48 - 01:30:07] בסדר, אני צריך קצת הפוך ממה שרציתי להגיד אבל אם אנחנו נצליח להשיג כזה דבר אז בעצם יהיה לנו
[01:30:11 - 01:30:15] קל מהבחינה הזאת שאנחנו לא צריכים לדאוג לאיך המודל הזה בדיוק מתפרק
[01:30:16 - 01:30:18] פשוט יש לנו משהו שאנחנו צריכים לדאוג שהוא יהיה חיובי
[01:30:18 - 01:30:25] ואחר כך נוכל לנרמל אותו, אוקיי? אז זה יהיה היתרון, שיהיה לנו יותר גמישות ואיך לבחור את זה, בסדר, בסדר, בסדר.
[01:30:25 - 01:30:29] החיסרון הוא שברגע שאין לנו את המקדם נרמול הזה דברים נעשים קשים
[01:30:30 - 01:30:34] אז קודם כל שני דברים נעשים קשים, אחד שכבר קצת דיברנו עליו זה הלמידה עצמה
[01:30:35 - 01:30:40] ושתיים זה איך אנחנו משתמשים במודל הזה בסופו של דבר, אוקיי? יש לנו רק את המודל אנרגיה,
[01:30:41 - 01:30:45] אנחנו לא יודעים, אין לנו דרך קלה לדגום ממודל כזה
[01:30:46 - 01:30:55] בניגוד ל-flow, שזה פשוט יש לנו את הסדר של הדגימות שאנחנו יכולים לעצות או מודל יותר רגרסיב או VE שאנחנו יודעים לדגום מהם
[01:30:55 - 01:31:01] וגם אנחנו לא יודעים, שוב, מה שאמרנו קודם אנחנו לא יודעים ללמוד אותו כי אנחנו לא יודעים לחשב את ה-likelihood
[01:31:01 - 01:31:11] זה לא יעזור סתם להגביר את האנרגיה או להקטין את האנרגיה לכל הדאטה שאנחנו רואים כי זה לא ידאג לזה שהמודל הזה הוא מתמרמל
[01:31:12 - 01:31:14] זה לא אומר שבאופן יחסי הנקודות האלה יקבלו
[01:31:15 - 01:31:16] הסתברות יותר גבוהה
[01:31:19 - 01:31:21] עדיין אפשר להשתמש במודל כזה,
[01:31:21 - 01:31:22] אפילו אם אנחנו לא יודעים לדגום ממנו,
[01:31:23 - 01:31:27] למשל אנחנו יכולים לחשב את היחס בין שתי הסתברויות,
[01:31:28 - 01:31:29] אם יש לנו כזה מודל
[01:31:30 - 01:31:31] בין הסתברויות של שתי נקודות
[01:31:32 - 01:31:34] נגיד אם יש לנו מודל כזה אנחנו יכולים לחשב
[01:31:34 - 01:31:38] איזה נקודה יש לה הסתברות יותר גבוהה מהשנייה
[01:31:38 - 01:31:41] פשוט אנחנו מתחשבים, המקדם נרמול או אותו מקדם נרמול
[01:31:42 - 01:31:44] לשתי הדוגמאות האלה למרות שאנחנו לא יודעים אותו
[01:31:44 - 01:31:48] ‫ואנחנו יכולים לחשב את היחס ביניהם.
[01:31:50 - 01:31:53] ‫היחס ביניהם פשוט יהיה שווה ‫להפרש של האנרגיות.
[01:31:54 - 01:31:57] ‫מי שמקבל אנרגיה יותר נמוכה,
[01:31:57 - 01:32:00] ‫שוב, פה זה לא כתוב ‫עם הסימן השלילי,
[01:32:02 - 01:32:05] ‫אבל בדרך כלל כשאנחנו קוראים לזה אנרגיה, ‫אז אנרגיה נמוכה זה טוב.
[01:32:06 - 01:32:13] ‫אז דגימה או data point, נקודה שמקבלת ‫אנרגיה יותר נמוכה,
[01:32:14 - 01:32:16] ‫תהיה נקודה שמקבלת הסתברות יותר גבוהה.
[01:32:17 - 01:32:18] ‫אז את זה אנחנו כן נוכל לחשב עדיין.
[01:32:20 - 01:32:24] ‫אנחנו יכולים לעשות עוד הרבה דברים ‫עם מודל כזה שהוא נראה ככה,
[01:32:24 - 01:32:25] ‫אנחנו נראה כמה דוגמאות.
[01:32:26 - 01:32:38] ‫אחת מהדוגמאות הקלאסיות ‫נקרא אייזינג מודל, אוקיי? ‫זה גם פיזיקה, אבל גם בעיבוד תמונות ‫משתמשים בזה הרבה, ‫וקצת נתנו דוגמה לזה ‫גם בשיעורים הראשונים.
[01:32:38 - 01:32:41] ‫מודלים כאלה, הרבה פעמים ‫אנחנו כותבים אותם ‫בצורה של גרף
[01:32:42 - 01:32:43] לא מכוון,
[01:32:44 - 01:32:49] ובניגוד לגרף מכוון, ‫שאנחנו יודעים איך לדגום דגימה ממנו ‫לפי איזשהו סדר,
[01:32:49 - 01:32:51] ‫פה אנחנו לא יודעים לדגום, ‫אנחנו רק יודעים,
[01:32:52 - 01:32:55] ‫בעצם יש לנו פקטורים ‫על כל מיני קבוצות של ממדים,
[01:32:57 - 01:32:59] ‫ממדים במשתנה שלנו, הרב-ממדים,
[01:33:00 - 01:33:06] ‫וכל פקטור כזה הוא נסכם לנו באנרגיה. ‫אז יש משהו שהוא מחיר על XI ו-YI,
[01:33:07 - 01:33:10] ‫ועוד איזשהו מחיר על YI שהם שכנים.
[01:33:11 - 01:33:18] אוקיי, אז עם Y ו-Y2, ‫ערכים שונים של Y ו-Y2 ‫בעצם נותנים לנו ערך אחר לאנרגיה.
[01:33:19 - 01:33:21] ‫אז הרבה פעמים ככה, ‫זה דרך שעושים ייצוג
[01:33:22 - 01:33:26] למודלי אנרגיה, ‫לפחות זה לפני שהיו רשתות עמוקות,
[01:33:27 - 01:33:29] ‫זו הייתה אחת מהדרכים הפופולריות,
[01:33:29 - 01:33:32] למשל בתמונות, ‫אפשר לנדל תמונות רועשות ככה.
[01:33:33 - 01:33:36] ‫כל ה-Y זה הערכים של התמונה המקורית,
[01:33:36 - 01:33:38] ‫ובאמת יש איזשהו פיקסלים שכנים,
[01:33:39 - 01:33:42] ‫הם צריכים להתנהג באיזושהי צורה ‫שקשורה אחת לשנייה.
[01:33:43 - 01:33:45] ‫אוקיי, למשל תחשבו על דאטה בינארית,
[01:33:45 - 01:33:48] ‫אז אם שני הפיקסלים ‫היו מקבלים אותו ערך,
[01:33:48 - 01:33:52] ‫אז האנרגיה היא נמוכה, ‫אבל אם הם מקבלים ערך שונה, ‫אז האנרגיה תהיה גבוהה.
[01:33:53 - 01:34:01] ‫וה-Xים, לכל פיקסל יש איזושהי הרעשה ‫שהפיקסל הזה קיבל, נגיד כל פיקסל ‫קיבל איזה רעש גאוסיאני.
[01:34:03 - 01:34:06] ‫אז גם עבור כל זוג כזה ‫יש איזשהו ערך של אנרגיה.
[01:34:06 - 01:34:10] ‫אז אפשר לשאול כל מיני שאלות, ‫בהינתן ה-Xים שאנחנו רואים, ‫מה ה-Y הכי סביר?
[01:34:11 - 01:34:16] ‫אז אפשר למצוא את ה-Y ‫שימזער את האנרגיה של הדבר.
[01:34:17 - 01:34:21] ‫זה סוג הדברים שאפשר לעשות ‫עם מודלי אנרגיה.
[01:34:28 - 01:34:30] ‫איך אפשר ללמוד את המודלים בעת?
[01:34:31 - 01:34:36] ‫אז אנחנו לא נכנסים לדוגמאות, ‫כי דיברנו על זה טיפה ב...
[01:34:37 - 01:34:41] ‫בחלק הראשון, ואני רוצה ישר ‫לקפוץ לסקור.
[01:34:42 - 01:34:44] ‫אז בגדול אנחנו, יש כמה גישות,
[01:34:45 - 01:34:49] ‫שראינו חלק מהן, ואני רק רוצה ‫לשים לכם את זה בתוך ה...
[01:34:50 - 01:34:56] בתוך המסגרת הזאת של ‫אנרגי-בייסט מודל, ‫שאנחנו לא יודעים איך לנרמל את המודל.
[01:34:57 - 01:34:57] ‫אז
[01:34:58 - 01:34:59] ראינו בעצם דגימות ש...
[01:35:00 - 01:35:03] ‫דיברנו על זה בהקשר של ‫Laten-Varible מודל, אוקיי?
[01:35:03 - 01:35:06] ‫גם בהקשר של... גם בהקשר של ‫Laten-Varible מודל,
[01:35:06 - 01:35:08] ‫הבעיה היא שאנחנו לא יודעים ‫לנרמל את המודל.
[01:35:09 - 01:35:11] ‫כי היה לנו שם אינטגרל ‫שאנחנו לא יכלנו לפתור אותו,
[01:35:12 - 01:35:13] ‫שהיינו צריכים אותו ‫כדי לנרמל את המודל.
[01:35:14 - 01:35:20] אז זה בדיוק אותה בעיה. ‫אפשר... אפשר ממש למפות ‫את הבעיות האלה אחת לשנייה.
[01:35:21 - 01:35:23] ‫מודל שבו אני כן רואה את הכול, ‫אבל אין לי...
[01:35:24 - 01:35:29] יש לי רק את הפונקציית ‫אנרגיה של המודל הזה, ‫או מודל שאני מניח ‫שיש איזשהו לי הייתן-Varible ‫ואני לא רואה אותו.
[01:35:30 - 01:35:32] ‫אפשר למפות את שתי הבעיות האלה ‫אותו דבר,
[01:35:32 - 01:35:38] ‫והפתרונות הם בגדול בדרך כלל אותו דבר, ‫אז אני יכול לעשות ‫ווירי של אינפרנס או MCMC.
[01:35:39 - 01:35:42] זה גם מה שהיה לכם בתרגיל ‫לשתי השיטות שיכולתם להשתמש בהן
[01:35:43 - 01:35:46] ‫כדי לפתור את האינטגרל הזה ‫שאנחנו לא יודעים איך לפתור אותו.
[01:35:47 - 01:35:53] ‫אבל שתי השיטות האלה, ‫בעצם אפשר לחשוב עליהן ‫בתור דרכים לקרב את הלייק לראותו.
[01:35:55 - 01:35:58] ‫אז ב-Ration אינפרנס, ‫אנחנו ראינו למשל ב-VaE,
[01:35:59 - 01:36:02] ‫עוד לא ראינו אף פעם דוגמה ‫של איך לעשות למידה עם MCMC,
[01:36:03 - 01:36:06] ‫אז ראינו איך לעשות אינפרנסים עם MCMC, ‫כל זה דבר שהייתי צריכים לעשות יותר רגיל.
[01:36:07 - 01:36:19] ‫לדגום כל פעם, למשל, עשיתם או ראינו Deep Sampling, ‫שכל פעם אנחנו דוגמים רק מימד אחד, ‫ואז בסופו של דבר מובטח לנו ‫שנקבל בדימה
[01:36:19 - 01:36:23] מתוך המודל הזה, ‫למרות שאין לנו את המודל הזה ‫ממש ביד.
[01:36:24 - 01:36:25] ‫אבל איך אנחנו לומדים מודל כזה?
[01:36:26 - 01:36:27] ‫אנחנו לומדים את הפרמטרים שלו,
[01:36:28 - 01:36:38] ‫אז שוב, אני לא נכנס לזה, ‫אבל יש למשל שיטות, ‫בגדול אפשר לחשוב על שיטות בצורה כזאת. ‫אנחנו, יש לנו דאטה מתוך ה-training set שלנו,
[01:36:39 - 01:36:42] ‫עבור הדאטה הזה אנחנו רוצים ‫להוריד את האנרגיה,
[01:36:42 - 01:36:43] ‫היא יותר טובה,
[01:36:44 - 01:36:50] ‫אבל אנחנו צריכים איכשהו לנרמל. ‫אז אחת מהדרכים ‫שאפשר לחשוב על נרמול זה לייצר דגימה ‫מתוך המודל שלנו,
[01:36:51 - 01:36:54] ‫וזה מודל שאנחנו, זו דגימה ‫שאנחנו רוצים להעלות את האנרגיה שלה.
[01:36:55 - 01:36:59] ‫אנחנו בעצם רוצים להעלות את האנרגיה ‫לדגימות שהן לא אמיתיות,
[01:37:00 - 01:37:01] ‫ולהוריד את האנרגיה ‫לדגימות שהן אמיתיות.
[01:37:02 - 01:37:06] ‫אז זה בגדול הגישה שמבוססת על MCC,
[01:37:07 - 01:37:09] ‫אבל היום אנחנו נדבר על גישה אחרת,
[01:37:09 - 01:37:14] ‫שהיא בעצם לא קשורה, ‫לא מחשבת את הלייקליות באופן ישיר,
[01:37:14 - 01:37:16] ‫והגישה הזאת מבוססת על סקור.
[01:37:17 - 01:37:18] ‫אוקיי, אתם זוכרים איזה סקור?
[01:37:22 - 01:37:24] ‫דיברנו על זה גם במובן קצת.
[01:37:24 - 01:37:26] ‫מישהו בוחר?
[01:37:29 - 01:37:38] ‫סקור מוגדר ככה, זה המגזרת לפי X
[01:37:38 - 01:37:45] ‫של הלוב של ההסתברות של נקודת X. ‫קוראים לזה לפעמים סטיין סקור.
[01:37:54 - 01:38:01] ‫אבל שימו לב שהנגזרת הזאת ‫היא לפי X, אוקיי? ‫לא לפי תטא. ‫כשאנחנו לומדים, בדרך כלל, ‫שעושים גרדיאנט דיסנט
[01:38:02 - 01:38:03] ‫על...
[01:38:05 - 01:38:09] ‫כשעושים מקסימום לייקליות, למשל, ‫עם גרדיאנט דיסנט, ‫אנחנו מסתכלים על איזושהי דוגמה שקיבלנו,
[01:38:11 - 01:38:13] ‫מחפשים את הגרדיאנט ‫לפי הפרמטרים שלנו תטא,
[01:38:14 - 01:38:17] ‫ואז הם נעים בכיוון שהפוך
[01:38:19 - 01:38:22] ‫לכיוון של הגרדיאנט הזה.
[01:38:22 - 01:38:26] ‫פה אבל הגרדיאנט הוא לא לפי תטא, ‫אלא הוא לפי X, לפי הנקודה עצמה.
[01:38:27 - 01:38:28] ‫בעצם זה אומר,
[01:38:28 - 01:38:36] ‫אם יש לי איזושהי נקודה, ‫לאיזה כיוון כדאי לי לשנות את הדאטה ‫ככה שההסתברות תחת המודל שלי תגדל.
[01:38:37 - 01:38:38] ‫אני מחפש את הדוגמאות
[01:38:39 - 01:38:42] ‫שעבורן ההסתברות... ‫המודל שלי אומר שההסתברות היא גבוהה.
[01:38:46 - 01:38:51] ‫בדו-ממד אפשר להסתכל על זה ככה, ‫אז נגיד שהצבע פה זה, שוב פעם,
[01:38:51 - 01:38:52] ‫זה דאטה דו-מימדי,
[01:38:53 - 01:38:58] ‫שיש לו שני מודים, ‫יש לו מוד אחד גדול כאן ‫ועוד מוד קטן כאן.
[01:39:00 - 01:39:07] ‫ומה שאתם רואים כאן זה חץ, ‫שזה בעצם גם מספר דו-מימדי, ‫שאומר כיוון דו-מימדי.
[01:39:08 - 01:39:09] ‫בכל נקודה כאן,
[01:39:09 - 01:39:15] ‫הוא נותן את הכיוון של ה... ‫את ה-score, אוקיי? ה-score זה משהו שהוא... ‫המימד של ה-score זה אותו מימד כמו X,
[01:39:16 - 01:39:17] ‫כי זה הגרדיאנט לפי X,
[01:39:18 - 01:39:24] ‫והוא אומר לאיזה כיוון כדאי לזוז ‫כדי להגדיל כמה שיותר מהר את ההסתברות, ‫או את לוג ההסתברות.
[01:39:26 - 01:39:31] ‫אז אם אני פה, אני כבר בנקודה הכי גבוהה, ‫אז זה יהיה 0, ‫לא יהיה פה שום כיוון.
[01:39:32 - 01:39:35] ‫אם אני כאן, ‫אז הכיוון לגדול זה לכיוון הזה, ‫פה זה לכיוון הזה.
[01:39:35 - 01:39:40] ‫בעצם כל האזור הזה, זה אומר לו ‫שכדאי לזוז לכיוון הזה, ‫וכל האזור הזה,
[01:39:41 - 01:39:44] זאת אומרת שכדי לגדול כמה שיותר מהר, ‫כדאי לזוז לכיוון ההפוך.
[01:39:47 - 01:39:51] פיקסלים, אני לא אומר לך איזה אזור בפגנון חשוב?
[01:39:54 - 01:39:56] נגיד במקרה של פרסיטי פייקסלים.
[01:39:56 - 01:39:58] יכול להיות איזה אזור חשוב,
[01:39:58 - 01:40:01] אבל הoutput שלך פה הוא ההסתברות.
[01:40:02 - 01:40:04] אז איזה אזור חשוב להסתברות, כן.
[01:40:04 - 01:40:11] אז אם יש למשל פיקסל שהוא ממש לא מתאים לכל שער הפיקסלים, הוא יגיד שכדאי לך להזיז אותו יותר משער הפיקסלים.
[01:40:14 - 01:40:16] כן, אז נגיד כאן, פה יש כאילו שני פיקסלים, נכון?
[01:40:17 - 01:40:21] אז פה זה יגיד לך שכדאי לך בעיקר להזיז את הפיקסל הזה,
[01:40:21 - 01:40:23] ולפחות להזיז את הפיקסל הזה.
[01:40:24 - 01:40:28] אז כאילו אתה יכול לחשוב על זה ככה, שהפיקסל הזה הוא יותר חשוב באזור הזה.
[01:40:29 - 01:40:37] יצא לך סאמפל שהפיקסל שמתאים כאן לציר ה-X הוא מאוד לא הגיוני, אבל הפיקסל השני הוא דווקא בסדר.
[01:40:39 - 01:40:40] אז כדאי להזיז אותו רק.
[01:40:43 - 01:40:45] אתה צודק, אם יש לך תמונה שהיא נראית ממש טוב,
[01:40:46 - 01:40:47] אבל יש לך רק אזור אחד,
[01:40:47 - 01:40:49] אז רק שם בעצם יהיה את הוודיאנט.
[01:40:52 - 01:40:56] ברור לכם מה זה סקורה, אנחנו עכשיו נדבר על סקורה במשך שני שיעורים וחצי,
[01:40:57 - 01:40:58] כדאי להבין מה זה.
[01:40:58 - 01:41:09] מה הכוונה בלהזיז את הפיקסל? כאילו ממש את התמונה... זה גרדיאנט, אז זה כאילו אומר לאיזה כיוון ידאי לי לשנות את הפיקסל, לא להזיז אותו בתוך התמונה, אלא להזיז את הערכים שלו בעצם.
[01:41:11 - 01:41:15] התמונות הדו-מדיות האלה לפעמים קצת מבלבלות, כי הם חושבים על זה בתור התמונה.
[01:41:16 - 01:41:18] אבל זה כאילו שיש לי תמונה עם שני פיקסלים.
[01:41:20 - 01:41:23] אוקיי? ויש פה אזור של תמונות מאוד סבירות.
[01:41:25 - 01:41:29] התמונות האלה הן מאוד סבירות, וגם התמונה הזאת היא די סבירה, וכל התמונות האלה שבאזור הזה הן לא סבירות.
[01:41:30 - 01:41:33] זאת אומרת, כל הערכים של שני פיקסלים שנמצאים כאן הם לא סבירים.
[01:41:34 - 01:41:37] ואני אומר לך, אם במקרה דגמת תמונה כזאת,
[01:41:37 - 01:41:39] כדאי לך לזוז לפה.
[01:41:39 - 01:41:43] זה אותו דבר.
[01:41:43 - 01:41:45] X במלמד שניים,
[01:41:46 - 01:41:49] וזה אומר שיש לי תמונה, אם אני אפשר לראות אותה תמונה,
[01:41:50 - 01:41:51] הדאטה שלי הוא מלמד שתיים,
[01:41:53 - 01:41:54] שני מלמדים.
[01:41:54 - 01:41:57] זאת אומרת שהגרדיאנט לפי X הוא גם יד גודל שתיים.
[01:41:58 - 01:42:00] והמשמעות שלו תהיה לאיזה כיוון כדאי לי לשנות את
[01:42:01 - 01:42:02] שני ה-Xים האלה,
[01:42:03 - 01:42:05] שני המלמדים של Xים, כדי להגדיל את הלוג מסבירות.
[01:42:12 - 01:42:14] זה הביטוי, אוקיי? זה לא...
[01:42:14 - 01:42:15] זה מספר,
[01:42:16 - 01:42:18] זה פונקציה שמקבלת תמונה,
[01:42:19 - 01:42:22] מחזירה סקלר, שאומר מה ההסתברות של התמונה הזאת,
[01:42:23 - 01:42:33] אבל אני גוזר את הפונקציה הזאת לפי X. אז חוזר לי וקטור X שאומר לאיזה כיוון כדאי לשנות את האינפוט X ככה שהדבר הזה יגדל, פונקציה זו תמונה.
[01:42:34 - 01:42:35] למה אנחנו רוצים את הלוג?
[01:42:36 - 01:42:37] למה אני לא?
[01:42:38 - 01:42:41] זה פשוט מסתדר יותר טוב,
[01:42:41 - 01:42:42] בחישוב אחת או אחת.
[01:42:43 - 01:42:48] אוקיי, אבל כן, ככה מוגדר ה-score. ה-score זה הנגזרת של הלוג של ההסתברות.
[01:42:49 - 01:42:51] אבל לא חוץ מהחישובים גורמים אפשר לחיות את זה דיאלוג?
[01:42:52 - 01:43:04] באופן קונספטואלי הנגזרת לפי ההסתברות זה גם מבחינת הכיוונים זה יהיה אותם כיוונים בגדול, אבל כאילו שוב העניין היחסיות היא שונה קצת, זה לא יראה בדיוק אותו כיוון וחצים, אבל
[01:43:05 - 01:43:08] הכיוון שגדל פה גם יגדיל את הלוג.
[01:43:09 - 01:43:12] אבל זה מוגדר ככה כי דברים הסתדרו.
[01:43:12 - 01:43:13] בהמשך יותר טוב.
[01:43:18 - 01:43:18] בסדר?
[01:43:18 - 01:43:19] אז זה ה-score.
[01:43:22 - 01:43:33] למה הדבר הזה הוא טוב? כי בעצם הוא מאפשר לנו להתעלם מה-partition function באנרג'י-בייסט מודל.
[01:43:33 - 01:43:34] למה?
[01:43:34 - 01:43:35] אם אני מחשב
[01:43:36 - 01:43:40] ושוב, יש לי מודל כזה, נכון? שזה החלק שהמודל שלי ממש עושה.
[01:43:40 - 01:43:42] יש לי ממש איזו רשת שנותנת את הדבר הזה.
[01:43:43 - 01:43:46] וזה יהיה משהו מאוד מורכב, שאני לא יודע איפה תשב את האינטגרל הזה.
[01:43:47 - 01:43:49] אבל אם כל מה שמעניין אותי זה ה-score,
[01:43:50 - 01:43:53] אז זה לא נורא, כי הדבר הזה פשוט נעלם ב-score. אם אני גוזר
[01:43:54 - 01:43:57] את לוג ההסתברות שלי לפי x,
[01:43:58 - 01:44:01] אז זה לא תלוי ב-x, כי יש כאן אינטגרל לכל ה-x אינס.
[01:44:03 - 01:44:05] כבר אין, אין, ה-x לא משפיע פה.
[01:44:05 - 01:44:07] ה-x שמופיע פה זה לא באמת x, זה x'
[01:44:08 - 01:44:09] שאני עובר עליו
[01:44:09 - 01:44:12] בכל ה-xים האפשריים.
[01:44:13 - 01:44:15] ה-x שנכנס פה באינפוט הוא לא מופיע כאן במכנה.
[01:44:16 - 01:44:20] ואז אני אקח לוג של זה, יש כאן איבר שהנגזרת שלו היא 0 ל-px.
[01:44:22 - 01:44:27] נשארתי רק עם הנגזרת של ה... אם זה רשת ניורונים שנותנת לי מספר,
[01:44:28 - 01:44:31] נשארתי רק עם הנגזרת של הרשת ניורונים הזאת.
[01:44:32 - 01:44:36] זה משהו שקל לחשב, פשוט גוזר את הרשת שלי לפי האינפוט.
[01:44:38 - 01:44:41] בסדר? אז אם אני... מעניין אותי רק score, אז
[01:44:42 - 01:44:45] לא כזה חשוב לי שאין לי, אין לי דרך לחשב את המקדם נרמול.
[01:44:48 - 01:44:50] אהמ... אוקיי, אבל השאלה, אהמ...
[01:44:51 - 01:44:55] אוקיי, אז הרעיון בעצם הוא במקום ללמוד את p של x,
[01:44:55 - 01:44:58] שיכריח אותי איכשהו להתמודד עם המקדם נרמול,
[01:44:58 - 01:45:01] ללמוד רק את ה-score, אוקיי? ללמוד רק מודל
[01:45:02 - 01:45:04] שזה ה... שמייצר לי את זה,
[01:45:05 - 01:45:06] אוקיי?
[01:45:08 - 01:45:11] איך אני יכול... איך אני יכול להשתמש במודל כזה?
[01:45:12 - 01:45:13] אז אה...
[01:45:14 - 01:45:16] זה היה אמור להופיע אחר כך, זה היה אמור להיות שאלה שאני שואל אתכם.
[01:45:17 - 01:45:18] אה... ראינו את זה כבר.
[01:45:18 - 01:45:21] אם אני אהמ... יודע איך לחשב
[01:45:22 - 01:45:25] את הנגזרת של ה-Nergy function, שזה ה-score,
[01:45:26 - 01:45:30] אהמ... אז אני יכול עדיין, למשל, לדגום מהמודל הזה על ידי אהמ...
[01:45:31 - 01:45:33] Langevin dynamics, של משהו שראינו.
[01:45:33 - 01:45:34] זוכרים את זה?
[01:45:35 - 01:45:35] כן?
[01:45:36 - 01:45:40] אז יש פה תזכורת, וגם אתם יכולים להסתכל על השיעור שדיברנו על זה.
[01:45:41 - 01:45:47] אז בעצם, מה זה אנג'אמין דינאמיק? זה סוג של MCMC, הוכחנו שזה אפילו אה... סוג של גיפ סמפלינג.
[01:45:48 - 01:45:51] ככה הוכחנו שזה... שזה מתכנס לדגימות מתוך ההסתברות,
[01:45:51 - 01:45:53] אבל זה בעצם דרך להשיג דגימות
[01:45:54 - 01:45:57] מ-P של X, כאשר כל מה שיש לי זה את הדבר הזה.
[01:45:58 - 01:46:00] אם יש לי רק את ה-score, יש לי רק גישה ל-score,
[01:46:01 - 01:46:03] אני עדיין יכול לדגום דגימות מ...
[01:46:04 - 01:46:04] מההסתברות הזאת.
[01:46:05 - 01:46:06] איך אני עושה את זה?
[01:46:07 - 01:46:09] אני... זה משהו אינטרטיבי, אוקיי? זה MCMC,
[01:46:10 - 01:46:12] MC MC, אני מתכוון בלי סתם איזשהו X,
[01:46:13 - 01:46:21] וכל פעם אני צועד בכיוון הזה, זאת אומרת אני הולך בכיוון שתותן לי ערך יותר גבוה,
[01:46:21 - 01:46:22] אבל אני גם אוסיף
[01:46:22 - 01:46:23] קצת רעש.
[01:46:24 - 01:46:27] אוקיי? אתם זוכרים? אני נותן לנו דוגמה כזאת שבחד-ממד,
[01:46:28 - 01:46:30] אם יש לי איזושהי פונקציה כזאת,
[01:46:31 - 01:46:33] אני רוצה לדגום ממנה,
[01:46:34 - 01:46:36] ויש לי כל פעם רק את הנגזרת.
[01:46:37 - 01:46:39] אז אם הייתי סתם מתחיל מאיזושהי נקודה,
[01:46:39 - 01:46:42] ועושה gradient descent, הייתי מגיע לאיזשהו מקסימום.
[01:46:43 - 01:46:44] או, אני פתאום נזקר, הייתי מגיע לפעם.
[01:46:45 - 01:46:46] אבל זה לא מה שאני רוצה,
[01:46:46 - 01:46:49] אני לא רוצה להגיע למקסימום של ההסתברות,
[01:46:49 - 01:46:51] אני רוצה לדגום מההסתברות. זאת אומרת שיהיו לי
[01:46:52 - 01:46:55] הרבה דגימות באזור הזה, ככל שאני יורד יהיה לי קצת פחות,
[01:46:55 - 01:46:58] ויש אזורים שהם ממש נדיר שאני אקבל שם דגימות.
[01:46:59 - 01:47:06] אבל התהליך הזה מבטיח לנו את הדבר הזה, כי באופן אינטואיטיבי אנחנו הולכים לכיוון שהוא יותר גבוה תמיד,
[01:47:07 - 01:47:08] אבל מוסיפים קצת רעש.
[01:47:08 - 01:47:13] אז אנחנו רוב הזמן נהיה באזור של מקומות גבוהים,
[01:47:13 - 01:47:19] מדי פעם הרעש הזה ייתן לנו ערך ממש גדול, אז פתאום נקפוץ למקום נבוא, ואז גם נגיע אולי לפיק אחר,
[01:47:20 - 01:47:25] ממש להקים לדירות נגיע לאזורים שהם בהסתגרות נמוכה.
[01:47:26 - 01:47:34] תחת תנאים מסוימים אפשר ממש להבטיח שהדבר הזה יתכנס לדגימות מתוך ההתפלגות האלה.
[01:47:35 - 01:47:38] דיברנו על זה, אפילו הראינו את ההוכחה של הדבר הזה,
[01:47:39 - 01:47:40] כשדיברנו על MCMC,
[01:47:41 - 01:47:41] גימות MCMC.
[01:47:44 - 01:47:45] אז אוקיי, אז
[01:47:46 - 01:47:50] למה אנחנו אומרים את זה? כי שוב, זה אומר שהדבר הזה הוא יכול להיות שימושי, אוקיי? אז יכול להיות שבמקום
[01:47:51 - 01:47:54] שהמודל שלנו יתפוס את הדבר הזה,
[01:47:55 - 01:47:57] את האנרגי פאנקשן, ושאחר כך נצטרך
[01:47:57 - 01:47:58] לנרמל אותו,
[01:47:59 - 01:48:02] אולי אנחנו יכולים באופן ישיר,
[01:48:03 - 01:48:06] רק שהמודל שלנו יתפוס את ה-score function,
[01:48:07 - 01:48:09] ואז אנחנו לא צריכים לדאוג לכל הדברים האלה.
[01:48:12 - 01:48:17] שיטה אחרת שגם משתמשים זה למדל את האנרגי,
[01:48:19 - 01:48:21] אבל להשתמש בו אחר כך רק בשביל לחשב את ה-score.
[01:48:23 - 01:48:27] אבל בזמן האחרון יותר נוטים למדל באופן ישיר את ה-score.
[01:48:28 - 01:48:30] אחת מהסיבות שחושבים שאולי זה עובד יותר טוב,
[01:48:30 - 01:48:35] כי הדבר הזה בסופו של דבר לכל דגימה הוא נותן רק מספר אחד,
[01:48:36 - 01:48:40] ופה זה נותן משהו מממד יותר גבוה, אז יש פה סיגנל קצת יותר עשיר,
[01:48:41 - 01:48:44] שאולי המודלים יכולים לתפוס בצורה יותר יעילה.
[01:48:45 - 01:48:46] אבל זה לא לגמרי ברור מה עדיף.
[01:48:49 - 01:48:50] אז עכשיו אנחנו נדבר על...
[01:48:50 - 01:48:50] אז יש הרבה,
[01:48:52 - 01:48:53] אם תסתכלו על אנרגי-בייסט מודלס,
[01:48:54 - 01:48:56] אנחנו לא נכנסים אליו כל כך לעומק,
[01:48:56 - 01:48:58] אז יש הרבה מודלים שממש מבלים את הדבר הזה,
[01:48:58 - 01:49:02] ואז מראים על כל מיני דרכים להשתמש בזה, ואחת מהן זה לחשב דרך זה את ה-score.
[01:49:03 - 01:49:08] אנחנו נדבר על מודלים שבאופן ישיר ממדלים את זה. זאת אומרת, הרשת שלנו, האינפוט שלה יהיה X,
[01:49:09 - 01:49:12] וה-artput שלה יהיה הנגזרת הזאת, זאת אומרת שזה יהיה באותו מימד כמו X.
[01:49:14 - 01:49:22] זה יגיד לאיזה כיוון כדאי לזוז כדי לשפר את ההסתברות של X. זה מה שהרשת שלנו, הרשת ניורנים שלנו תלמד.
[01:49:25 - 01:49:26] שוב,
[01:49:27 - 01:49:31] הדגרם הזאת מראה את מה שאמרתי עכשיו, מה אנחנו רוצים לעשות, נגיד שיש לנו דאטה,
[01:49:32 - 01:49:36] שוב, ההמחשה כאן היא בדו-מימד, אבל בעצם תחשבו שכל נקודה כאן זה תמונה אחרת.
[01:49:38 - 01:49:42] נגיד שכל התמונות שלנו מרוכזות באזורים האלה,
[01:49:43 - 01:49:46] אז הדאטה שלי תמיד, הטרנינג סט שלי יגיע מכאן.
[01:49:47 - 01:49:48] אני רוצה להשתמש בדאטה
[01:49:49 - 01:49:49] כדי ללמוד
[01:49:50 - 01:49:51] את ה-score.
[01:49:52 - 01:49:54] זאת אומרת, בכל נקודה ומרחב, לאן כדאי לי לזוז,
[01:49:55 - 01:49:59] כדי להגיע לנקודה יותר טובה.
[01:50:00 - 01:50:01] הכיוון שבו,
[01:50:02 - 01:50:03] זהו שנגזרת זה משהו שהוא,
[01:50:04 - 01:50:09] נדבר על סביבות קטנות, כן? אני רוצה למצוא את הכיוון שבו אני אשתפר הכי מהר,
[01:50:10 - 01:50:11] אם אני אעשה צעד קטן.
[01:50:13 - 01:50:18] זה מה שאני רוצה ללמוד, ואחרי שיהיה לי את הדבר הזה אני אוכל להשתמש בזה למשל כדי למדון דגומות חדשות
[01:50:19 - 01:50:22] עם Langer וינברנמיקס. זאת אומרת, אני אתחיל מנקודה בתקראית,
[01:50:23 - 01:50:25] ואתקדם, וכל פעם מוסיף קצת רעש,
[01:50:25 - 01:50:31] עד שאני אגיע בגדול לאזורים האלה, הרבה דגימות יהיו כאן, וחלק מהדגימות יהיו כאן.
[01:50:32 - 01:50:36] יש משמעות לגודל של ה-Fekto, נגיד לצה״ל, אם אתה מדבר עם יותר קטן?
[01:50:37 - 01:50:45] כן, זה, בעיקרון מה שמצוין כאן זה גודל הגרדיאנט. יש אזורים שהגרדיאנט יותר גדול, לא רק הכיוון משתנה, גם הגודל שלו יכול להשתנות.
[01:50:46 - 01:50:50] יש אזורים שהגרדיאנט יותר קטן, למשל כאן זה נקודת מקסימום, הגודל של הגרדיאנט הוא 0.
[01:50:53 - 01:50:54] אוקיי,
[01:50:56 - 01:50:59] שוב, עוד המחשש לדבר הזה, מה זה אומר? אנחנו,
[01:50:59 - 01:51:01] יש לנו איזושהי הסתברות
[01:51:02 - 01:51:05] שהיא ההתפלגות הלא ידועה של הדאטה,
[01:51:06 - 01:51:09] אנחנו דוגמים את ה-training data שלנו מתוך ההתפלגות הזאת,
[01:51:10 - 01:51:11] התפלגות כל הנקודות האלה,
[01:51:11 - 01:51:21] ואנחנו רוצים להשתמש בנקודות האלה כדי למדל איזושהי פונקציה שהיא מקרבת כמה שיותר טוב את ה-score function.
[01:51:21 - 01:51:23] הדבר הזה, זה צריך להיות איזושהי,
[01:51:23 - 01:51:25] למשל רשת נוירונים, איזשהו מודל,
[01:51:26 - 01:51:31] שהוא פונקציה בעצם מ-RD ל-RD, הוא מקבל X שהוא במימד D,
[01:51:31 - 01:51:35] וה-artput שלו הוא הנגזרת לפי X של לוג פי דאטה, שזה גם משהו במימד D.
[01:51:38 - 01:51:39] זה נקרא ה-score function.
[01:51:44 - 01:51:47] עוד פעם, בדיוק כמו שאמרתי, רק במילים, אוקיי, יש לנו דאטה סט,
[01:51:48 - 01:51:50] או רוצים לשערך את זה,
[01:51:51 - 01:51:53] יש לנו מודל מ-RD ל-RD,
[01:51:54 - 01:51:57] אנחנו רוצים לקרב, הרבה שלנו יקרב את ה-score.
[01:51:59 - 01:52:01] זה הרבה פעמים, כי אני רוצה שזה יהיה ברור.
[01:52:03 - 01:52:05] איך אנחנו עושים fit לדבר הזה?
[01:52:11 - 01:52:16] אז תראו, כמו שזה מצוייר כאן, באמת ה-score זה וקטור-פילד כזה, אוקיי? זה בעצם לכל נקודה,
[01:52:16 - 01:52:17] צריך לייצר וקטור,
[01:52:18 - 01:52:18] לייצר חץ,
[01:52:18 - 01:52:21] שכל נקודות במרחב צריך לייצר וקטור באותו מימד,
[01:52:22 - 01:52:30] והיינו רוצים שאם יש לנו את המרחב וקטורים האמיתיים, את השדה וקטורים האלה האמיתיים, שהשדה שאנחנו לומדים יהיה אותו דבר.
[01:52:34 - 01:52:35] אוקיי, אז
[01:52:36 - 01:52:41] יש משהו שנקרא Feature Divergence, שבעצם הוא עושה את זה בצורה הזאת,
[01:52:41 - 01:52:44] זה בעצם Objective function שמבוסס על ה-score,
[01:52:45 - 01:52:46] שהוא אומר ככה,
[01:52:46 - 01:52:49] אני, מה שאני רוצה למזער זה המרחק בין
[01:52:50 - 01:52:52] המודל שלי לסקורה האמיתי,
[01:52:52 - 01:52:55] שהמרחק הזה הוא נורמה 2,
[01:52:56 - 01:52:57] תזכרו, שני הדברים האלה הם וקטורים,
[01:52:58 - 01:52:59] אז יש לי כאן נורמה,
[01:53:00 - 01:53:01] נורמה 2 בריבוע,
[01:53:03 - 01:53:06] ואת המרחק הזה אני מחשב בתוחלת
[01:53:06 - 01:53:09] על פני ההתפלגות האמיתית של הדאטה.
[01:53:11 - 01:53:15] מה המשמעות שזה תוחלת על פני ההתפלגות האמיתית של הדאטה?
[01:53:15 - 01:53:19] למשל, אם יש לי אזור שבו אין לי בכלל בגימות,
[01:53:20 - 01:53:21] כי ההסתברות שם היא 0,
[01:53:21 - 01:53:27] אז שם זה פחות חשוב לתפוס את ה-score כמו שצריך,
[01:53:27 - 01:53:28] כי זה לא יופיע לי.
[01:53:30 - 01:53:32] למה חצי שזה התגובות כאילו?
[01:53:33 - 01:53:34] מה? למה חצי?
[01:53:35 - 01:53:41] לא, חצי זה סתם, רק כשיש Objective בריבוע שמים כאן חצי, כדי שאחר כך, כשנגזור לזה, החצי זה יהיה...
[01:53:42 - 01:53:46] איזה Objective Function, לא משנה
[01:53:55 - 01:53:59] אוקיי, אז הדבר הזה יתאפס באמת, כשזה יהיה שווה בדיוק לזה,
[01:54:00 - 01:54:02] אבל רק במקומות שבהם הדאטה קיים.
[01:54:03 - 01:54:04] במקומות אחרים הוא
[01:54:05 - 01:54:08] יכול להיות מאוד שונה, כי זה אף פעם לא יופיע בתוחלת הזאת.
[01:54:09 - 01:54:11] אוקיי,
[01:54:14 - 01:54:21] מה הבעיה? אנחנו לא, אין לנו, איך אנחנו ראינו, רוצים כאילו, נגיד שזה Objective Function שלנו, אנחנו רוצים לאמן את המודל, אוקיי? יש לנו את המודל שלנו,
[01:54:22 - 01:54:28] זה סוג של regression כזה, נכון? יש לנו מודל, יש לנו Label כזה, שאנחנו רוצים לאמן לכיוון הזה, אבל מה הבעיה?
[01:54:29 - 01:54:30] אין לנו באמת את הדבר הזה.
[01:54:31 - 01:54:32] יש לנו דגימות
[01:54:33 - 01:54:38] מההתפלגות של דאטה, אנחנו לא יודעים מה הנגזרת של הדגימות לפי X.
[01:54:39 - 01:54:39] בהתפלגות האמיתית.
[01:54:42 - 01:54:43] אז מה עושים? אז יש כמה
[01:54:45 - 01:54:48] פתרונות לזה, אחד נקרא Score Matching,
[01:54:49 - 01:54:53] שזה משהו שכותח ב-2005, על ידי
[01:54:53 - 01:54:55] אפו-יוריינן,
[01:54:56 - 01:54:59] ובעצם אפשר להראות,
[01:54:59 - 01:55:04] תכף נוכיח את זה, שה Objective Function שראינו קודם,
[01:55:04 - 01:55:05] אפשר לכתוב אותו בצורה הזאת,
[01:55:06 - 01:55:08] שכאן רק מה שמופיע זה
[01:55:09 - 01:55:10] זה המודל שלנו.
[01:55:11 - 01:55:14] אוקיי? יש תוחלת על הדאטה,
[01:55:16 - 01:55:20] אבל לא מופיע כאן בתוך ה-Objective Function שום דבר שקשור לנגזרת האמיתית,
[01:55:21 - 01:55:22] לסקורה האמיתית.
[01:55:22 - 01:55:25] אז איך נראה, איך אפשר לעשות את זה.
[01:55:27 - 01:55:33] ובעניין שיש לנו את זה, אז אין בעיה למקסם את זה בדרך הרגילה, תוחלת על פני הדאטה זאת לא בעיה,
[01:55:33 - 01:55:40] את זה אנחנו תמיד פשוט מקרבים על ידי קירוב נותקר,
[01:55:40 - 01:55:42] פשוט לוקחים את כל ה-planning set שלנו,
[01:55:42 - 01:55:43] ואנו מציעים.
[01:55:43 - 01:55:45] אני מתוחלת על פשוט הבעיה,
[01:55:47 - 01:55:50] והדבר הזה, זה מה שעוזר לנו לפתור את זה, אז בואו נראה
[01:55:51 - 01:55:51] למה זה נכון.
[01:55:53 - 01:55:57] זה פשוט דרך האינטגרציה בחלקים,
[01:55:57 - 01:56:00] אוקיי? אז הדבר הזה, אפשר לפתוח את ה-Objective,
[01:56:00 - 01:56:07] אני חושב שזה אפשר שאולי בתור סוגריים כפול עצמם, נכון? אז נפתוח את זה, ויש לנו את
[01:56:08 - 01:56:10] האיבר הזה, הוא מופיע פעמיים בריבוע,
[01:56:10 - 01:56:12] האיבר הזה מופיע בריבוע,
[01:56:13 - 01:56:17] ויש לנו פעמיים את האחד כפול השני,
[01:56:18 - 01:56:22] אוקיי? ויש פה גם, פשוט פתחו את התוחלת, אוקיי?
[01:56:22 - 01:56:24] אינטגרל לפי איקס של הדבר הזה.
[01:56:25 - 01:56:29] ברור מהעבר הזה, פשוט לפתוח את הסוגריים האלה ולפתוח את ההגדרה של התוחלת.
[01:56:30 - 01:56:33] עכשיו, האיבר הראשון הוא קבוע, והוא לא תלוי בכלל במודל שלנו,
[01:56:34 - 01:56:35] לא מעניין.
[01:56:37 - 01:56:39] האיבר הזה, אין לנו בעיה לחשב אותו,
[01:56:40 - 01:56:41] הוא רק מכיר את
[01:56:42 - 01:56:43] הריבוע של המודל שלנו,
[01:56:43 - 01:56:47] ואת הדבר הזה אפשר לפתוח עם הנוסחה של אינטגרציה בחלקים.
[01:56:48 - 01:56:49] מי שזוכר,
[01:56:50 - 01:56:53] צריך, יש פה עוד טריק באמצע, אנחנו צריכים,
[01:56:53 - 01:56:56] יש לנו את הטריק הרגיל של פי איקס נגזרת, לוג פי איקס,
[01:56:57 - 01:56:59] אנחנו יכולים לכתוב בתור
[01:57:00 - 01:57:01] נגזרת של פי איקס פשוט,
[01:57:02 - 01:57:04] כי הנוסחה של נגזרת של לוג
[01:57:05 - 01:57:08] זה נגזרת של מה שכתוב בתוך הלוג, חלקי
[01:57:08 - 01:57:10] מה שכתוב בתוך הלוג,
[01:57:10 - 01:57:13] אז אלה מגבטים, נשארנו רק עם הנגזרת של פי איקס כפול
[01:57:16 - 01:57:17] המודל שלנו,
[01:57:17 - 01:57:20] זה האיבר הזה, שווה לאיבר הזה,
[01:57:21 - 01:57:23] ועכשיו את זה אנחנו יכולים לעשות,
[01:57:23 - 01:57:26] יש לנו כאן אינטגרציה של נגזרת של משהו כפול משהו,
[01:57:27 - 01:57:30] אז לפי הנוסחה של אינטגרציה בחלקים,
[01:57:31 - 01:57:36] זה שווה לפונקציה שהתקבלה כאן בלי הנגזרת,
[01:57:37 - 01:57:38] בין מינוס אינסוף לאינסוף,
[01:57:39 - 01:57:41] פחות אותו דבר רק שהנגזרת בצד השני
[01:57:42 - 01:57:44] זה הדבר השני, עכשיו הדבר הזה,
[01:57:45 - 01:57:52] זה פשוט שווה 0 בהנחה שההסתברות הזאת היא באיזשהו support
[01:57:54 - 01:57:54] סופי,
[01:57:55 - 01:57:59] ובמינוס את סוף ואינסוף אנחנו מניחים שההסתברות שווה 0,
[01:58:00 - 01:58:02] זאת ההנחה סבירה, אז נשארנו רק עם האיבר הזה,
[01:58:03 - 01:58:10] בעצם אין פה יותר את ההסתברות של איקס, כי זה יוצא שוב פעם בתוכלת,
[01:58:13 - 01:58:17] אין פה את השלב הזה, אבל יש פה את השלב עם האיבר הזה והאיבר הזה,
[01:58:18 - 01:58:20] שניהם באינטגרל על פי איקס, זאת אומרת זה התוכלת
[01:58:21 - 01:58:24] לפי פי איקס של זה ועוד זה.
[01:58:25 - 01:58:31] אוקיי, עכשיו יש פה כל מיני דקויות של גזירה של וקטורים,
[01:58:33 - 01:58:42] אז האיבר שכתוב כאן זה לא כתוב בצורה מפורשת, אבל זה הגזירה של כל מימד ב-X כפול
[01:58:46 - 01:58:49] הגזירה של כל מימד ב-X לפי אותו מימד.
[01:58:55 - 01:58:58] אם אני רוצה לגזור, הדבר הזה הוא וקטור,
[01:58:58 - 01:59:00] ואני גוזר אותו לפי וקטור,
[01:59:01 - 01:59:04] אז המגזרת המלאה שנותנת את כל האפשרויות זה מטריצה.
[01:59:05 - 01:59:09] אבל מה שחשוב כאן בשביל החישוב הזה, בגלל שיש לנו כאן מכתלה פנימית בעצם,
[01:59:10 - 01:59:12] חשוב, כאן זה התחיל ממכתלה פנימית,
[01:59:13 - 01:59:19] אז לא כזה חשוב בדיוק מה שיוצא שם, אבל מה שיוצא זה רק האיברים האלכסונים חשובים לנו, זאת אומרת,
[01:59:19 - 01:59:21] יש לנו, אתם הולכים איך שקוראים למטריצה הזאת?
[01:59:22 - 01:59:26] של יעקוביאן. יעקוביאן, כן, זה הגזירה של כל הממדים
[01:59:27 - 01:59:30] של הפונקציה לפי כל הממדים של האינפוטים של הפונקציה.
[01:59:31 - 01:59:32] אבל חשוב לנו רק האלכסון פה,
[01:59:34 - 01:59:36] אז בגלל זה מופיע כאן ה-Trace הזה, אוקיי?
[01:59:37 - 01:59:40] אם כותבים את זה ככה, בלי לפרט אז זה בעצם כל המטריצה,
[01:59:40 - 01:59:42] אז זה רק ה-Trace של המטריצה.
[01:59:43 - 01:59:47] אוקיי, אז זה ה-objective function שלנו, שאנחנו יכולים לחשב אותו בעיקרון,
[01:59:47 - 01:59:48] כי הוא מכיר את המודל,
[01:59:49 - 01:59:49] נכון,
[01:59:50 - 01:59:53] לוקחים את כל האיקסים שלנו מתוך ה-training set,
[01:59:53 - 01:59:54] מחשבים את המודל,
[01:59:55 - 01:59:57] אוקיי? זה נותן לנו וקטור,
[01:59:59 - 02:00:03] נכון? אזכירו שה-S הזה, האינפוט שלו זה וקטור והאוטבוט שלו זה גם וקטור, ואותו מימד.
[02:00:03 - 02:00:04] זה נותן לנו את הווקטור,
[02:00:05 - 02:00:08] אנחנו מחשבים את הנורמה הריבועית,
[02:00:08 - 02:00:10] נורמה 2 בריבוע של הווקטור הזה,
[02:00:11 - 02:00:12] זה האיבר הזה,
[02:00:12 - 02:00:15] והאיבר הזה אנחנו צריכים לעשות נגזרת
[02:00:16 - 02:00:19] של הפונקציה הזאת לפי
[02:00:20 - 02:00:22] כל הממדים של X'ים, זה נותן לנו מטריצה,
[02:00:22 - 02:00:24] ואז אנחנו סוכמים את כל האלכסון במטריצה,
[02:00:25 - 02:00:26] זה ה-trace של זה.
[02:00:27 - 02:00:31] אז אפשר לחשב את הדבר הזה, אבל כש-X הוא ממד גבוה זה מאוד לא יעיל.
[02:00:32 - 02:00:33] מה החלק שלא יעיל פה?
[02:00:35 - 02:00:41] כן, אבל למרות שאנחנו צריכים לחשב רק את האלכסון של היעקוביאן,
[02:00:41 - 02:00:43] אני חושב שורה אחת ביעקוביאן,
[02:00:44 - 02:00:49] זה קל, כי אנחנו עושים פשוט black propagation אחד של הרשת הזאת,
[02:00:49 - 02:00:52] נכון? מ-output אחד של הרשת
[02:00:53 - 02:00:54] לכל האינפוטים.
[02:00:55 - 02:00:59] אבל פה אנחנו צריכים לגזור כל פעם לפי output אחר של הרשת.
[02:01:00 - 02:01:02] כל שורה כאן זה גזירה לפי output אחר.
[02:01:03 - 02:01:08] נכון שאנחנו צריכים רק איבר אחד מכל שורה, אבל עדיין אנחנו צריכים כנראה לגזור דרך כל הרשת
[02:01:09 - 02:01:10] d פעמים.
[02:01:12 - 02:01:16] אז זה משהו שעבור X'ים בממד קטן אפשר לעשות את זה,
[02:01:17 - 02:01:20] אבל באופן כללי זה לא מאוד סקיילבילי.
[02:01:23 - 02:01:25] אז איזה פתרונות יש לדבר הזה?
[02:01:25 - 02:01:30] אז יש את הפתרון שאנחנו לא נדבר עליו, שנקרא Slice Score Matching.
[02:01:30 - 02:01:40] אני רק אזכיר בקצרה מה הוא אומר. הוא אומר, אוקיי, אנחנו פשוט נבחר כל פעם, בכל איטרציה, איזושהי הטלה רנדומלית של המימד הגדול הזה של ה-output שלנו,
[02:01:40 - 02:01:43] לאיזשהו כיוון אחד רנדומלי.
[02:01:43 - 02:01:45] זה נקרא Slice Score Matching.
[02:01:46 - 02:01:53] במקום להסתכל על הנגזרת של כל הדבר הזה, אני מסתכל על הנגזרת של זה כפול איזשהו V. זאת אומרת,
[02:01:53 - 02:01:58] על ההטלה של ה-objective function הזה לאיזשהו כיוון אחד,
[02:01:58 - 02:02:02] ואני יכול להכניס את הכיוון הזה בסופו של דבר לתוך הנגזרת,
[02:02:03 - 02:02:05] ולכן אני יכול לגזור רק פעם אחת
[02:02:05 - 02:02:09] בכל איטרציה, לכל מיני-בת שאני אבחר איזשהו V רנדומלי,
[02:02:10 - 02:02:11] ואני אמפה את הכל,
[02:02:11 - 02:02:14] אני אעשה נגזרת רק לפי הכיוון הרנדומלי הזה.
[02:02:15 - 02:02:19] ואני צריך לגזור רק פעם אחת כל דבר, בגלל שאני כל פעם אבחר כיוון אחר, אז
[02:02:20 - 02:02:25] בסופו של דבר אני אסתכל על מספיק כיוונים בדאטה כדי שהנגזרות האלה יתגידו.
[02:02:26 - 02:02:29] אבל זה גם הרבה פעמים לא מאוד יעיל.
[02:02:30 - 02:02:33] כיוון אחר שאנחנו נדבר עליו נקרא Denoising Score Matching.
[02:02:34 - 02:02:43] Denoising Score Matching מבוסס על בעצם לחשב את ה-score על ידי זה שעושים Denoising
[02:02:44 - 02:02:48] להוסיף רעש ואז לעשות לזה Denoising.
[02:02:48 - 02:02:54] אז בואו נראה למה זה עובד. אז יש נוסחה שנקראת Twidies formula.
[02:02:56 - 02:02:58] בואו נסתכל מה הנוסחה הזאת אומרת.
[02:03:00 - 02:03:01] הנוסחה הזאת אומרת ככה, שאם
[02:03:04 - 02:03:05] יש לנו
[02:03:07 - 02:03:10] נקודה X, איזשהו זאת הפוינט X,
[02:03:11 - 02:03:12] והוספנו לנקודה הזאת רעש,
[02:03:12 - 02:03:15] שיש לנו גרסה מורעשת של הנקודה,
[02:03:17 - 02:03:24] ויש לנו את ה-MMSC של X בהינתן Y. זוכרים מה זה MMSC?
[02:03:25 - 02:03:26] קוראים על זה בשיעורים הראשונים?
[02:03:29 - 02:03:40] מין MSE, כן, אבל מה זה כתוב כאן על נוסחה של זה? אתם זוכרים איך מחשבים את זה ומה המשמעות של זה?
[02:03:40 - 02:03:42] מה S לא מבוא, שבערים זה לא מבוא?
[02:03:45 - 02:03:49] אפשר להראות שזה מתפרק לכל מיני רכיבים כאלה, אבל זה לא מה שהתכוונתי.
[02:03:49 - 02:03:51] מה המשמעות של MMSC?
[02:03:52 - 02:03:54] איך אנחנו חושבים על זה, או מה זה אומר?
[02:03:54 - 02:03:54] תוחלת אביזיאנית.
[02:03:56 - 02:03:58] כן, זה קשור לביזיאנית תוחלת,
[02:03:59 - 02:04:01] זה התוחלת,
[02:04:02 - 02:04:08] מה שכתוב כאן, זה בעצם התוחלת של הפוסטריו על X. אז יש לי Y,
[02:04:09 - 02:04:14] יש לי X שהוא משתנה חבוי, זו התמונה הנקייה שאני לא רואה אותו,
[02:04:14 - 02:04:15] לא רואה אותה,
[02:04:17 - 02:04:20] ואני רואה רק את Y, את התמונה המורעשת שלי,
[02:04:21 - 02:04:27] ואני צריך לתת תחזית ל-X, אוקיי? לעשות איזשהו שיעורך ל-X. אז דיברנו על שתי אפשרויות לעשות את זה,
[02:04:27 - 02:04:28] אחת זה map,
[02:04:29 - 02:04:33] קודם כל אני יכול לחשב את הפוסטריו של X בהינתן Y,
[02:04:34 - 02:04:36] ואז למצוא את הנקודת מקסימום של הפוסטריו הזאת,
[02:04:37 - 02:04:40] ודיברנו על MSE, שזה לחשב את התוחלת של הפוסטריו.
[02:04:42 - 02:04:46] אוקיי? אז כאן הכוונה היא שנשתמש ב-MSE, תכף נראה איך נשתמש בו,
[02:04:47 - 02:04:51] אז זה משהו שמסומם כאן ב-X קרובה, אוקיי? זה התוחלת של X בהינתן Y.
[02:04:53 - 02:04:55] אוקיי? אז ברור הסטאפ, יש לנו X,
[02:04:55 - 02:04:56] זו התמונה הנקייה,
[02:04:57 - 02:04:57] יש לנו Y,
[02:04:58 - 02:04:58] זו התמונה,
[02:04:59 - 02:05:02] איזשהו תמונה, איזשהו רעש שהוספנו ל-X,
[02:05:03 - 02:05:08] ויש לנו משהו שמשרך את התוחלת של X בהינתן Y. הוא לא רואה את X, הוא רק רואה את Y,
[02:05:09 - 02:05:11] והוא חושב את התוחלת של X בהינתן Y.
[02:05:12 - 02:05:16] טווידיס פורמולה אומר שאם יש לנו את ה-X סקור הזה,
[02:05:17 - 02:05:20] אנחנו יכולים להשתמש בו כדי לחשב את הסקור.
[02:05:22 - 02:05:27] אבל את הסקור של Y, לא את הסקור של X. Y זה ההסתברות
[02:05:29 - 02:05:32] P של Y, הוא מסומם כאן P Sigma Y,
[02:05:32 - 02:05:33] זה ההסתברות
[02:05:33 - 02:05:36] של Y, ההסתברות השולית של Y,
[02:05:36 - 02:05:37] שזה ההסתברות של התמונות הרועשות.
[02:05:41 - 02:05:45] זו הנוסחה, אוקיי? זו הנוסחה, אז
[02:05:45 - 02:05:50] קודם כל בוא נסתכל מה זה P של Y. אם יש לנו P של X, יש לנו הרבה,
[02:05:51 - 02:05:54] זה נגיד ההסתברות של התמונות שלנו,
[02:05:54 - 02:05:56] שמייצרות את התמונות,
[02:05:57 - 02:05:59] ועכשיו לכל תמונה הוספנו קצת רעש.
[02:06:00 - 02:06:04] אז עכשיו יש לנו דגימות של תמונות מורשות, יש לנו Training Set של הרבה תמונות
[02:06:05 - 02:06:06] עם קצת רעש.
[02:06:07 - 02:06:09] ההתפלגות של התמונות האלה זה,
[02:06:09 - 02:06:16] אנחנו קוראים כאן P Sigma של Y. יש לה התפלגות קצת אחרת מההתפלגות של תמונות נקיות,
[02:06:16 - 02:06:17] שיש להן גם רעש.
[02:06:18 - 02:06:21] זה החישוב של ההתפלגות הזאת,
[02:06:21 - 02:06:23] אפשר לחשב את זה על ידי האינטגרל הזה.
[02:06:25 - 02:06:28] אז אם יש לנו עכשיו דאטה כזה,
[02:06:29 - 02:06:47] אנחנו לוקחים את הדאטה ויש לנו איזשהו מודל שיודע לשערך את התוחלת של X בהינתן Y. אז ה-score של ההתפלגות המורשת של ההתפלגות של Y שווה לדבר הזה.
[02:06:47 - 02:06:48] מה כתוב כאן?
[02:06:49 - 02:06:49] כתוב כאן
[02:06:52 - 02:06:58] ה-MMSE פחות Y. זה השיערוך בעצם של התמונה הנקייה
[02:06:59 - 02:07:01] פחות בתמונה המורשת שראינו,
[02:07:02 - 02:07:09] אוקיי? שזה בערך השיערוך, אפשר לחשוב על זה בתור השיערוך שלנו לרעש שהתווסף לתמונה.
[02:07:10 - 02:07:13] חלקי ה-Sigma בריבוע. Sigma זה ה-Variance של הרעש.
[02:07:15 - 02:07:25] אז הדבר הזה זה בעצם ה-score function. זה בעצם הכיוון שבו אנחנו צריכים לזוז כדי להגדיל את ההסתברות של Y.
[02:07:29 - 02:07:36] זה קצת, אני חושב, הרבה דברים להבין בבת אחת, אבל אינטואיטיבית זה דווקא די הגיוני. תחשבו, מה זה אומר אינטואיטיבית?
[02:07:37 - 02:07:42] יש לי איזושהי התפלגות שמורכבת מהרבה תמונות.
[02:07:43 - 02:07:46] עכשיו אני מוסיף לתמונות האלה קצת רעש ואני שואל,
[02:07:46 - 02:07:51] באיזה כיוון כדאי לי לזוז כדי להגדיל כמה שיותר מהר את ההסתברות של התמונות האלה?
[02:07:52 - 02:07:54] והכיוון הזה זה בדיוק הפוך מהרעש.
[02:07:54 - 02:07:55] כאילו להוריד את הרעש שהוספתי.
[02:07:56 - 02:08:05] אוקיי, זה האינטואיציה של הדבר הזה ונדבר על זה קצת על המשמעות של זה בשיעור הבא.
[02:08:06 - 02:08:08] אוקיי, אז זה נקרא Tweet is formula.
[02:08:13 - 02:08:14] יש פה את ההוכחה של זה,
[02:08:15 - 02:08:18] היא דבר מסובכת, אז בואו נעבור על זה, אני חושב שאנחנו בסוף השיעור,
[02:08:19 - 02:08:20] כשאנחנו נסתכל על ההוכחות.
[02:08:21 - 02:08:24] אבל זה דווקא לא כזה מוקף,
[02:08:24 - 02:08:26] זה פשוט להציב את הנגזרת
[02:08:27 - 02:08:28] של גרסיין.
[02:08:28 - 02:08:31] אני מסתכל, אני אחשב את הנגזרת הזאת של הדבר הזה.
[02:08:33 - 02:08:35] P של Y זה שוב פעם, זה האינטגרל הזה.
[02:08:36 - 02:08:41] P של Y, אני יכול לחשוב על זה, יש לי כאן התפלגות משותפת של X ב-Y,
[02:08:41 - 02:08:44] אבל אני עושה אינטגרציה על כל ה-Y'
[02:08:49 - 02:08:53] ונשאר לי בעצם רק P של Y, אוקיי? אז P של Y זה האינטגרל הזה.
[02:08:55 - 02:08:57] למה יצא לי כאן ה-P של Y?
[02:08:57 - 02:09:00] שוב פעם, זה הטריק של נגזרת של לוג של P של הסתברות,
[02:09:00 - 02:09:01] זה אחד חלקי,
[02:09:02 - 02:09:07] סליחה, נגזרת של לוג של איזושהי פונקציה, זה אחד חלקי הפונקציה כפול הנגזרת של הפונקציה.
[02:09:09 - 02:09:10] ועכשיו אני מציב כאן
[02:09:11 - 02:09:17] את האינטגרל הזה של P של Y. עכשיו אני יכול להציב בדיוק מה יש לי כאן, יש לי כאן P של X,
[02:09:18 - 02:09:22] זה ההסתברות הלא ידועה שאני לא יודע אותה של התמונות הנקיות,
[02:09:23 - 02:09:26] כפול ההסתברות של הרעש שלי,
[02:09:26 - 02:09:26] אוקיי?
[02:09:28 - 02:09:31] לא אמרתי את זה, אבל הרעש הוא גאוסיאני, זה לא היה כתוב פה קודם?
[02:09:33 - 02:09:37] אז צריך להוסיף את זה כאן, כן? ה-Y זה רעש,
[02:09:37 - 02:09:38] זה רעש,
[02:09:38 - 02:09:41] זה תמונה שהוספתי לרעש גאוסיאני X,
[02:09:43 - 02:09:50] אז זה ההתפלגות של רעש גאוסיאני, אוקיי? זה Y, התוחרת של ההתפלגות של הרעש ל-X,
[02:09:51 - 02:09:53] והוספתי לו כל פיקסל
[02:09:54 - 02:09:55] בצורה בלתי תלוית
[02:09:56 - 02:09:57] איזשהו וריאנס,
[02:09:57 - 02:10:04] אוקיי? אז בעצם לקחתי תמונה וכל פיקסל הוספתי באיזושהי התפלגות דאוסיאנית עם וריאנס סיגמאס,
[02:10:05 - 02:10:10] אוקיי, אז אני עכשיו באמצע הגזירה של הדבר הזה, נכון? אז מה שנשאר לי זה רק לגזור את זה,
[02:10:11 - 02:10:13] כי כל השאר לא תלוי ב...
[02:10:14 - 02:10:15] זאת אומרת, יש לי את זה שיצא החוצה,
[02:10:16 - 02:10:19] ו-P של X לא תלוי ב-Y, אז הוא גם יוצא החוצה מהנגזרת,
[02:10:20 - 02:10:22] ונשארתי רק עם הנגזרת של גאוסיאן.
[02:10:23 - 02:10:26] הנגזרת של גאוסיאן נתתי לכם, אני חושב, בתרגיל הראשון,
[02:10:27 - 02:10:28] בשיעור הראשון,
[02:10:29 - 02:10:30] פשוט נשאר,
[02:10:31 - 02:10:33] הגאוסיאן עצמו יצא החוצה, נכון? בגלל האקספונט,
[02:10:34 - 02:10:40] אז האקספונט יצא החוצה, ונשאר פשוט גם עם ההתפלגות הזאת, כפול הנגזרת הפנימית של מה שיש בתוך האקספונט,
[02:10:41 - 02:10:48] שזה פונקציה ריבועית, אז נשארתי רק עם ה-1 חלקי סימובה בריבוע X פחות Y,
[02:10:49 - 02:10:50] ועכשיו
[02:10:51 - 02:10:53] אז נציב את כל זה במקום הגרדיאנט הזה,
[02:10:55 - 02:10:57] ואנחנו מקבלים את
[02:11:00 - 02:11:01] מה שכתוב כאן,
[02:11:02 - 02:11:05] כפול זה, נכון?
[02:11:05 - 02:11:09] זה היה הגרסיון הזה,
[02:11:10 - 02:11:17] זה P של Y בגרדיאנט X. פי של X חלקי פי של Y, ונשאר לי כאן עוד פי של Y בגרדיאנט X,
[02:11:18 - 02:11:19] כפול הנגזרת הפנימית,
[02:11:20 - 02:11:23] דבר הזה זה vivo-base,
[02:11:23 - 02:11:25] זו פשוט ההתפגות של X פינת NY,
[02:11:26 - 02:11:28] זה הפוסטיריון, נכון? של X פינת NY,
[02:11:28 - 02:11:30] ומה שנשאר לי כאן זה תוחלת
[02:11:31 - 02:11:34] של הפוסטריאו, לפי הפוסטריאו של הפונקציה הזאת,
[02:11:37 - 02:11:39] והיא זאת חלקי פות Y, אז התוחלת של הפונקציות לפי X,
[02:11:40 - 02:11:42] זה בדיוק ה-MMROCE,
[02:11:44 - 02:11:49] והתוחלת של הפוסטריאו לפי Y, אז התוחלת לא משנה כי זה תוחלת על פי X, זה פשוט נשאר לי Y בחוץ.
[02:11:49 - 02:12:02] נשאר לי ה-MSE פחות Y. זו פשוט ההוכחה שה-score של ההסתברות של הדאטה המורעש שווה פשוט לדבר הזה.
[02:12:03 - 02:12:15] ושוב, אינטואיטיבית זה אמור להיות יחסית הגיוני שאם נאספתי רעש כיוון שאני צריך כדי להגדיל בחזרה את ההסתברות של הדאטה שלי זה להוריד את הרעש שלי.
[02:12:20 - 02:12:32] אוקיי, אז זה בעצם השיטה הזאת שנקראת Denoising Score Matching במובן הזה של להשתמש בזה ממש כדי לחשב את ה-score
[02:12:33 - 02:12:44] פורמולה זה משהו מאוד ישן אבל ממש להשתמש בזה עם הרשתות ניורונים כדי לחשב את ה-score זה רעיון מ-2011 שממש בשנים האחרונות קיבל
[02:12:48 - 02:12:49] פנייה מחדש
[02:12:50 - 02:12:52] ובעצם אפשר לכתוב את זה ככה
[02:12:52 - 02:12:53] אנחנו עכשיו
[02:12:53 - 02:12:58] יש לנו איזה שהיא, שוב, מודל שאנחנו רוצים שהוא יהיה ה-score שלנו
[02:12:59 - 02:13:02] אנחנו מאמנים אותו בצורה הזאת שאנחנו
[02:13:05 - 02:13:12] יש לנו התפלגות אחרת שהיא תהיה לאוסיאן שמוסיפה רעש לתמונות כאן זה מוסימן בתור X תילדה
[02:13:13 - 02:13:17] וכאן מה שקראו לו P Sigma אז קוראים לו Q אתם תראו למה
[02:13:17 - 02:13:19] נהוג לקרוא לזה דווקא Q
[02:13:20 - 02:13:27] אז בעצם הדרך שאנחנו, מה שאנחנו רוצים לקרב זה ש-S יהיה קרוב
[02:13:27 - 02:13:28] לנגזרת
[02:13:29 - 02:13:31] הרועשת של הדבר הזה שזה פשוט
[02:13:32 - 02:13:32] ה-X
[02:13:49 - 02:13:51] פה-טו-פה אלא אין לי בעצם גישה ל-X
[02:14:12 - 02:14:19] אוקיי אז שבוע הבא אנחנו נכנס לזה יותר לעומק אבל בעצם מה שאנחנו עושים כאן זה השיטה שאנחנו נאמן את המודל
[02:14:20 - 02:14:29] בסופו של דבר אפשר להראות שהדבר הזה יקרב MLC אבל בעצם מה שכתוב כאן זה פשוט האלגוריתם אוקיי אנחנו לוקחים
[02:14:30 - 02:14:33] את הדאטה מתוך P מתוך הטרנינסט שלנו
[02:14:34 - 02:14:36] דוגמים גרסה מורעשת
[02:14:37 - 02:14:39] של הרעש הזה, של הדאטה הזאת
[02:14:40 - 02:14:40] זה X פילדה
[02:14:41 - 02:14:48] ומחשבים את השגיאה בין המודל שלנו לבין הדבר הזה
[02:14:48 - 02:14:50] זה X פחות הרעש חלקי סילדה
[02:14:54 - 02:14:54] בסדר?
[02:14:54 - 02:14:59] אפשר להראות שכשעושים את הדבר הזה בתוחלת על פני כל ה-Xים
[02:14:59 - 02:15:08] וכל ה-X תילדה אז זה מקרב את ה-MMSE של X ואז טווידיס פורמולה מתקיים
[02:15:18 - 02:15:22] אז נזכיר לך מנסה ללמוד את הרעש סימפשר?
[02:15:23 - 02:15:23] כן
[02:15:24 - 02:15:29] אז באמת פה איך שזה כתוב זה לא את הרעש כן גם פה איך שזה כתוב זה בעצם את הרעש
[02:15:29 - 02:15:31] הרעש הם פרקי סיגמא
[02:15:32 - 02:15:33] כמה רעש עוספנו
[02:15:34 - 02:15:40] אז בעצם הסיגנל אימון שלי זה עד כמה ואני צריך לעשות פרדיקציה על הרעש שעוספתי
[02:15:44 - 02:15:48] אבל אפשר להראות שהדבר הזה זה בעצם מה שטווידיס פורמולה מראה שאם
[02:15:48 - 02:15:54] הדבר הזה בתוחלת הוא מקרב את ה-MMSE אז הדבר הזה הוא בעצם ממדל את ה-Score
[02:15:55 - 02:15:58] אפשר להשתמש בו אחר כך כדי לעשות דגימה עם Langer & Dynabys
[02:16:00 - 02:16:03] אז אבל האימון הוא בדיוק כמו שאמרת אוקיי אז ניתן,
[02:16:03 - 02:16:05] הוא דוגם תמונה מהדאטה-סט,
[02:16:05 - 02:16:06] דוגם רעש,
[02:16:06 - 02:16:07] מוסיף את הרעש
[02:16:08 - 02:16:15] ונותן את הדוגמה עם הרעש למודל שלי והמודל שלי צריך לחזות את הרעש שהוספתי
[02:16:16 - 02:16:19] ופשוט שגיאה רגועית בין מה שיצא לרעש שיצא.
[02:16:21 - 02:16:27] אוקיי אז בעצם הדבר הזה זה הבסיס ל-Defusion Models
[02:16:28 - 02:16:31] שנדבר עליהם בשבוע הבא או בשבועיים
[02:16:32 - 02:16:39] שעושים בדיוק את זה אבל אפשר להגיע ל-objective function מכמה כיוונים,
[02:16:39 - 02:16:41] אנחנו נדבר על הכיוונים האלה בשבוע הבא
[02:16:43 - 02:16:44] אבל חשוב שתבינו
[02:16:46 - 02:16:54] עוד לפני שבוע הבא בדיוק מה זה score וכל הדברים שאמרנו שדיברנו עליהם עכשיו אז שלוש שאלות עובדים, יש לנו עוד כמה דקות,
[02:16:55 - 02:16:55] חמש
[02:16:56 - 02:17:00] אז תשאלו אם אפשר לעבור
[02:17:03 - 02:17:04] או לא
[02:17:08 - 02:17:13] אז תסתכלו על זה עוד קצת נורא ברור ובשבוע הבא נוכל
[02:17:13 - 02:17:19] להתקדם יחסית מהר, טוב תודה שבוע הבא