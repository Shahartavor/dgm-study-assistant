[00:00:00 - 00:00:10] טוב שלום היום אנחנו נדבר על מודל שנקרא diffusion models, זה בעצם יהיה המשך משבוע שעבר
[00:00:11 - 00:00:13] שדיברנו על score based models
[00:00:14 - 00:00:16] שזה היה בסיס ל-difusion models שנדבר עליו היום
[00:00:17 - 00:00:26] ואנחנו גם שבוע הבא כשניפגש לשיעור האחרון נמשיך לדבר על המודל הזה, אז זה בעצם יהיה המודל האחרון שנדבר עליו בקורס
[00:00:27 - 00:00:34] ואפשר לשאול על זה בצורה השיא של הקורס כי המודל הזה הוא באמת המודל כיום הכי פופולרי,
[00:00:34 - 00:00:39] אז בשנתיים שלוש האחרונות המודל העיקרי שהם משתמשים בו
[00:00:40 - 00:00:41] כדי למדל תמונות,
[00:00:42 - 00:00:49] אני מניח שכולכם ראיתם את התוצאות האלה בסגנון הזה לפחות, דיברנו על זה גם בשיעור הראשון,
[00:00:50 - 00:00:53] אז הם מודלים שיכולים לייצר תמונות באיכות מאוד גבוהה,
[00:00:54 - 00:00:55] לא רק שהם יכולים לייצר תמונות באיכות גבוהה,
[00:00:56 - 00:01:09] אפשר ממש לשלוט בהם ובעצם התמונות האלה הן מותנות באיזשהו משפט שמתאר משהו ואפשר בהינתן המשפט הזה לייצר כל מיני תמונות בסגנונות שונים,
[00:01:10 - 00:01:11] יש כל מיני יכולות גם של עריכה,
[00:01:13 - 00:01:16] אז יש כמה מודלים שמאפשרים את זה,
[00:01:17 - 00:01:18] דלי 2, דלי 3,
[00:01:19 - 00:01:20] Imogen,
[00:01:20 - 00:01:22] Stable Diffusion, Me journey,
[00:01:22 - 00:01:25] כל המודלים האלה מבוססים על המודל שנדבר עליו היום,
[00:01:26 - 00:01:27] שנקרא Diffusion Models.
[00:01:30 - 00:01:36] אז התוכנית שלנו זה, בעצם אנחנו נתחיל מ-score-Based Models ואני Lendgevין Dynamics,
[00:01:36 - 00:01:40] שזה המשך של מה שדיברנו עליו בשבוע שעבר,
[00:01:42 - 00:01:47] זה דרך אחת בעצם להציג את המודל הזה של Diffusion Models,
[00:01:48 - 00:01:51] אחר כך נדבר על דרך אחרת להציג בדיוק את אותו מודל,
[00:01:51 - 00:01:57] שבאה מהכיוון של משתנה חבוי של Laetent Variable Models ו-VaE,
[00:01:57 - 00:01:59] אפשר לחשוב על המודל הזה בתור סוג של VEE,
[00:02:00 - 00:02:04] שאנחנו נאמן אותו עם אותו אובג'קטיב שמאמנים ב-VEE, שזה אלבו,
[00:02:05 - 00:02:10] לפורמולציה הזאת של המודל קוראים לה DDPM,
[00:02:10 - 00:02:13] שזה Denoising Diffusion Propobליסטיק Models.
[00:02:14 - 00:02:16] ועוד דרך להסתכל על המודל הזה של Diffusion Models,
[00:02:17 - 00:02:20] זה מקשר אותו למודלים אחרים שדיברנו עליהם,
[00:02:20 - 00:02:21] Normalizing Flows,
[00:02:22 - 00:02:27] אבל כשהזמן הזה, במקום שהוא יהיה זמן בדיד
[00:02:28 - 00:02:30] שעובר מהתפלגות להתפלגות,
[00:02:31 - 00:02:32] אנחנו חושבים על זמן הזה בתור זמן רציף,
[00:02:33 - 00:02:36] ואז התהליך הזה אפשר
[00:02:41 - 00:02:46] לדבר עליו בתור תהליך שמוגדר על ידי משוואה דיפרנציאלית,
[00:02:47 - 00:02:47] או רגילה,
[00:02:48 - 00:02:48] או סטוכסטית,
[00:02:49 - 00:02:52] אז זה ODE או SDE, אז פה אנחנו נדבר רק בקצרה על הקשר של
[00:02:53 - 00:02:54] Diffusion Models
[00:02:57 - 00:02:59] לכיוון המחשבה הזה,
[00:02:59 - 00:03:02] לכיוון הזה של מודלים בזמן רציף.
[00:03:04 - 00:03:06] גם התרגיל האחרון שתקבלו,
[00:03:07 - 00:03:09] שאנחנו נשחרר אותו בשבוע הבא, הוא יהיה תרגיל על Diffusion Models.
[00:03:10 - 00:03:13] ואז תצטרכו בעצם, המודל האחרון שתממשו,
[00:03:14 - 00:03:17] זה יהיה המודל הזה שנדבר עליו היום.
[00:03:18 - 00:03:19] אוקיי, אז בואו נתחיל,
[00:03:20 - 00:03:32] אז תזכורת קטנה שוב למה שאנחנו מזכירים לעצמנו תמיד בתחילת כל שיעור, מה אנחנו עושים, אנחנו רוצים ללמוד מודל גנרטיבי, שאנחנו מדברים את זה בתור מודל הסתברותי,
[00:03:33 - 00:03:37] ארכות שלו הוא איזשהו דאטה בממד גבוה,
[00:03:38 - 00:03:45] והמודל הזה הוא הסתברותי במובן הזה שאנחנו מניחים שהדאטה נוצר מאיזשהו התפלגות אמיתית,
[00:03:46 - 00:03:50] אז התמונות מגיעו מאיזושהי התפלגות אמיתית לא ידועה, שאנחנו קוראים לה pData,
[00:03:50 - 00:03:55] ואנחנו רוצים למצוא פרמטרים של איזושהי משפחה של התפלגויות,
[00:03:56 - 00:04:00] ככה שהמרחק בין pData לפידאטה יהיה כמה שיותר קרוב,
[00:04:02 - 00:04:04] ושהמרחב הזה שנלמד
[00:04:04 - 00:04:09] הוא יהיה כזה שהתמונות שאנחנו רוצים לקבל,
[00:04:09 - 00:04:11] יהיה להן הסתברות גבוהה,
[00:04:11 - 00:04:17] וכל שאר הדאטה שאנחנו לא רוצים לקבל יקבל הסתברות מאוד נמוכה,
[00:04:19 - 00:04:23] וכבר אמרנו שאנחנו יכולים להשתמש במודלים כאלה מכל מיני דרכים, אחת זה לייצר דאטה,
[00:04:24 - 00:04:29] פשוט לדגום מתוך המודל ההסתברותי ולקבל דגימות חדשות של דאטה,
[00:04:30 - 00:04:40] דרך אחרת זה להשתמש בו כדי ללמוד איזשהו משהו על המבנה של הדאטה ולאפשר ייצוג חדש ויותר יעיל של הדאטה.
[00:04:41 - 00:04:42] זה גם representation learning.
[00:04:43 - 00:04:51] עוד נקודה חשובה זה זה שהמודל ההסתברותי ויכול לאפשר לנו לכמת לא רק את ה... עד כמה
[00:04:53 - 00:04:57] איזושהי תמונה ספציפית, אם היא טובה או לא, אלא מה מרחב
[00:04:58 - 00:04:59] אי-הוודאות שיש סביבה.
[00:05:01 - 00:05:01] וזה יותר רלוונטי אולי
[00:05:04 - 00:05:06] למצבים שבהם אנחנו צריכים לקבל איזושהי החלטה
[00:05:07 - 00:05:09] שהיא בעצמה צריכה...
[00:05:10 - 00:05:14] יש לה איזושהי מחיר ואנחנו רוצים לעשות איזשהו שיקול,
[00:05:15 - 00:05:18] שקלול של כל האפשרויות שיכול להיות.
[00:05:19 - 00:05:27] ובאופן כללי אנחנו כבר ראיתם גם בתרגיל דוגמה לזה שאנחנו רוצים מודל כזה שאנחנו יכולים לשאול כל מיני שאלות בשפה הסתברותית
[00:05:27 - 00:05:29] ולענות עליהן בשפה הסתברותית.
[00:05:30 - 00:05:32] אז לזה אנחנו קוראים פרבוליסטיק אינפרנס.
[00:05:33 - 00:05:34] אוקיי,
[00:05:35 - 00:05:37] איזה מודלים ראינו? אז ראינו מודל אוטו-רגרסיב
[00:05:38 - 00:05:44] שבו אנחנו מפרקים בעצם את המימד הגבוה של הדאטה למכפלה של הרבה
[00:05:44 - 00:05:46] התפלגויות בממד יותר נמוך
[00:05:47 - 00:05:53] וזה מאפשר לנו ללמוד את המודל בצורה יותר יעילה,
[00:05:54 - 00:05:58] לייצג את ההתפלגות הזאת בפחות פרמטרים.
[00:05:59 - 00:06:02] וזה מודל שהוא עובד, יכול לעבוד די טוב,
[00:06:03 - 00:06:07] אבל אחת מהבעיות שלו זה שהדגימה היא איטית.
[00:06:07 - 00:06:15] אנחנו צריכים בעצם לדגום, כשאנחנו רוצים לדגום אנחנו צריכים לדגום מודל בפיקסל אחד אחרי השני.
[00:06:15 - 00:06:20] אם יש לנו תמונה עם מיליון פיקסלים אנחנו נצטרך לעשות מיליון צעדים
[00:06:20 - 00:06:22] עבור דגימה של תמונה אחת.
[00:06:24 - 00:06:31] זאת אחת מהבעיות של המודל הזה. עוד בעיה אחרת זה שהוא לא כל כך נוח ל-representation learning, אין לו איזשהו ייצוג טבעי.
[00:06:32 - 00:06:37] בגלל שאנחנו מפרקים את הדאטה למשהו שהוא קצת מלאכותי, זה של פיקסל אחרי פיקסל,
[00:06:37 - 00:06:42] אין לנו פה איזשהו משהו שתופס איזשהו ייצוג טבעי של התמונה בצורה גלובלית.
[00:06:43 - 00:06:47] זה היה אחד מהמודלים שראינו. מודל אחר זה היה VAE, בר יש לנו לאוטו-רקודר,
[00:06:48 - 00:06:58] שזה מודל שמבוסס על זה שאנחנו מוסיפים, בעצם משתנה למשתנים שלנו, שאנחנו קוראים לזה, זה משתנה חבוי כי אנחנו לא רואים אותו בדאטה,
[00:06:59 - 00:07:00] וזה עוזר לנו
[00:07:03 - 00:07:07] לפרמל להשתמש בהתפלגויות שאנחנו יודעים לעבוד איתן,
[00:07:07 - 00:07:13] אבל בצורה כזאת שסך הכל ההתפלגות השולית על איקס תהיה התפלגות יותר מורכבת מלמשל סתם גאוסיאן.
[00:07:14 - 00:07:16] אז ראינו כמה דוגמאות לזה,
[00:07:17 - 00:07:22] ואת זה אנחנו אימנו עם אמורטייז וריאשנל אינפרנס,
[00:07:23 - 00:07:23] דרך,
[00:07:24 - 00:07:26] קראנו לחסם הזה אלבו,
[00:07:28 - 00:07:31] שבעצם מאפשר לנו לחסום את ההתפלגות האמיתית,
[00:07:32 - 00:07:33] את ההסתברות של איקס,
[00:07:33 - 00:07:40] וראינו שזו דרך יעילה לאמן מודלים מהסוג הזה.
[00:07:41 - 00:07:45] למרות שבסופו של דבר אנחנו לא יכולים לקבל בהתנתן איקס, אנחנו לא יודעים להגיד את ההסתברות של איקס,
[00:07:46 - 00:07:48] אנחנו רק יכולים לחסום אותה על ידי החסם הזה.
[00:07:49 - 00:07:52] אבל האופטימום של החסם, אם המשפחה של
[00:07:54 - 00:07:57] הפוסטריו שלנו היא מספיק עשירה,
[00:07:57 - 00:07:59] האופטימום של זה הוא באמת ההסתברות האמיתית.
[00:08:00 - 00:08:02] אנחנו יכולים לקוות שאנחנו הולכים ומתקרבים
[00:08:03 - 00:08:03] ההסתברות האמיתית.
[00:08:05 - 00:08:09] מודל מהסוג הזה הוא כן מאפשר דגימה בצורה יחסית,
[00:08:10 - 00:08:13] בצורה מהירה כי בעצם יש רק שני שלבים אנחנו דוגמים את ה-prior z
[00:08:14 - 00:08:17] ובתוך z אנחנו יכולים לדגום את כל ה-x'ים בצורה מגבילה.
[00:08:19 - 00:08:23] שוב זה תלוי איך הדקודר הזה בנוי אבל בדוגמאות שאנחנו ראינו זה היה המצב.
[00:08:25 - 00:08:28] הבעיה שהדגימות פה הן לא תמיד דגימות הכי טובות,
[00:08:29 - 00:08:32] ראינו דוגמה שאם יש לנו היררכיה של הרבה
[00:08:32 - 00:08:35] latency מצליחים להגיע לתוצאות די טובות
[00:08:36 - 00:08:40] וזה הולך להיות די דומה למודל שאנחנו נדבר עליו היום בעצם בסופו של דבר.
[00:08:43 - 00:08:45] בשבוע שעבר דיברנו על עוד סוג של מודל,
[00:08:46 - 00:08:51] דיברנו על זה קצת בקצרה אבל אמרנו שבעצם אנחנו יכולים לכתוב את ההסתברות
[00:08:51 - 00:08:57] של x בתור פורמולציה כזאת שקוראים לה לפעמים אנרגי בייסט מודל,
[00:08:57 - 00:08:59] יש לנו פה איזושהי פונקציה של x
[00:09:00 - 00:09:06] שהפונקציה הזאת אנחנו לא מכריחים אותה להיות פונקציה כזאת שמתנרמלת לאחד
[00:09:07 - 00:09:13] שזה הקושי העיקרי להצליח למדל הסתברויות.
[00:09:13 - 00:09:20] ברגע שאנחנו לא צריכים לדאוג לנרמול אז זה נותן לנו חופש לבחור
[00:09:20 - 00:09:27] מודלים עשירים ולאמן אותם בכל מיני שיטות אבל אנחנו לא יכולים
[00:09:27 - 00:09:32] בעצם אנחנו לא יכולים להשתמש בהם בצורה ישירה במודל הזה ואנחנו לא יכולים לאמן אותה בצורה ישירה
[00:09:33 - 00:09:34] כי חסר לנו המקדם נרמול הזה.
[00:09:36 - 00:09:40] אז יש כל מיני שיטות, ראינו כל מיני שיטות למשל שמבוססות על
[00:09:41 - 00:09:42] דגימה
[00:09:43 - 00:09:47] אבל גם שיטות של ורישנן אינפרנס יכולות לעבוד פה כדי לעשות את זה
[00:09:48 - 00:09:51] אבל השיטה שדיברנו עליה, שהתחלנו לדבר עליה השבוע שעבר,
[00:09:51 - 00:09:52] היא מבוססת על ה-score.
[00:09:53 - 00:09:57] אז score זה פונקציה שהגדרנו אותה ככה
[00:09:57 - 00:09:58] זה הנגזרת
[00:09:59 - 00:10:00] לפי x של לוג
[00:10:01 - 00:10:02] של ההסתברות
[00:10:02 - 00:10:03] של x.
[00:10:04 - 00:10:04] אז אם יש לנו מודל
[00:10:06 - 00:10:08] שאמור לתפוס את ההסתברות של x
[00:10:10 - 00:10:10] אז ה-score שלו
[00:10:11 - 00:10:18] או ה-score של x תחת המודל הזה זה הנגזרת לפי x של לוג ההסתברות של x.
[00:10:19 - 00:10:21] x זה תחשבו על x תמונה שאני מקבל
[00:10:22 - 00:10:27] אז ה-score זה הנגזרת לפי x זאת אומרת
[00:10:28 - 00:10:30] כמה אני צריך לשנות כל אחד מהפיקסלים בתמונה
[00:10:31 - 00:10:33] של לוג ההסתברות של התמונה
[00:10:34 - 00:10:37] איך אני צריך לשנות את כל הפיקסלים בתמונה, שינוי קטן,
[00:10:38 - 00:10:40] ככה שההסתברות של התמונה תגדל.
[00:10:41 - 00:10:46] זאת אומרת מה אני צריך לשנות בתמונה כדי להגדיל כמה שיותר מהר את ההסתברות שלה
[00:10:48 - 00:10:49] זה ה-score function
[00:10:51 - 00:10:56] אז אנחנו בדרך כלל מדברים על do-mמד אבל רוב הויזואליזציות שיש פה זה ב-do-m...
[00:10:56 - 00:10:57] סליחה,
[00:10:57 - 00:11:01] אנחנו מדברים על תמונות בממדים מאוד גבוהים אבל הוויזואליזציות שאפשר להראות
[00:11:01 - 00:11:02] הן ב-do-mימד
[00:11:03 - 00:11:10] אז זה למשל דוגמה של do-mימד אז מה יש לנו כאן בעצם תחשבו על זה בתור תמונה עם שני פיקסלים למשל
[00:11:11 - 00:11:14] ציר ה-y זה הפיקסל הראשון, ציר ה-x זה הפיקסל השני
[00:11:15 - 00:11:22] וההסתברות של הדאטה היא הכתמים האדומים פה בעצם זה שני גאוסיאנים בשתי הפינות
[00:11:23 - 00:11:28] אמ... וככל שאנחנו מתרחקים מהגאוסיאנים האלה ההסתברות שלנו קטנה
[00:11:29 - 00:11:31] זאת אומרת ההסתברות לקבל את שני הפיקסלים
[00:11:32 - 00:11:35] בנקודה הזאת היא הרבה יותר נמוכה מבנקודה הזאת.
[00:11:37 - 00:11:39] אוקיי, זו ההסתברות
[00:11:39 - 00:11:44] של התמונות, של התמונות הדו-ממדיות האלה של שני הפיקסלים
[00:11:46 - 00:11:48] והחצים שאתם רואים כאן זה ה-score,
[00:11:49 - 00:11:54] זה בכל נקודה מה הכיוון שכדאי לי לזוז כדי להגדיל את ההסתברות כמה שיותר מהר.
[00:11:55 - 00:11:59] ואז כל עוד אני באזור הזה של המרחב
[00:12:00 - 00:12:03] עדיף לי ללכת לכיוון של הגאוסיאן הזה,
[00:12:04 - 00:12:07] כשאני באזור הזה של המרחב כבר עדיף לי ללכת לכיוון של הגאוסיאן השני.
[00:12:08 - 00:12:10] אם אני בדיוק נמצא ב-C של הגאוסיאן
[00:12:11 - 00:12:15] הסcore הוא 0, זאת אומרת אני נמצא במקום הכי גבוה,
[00:12:15 - 00:12:19] אין שום מקום שאני יכול לזוז אליו כדי שיגדיל את ההסתברות
[00:12:20 - 00:12:24] של X. אוקיי, אז זה פונקציית ה-score.
[00:12:25 - 00:12:26] למה זה טוב פונקציית ה-score?
[00:12:27 - 00:12:31] אז דיברנו קודם על אנרגי-בייסט מודל והצורך
[00:12:32 - 00:12:33] לנרמל.
[00:12:34 - 00:12:36] אם אנחנו במקום לדבר על F נדבר על ה-score,
[00:12:38 - 00:12:40] בעצם אנחנו כבר לא נצטרך יותר לנרמל.
[00:12:41 - 00:12:44] למה? כי אם אנחנו עכשיו נחשב את ה-score, זאת אומרת אנחנו נחשב את הנגזרת לפי X,
[00:12:45 - 00:12:46] של לוג ההסתברות של X,
[00:12:48 - 00:12:51] אז אפשר לחשב את זה בתור הנגזרת של,
[00:12:52 - 00:12:55] יש לנו לוג כאן של המקדם הזה,
[00:12:55 - 00:12:58] יש לנו לוג של ה-1 חלקי Z,
[00:12:59 - 00:13:01] כפול לוג של האקספוננט,
[00:13:01 - 00:13:02] אז זה מכפלה,
[00:13:02 - 00:13:06] אז לוג של מכפלה זה מתפרק לסכום של לוגים,
[00:13:07 - 00:13:10] ולוג של אקספוננט זה פשוט מה שיש בתוך האקספוננט.
[00:13:10 - 00:13:13] אז יש לנו את הנגזרת של הדבר הזה,
[00:13:14 - 00:13:15] זה האיבר הראשון שמופיע כאן,
[00:13:16 - 00:13:18] והנגזרת של הלוג של הנרמול,
[00:13:19 - 00:13:22] אבל הנרמול היא פונקציה שלא תלויה ב-X,
[00:13:22 - 00:13:26] כי זה אינטגרציה לכל ה-Xים האפשריים, זה משהו שתלוי בפרמטרים רק של המודל שלי,
[00:13:27 - 00:13:28] במודל עצמו, בהסתברות,
[00:13:29 - 00:13:32] בהתפלגות על פני כל ה-Xים האפשריים.
[00:13:33 - 00:13:36] ולכן הנגזרת של האיבר הזה לפי X שווה 0.
[00:13:37 - 00:13:43] אז ה-score של איזושהי פונקציית התפלגות
[00:13:44 - 00:13:47] בשביל לחשב אותה, מספיק לי רק לחשב את הנגזרת של האנרגי,
[00:13:48 - 00:13:49] של האנרגיה, של F.
[00:13:50 - 00:13:53] אם אני אצליח להתחיל לעבוד רק עם score,
[00:13:54 - 00:13:57] אני לא אצטרך לנרמל את הפונקציות שלי.
[00:13:59 - 00:13:59] אז זה יתרון.
[00:14:00 - 00:14:05] אז הרעיון הוא באמת, במקום לעבוד עם ההתפלגויות,
[00:14:06 - 00:14:12] ללמוד מודל שהוא פשוט ללמוד באופן ישיר את ה-score של איזושהי התפלגות.
[00:14:13 - 00:14:18] אנחנו נרצה למצוא את score שהוא קרוב ל-score האמיתי של הדאטה.
[00:14:20 - 00:14:22] במקום למצוא התפלגות שהיא קרובה להתפלגות
[00:14:23 - 00:14:24] האמיתית של הדאטה,
[00:14:25 - 00:14:27] אנחנו נחפש score שהוא קרוב ל-score
[00:14:28 - 00:14:30] של ההתפלגות האמיתית של הדאטה.
[00:14:32 - 00:14:32] אוקיי,
[00:14:33 - 00:14:36] איך אפשר להשתמש? נגיד שהצלחנו, מצאנו את ה-score, איך אנחנו יכולים להשתמש?
[00:14:36 - 00:14:38] אז ראינו כבר השבוע שעבר,
[00:14:38 - 00:14:41] זאת אומרת נזכרנו במה שהגדרנו כבר לפני
[00:14:43 - 00:14:44] חודש בערך
[00:14:45 - 00:14:56] בשיטה שבהינתן ה-score יכולה לייצר דגימות, כמו שם, אוקיי? זה למשל דרך שאנחנו יכולים להשתמש ב-score. אם הצלחנו ללמוד את ה-score function, יש לנו את ה-score function,
[00:14:57 - 00:15:04] אנחנו יכולים פשוט לדגום מתוך המודל ההסתברותי על ידי זה שנעשה את התהליך הזה שנקרא Lenge of in Dynamics,
[00:15:04 - 00:15:05] שזה תהליך
[00:15:05 - 00:15:07] של דגימת MCMC,
[00:15:07 - 00:15:11] זאת אומרת אנחנו כל פעם דוגמים בהינתן הדגימה הקודמת שלנו,
[00:15:12 - 00:15:16] ובסוף מובטח לנו שהדגימות שנקבל הן דגימות מההתפלגות האמיתית.
[00:15:17 - 00:15:18] אז איך זה עובד?
[00:15:19 - 00:15:21] אנחנו מתחילים מאיזשהו מקום
[00:15:23 - 00:15:24] רנדומלי,
[00:15:25 - 00:15:32] ובכל צעד אנחנו בעצם מעדכנים את המיקום שלנו,
[00:15:33 - 00:15:34] על ידי זה שאנחנו מוסיפים
[00:15:35 - 00:15:36] את ה-score,
[00:15:36 - 00:15:40] זאת אומרת אנחנו צועדים בכיוון שאנחנו יגדיל את ההסתברות,
[00:15:41 - 00:15:42] עם איזשהו גודל צעד,
[00:15:43 - 00:15:50] אבל אנחנו לא רק מתקדמים ככה, אם היינו רק מתקדמים ככה זה היה gradient descent, אנחנו היינו פשוט לפי הגרדיאנט מתקדמים עד שהיינו מגיעים למקסימום הלוקאלי.
[00:15:51 - 00:15:54] פה אנחנו עושים לא רק את זה, אנחנו גם מוסיפים רעש,
[00:15:54 - 00:15:56] Epsilon כאן זה איזשהו רעש באוסיאני,
[00:15:57 - 00:15:59] שהווריאנס שלו הוא כאן אלפתי.
[00:16:01 - 00:16:03] אז אם אנחנו נחזור לפעולה הזאת כמה,
[00:16:04 - 00:16:05] הרבה פעמים,
[00:16:05 - 00:16:10] מובטח לנו שצריכים גם להקטין את גודל הצעד,
[00:16:11 - 00:16:17] אבל אם אנחנו עושים את זה בצורה נכונה, אז מובטח לנו שבסופו של דבר נקבל דגימות מההסתברות,
[00:16:17 - 00:16:20] מההתפלגות של פי תטא.
[00:16:24 - 00:16:30] דיברנו על זה גם שבוע שעבר, אפשר לחשוב על זה בתור זה שאנחנו עושים gradient descent, זאת אומרת אנחנו עולים למקסימום הלוקאלי
[00:16:32 - 00:16:33] כמה שיותר מהר,
[00:16:34 - 00:16:38] אבל מוסיפים רעש, זאת אומרת אנחנו גם, הרעש הזה נותן לנו גם
[00:16:38 - 00:16:43] להישאר לא רק בנקודת המקסימום אלא בסביבה של המקסימום,
[00:16:44 - 00:16:49] וגם מדי פעם אנחנו מקבלים ערך גדול של הרעש וזה מקפיץ אותנו למקסימום אחר,
[00:16:49 - 00:16:50] מקסימום לבעלי אחר,
[00:16:51 - 00:16:59] ומה שיפה זה שמובטח לנו שאנחנו אם נחזור על הפעולות האלה הרבה פעמים נקבל דגימות
[00:16:59 - 00:17:03] שהן פרופורציונליות ממש למפת ההתפלגות הזאת.
[00:17:04 - 00:17:10] יש פה איזה גיף שממחיש את זה, אנחנו מתחילים מכל מיני נקודות,
[00:17:11 - 00:17:14] והדגימות בסופו של דבר יתרכזו סביב
[00:17:16 - 00:17:18] שני המודים של ההתפלגות.
[00:17:28 - 00:17:33] אוקיי, אז מה התוכנית, איך אנחנו נשתמש בזה? אנחנו רוצים לקחת דאטה, נגיד תמונות,
[00:17:34 - 00:17:37] יש לנו אוסף של נקודות במרחב שלנו,
[00:17:38 - 00:17:47] אנחנו רוצים להשתמש איכשהו בנקודות האלה כדי ללמוד את ה-score, זאת אומרת מכל נקודה במרחב איך אני יכול, לאן אני צריך להתקדם כדי להשתפר,
[00:17:48 - 00:17:52] כדי לשפר את הלוג של p של דאטה,
[00:17:53 - 00:17:53] ואז
[00:17:54 - 00:17:58] ברגע שיהיה לי את ה-score אני אוכל להשתמש למשל ב-Lenge of אינטיימקס ולייצר לי דגימות חדשות.
[00:18:00 - 00:18:01] זאת התוכנית.
[00:18:03 - 00:18:09] איך אנחנו עושים את זה? אז אוקיי, פה עדיין לא מוסבר איך, אבל שוב, איך הבעיה,
[00:18:10 - 00:18:13] אפשר להגדיר את הבעיה ככה, בעצם יש לנו איזושהי התפלגות
[00:18:14 - 00:18:17] של הדאטה, התפלגות הלא ידועה הזאת, p-data.
[00:18:18 - 00:18:20] תוך ההתפלגות הזאת אנחנו דוגמים דגימות,
[00:18:21 - 00:18:22] זה ה-training set שלנו,
[00:18:23 - 00:18:28] זה הנקודות, ועכשיו אנחנו רוצים מתוך הנקודות האלה איכשהו ללמוד את ה-score function,
[00:18:29 - 00:18:30] אנחנו רוצים ללמוד מודל,
[00:18:30 - 00:18:31] אנחנו נקרא לו s,
[00:18:32 - 00:18:33] יהיה לו פרמטרים תטא,
[00:18:34 - 00:18:35] שהוא יקבל x
[00:18:36 - 00:18:44] והוא יחזיר לנו וקטור שאומר לנו את הכיוון שבו לוג p-data גדל הכי מהנקודה x.
[00:18:45 - 00:18:49] שוב, בכל המחשות כאן x זה נקודה דו-ממדית,
[00:18:50 - 00:18:52] אבל אנחנו נתעניין ב-x'ים שהם תמונות,
[00:18:53 - 00:18:55] נושא שהם לימד הרבה יותר גבוה.
[00:18:56 - 00:18:59] אבל s הזה, רק להדגיש, היא מקבלת בתור input תמונה,
[00:19:00 - 00:19:05] והoutput שלה זה data באותו מימד כמו של התמונה,
[00:19:06 - 00:19:10] שהוא אומר לאיזה כיוון כדאי להתקדם, זאת אומרת כמה לשנות כל פיקסל בתמונה,
[00:19:10 - 00:19:14] ככה שלוג p-data יגדל כמה שיותר מהר.
[00:19:18 - 00:19:21] כן, אז זה מה שאמרנו,
[00:19:22 - 00:19:26] ובעצם אנחנו רוצים לעשות איזשהו fit של בין וקטור לווקטור,
[00:19:26 - 00:19:29] אנחנו רוצים שהווקטורים שאנחנו לומדים יהיו כמה שיותר דומים,
[00:19:29 - 00:19:32] לווקטורים של הדאטה בכל המקומות במרחב.
[00:19:36 - 00:19:40] אוקיי, אז אחת מהדרכים לפרמל את זה, זה מה שנקרא Feature Divergence,
[00:19:41 - 00:19:42] זה פשוט שגיאה ריבועית,
[00:19:43 - 00:19:49] או נורמה בריבוע, יותר נכון, של הפרדיקציה שלנו והסקורה הנכון.
[00:19:51 - 00:19:53] אוקיי, אז כל אחד מהדברים האלה פה זה וקטור
[00:19:54 - 00:19:59] בממדי, לכן אנחנו נמדוד את זה בנורמה שתיים בריבוע.
[00:20:00 - 00:20:08] אנחנו נחשב את זה בתוחלת על פני איקסים שנדגמו מההתפלגות האמיתית.
[00:20:15 - 00:20:18] אוקיי, אם הצלחנו והס-תטא הוא בדיוק
[00:20:19 - 00:20:20] הסקורה נכון,
[00:20:20 - 00:20:23] אז הוקטור פה יהיה תמיד וקטור האפסים.
[00:20:24 - 00:20:27] ובכן הנורמה הזאת תהיה תמיד אפס, אבל בכל מצב אחר
[00:20:28 - 00:20:31] אנחנו יהיה פה ערך שהוא יהיה חיובי,
[00:20:33 - 00:20:34] ולכן גם התוחלת הזאת היא חיובית.
[00:20:36 - 00:20:41] כתוב פה שווה בערך, בגלל שעדיין יכול להיות, אם יש מקומות שבהם ההסתברות היא אפס,
[00:20:42 - 00:20:45] זאת אומרת עבור אזורים שבהם ההסתברות לקבל נקודה
[00:20:46 - 00:20:46] היא אפס,
[00:20:47 - 00:20:48] אז שם מותר לנו לטעות,
[00:20:49 - 00:20:53] כי שם התוחלת הזאת היא בעצם תכפיל לנו את
[00:20:53 - 00:20:54] העבר הזה באפס.
[00:20:56 - 00:20:56] אז זה לא נורא.
[00:20:57 - 00:21:00] כן, אנחנו רק צריכים להיות טובים במקומות שבהם באמת יש דאטה.
[00:21:04 - 00:21:07] אוקיי, מה הבעיה? שאנחנו לא יודעים את הגאונטרות של ה-score.
[00:21:08 - 00:21:09] אנחנו לא יודעים את ההסתברות האמיתית,
[00:21:10 - 00:21:12] פי-דאטה, אז אנחנו גם בטח לא יודעים את הנגזרת של פי-דאטה.
[00:21:13 - 00:21:15] אבל שבוע שעבר ראינו בעצם שני פתרונות לזה,
[00:21:15 - 00:21:17] אחד נקרא score-matching,
[00:21:18 - 00:21:23] score-matching על ידי איזשהו טריק שמתבסס על אינטגרציה בחלקים,
[00:21:23 - 00:21:27] ראינו שאנחנו יכולים לחשב את
[00:21:29 - 00:21:30] האובג'קטיב
[00:21:31 - 00:21:33] בלי שיהיה לנו גישה ל-score,
[00:21:34 - 00:21:39] אבל זה יכול להיות יקר, יש כאן trace שמצריך נגזרת
[00:21:40 - 00:21:45] d פעמים לגזור את המודל שלנו עבור כל דאטה פורט X.
[00:21:47 - 00:21:50] אז עבור מקרים של תמונות גדולות זה לא שיטה יעילה,
[00:21:51 - 00:21:52] ואז דיברנו על שיטה אחרת,
[00:21:52 - 00:21:54] שזאת תהיה השיטה שאני מתרכז בה גם היום,
[00:21:55 - 00:21:56] שנקראת Denoising Score Matching.
[00:21:57 - 00:21:58] Denoising Score Matching,
[00:21:59 - 00:22:03] אנחנו בעצם דוגמים דת נקודה מהמרחב שלנו, דוגמים איזשהו רעש,
[00:22:04 - 00:22:05] מוסיפים ל-X רעש,
[00:22:07 - 00:22:11] זה מה שנכנס למודל שלנו, X ועוד הרעש,
[00:22:11 - 00:22:14] ואנחנו צריכים שהארטפוד של המודל שלנו יהיה בדיוק הרעש שהוספנו.
[00:22:15 - 00:22:19] רוצים בעצם לחזות מה הרעש שהוספנו לתמונה הנקייה.
[00:22:20 - 00:22:22] תכף נראה איך מגיעים לזה.
[00:22:25 - 00:22:29] אוקיי, אז מה שגם ראינו שבוע שעבר זה נקרא Tweet is Formula, אז בואו נחזור על זה רק.
[00:22:32 - 00:22:35] זה אומר ככה, אם יש לנו נקודה שהיא רואה אשת Y,
[00:22:36 - 00:22:39] שהשגנו אותה על ידי זה שהיה לנו נקודה דאטה פוינט,
[00:22:40 - 00:22:41] זאת אומרת תמונה נקייה X,
[00:22:42 - 00:22:44] הוספנו לה רעש כאוסיאני,
[00:22:46 - 00:22:48] ויש לנו את המשערך
[00:22:49 - 00:22:51] ה-MMSE של X.
[00:22:53 - 00:22:55] כן, אני מזכיר, מה זה המשערך ה-MMSE של X?
[00:22:56 - 00:23:01] זה המשערך שממזער את ה-MSE הבייזיאני
[00:23:02 - 00:23:04] של X. זאת אומרת,
[00:23:05 - 00:23:08] אם אני אדגום הרבה Xים לפי הפריור של X,
[00:23:10 - 00:23:16] ואני אוסיף בכל X כזה את הרעש לפי ההסתברות של הרעש, במקרה הזה זה גאוסיאן,
[00:23:16 - 00:23:21] ועכשיו אני רוצה לשחזר בחזרה את X. אם אני אחזור לפעולה הזאת אינסוף פעמים,
[00:23:22 - 00:23:25] מה יהיה האלגוריתם שישחזר הכי טוב,
[00:23:26 - 00:23:27] שימזער את השגיאה הריבועית
[00:23:28 - 00:23:31] בין השחזורים שלי ל-Xים האמיתיים.
[00:23:33 - 00:23:35] אז אנחנו מסיימים את זה בתור X כובע,
[00:23:35 - 00:23:40] וגם הראינו בשיעורים הראשונים שהדבר הזה הוא פשוט התוחלת
[00:23:41 - 00:23:43] של הפוסטריור של X בהינתן Y.
[00:23:43 - 00:23:48] זה מה שממזער את ה-Basian MSE.
[00:23:50 - 00:23:51] אוקיי, אז אם יש לנו את ה-Y,
[00:23:52 - 00:23:56] הדוגמה רועשת, ויש לנו משערך MMSE של X,
[00:23:57 - 00:24:00] אז הסקור של ההסתברות,
[00:24:01 - 00:24:07] של ההתפלגות על Y שווה לאחד חלקי הוואריאנס של הרעש, סימא זה הוואריאנס של הרעש,
[00:24:08 - 00:24:11] כפול המשערך MMSE פחות Y.
[00:24:14 - 00:24:19] אז רק להסביר, מה זה ההתפלגות על Y? זה בעצם ההתפלגות השולית
[00:24:20 - 00:24:25] שאנחנו מקבלים מתוך התהליך שתיארתי קודם. זאת אומרת, אנחנו דוגמים את X מתוך הפריור,
[00:24:26 - 00:24:27] מתוך ה-Data,
[00:24:28 - 00:24:29] ומוסיפים לו רעש,
[00:24:29 - 00:24:32] אבל אז מסתכלים רק על התמונה הרועשת,
[00:24:32 - 00:24:34] אנחנו נקבל בעצם הרבה תמונות רועשות.
[00:24:35 - 00:24:38] ההתפלגות של התמונות האלה, לזה אנחנו קוראים PY.
[00:24:39 - 00:24:43] אוקיי, אז מה שהדבר הזה אומר זה שאם יש לנו משערך MMS,
[00:24:43 - 00:24:46] C אנחנו יכולים להשתמש בו כדי לשערך את ה-score.
[00:24:48 - 00:24:49] אבל זה לא יהיה ה-score של X,
[00:24:49 - 00:24:51] של מה שמעניין אותנו בסופו של דבר, אלא ה-score של Y.
[00:24:53 - 00:24:56] וזה אולי לא נורא, כי אנחנו נוכל לשחק קצת עם הסיגמה של הרעש,
[00:24:56 - 00:24:59] ובסופו של דבר להסתכל על רעש שהוא מאוד מאוד קטן.
[00:25:00 - 00:25:01] ואז ה-score
[00:25:02 - 00:25:06] של ההתפלגות הזאת, אנחנו יכולים להניח שהוא הולך ומתקרב
[00:25:07 - 00:25:08] ל-score של ההתפלגות של X.
[00:25:09 - 00:25:18] אוקיי, לא נחזור על ההוכחה של טווידי'ס פורמולה, זה בגדול פשוט גזירה של גאוסיאן,
[00:25:19 - 00:25:20] ראינו את זה בשבוע שעבר.
[00:25:24 - 00:25:27] פה זה הסוף של ההוכחה, מה שאנחנו קוראים זה שה-score
[00:25:27 - 00:25:32] שווה לאחד חלקי סיגמה בריבוע של ה-MMSE של X פחות Y.
[00:25:34 - 00:25:35] אוקיי, אז מה זה אומר?
[00:25:36 - 00:25:38] אז זה אומר שאנחנו יכולים לחשב את ה-score ככה.
[00:25:39 - 00:25:40] אם, לפי הנוסחה הזאת.
[00:25:42 - 00:25:45] ובואו נשחק קצת עם הנוסחה הזאת, אוקיי? אז אנחנו
[00:25:52 - 00:25:54] נסתכל על ה-objective שראינו קודם.
[00:25:55 - 00:25:56] הוא היה, קראנו לו
[00:25:57 - 00:25:58] Fישר Divergence
[00:26:02 - 00:26:03] כאן, Fישר Divergence
[00:26:04 - 00:26:07] ובמקום ה-score, קודם כל אנחנו עושים את ה-Fישר
[00:26:08 - 00:26:17] Divergence על P של Y. ובמקום ה-score האמיתי נכניס את הנוסחה של 2D פורמולה.
[00:26:22 - 00:26:24] אוקיי, אז זה ה-Fישר
[00:26:25 - 00:26:27] Divergence,
[00:26:27 - 00:26:31] שבשביל לייצר את ה-Y' אני צריך לייצר קודם את X מתוך הדאטה,
[00:26:31 - 00:26:35] ואז לייצר את Y שזה הרעש הגאוסיאני עם וריאנט סיגמה בריבוע.
[00:26:36 - 00:26:40] ועכשיו אני מציב פה את ה-
[00:26:45 - 00:26:51] את ה-Tweetie's formula עד כדי הקבוע של סיגמה בריבוע,
[00:26:52 - 00:26:53] איך נתעלם ממנו כרגע?
[00:26:55 - 00:27:02] כי תמיד אני יכול להניח שמה שיוצא לי מהמודל זה לא ה-score, אלא זה ה-score עד כדי הקבוע הזה של סיגמה.
[00:27:04 - 00:27:05] אז זה מה שכתוב כאן.
[00:27:06 - 00:27:06] ועכשיו
[00:27:07 - 00:27:12] המעבר בין השורה הזאת לשורה הזאת זה שפשוט במקום
[00:27:12 - 00:27:14] לשים כאן את המשערך MMSE של X,
[00:27:15 - 00:27:17] אני פשוט שם את ה-X האמיתי,
[00:27:17 - 00:27:20] את ה-X שהשתמשתי בו כשייצרתי את התמונה Y.
[00:27:23 - 00:27:25] אז זה קירוב,
[00:27:26 - 00:27:28] אבל תכף נראה למה זה הגיוני
[00:27:29 - 00:27:30] לעשות את זה.
[00:27:31 - 00:27:32] עכשיו מה זה X פחות Y?
[00:27:33 - 00:27:35] X פחות Y זה פשוט הרעש שאני הוספתי.
[00:27:36 - 00:27:38] או מינוס הרעש שאני הוספתי.
[00:27:39 - 00:27:43] אז בעצם הגענו לדבר הזה, שזה מה שראינו ב-Denoising
[00:27:45 - 00:27:48] Diffusion, ב-Denoising Score Matching.
[00:27:49 - 00:27:53] יש לי מודל שמקבל בתור אינפוט את התמונה הרועשת,
[00:27:54 - 00:27:57] זאת אומרת X פלוס הרעש כפול ה-Sיגמה של הרעש,
[00:27:58 - 00:28:01] וה-Output שלה צריך להיות כמה שיותר קרוב ל-Epsilon.
[00:28:02 - 00:28:04] אז פה זה מינוס Epsilon, אבל
[00:28:04 - 00:28:06] בדרך כלל כותבים את זה בלי המינוס.
[00:28:07 - 00:28:09] זאת אומרת ה-S הוא לא יהיה בדיוק ה-score,
[00:28:10 - 00:28:14] הוא יהיה מינוס ה-score כפול Sigma בריגוע.
[00:28:16 - 00:28:17] אוקיי.
[00:28:18 - 00:28:22] אז ראינו למה בעצם ללמוד את ה-score.
[00:28:23 - 00:28:25] אנחנו יכולים ללמוד את ה-score על ידי זה שאנחנו פשוט
[00:28:25 - 00:28:28] מנסים ללמוד את הרעש שהוספנו לתמונות.
[00:28:32 - 00:28:34] עוד דרך לחשוב על מה שכתוב פה,
[00:28:35 - 00:28:37] זה שבעצם אם אני מסתכל על
[00:28:38 - 00:28:41] שני האיברים האלה ביחד, Y ו-S,
[00:28:43 - 00:28:50] שניהם, אני יכול לחשוב עליהם בתור המודל שלי, שבהינתן Y מוציאים את ה-Output הזה, אז ה-Output הזה צריך להיות כמה שיותר קרוב ל-X.
[00:28:52 - 00:28:56] וזה בתוחלת על פני כל ה-X'ים שמגיעים מה-Prior של X,
[00:28:56 - 00:29:02] ובעצם הדבר הזה צריך להיות בדיוק האובג'קטיב של המשארי חם המסי.
[00:29:05 - 00:29:07] אוקיי,
[00:29:08 - 00:29:14] קצת אינטואיציה על-על-על-מה זה אומר, דרך מה שנקרא היפרותזה של היריעה,
[00:29:14 - 00:29:19] Mindful hypothesis. בעצם המרחב שלנו הוא מאוד גדול, נכון? המרחב של התמונות.
[00:29:20 - 00:29:25] אני יכול להניח שרוב הנקודות במרחב האלה הן לא תמונות טבעיות,
[00:29:26 - 00:29:28] או לא תמונות שמעניינות אותי בדאטה שלי.
[00:29:29 - 00:29:31] הייתי רוצה שההסתברות שם היא אפס בעצם.
[00:29:32 - 00:29:35] ההסתברות שאני אדגום נקודות מרוב המקומות במרחב,
[00:29:36 - 00:29:37] בטרנינג סט שלי הוא אפס,
[00:29:38 - 00:29:42] ויש רק איזשהו סט קטן שאפשר לקרוא לו ירייה מסוימת,
[00:29:44 - 00:29:48] שמכילה את הנקודות שהן הטמונות עם הסתברות קבועה.
[00:29:50 - 00:29:53] אז מה האינטואיציה של לעשות Denoising Score Matching,
[00:29:53 - 00:29:56] אם באמת כל הדאטה שלי יושב על ירייה כזאת?
[00:29:57 - 00:30:00] זה אינטואיציה כזאת, ברגע שאני מוסיף רעש קטן
[00:30:01 - 00:30:03] לעירייה הזאת,
[00:30:04 - 00:30:06] אז אני יוצא החוצה מהעירייה,
[00:30:07 - 00:30:13] אני מוסיף רשגה אוסיאני שהוא יכול ללכת לכל הכיוונים, אז אני יוצא קצת מהעירייה,
[00:30:13 - 00:30:19] ובעצם כדי להגדיל את ההסתברות כמה שיותר מהר אני צריך לחזור כמה שיותר מהר לעירייה.
[00:30:20 - 00:30:22] זה בדיוק לבטל את הרעש שהוספתי.
[00:30:23 - 00:30:25] זה מה שיחזיר אותי לאזור של התגונות החוקיות
[00:30:26 - 00:30:27] עם ההסתברות הגבוהה,
[00:30:28 - 00:30:29] אבל לכן זה גם הכיוון של ה-score,
[00:30:29 - 00:30:31] זה הכיוון שיגדיל כמה שיותר מהר את ההסתברות.
[00:30:32 - 00:30:36] כי ההסתברות הזאת היא מאוד מהר קטנה לאפס מחוץ לעירייה,
[00:30:37 - 00:30:37] כשאני מתרחק מהעירייה.
[00:30:39 - 00:30:42] אוקיי, אז זו אינטואיציה שאומרת לנו למה הדבר הזה עובד,
[00:30:43 - 00:30:50] אבל זה גם מעלה שאלה בעצם מה שזה אומר זה שיש איזשהו אזור של יריעה שקרוב מאוד ליריעה,
[00:30:50 - 00:30:55] יש לי משמעות להתפלגות ול-score,
[00:30:55 - 00:30:59] הקצב שבה ההסתברות יורדת.
[00:31:00 - 00:31:04] ששם זה באמת הגיוני להתקדם בכיוון ההפוך של הרעש,
[00:31:04 - 00:31:06] אבל ברגע שאני קצת מתרחק מהעירייה הזאת,
[00:31:07 - 00:31:08] כל המרחב,
[00:31:08 - 00:31:10] בעצם ההסתברות היא די אפס בגדול,
[00:31:11 - 00:31:13] וזה אומר שהכיוון,
[00:31:14 - 00:31:16] הגרדיאנט שם לא יהיה כל כך מוגדר או יהיה מאוד לא יציב.
[00:31:17 - 00:31:19] אז מה אנחנו עושים, איך אנחנו פותרים את הבעיה הזאת?
[00:31:22 - 00:31:27] אנחנו צריכים הרי כדי שלנדלווינט דיינאניקס יעבוד, אנחנו צריכים להתחיל מאיזושהי נקודה רנדומלית במרחב,
[00:31:28 - 00:31:37] ולהגיע לאיזושהי נקודה בעירייה הזאת, אבל אם הגרדיאנט, אם ה-score שלי הוא כל כך גרוע ולא יציב מחוץ לעירייה,
[00:31:37 - 00:31:40] הסיכוי שאני אגיע אל תוך העירייה הוא יהיה מאוד נמוך.
[00:31:43 - 00:31:45] הנה, זו המחשה של הנקודה הזאת בדו-מימד.
[00:31:46 - 00:31:49] אז יש לנו את האזור הזה,
[00:31:49 - 00:31:53] ששני האזורים האלה שבהם ההסתברות היא יחסית גבוהה,
[00:31:55 - 00:31:57] אז יש לנו נקודות באמת ליד האזורים האלה.
[00:31:58 - 00:32:00] כשאני מוסיף להם רעש אני קצת יוצא מהם, ואז ה-score
[00:32:01 - 00:32:06] הוא אומר לי איך אני צריך לחזור לכיוון שיעלה את ההסתברות,
[00:32:07 - 00:32:09] אבל בכל הנקודות באמצע אני בכלל,
[00:32:10 - 00:32:13] ה-training data שלי לא מכיל נקודות כאן בכלל,
[00:32:14 - 00:32:15] אני בכלל לא אראה נקודות פה.
[00:32:19 - 00:32:19] אז
[00:32:20 - 00:32:26] יכול להיות שההסתברות כאן היא קטנה מאוד מהר, אז יכול להיות שה-score כאן מוגדר, אז בדוגמה כאן ה-score מוגדר כאן,
[00:32:26 - 00:32:28] זה הדוגמה שראינו גם קודם,
[00:32:28 - 00:32:30] יש כיוון שצריך להתקדם אליו,
[00:32:31 - 00:32:34] אבל בגלל שאני לא אקבל שם כמעט דוגמאות בטריינינג סט שלי,
[00:32:35 - 00:32:38] אני לא אצליח ללמוד את ה-score באזורים האלה.
[00:32:38 - 00:32:40] אז יהיו אזורים שבהם ה-score שלי יהיה מדויק,
[00:32:41 - 00:32:44] כי אני אקבל הרבה דוגמאות שם, אבל יהיו הרבה אזורים שבהם ה-score שלי יהיה
[00:32:45 - 00:32:47] מאוד לא מדויק, אז אני לא אוכל ללמוד,
[00:32:47 - 00:32:50] או גם אם הצלחתי זה יהיה מאוד לא יציב,
[00:32:51 - 00:32:54] והסיכוי שאני אצליח לדגום,
[00:32:54 - 00:32:56] להשתמש ב-Lenge אביב דינמיס כדי לייצר דגימה
[00:32:57 - 00:33:01] שתגיע עד למקום טוב, הוא יהיה מאוד מאוד נמוך.
[00:33:04 - 00:33:04] אז איך פותרים את זה?
[00:33:05 - 00:33:09] אחד מהפתרונות זה לא רק להוסיף קצת רעש כדי להתרחל קצת מהעירייה,
[00:33:10 - 00:33:12] אלא להוסיף הרבה רעש.
[00:33:13 - 00:33:16] מה קורה אם אנחנו מוסיפים הרבה רעש? אם אנחנו מוסיפים הרבה רעש, אז עכשיו
[00:33:18 - 00:33:20] הגאוסיאנים האלה בעצם גדלים,
[00:33:21 - 00:33:29] יש הרבה אזורים שמוגדרים בתוך ההתפלגות שלי. תזכור שאנחנו מדברים על ההסתברות של Y, לא ההסתברות של X. זאת אומרת ההסתברות של הדאטה עם הרעש.
[00:33:30 - 00:33:34] אז ההסתברות של הדאטה עם קצת רעש, היא תהיה קצת יותר
[00:33:39 - 00:33:44] מהעירייה המקורית, מהמקומות שבהם ההסתברות של X הייתה
[00:33:44 - 00:33:45] גדולה מ-0.
[00:33:46 - 00:33:47] ואם אני אוסיף הרבה רעש,
[00:33:47 - 00:33:52] אז הרבה יותר ויותר אזורים במרחב שלי יקבלו הסתברות גבוהה.
[00:33:54 - 00:33:54] ואז
[00:33:55 - 00:34:03] אני אוכל, למשל פה בדוגמה הזאת הוספנו הרבה רעש, זאת אומרת אני כבר עכשיו אקבל, כשאני אדגום מתוך ההסתברות הזאת אני אקבל נקודות בכל המרחב,
[00:34:04 - 00:34:08] ולכן אני אוכל ללמוד סקור יחסית מדויק
[00:34:08 - 00:34:09] בכל המרחב.
[00:34:11 - 00:34:11] מה הבעיה?
[00:34:12 - 00:34:16] הבעיה היא שאני אלמד סקור מאוד מדויק, אבל זה יהיה של הסתברות אחרת, של התפלגות אחרת,
[00:34:17 - 00:34:19] של התפלגות של Y עם הרבה רעש.
[00:34:20 - 00:34:23] זה כבר יהיה מאוד רחוק מההתפלגות של X המקורי שאני רציתי.
[00:34:24 - 00:34:25] אז בעצם יש כאן איזשהו trade-off,
[00:34:26 - 00:34:28] שזה מה שאנחנו נפתור,
[00:34:30 - 00:34:32] והדרך לפתור את זה זה בצורה איטרטיבית.
[00:34:33 - 00:34:35] אנחנו מתחילים מרעש מאוד מאוד גבוה,
[00:34:36 - 00:34:37] ולאט לאט
[00:34:37 - 00:34:43] חוזרים לרעש קטן שהוא מתקרב להתפלגות המקורית שלנו של X.
[00:34:43 - 00:34:47] אנחנו נאמן גם עם רעשים מאוד גדולים,
[00:34:48 - 00:34:48] גם עם רעשים קטנים.
[00:34:50 - 00:35:00] ואנחנו נצטרך לדעת מתי להשתמש בכל אחד מההתפלגויות האלה בעצם יהיה סקור פונקשן אחר.
[00:35:01 - 00:35:12] אנחנו נצטרך להשתמש בהדרגה בסקור פונקשן ששייך להתפלגות עם הרעש הגדול, עד שאנחנו מגיעים חזרה להתפלגות עם הרעש הקטן.
[00:35:14 - 00:35:17] אוקיי, אז אנחנו, אוקיי,
[00:35:18 - 00:35:23] כמו שאמרתי קודם אנחנו נצטרך בעצם סקור פונקשן, ללמוד סקור פונקשן לכל אחת מההתפלגויות האלה.
[00:35:24 - 00:35:28] אבל זה לא יעיל וההתפלגויות האלה בסופו של דבר אם נסתכל על שתי התפלגויות קרובות
[00:35:29 - 00:35:33] הן יהיו מאוד דומות אחת לשנייה אז חבל ללמוד סקור פונקשן שונה לכל אחד מהם.
[00:35:33 - 00:35:37] אז הדרך שעושים את זה בדרך כלל זה לומדים את אותו, לומדים סט של,
[00:35:38 - 00:35:43] זאת אומרת לומדים מודל אחד שהוא מקבל בתור input גם את
[00:35:43 - 00:35:51] סיגמא והוא בעצם יודע להוציא את הסקור לפי הסיגמא שהוא קיבל.
[00:35:52 - 00:35:58] אוקיי, אז בדרך אחרת לחשב על זה זה שבעצם אנחנו חולקים את אותו המודל עבור כל הסיגמאות
[00:35:59 - 00:36:02] אבל יש לנו עוד איזה input שאומר לנו באיזה סיגמא אנחנו נמצאים עכשיו.
[00:36:06 - 00:36:09] קוראים לזה כאן noise conditional score network,
[00:36:09 - 00:36:14] אוקיי זה score network שהוא מותנה בדיוק בערך של הרעש שהוספנו.
[00:36:16 - 00:36:22] ואת המודל הזה אנחנו מאמנים על כל סוגי הרעשים שכל פעם אנחנו אומרים לו באיזה רעש השתמשנו.
[00:36:24 - 00:36:27] אוקיי, זה מה שכתוב כאן, התוחלת הפנימית זה מה שראינו קודם,
[00:36:28 - 00:36:31] רק שעכשיו כל פעם אנחנו נותנים סיגמא אחר,
[00:36:32 - 00:36:35] אנחנו גם דוגמים יש כאן תוחלת חיצונית על פני כל הסיגמאות
[00:36:36 - 00:36:42] החל מהסיגמא המאוד מאוד גבוה עד לסיגמא הקטן שהוא כבר די קרוב לאפס
[00:36:44 - 00:36:47] זאת אומרת ההתפלגות של y פה תהיה די קרובה להתפלגות של x
[00:36:49 - 00:36:58] ואני מאמן את אותו מודל על כל הסיגמאות האלה רק שעכשיו למודל הזה יש שני אינפוטים יש גם את התמונה הרועשת כמו שהיה קודם וגם את הסיגמא עצמו כדי שהמודל ידע
[00:36:59 - 00:37:00] באיזה סיגמא הוא נמצא כרגע.
[00:37:02 - 00:37:05] ושוב האאוטפוט של המודל הזה זה ה-εpsilon,
[00:37:05 - 00:37:06] זה הרעש שהוספנו,
[00:37:07 - 00:37:12] שהוא כמו שראינו זה הפוך מהכיוון שבו צריך
[00:37:13 - 00:37:15] להתקדם כדי להגדיל את ההסתברות.
[00:37:19 - 00:37:24] אוקיי אז איך אנחנו ברגע שיש לנו את המודל הזה איך אנחנו משתמשים בו כדי לדגום,
[00:37:24 - 00:37:29] אז הדגימה שמשתמשת במודלים עם רעשים שונים נקראת Unneed Langevin Dynamics
[00:37:30 - 00:37:33] אנחנו מתחילים עם הרבה רעש ואת הסקור של הרבה רעש
[00:37:34 - 00:37:39] אנחנו דוגמים עושים כמה צעדי Langevin Dynamics
[00:37:40 - 00:37:45] ואז אנחנו מניחים שכבר הגענו למקום שההסתברות היא יחסית גבוהה אז גם אם ניקח רעש
[00:37:45 - 00:37:47] נסתכל עכשיו על רעש קצת יותר קטן
[00:37:48 - 00:37:56] אז עדיין נהיה באזורים שבהם ההסתברות היא גבוהה כבר התרחקנו מהאזור שההסתברות היא הייתה ממש אפס
[00:37:57 - 00:38:01] אז גם פה Langevin Dynamics יעבוד נעשה כמה צעדי Langevin
[00:38:01 - 00:38:05] Dynamics פה ואז ככה לאט לאט אנחנו נגיע לאזורים שהם יותר קרובים
[00:38:05 - 00:38:08] לאזורים עם ההסתברות הגבוהה של X המקורי
[00:38:11 - 00:38:15] אז יש פה את האלגוריתם עצמו זה די דומה למה שהיה קודם
[00:38:15 - 00:38:20] רק שיש פה לולאה חיצונית
[00:38:21 - 00:38:22] על ה-Noise Levels
[00:38:23 - 00:38:24] אנחנו מתחילים מ�-Noise Level
[00:38:25 - 00:38:27] מאוד גדול על ה-Noise Level הקטן
[00:38:28 - 00:38:34] מגדירים איזשהו step size יש כל מיני דרכים שאפשר להגדיר את זה יש פה עוד פרמטר בטא שקובע את ה-step size
[00:38:35 - 00:38:39] ואז אנחנו עושים T צעדי Langevin Dynamics
[00:38:41 - 00:38:43] הרבה פעמים בוחרים את ה-T גדול הזה להיות אחד
[00:38:44 - 00:38:45] בעצם אנחנו יכולים
[00:38:46 - 00:38:50] עבור אם יש לנו הרבה הרבה מאוד סיגמאות
[00:38:51 - 00:38:54] אנחנו נגיד שאנחנו החלטנו מראש שאנחנו רוצים לעשות אלף צעדים
[00:38:54 - 00:38:58] אז אנחנו גם נחליט מראש שיש לנו אלף אחרי סיגמא
[00:38:58 - 00:39:02] וכל פעם אנחנו עושים צעד אחד ואנחנו גם משנים את הסיגמא שאנחנו משתמשים בו
[00:39:03 - 00:39:07] עד שאנחנו מגיעים, אם אנחנו מתחילים עם סיגמא מאוד גדול עד שאנחנו מגיעים לסיגמא קטן
[00:39:10 - 00:39:14] אוקיי אז זה דוגמה לתוצאות של שימוש במודל הזה
[00:39:15 - 00:39:22] אוקיי אז זה ממש האלגוריתם שראינו Langevin Dynamics בדיוק כמו שתיארתי אותו רק על תמונות במקרה הזה משמאל זה תמונות של פרצופים
[00:39:23 - 00:39:26] מימין זה תמונות מדאטה סט שנקרא CFAR10
[00:39:28 - 00:39:34] ואתם רואים פה את התהליך של Langevin Dynamics מתחיל מאיזושהי נקודה רנדומלית במרחב הזה שהיא נראית כמו
[00:39:35 - 00:39:35] רעש לבן
[00:39:37 - 00:39:40] ועושים לאט לאט צעדי Langevin Dynamics
[00:39:41 - 00:39:44] עד שמגיעים לנקודה שיש לה הסתברות גבוהה
[00:39:46 - 00:39:49] בהתפלגות המקורית של X של הדאטה X
[00:39:53 - 00:39:56] אוקיי אז שוב אני מסכים מה התהליך שהיה כאן
[00:39:58 - 00:40:00] היה פה קודם כל למידה
[00:40:01 - 00:40:06] עם Objective Function הזה זאת אומרת אנחנו דגמנו X מהדאטה סט שלנו
[00:40:07 - 00:40:09] דגמנו איזשהו Epsilon
[00:40:10 - 00:40:17] וגם דגמנו Sigma איזשהו ערך של איזשהו Sigma בריבוע זה יהיה הווריאנס של הרעש
[00:40:18 - 00:40:18] שנוסיף
[00:40:19 - 00:40:26] אוקיי אז יש לנו בכל באצ' או כל דוגמה שאנחנו לוקחים מהטיינינג סט אנחנו דוגמנו שלושה דברים
[00:40:27 - 00:40:27] את התמונה,
[00:40:28 - 00:40:30] את הרעש שאנחנו נוסיף זאת אומרת הרעש
[00:40:31 - 00:40:34] זה יהיה ממש כמה אנחנו מוסיפים לכל פיקסל
[00:40:35 - 00:40:37] זה יהיה גם משהו בגודל התמונה
[00:40:38 - 00:40:42] וסקלר אחד שהוא יהיה Sigma זה יהיה Standard Deviation של הרעש
[00:40:43 - 00:40:47] ברגע שדגמנו על שלושת המספרים האלה אנחנו מחשבים את ה-objective
[00:40:47 - 00:40:55] function הבא שזה X ועוד Sigma כפול Epsilon זה יהיה ה-Input שאנחנו נותנים לרשת שלנו
[00:40:56 - 00:40:59] אנחנו נותנים לרשת הזאת גם את Sigma אז יש לה שני אינפוטים
[00:41:00 - 00:41:02] ה-Output של הרשת שלנו זה יהיה משהו בגודל של תמונה
[00:41:04 - 00:41:10] אנחנו משווים את זה ל-Epsilon שדגמנו לרעש הזה שהוספנו ומחשבים את השגיאה הריבועית
[00:41:11 - 00:41:14] ועושים Back Propagation לפרמטרים של תטא
[00:41:14 - 00:41:17] ככה חוזרים לזה עבור כל התמונות
[00:41:17 - 00:41:25] מהטרנינג סט שלנו עבור כמה אפוקים עד שאנחנו מקבלים אסתטה שהוא מספיק טוב
[00:41:26 - 00:41:29] שהוא תופס את ה-score בעצם או את מינוס ה-score
[00:41:30 - 00:41:31] עכשיו איך אנחנו משתמשים בו?
[00:41:31 - 00:41:33] על ידי האלגוריתם הזה רנג'מין דיינאמיקס
[00:41:34 - 00:41:40] אנחנו רוצים לדגום דוגמה חדשה אנחנו מתחילים מאיזושהי נקודה רנדומלית
[00:41:41 - 00:41:43] ודוגמים את הסיגמאות
[00:41:43 - 00:41:46] מהגדול לקטן
[00:41:47 - 00:41:54] ועבור כל אחד אנחנו עושים צעד אחד או כמה צעדים של אנג'מין דיינאמיקס זאת אומרת אנחנו מוסיפים אנחנו מחשבים
[00:41:55 - 00:42:00] מכניסים את הנקודה הנוכחית שאנחנו נמצאים בה לתוך הרשת שלנו
[00:42:00 - 00:42:04] היא מוציאה לנו את ה-score זה הכיוון שבו אנחנו צריכים להתקדם
[00:42:05 - 00:42:10] אנחנו מוסיפים את זה ל-XT שלנו וגם מוסיפים קצת רש
[00:42:12 - 00:42:13] וזה נותן לנו את ה-X הבא
[00:42:14 - 00:42:18] ככה אנחנו חוזרים כמה צעדים ומובטח לנו שהדגימות שנקבל יהיו דגימות
[00:42:19 - 00:42:20] מתוך ההתפלגות של X
[00:42:22 - 00:42:23] זה מה שראינו פה
[00:42:28 - 00:42:32] אוקיי אז בעצם מה שהקדמנו עכשיו זה כבר diffusion model
[00:42:32 - 00:42:38] ועכשיו אנחנו נראה דרך אחרת להגיע בדיוק לאותו אלגוריתם
[00:42:44 - 00:42:49] אוקיי אז ראינו בחלק הקודם את ההגדרה בעצם הגענו לדיפיוז'ן מודל דרך
[00:42:49 - 00:42:54] score-based models ועכשיו אנחנו נראה שאותו מודל בדיוק אפשר להגיע אליו
[00:42:55 - 00:42:57] מהכיוון של latent variable models
[00:42:59 - 00:43:07] בסוף אנחנו גם נראה את הקשר ל-Normalizing flows עבור זמן, עבור תהליכים בזמן רציף
[00:43:08 - 00:43:12] אוקיי אז מה שהראינו עד עכשיו זה בעצם היה תוצאה מהמאמר
[00:43:12 - 00:43:17] בעצם סדרה של מאמרים שהסתיימו במאמר הזה מ-2019 שאיראו את המודלים
[00:43:18 - 00:43:25] שמבוססים על score ושם הגיעו לתוצאות גם שראינו בסוף החלק הקודם
[00:43:26 - 00:43:35] אבל זה מ-2019 אבל כבר ב-2015 בעצם יצא מאמר שהגדיר את המודל
[00:43:35 - 00:43:38] לדיפיוז'ן הזה בצורה אחרת
[00:43:40 - 00:43:44] לא יודע אם שמתם לב אבל כבר כמה פעמים שהראינו השוואות של מודלים שונים
[00:43:45 - 00:43:47] ל-likelihood שלהם
[00:43:48 - 00:43:53] אז היה שם איזשהו דיפיוז'ן מודל מ-2015 זה המודל הזה
[00:43:54 - 00:44:01] וב-2020 יצא מאמר שנקרא Denoising Diffusion Probableistic Models או בקיצור DDPM שהראה שבעצם שתי המודלים האלה
[00:44:02 - 00:44:07] הם פחות או יותר זהים מאוד דומים
[00:44:08 - 00:44:16] ושאפשר לחשוב בעצם להראות שהאימון הוא בעצם אותו תהליך אימון ותהליך הדגימה הוא בסופו של דבר אותו תהליך דגימה
[00:44:17 - 00:44:25] אוקיי אז בוא... אה אוקיי סליחה ועוד נקודה זה ששנה אחרי זה יצא מאמר שהראה שאפשר גם לחשוב על כל הדבר הזה בתור
[00:44:28 - 00:44:31] איזשהו פתרון נומרי לתהליך
[00:44:32 - 00:44:33] שקורה בזמן רציף
[00:44:34 - 00:44:41] אז בסוף השיעור עכשיו אני גם אדבר על זה אבל בקצרה לא ניכנס לעומק של זה
[00:44:43 - 00:44:44] אוקיי
[00:44:45 - 00:44:47] אז איך המודל הזה מוגדר?
[00:44:48 - 00:44:53] DDPM מוגדר ככה יש לנו שני תהליכים תהליך אחד שנקרא תהליך הדיפוזיה,
[00:44:53 - 00:44:57] דיפיוז'ן שקוראים לו פה גם ה-Forward Diffusן Process
[00:44:58 - 00:45:01] ותהליך אחד שהוא תהליך הפוך ה-Reverse
[00:45:01 - 00:45:03] שהוא נקרא Denoising Process
[00:45:04 - 00:45:07] אוקיי אז מה זה ה-Diffusion Process? Diffusion Process מתחיל מ-Data,
[00:45:07 - 00:45:13] מ-תמונה במקרה שלנו ולאט לאט מוסיף לתמונה רעש
[00:45:13 - 00:45:16] עד שאנחנו מגיעים למצב שיש לנו רק רעש
[00:45:17 - 00:45:20] אין לנו יותר סיגנל נשארנו רק עם הרעש אז זה תהליך ה-Forward
[00:45:21 - 00:45:22] זה תהליך ה-Defusion
[00:45:23 - 00:45:24] ה-Reverse
[00:45:25 - 00:45:27] זה תהליך שמתחיל מרעש
[00:45:28 - 00:45:31] ולאט לאט מוריד את הרעש
[00:45:31 - 00:45:34] מוסיף יותר ויותר סיגנל עד שאנחנו מגיעים
[00:45:35 - 00:45:39] למשהו שהוא נראה כמו תמונה מהדאטה שלנו
[00:45:40 - 00:45:44] התהליך הגנרטיבי בעצם שמייצר דאטה זה התהליך ההפוך ה-Reverse
[00:45:45 - 00:45:52] יש כל מיני פורמולציות שבהם הכיוונים האלה הם הפוכים מה שנקרא Forward הוא הפוך ממה שנקרא ה-Reverse
[00:45:53 - 00:46:01] אבל כשמגדירים את זה ככה בפורמולציה של DDPM אז זה הדרך Forward זה מהדאטה לרעש
[00:46:02 - 00:46:04] ו-Reverse זה מהרעש
[00:46:04 - 00:46:05] חזרה לדאטה
[00:46:07 - 00:46:11] אוקיי אז יש לנו אנחנו מגדירים את שני התהליכים האלה תכף נדבר עליהם קצת יותר לעומק
[00:46:12 - 00:46:17] הנקודה החשובה זה שהתהליך ה-Forward הוא קבוע זאת אומרת אנחנו קובעים אותו מראש
[00:46:18 - 00:46:21] והתהליך ההפוך הוא התהליך הנלמד
[00:46:29 - 00:46:31] אז בואו נדבר על התהליך
[00:46:31 - 00:46:47] ה-Difusion קדימה אוקיי אז בעצם מה שקורה זה שבכל אנחנו מחלקים את התהליך הזה לנקודות זמן ל-T נקודות זמן נגיד T סטנדרטי הוא אלף
[00:46:48 - 00:46:51] אלף נקודות זמן ובעצם
[00:46:53 - 00:46:59] בכל נקודת זמן אנחנו לוקחים את הנקודה שלנו מהנקודה הקודמת
[00:47:00 - 00:47:01] נקודת הזמן הקודמת את הדאטה מהנקודה
[00:47:01 - 00:47:02] הזמן הקודמת
[00:47:03 - 00:47:06] ומוסיפים לה רעש גאוסיאני
[00:47:07 - 00:47:12] איך זה קורה אז יש פה בדיוק את ההסתברות שעושה את זה
[00:47:13 - 00:47:17] זה הסתברות מותנית של XT בהינתן XT מינוס 1
[00:47:18 - 00:47:19] זו התפלגות נורמלית
[00:47:21 - 00:47:24] מפני המשתנה XT שהתוחלת שלו
[00:47:25 - 00:47:27] זה מה שכתוב כאן זה ה-XT מינוס 1
[00:47:28 - 00:47:29] זה הדאטה
[00:47:30 - 00:47:30] בצעד הקודם
[00:47:31 - 00:47:33] אבל קצת מונחת
[00:47:34 - 00:47:35] בטא זה מספר בין 0 ל-1
[00:47:36 - 00:47:39] והדבר הזה הוא נותן לנו איזושהי הנחתה של הסיגנל
[00:47:41 - 00:47:42] אז זה התוחלת
[00:47:43 - 00:47:44] ויש גם רעש
[00:47:44 - 00:47:45] שהוא בטא T
[00:47:46 - 00:47:52] אז לכל נקודת זמן אנחנו יכולים להוסיף רעש בסטיית תקן שונה
[00:47:52 - 00:47:53] שמוגדרת על ידי בטא T
[00:48:01 - 00:48:14] אז זה התהליך שמוסיף לנו רעש שימו לב שהתהליך הזה הוא תהליך מרקובי מה זה אומר? זה אומר שבזמן T מסוים אני רק צריך לדעת מה קרה ב-T מינוס 1
[00:48:15 - 00:48:19] כדי להגדיר את ההתפלגות החדשה אני לא צריך לדעת מה קרה בהתחלה
[00:48:21 - 00:48:29] אוקיי אז זה מה שכתוב פה זה משרה גם התפלגות משותפת זאת אומרת בהינתן X0 אני יכול להגיד מה ההתפלגות של כל
[00:48:31 - 00:48:33] X'ים מ-1 עד T
[00:48:34 - 00:48:37] בתור המכפלה
[00:48:38 - 00:48:39] של ההתפלגויות האלה
[00:48:41 - 00:48:49] אני כל פעם יכול לחשב את ה-XT הבא הוא ייתן לי את התוחלת של ההתפלגות הבאה אז ההתפלגות הבאה היא תהיה מוגדרת
[00:48:50 - 00:48:57] זה מאפשר לי בעצם להגדיר את ההתפלגות המשותפת של כל ה-X'ים מ-1 עד T בהינתן X0
[00:48:58 - 00:49:01] אוקיי אז זה ה-Fורוורד
[00:49:06 - 00:49:09] עוד נקודה על ה-Fורוורד זה שבגלל שהכל כאן גאוסיאני
[00:49:10 - 00:49:12] אני יכול להגדיר
[00:49:13 - 00:49:20] זה מאפשר לנו בעצם להגדיר קפיצות כאלה זאת אומרת אני יכול להגדיר את X4 בהינתן X0
[00:49:21 - 00:49:25] בלי לעבור ב-X1, X2 ו-X3
[00:49:25 - 00:49:30] אוקיי אז אם אתם זוכרים זה בשיעורים הראשונים ראינו שיש לנו סכום של
[00:49:31 - 00:49:32] גאוסיאנים
[00:49:34 - 00:49:36] אז התוצאה היא גם גאוסיאנים
[00:49:39 - 00:49:42] והווריאנס זה פשוט יהיה סכום הווריאנסים
[00:49:42 - 00:49:46] אוקיי כל גאוסיאן כאן הוא בעצם בלתי תלוי בגאוסיאן הקודם
[00:49:47 - 00:49:56] אהמ.. והווריאנס ש.. לכן הווריאנס שמתקבל זה פשוט סכום הווריאנסים
[00:49:58 - 00:50:03] אז את הדבר הזה אפשר להגדיר ככה אז אוקיי אז יש פה את ההסתברות של XT בהינתן X0
[00:50:07 - 00:50:16] זה מודע ל-XT התוחלת של זה זה יהיה X0 מונחת על ידי מספר חדש שנקרא Alpha T
[00:50:17 - 00:50:23] הוא בעצם שווה למכפלה של כל ההנחתות שהיה לנו עד עכשיו אוקיי אז אם קודם
[00:50:23 - 00:50:31] הנחתנו את XT בשורש של 1 מינוס Beta T כל פעם אנחנו מלחיתים את זה ב-1 מינוס Beta T נוסף
[00:50:31 - 00:50:38] בסך הכל אנחנו מקבלים מכפלה של כל ההנחתות האלה וזה בדיוק מה שמוגדר כאן בתור Alpha T בר
[00:50:40 - 00:50:42] יש כאן כמה כמה גדרות לפעמים קצת מבלבל
[00:50:42 - 00:50:46] אני חושב שעדיין לא כתוב כאן Alpha T הוא בדרך כלל
[00:50:46 - 00:50:54] מוגדר בתור 1 מינוס Beta T ו Alpha T בר זה המכפלה של כל ה Alpha T עד עכשיו זאת אומרת המכפלה של כל ה-1 מינוס
[00:50:55 - 00:50:57] Beta T שהיו עד זמן T
[00:50:58 - 00:51:02] זה אומר כמה סך הכל הנחתנו את הסיגנל המקורי
[00:51:03 - 00:51:08] אוקיי אז זה מה שמתקבל כאן זה התוחלת בעצם של X0
[00:51:08 - 00:51:12] והווריאנס יוצא 1 פחות
[00:51:13 - 00:51:13] לדבר הזה
[00:51:14 - 00:51:17] אוקיי אז זה הווריאנס שאנחנו מוסיפים
[00:51:19 - 00:51:25] הווריאנס של הרעש שאנחנו מוסיפים כדי לקפוץ באופן ישיר מ-X0 ל-XT
[00:51:27 - 00:51:34] אוקיי אז מה זה אומר אם זה הווריאנס איך אני יכול עכשיו לדגום בהינתן שיש לי את X0 איך אני יכול לדגום דגימה מ-XT
[00:51:35 - 00:51:38] אז זה גם משהו שראינו ראיתם אני חושב בתרגיל הראשון
[00:51:38 - 00:51:42] אני ככה דוגמים גאוסיין אוקיי זה התוחלת
[00:51:43 - 00:51:45] אוקיי שורש של Alpha T-BAR
[00:51:46 - 00:51:48] כפול X0 ועוד
[00:51:49 - 00:51:55] הסטיית תקן כפול רעש גאוסייאני סטנדרטי
[00:51:55 - 00:52:00] מתוחלת 0 וקובריאנס I
[00:52:02 - 00:52:05] זה Epsilon זה הרעש הגאוסייאני הזה ואני מכפיל אותו פשוט
[00:52:06 - 00:52:07] בסטיית תקן
[00:52:07 - 00:52:10] שזה שורש של הווריאנס
[00:52:10 - 00:52:13] אוקיי עכשיו כל הדבר הזה מוגדר עבור
[00:52:14 - 00:52:21] Beta T שונים שבטא T זה בעצם הקצב שבו אנחנו מוסיפים את הרעש ב-Fore-Work-Process הזה
[00:52:22 - 00:52:25] בכל איטרציה, Beta T זה כמה רעש אנחנו מוסיפים בכל איטרציה
[00:52:26 - 00:52:27] ואפשר לבחור כל מיני
[00:52:29 - 00:52:29] Scheduling
[00:52:30 - 00:52:30] כל מיני
[00:52:34 - 00:52:36] דרכים שונות להגדיל את Beta T
[00:52:36 - 00:52:43] אבל באופן כללי אנחנו רוצים להגיע למצב שבסופו של דבר Alpha T Bar כש-T גדול
[00:52:44 - 00:52:47] זאת אומרת שעברנו את כל האלף איטרציות שלנו
[00:52:48 - 00:52:53] זה מספר שהולך וקרב לאפס ככה שבעצם הנחתנו את הסיגנל עד כדי כך שהוא נעלם
[00:52:54 - 00:52:59] והוספנו רעש שיהיה 1 פחות הדבר הזה זאת אומרת הרעש שמתווסף לנו
[00:53:00 - 00:53:02] הוא פשוט רעש גאוסייאני סטנדרטי
[00:53:03 - 00:53:03] עם סטיית תקן 1
[00:53:04 - 00:53:12] אוקיי, אז אנחנו מתחילים מהדאטה שלנו ומסיימים ברעש גאוסיאני עם סטיית תקן 1
[00:53:13 - 00:53:19] אוקיי, וה-Covarense זה I, היא אלכסונית, זאת אומרת שכל הפיקסלים פה הם בלתי תלויים אחד בשני
[00:53:23 - 00:53:24] אוקיי
[00:53:24 - 00:53:31] איך זה נראה אם אנחנו חושבים על ההתפלגות של הדאטה אז כאן זו המחשה של זה בהנחה ש-XT היה חד-ממדי
[00:53:31 - 00:53:33] אז אם XT היה חד-ממדי
[00:53:33 - 00:53:35] בעצם הוא-והוא לא היה גאוסיאן, הוא היה איזושהי התפלגות מוזרה,
[00:53:37 - 00:53:39] כל פעם היינו מוסיפים לו גאוסיאן,
[00:53:39 - 00:53:44] בעצם דרך לחשוב על זה זה שה-התפלגות הולכת ו-הופכת להיות גאוסיאנית.
[00:53:45 - 00:53:47] מוסיפים עוד ועוד גאוסיאנים עד שבסוף
[00:53:48 - 00:53:49] הסיגנל נעלם,
[00:53:50 - 00:53:55] ואנחנו נשארים רק עם הגאוסיאן המצטבר של כל הגאוסיאנים שהוספנו בדרך.
[00:53:57 - 00:53:59] אז-זה נוח להסתכל על המחשה החד-ממדית הזאת,
[00:53:59 - 00:54:03] אבל לפעמים יש משהו מהותי כאן שמתפספס,
[00:54:04 - 00:54:06] אז אני מראה את זה כאן גם בדו-ממד.
[00:54:07 - 00:54:10] זה קשור למה שאמרתי קודם, שבסופו של דבר הדאטה הוא בלתי תלוי.
[00:54:12 - 00:54:16] אז כאן אנחנו רואים למשל איזושהי התפלגות התחלתית דו-ממדית,
[00:54:16 - 00:54:19] שכאן הדאטה הוא כן תלוי, זאת אומרת יש כאן איזושהי קורולציה
[00:54:22 - 00:54:23] באזור מסוים שלילית,
[00:54:25 - 00:54:27] באזור השני אולי אין קורולציה,
[00:54:27 - 00:54:30] ובכל מקרה יש כאן איזושהי תלות בין שני המשתנים שלי
[00:54:32 - 00:54:34] שמוגדרים כאן ציר ה-X וציר ה-Y,
[00:54:35 - 00:54:38] וככל שאני מוסיף להתפלגות הזאת גאוסיאן,
[00:54:39 - 00:54:45] יותר ויותר גאוסיאנים שהם התפלגות בלתי תלויה בין המשתנים,
[00:54:46 - 00:54:51] הדאטה המקורי נעלם, ההתפלגות המקורית נעלמת, ואני נשאר רק עם ההתפלגות של הרעש
[00:54:52 - 00:54:54] הסתבר שלי שהיא בלתי תלויה.
[00:54:54 - 00:54:58] אז אני מקבל עכשיו סיגנל שהוא בלתי תלוי, ה-X ל-Y כאן הם בלתי תלויים.
[00:54:58 - 00:55:00] זאת אומרת מידע על ה-X לא ייתן לי שום מידע על ה-Y.
[00:55:01 - 00:55:04] אז ככל שאני מתקדם בתהליך הדיפוזי הזה,
[00:55:05 - 00:55:09] הממדים השונים נהיים יותר ויותר בלתי תלויים אחד מהשני.
[00:55:11 - 00:55:13] שוב, זו המחשה באחד מימד ובדו-מימד,
[00:55:14 - 00:55:16] בפועל אנחנו עובדים בתמונות בממד הרבה הרבה יותר גבוה.
[00:55:17 - 00:55:23] המשמעות של זה זה שאם בהתחלה הפיקסלים הם תלויים אחד בשני, למשל פיקסלים שכנים נוטים לקבל ערכים מאוד מאוד דומים,
[00:55:24 - 00:55:31] במהלך הזה הפיקסלים יהיו בלתי תלויים אחד בשני. הפיקסלים, אפילו שהם יהיו מאוד קרובים, צמודים אחד לשני, יכולים לקבל ערכים שונים לגמרי.
[00:55:34 - 00:55:35] אוקיי,
[00:55:35 - 00:55:39] איך אנחנו יכולים לכמת את ההתפלגות השולית הזאת?
[00:55:40 - 00:55:42] זאת אומרת, מה קורה בכל צעד?
[00:55:43 - 00:55:45] מתחילים מההתפלגות של הדאטה, זה ההתפלגות של X0,
[00:55:46 - 00:55:50] מסיימים בהתפלגות של XT. איך נראות ההתפלגויות האלה בדרך?
[00:55:50 - 00:55:56] אז ראינו איך אנחנו יכולים לייצר דגימות מההתפלגויות האלה, ואיך ההתפלגויות עצמן נראות.
[00:55:57 - 00:56:02] אז זה מוגדר על ידי האינטגרל הזה, זה פשוט
[00:56:03 - 00:56:05] ההסתברות השלמה,
[00:56:05 - 00:56:09] זה ההסתברות של X0 כפול ההסתברות של XT בהינתן X0,
[00:56:09 - 00:56:13] זה פשוט ההסתברות המשותפת של X0 ו-XT,
[00:56:13 - 00:56:17] ואני עושה אינטגרל על כל ה-X0, אני נשאר אז רק עם XT.
[00:56:17 - 00:56:18] אוקיי?
[00:56:19 - 00:56:24] ובעצם מה שאנחנו רואים שקורה כאן זה שאנחנו בעצם עושים קונבולוציה
[00:56:24 - 00:56:25] בין ההתפלגות
[00:56:27 - 00:56:27] של הדאטה
[00:56:28 - 00:56:30] לקרנל כזה,
[00:56:30 - 00:56:31] כמו שהיינו קודם,
[00:56:31 - 00:56:32] הקרנל הזה הוא גאוסיאן,
[00:56:32 - 00:56:34] אנחנו עושים פשוט קונבולוציה עם גאוסיאן.
[00:56:37 - 00:56:41] והגאוסיאן הזה קוראים לו לפעמים ה-diffusion kernel.
[00:56:42 - 00:56:46] ככל ש-T הוא מספר יותר גבוה,
[00:56:46 - 00:56:49] אז הקרנל הזה יש לו וריאנס יותר גדול בעצם,
[00:56:50 - 00:56:54] והוא יותר ויותר מטשטש את ההתפלגות הזאתי,
[00:56:55 - 00:57:02] ככה שהדאטה נהיה ביותר ויותר רעש נקי ופחות סיגנל.
[00:57:12 - 00:57:15] אז זה היה הכיוון ה-forward.
[00:57:16 - 00:57:19] קדימה של ה-diffusion, עכשיו אנחנו נדבר על הכיוון ההפוך.
[00:57:20 - 00:57:23] בכיוון ההפוך אנחנו יכולים בעצם,
[00:57:24 - 00:57:27] מה שאנחנו רוצים לעשות, אנחנו רוצים להתחיל מאיזושהי נקודה
[00:57:28 - 00:57:32] בגאוסיאן הזה הנקי שהגענו אליו בסוף.
[00:57:34 - 00:57:39] זה לא נכון לקרוא לזה נקי, הגאוסיאן שמתאר כאן בעצם רעש,
[00:57:40 - 00:57:43] רעש איזוטרופי מוחלט.
[00:57:44 - 00:57:47] לאט לאט אנחנו רוצים לדגום דגימות
[00:57:49 - 00:57:53] מתוך ההתפלגויות האלה שהופכות להיות בסופו של דבר ההתפלגות של הדאטה.
[00:57:55 - 00:58:01] אנחנו רוצים לקחת דגימה מכאן ולשנות אותה, לשנות את הערך שלה,
[00:58:02 - 00:58:05] ככה שבסופו של דבר נקבל דגימה מההתפלגות המקורית של הדאטה.
[00:58:09 - 00:58:13] השאלה היא איך אנחנו יכולים לקחת דגימה מכאן ולהשתמש בה,
[00:58:13 - 00:58:18] כדי לדגום בעצם את הכיוון ההפוך את xt מינוס 1
[00:58:19 - 00:58:20] בהינתן xt
[00:58:22 - 00:58:23] אז
[00:58:23 - 00:58:28] באופן כללי ההתפלגות הזאת היא לא משהו שאפשר לחשב בצורה מדויקת
[00:58:30 - 00:58:35] והסיבה היא שהדבר הזה תלוי בעצם בסופו של דבר ב-x0
[00:58:36 - 00:58:36] זאת אומרת
[00:58:37 - 00:58:42] המשמעות של ללכת צעד אחד אחורה זה להתקרב בקצת להתפלגות של x0
[00:58:42 - 00:58:46] x0 זה ההתפלגות של הדאטה וההתפלגות לא ידועה אז אין לנו באמת דרך
[00:58:47 - 00:58:49] לחשב את ההתפלגות הזאת
[00:58:50 - 00:58:53] אבל אם אנחנו מניחים שאנחנו עושים צעד מאוד קטן
[00:58:54 - 00:58:56] אנחנו יכולים להניח
[00:58:56 - 00:58:59] שההתפלגות הזאת היא בעצמה גאוסיאן
[00:59:00 - 00:59:04] מה שלא נכון בצורה מדויקת אנחנו יכולים לקרב את ההתפלגות הזאת על ידי גאוסיאן
[00:59:04 - 00:59:08] ובעצם מה שנשאר לנו זה לחשב את התוחלת של הגאוסיאן הזה
[00:59:08 - 00:59:14] אוקיי, אז עבור כל דגימה, בהינתן הדגימה שקיבלתי, אני יכול לחשב מה התוכלת
[00:59:14 - 00:59:16] של ההתפלגות
[00:59:17 - 00:59:20] של הצעד t-1 בהינתן t.
[00:59:23 - 00:59:31] אוקיי, אז זה מה שאנחנו נעשה, ובעצם מה שזה אומר, זה שאומר שאנחנו צריכים ללמוד,
[00:59:33 - 00:59:37] אנחנו נשתמש בעצם בדאטה שנייצר,
[00:59:37 - 00:59:45] ובאימון כדי ללמוד את הגאוסיאן הזה של t-1 בהינתן t.
[00:59:47 - 00:59:50] אם עד עכשיו קראנו לתהליך שהולך קדימה קראנו לו q,
[00:59:53 - 00:59:55] זה משהו שאנחנו מגדירים מראש
[00:59:56 - 00:59:58] והוא לא הולך להיות משהו, אין לו פרמטרים.
[00:59:59 - 01:00:00] עכשיו אנחנו,
[01:00:01 - 01:00:03] את התהליך ההפוך אנחנו נקרא לו, נסמן אותו פה ב-p,
[01:00:04 - 01:00:05] p-teta,
[01:00:06 - 01:00:07] זה p שיש לו פרמטרים.
[01:00:07 - 01:00:09] teta שזה הפרמטרים שאנחנו נלמד.
[01:00:10 - 01:00:13] כמו שאמרתי אנחנו נניח שההתפלגות הזאת היא גאוסיאן,
[01:00:15 - 01:00:20] ושכל מה שנשאר לנו זה ללמוד את התוחלת של הגאוסיאן הזה בהינתן שיש לנו דגימה
[01:00:21 - 01:00:23] של צעד אחד
[01:00:26 - 01:00:30] קדימה, בהינתן xt אנחנו רוצים את ההתפלגות של xt-1
[01:00:31 - 01:00:35] אנחנו נניח שזה גאוסיאן וכל מה שנשאר לנו ללמוד מהגאוסיאן הזה זה את התוחלת שלו.
[01:00:37 - 01:00:39] אז הדבר שאנחנו נלמד,
[01:00:40 - 01:00:45] הרשת שלנו שאנחנו נלמד, זה יהיה בעצם הרשת שבהינתן XT נותנת לנו את התוחלת
[01:00:46 - 01:00:47] של XT מינוס אחד.
[01:00:50 - 01:00:52] תכף אנחנו נראה איך זה קורה.
[01:00:56 - 01:01:07] כן, מה שכתוב כאן שאפשר לחשוב על הדבר הזה בתור צעד של דנויזינג, אנחנו מנסים קצת להוריד את הרעש מהתמונה, נכון? כי כל פעם שאנחנו הולכים אחורה בזמן אנחנו אמורים להתקרב
[01:01:07 - 01:01:14] לדאטה הענקי בלי הרעש ובעצם כדי לעשות את זה נכון אנחנו צריכים ללמוד
[01:01:16 - 01:01:20] לזהות את הרעש הזה ולהוריד רק אותו בלי להוריד את הסיגנל.
[01:01:23 - 01:01:30] אפשר לחשוב על המודל שאנחנו לומדים כאן בתור דנויזינג אוטונקודר,
[01:01:31 - 01:01:36] אוטונקודר שהוא מקבל את הסיגנל עם הרעש והוא צריך להחזיר את הסיגנל בלי הרעש
[01:01:36 - 01:01:39] וזה ממש בדרך כלל עם יוניטס, תכף נראה
[01:01:40 - 01:01:40] מה זה אומר.
[01:01:44 - 01:01:47] אוקיי, אז איך זה קורה? איך הגענו לדבר הזה?
[01:01:48 - 01:01:55] אז אפשר לחשוב על כל הדבר הזה בתור מודל עם משתנה חבוי,
[01:01:56 - 01:01:57] מה המשתנה החבוי כאן?
[01:01:57 - 01:02:01] אם נסתכל רגע על מה שאנחנו רואים כאן, יש לנו כאן הרבה איקסים,
[01:02:01 - 01:02:02] x0 עד xt,
[01:02:03 - 01:02:05] x0 זה התמונות שאנחנו רואים,
[01:02:05 - 01:02:06] שיש לנו בטרנינגסט,
[01:02:07 - 01:02:08] כל שאר האיקסים
[01:02:09 - 01:02:11] הם בעצם המשתנה החבוי שלנו.
[01:02:12 - 01:02:14] יש לנו אפשר לחשוב על הרבה משתנים חבויים,
[01:02:15 - 01:02:19] x1 עד xt אנחנו יכולים להתייחס אליהם בתור משתנים חבויים,
[01:02:20 - 01:02:21] כי אנחנו לא יודעים את הערכים שלהם
[01:02:22 - 01:02:24] עבור x0 ספציפי.
[01:02:29 - 01:02:29] אוקיי,
[01:02:30 - 01:02:35] אז מה שבעצם מציעים לעשות במודל הזה,
[01:02:35 - 01:02:40] זה לחשוב על דבר הזה בתור VAE ולאמן אותו עם אלבו.
[01:02:41 - 01:02:43] אני רק רוצה לחזור עוד פעם, שוב פעם.
[01:02:44 - 01:02:48] אחורה.
[01:02:49 - 01:02:52] בעצם, אוקיי, אפשר לחשוב על ה...
[01:02:53 - 01:02:55] יש לנו כאן את ה-Forward וה-Reverse,
[01:02:56 - 01:03:00] אפשר לחשוב על זה כמו שראינו ב-VAE,
[01:03:01 - 01:03:04] שיש לנו את התהליך הגנרטיבי,
[01:03:05 - 01:03:06] שלוקח את ה-Latence,
[01:03:07 - 01:03:07] במקרה הזה יש לנו הרבה latency,
[01:03:08 - 01:03:09] ומייצר מהם
[01:03:09 - 01:03:10] את הדאטה,
[01:03:11 - 01:03:15] ואימנו את המודל הזה על ידי זה שאימנו גם מודל הפוך,
[01:03:15 - 01:03:19] שלקח דאטה ונתן לנו את ההתפלגות
[01:03:19 - 01:03:20] על ה-Latence,
[01:03:21 - 01:03:24] וזה קראנו inference network או posterior network,
[01:03:26 - 01:03:30] אימנו רשת שלקחה את הדאטה ונתנה לנו את ההתפלגות
[01:03:30 - 01:03:33] על ה-Latence שלנו,
[01:03:34 - 01:03:36] וקראנו להתפלגות הזאת Q,
[01:03:36 - 01:03:39] ולהתפלגות הגנרטיבית קראנו P,
[01:03:39 - 01:03:40] אז גם בדיוק פה זו הסיבה שקוראים
[01:03:41 - 01:03:45] לכיוון ה-Forward Q ולכיוון ה-Reverse P,
[01:03:45 - 01:03:48] רק למה שקוראים Forward ומה שקוראים Reverse זה קצת הפוך
[01:03:49 - 01:03:54] מאיך שבדרך כלל חושבים על VE. עוד נקודה חשובה שכאן ה-Forward,
[01:03:54 - 01:03:55] האינפרנס,
[01:03:55 - 01:03:58] מה שנותן לנו את ההתפלגות על ה-Latence בהינתן הדאטה,
[01:03:59 - 01:03:59] הוא קבוע,
[01:04:00 - 01:04:01] הוא לא משהו נלמד.
[01:04:01 - 01:04:02] ב-VE למדנו
[01:04:03 - 01:04:04] את ה-Posterior Network הזה,
[01:04:05 - 01:04:06] פה אנחנו פשוט מניחים
[01:04:07 - 01:04:10] שזה סדרה של גאוסיאנים.
[01:04:12 - 01:04:12] אוקיי,
[01:04:14 - 01:04:16] אז בהינתן שזה המצב,
[01:04:16 - 01:04:18] מה זה אומר ללמוד את ה-Lbow?
[01:04:18 - 01:04:26] אז אתם זוכרים, ה-Lbow זה משהו שחוסם את ההתפלגות,
[01:04:26 - 01:04:28] את ההסתברות, סליחה, מלמטה,
[01:04:29 - 01:04:32] אבל אם זה מינוס ההסתברות אז זה חוסם את זה מלמעלה,
[01:04:32 - 01:04:36] אוקיי, אז זה בכיוון שאנחנו חושבים על זה בתור Loss, משהו שאנחנו רוצים למזער,
[01:04:36 - 01:04:38] אז זה חסם עליון ל-Loss שלנו.
[01:04:39 - 01:04:40] אוקיי, ואיך הוא נראה?
[01:04:41 - 01:04:42] הוא נראה כמו תוחלת
[01:04:43 - 01:04:47] לפי ה-Posterior של לוג,
[01:04:48 - 01:04:52] של ההתפלגות המשותפת
[01:04:53 - 01:04:55] של הדאטה שאנחנו רואים
[01:04:56 - 01:04:56] והדאטה
[01:04:57 - 01:04:57] ה...
[01:05:01 - 01:05:02] החבוי, ה-Latent Variable,
[01:05:03 - 01:05:08] זה כל ה-X'ים במקרה הזה, גם X0 שהוא הדאטה שאנחנו רואים וגם X1 ה-T שזה המשתנה החבוי,
[01:05:09 - 01:05:12] חלקי הפוסטריאור שלנו,
[01:05:12 - 01:05:15] זאת אומרת זה פוסטריאור זה המשתנה החבוי,
[01:05:15 - 01:05:25] כל המשתנים החבויים בהינתן המשתנה שאנחנו רואים, X0. זה ככה גם הגדרנו, אם אתם לא צורכים תסתכלו על ההרצאה של VAE,
[01:05:25 - 01:05:26] ככה אנחנו הגדרנו את ה-L בו.
[01:05:27 - 01:05:31] ההסתברות, ה-KL Divergence בין ההסתברות המשותפת,
[01:05:34 - 01:05:45] סליחה, זה לא בדיוק KL Divergence, כי יש פה משתנים אחרים, אבל זה הלוג של היחס בין ההסתברות המשותפת של ה-X ו-Z,
[01:05:45 - 01:05:47] קראנו לזה ב-VAE,
[01:05:48 - 01:05:50] חלקי Q של Z בהינתן X.
[01:05:51 - 01:05:54] זה תפקיד של Z, ממלא כאן ה-X'ים 1 עד T.
[01:05:55 - 01:05:58] אוקיי, את הדבר הזה גם אפשר לכתוב ככה,
[01:05:59 - 01:06:02] זאת אומרת אנחנו מוציאים את XT החוצה,
[01:06:03 - 01:06:04] ואנחנו נשארים עם
[01:06:08 - 01:06:09] סכום, בגלל שיש כאן,
[01:06:09 - 01:06:19] אפשר לפרק את ההסתברויות האלה בתור מכפלות של הסתברויות, שוב זה בגלל התכונה המרקובית, אנחנו יכולים לפרק את השרשרת הזאת,
[01:06:19 - 01:06:22] כל אחד הוא בהינתן הקודם.
[01:06:23 - 01:06:25] אז יש לנו כאן סכום,
[01:06:25 - 01:06:26] של לוגים,
[01:06:26 - 01:06:30] של למעלה יש לנו את P של XT-1 בהינתן XT,
[01:06:32 - 01:06:34] זה הכיוון שהולך אחורה,
[01:06:34 - 01:06:35] ולמטה יש לנו את XT,
[01:06:36 - 01:06:38] Q של XT בהינתן XT-1,
[01:06:38 - 01:06:39] זה הכיוון שהולך קדימה.
[01:06:43 - 01:06:47] אוקיי, תכף אנחנו נראה שאת הדבר הזה אנחנו גם יכולים לפרק עוד יותר,
[01:06:48 - 01:06:48] בצורה הזאת,
[01:06:50 - 01:06:53] וזו הצורה שאנחנו נשתמש בה כדי לעשות את האימון,
[01:06:54 - 01:06:56] אז יש לנו כאן בעצם קייל דייברג'נס בין
[01:06:58 - 01:07:04] Q של ה-X של הרעש בהינתן התמונה X0,
[01:07:06 - 01:07:08] לבין P של X הרעש,
[01:07:09 - 01:07:11] P של X של הרעש זה פשוט,
[01:07:11 - 01:07:13] אנחנו מניחים שהוא גאוסיאן פשוט.
[01:07:15 - 01:07:18] אז זה איבר שהוא קצת פחות חשוב,
[01:07:18 - 01:07:19] גם האיבר הזה הוא קצת פחות חשוב,
[01:07:19 - 01:07:23] זה רק על האיטרציה האחרונה של מה קורה ב-X0 בהינתן X1,
[01:07:24 - 01:07:29] ובעצם האיברים החשובים כאן זה הסכום הזה עבור כל ה-Tים,
[01:07:30 - 01:07:32] שמ-2 עד T גדול,
[01:07:32 - 01:07:36] שפה יש לנו קייל דייברג'נס בין Q
[01:07:37 - 01:07:40] ו-Q הזה של XT-1 בהינתן XT ו-X0,
[01:07:40 - 01:07:43] לבין מה שאנחנו רוצים ללמוד
[01:07:43 - 01:07:49] שזה P תטא של XT-1 בהינתן XT.
[01:07:50 - 01:07:53] זה הדבר הזה שאנחנו רוצים ללמוד, אנחנו רוצים ללמוד מה ההסתברות של
[01:07:54 - 01:07:56] ה-X הקודם בהינתן ה-X הבא.
[01:07:58 - 01:08:00] שוב, בניגוד ל-VE אנחנו מניחים ש-Q,
[01:08:01 - 01:08:05] ב-VE אנחנו למדנו גם את P וגם את Q, פה אנחנו מניחים ש-Q הוא קבוע ואנחנו יודעים אותו,
[01:08:06 - 01:08:09] וכל מה שנשארנו זה רק לאמן את P, את ה-forward.
[01:08:13 - 01:08:20] אוקיי, אז יש כאן איזה נקודה שצריך לשים לב,
[01:08:20 - 01:08:25] שה-Q שאנחנו קבענו אותו מראש זה לא ה-Q שכתוב כאן,
[01:08:26 - 01:08:28] זה לא Q של XT-1 בהינתן XT ו-X0,
[01:08:29 - 01:08:31] אלא זה רק Q של,
[01:08:31 - 01:08:38] זה Q הפוך, זה Q של X, זה מה שכתוב כאן, זה Q של XT בהינתן XT-1.
[01:08:39 - 01:08:41] ה-Q הזה זה פשוט רעש גאוסיאני,
[01:08:42 - 01:08:50] עם איזושהי, עם Variants Beta T. אבל כאן ב-objective function שאנחנו רוצים למזער,
[01:08:51 - 01:08:55] יוצא לנו התפלגות אחרת, שתכף נראה שזו התפלגות שאנחנו יכולים לחשב אותה.
[01:08:56 - 01:09:01] זו התפלגות של XT-1 בהינתן XT, שזה 1 אחריו,
[01:09:02 - 01:09:04] ו-X0, שזה הדאטה שממנו התחלנו.
[01:09:06 - 01:09:08] אוקיי, אז אפשר להראות שההתפלגות הזאת
[01:09:09 - 01:09:10] היא גם גאוסיאן,
[01:09:12 - 01:09:15] עם תוחלת
[01:09:15 - 01:09:17] מסוימת ו-Varions מסוים,
[01:09:17 - 01:09:19] שהנוסחאות שלהן כתובות כאן.
[01:09:20 - 01:09:23] וזה שוב משהו שעשיתם
[01:09:27 - 01:09:28] בתרגיל
[01:09:28 - 01:09:31] הראשון או השני, או השני אני חושב,
[01:09:31 - 01:09:32] שעשיתם התרגיל על
[01:09:33 - 01:09:36] הסקה בייזיאנית.
[01:09:37 - 01:09:40] אז יכול להיות שבתרגיל הבא אני אתן לכם רק להיזכר בזה,
[01:09:40 - 01:09:42] ולהגיע לפיתוח הזה שוב פעם.
[01:09:43 - 01:09:44] אז התוחלת בעצם
[01:09:45 - 01:09:47] של XT-1,
[01:09:47 - 01:09:50] אם אני יודע מה כבר ה-X הבא שאני הולך להגיע אליו,
[01:09:50 - 01:09:53] ואני יודע מה ה-X0 שהתחלתי ממנו,
[01:09:54 - 01:09:59] התוחלת יוצאת איזושהי קומבינציה ליניארית בין ה-X שהתחלתי ממנו ל-X הבא.
[01:09:59 - 01:10:00] וזה המקדמים
[01:10:01 - 01:10:04] של הקומבינציה הליניארית הזאת.
[01:10:05 - 01:10:07] וגם ה-Varions אפשר לחשב מה הוא יוצא.
[01:10:11 - 01:10:16] אוקיי, אז בואו נראה רגע איך מגיעים לנסחה הזאת, אז זה האלבוג שאנחנו מתחילים ממנו,
[01:10:17 - 01:10:29] אוקיי? שוב, זה התוחלת לפי ה-Posterior Q של לוג של היחס בין ההסתברות ה-Forward של כל הדאטה שלי בין מ-0 עד T,
[01:10:31 - 01:10:32] או יותר נכון מ-T עד 0,
[01:10:34 - 01:10:36] והיחס בין זה לבין Q,
[01:10:38 - 01:10:39] סליחה, קודם קראתי לזה forward,
[01:10:39 - 01:10:41] זה ההתפלגות ה-inverse, reverse,
[01:10:42 - 01:10:43] שמייצרת את הדאטה,
[01:10:43 - 01:10:46] Q זה ההתפלגות שנקראת כאן forward, שזה מהדאטה
[01:10:47 - 01:10:48] אל הרעש,
[01:10:49 - 01:10:52] אבל פה זה רק בהינתן X0,
[01:10:52 - 01:10:56] זה רק של X1 עד T, שזה כל ה-Latend variables שלנו בהינתן X0.
[01:10:57 - 01:11:08] אוקיי, אז אנחנו רוצים לפרק את זה, למה אנחנו מפרקים את זה? כי אנחנו רוצים לפרק את זה לרכיבים שאנחנו יכולים לחשב בצורה קלה, כדי שנוכל לעשות אופטימיזציה ל-objective function הזה,
[01:11:08 - 01:11:09] וללמוד את הפרמטרים תטא.
[01:11:11 - 01:11:16] אוקיי, אז קודם כל אפשר להוציא מתוך ה-P הזה את האיבר הראשון,
[01:11:16 - 01:11:17] שהוא פשוט גאוסיאן
[01:11:20 - 01:11:21] 0I,
[01:11:22 - 01:11:23] אפשר להוציא אותו,
[01:11:24 - 01:11:29] ולכתוב את הכל בצורה כזאת שנובעת מהתכונה המרקוביות של שני הכיוונים,
[01:11:29 - 01:11:31] שני הכיוונים הם מרקוביים,
[01:11:32 - 01:11:33] אז אני מניח שיש לי,
[01:11:34 - 01:11:36] פה אני הולך אחורה ופה אני הולך קדימה.
[01:11:37 - 01:11:43] אני יכול לפרק את זה לשרשרת שהולכת אחורה ולחלקי השרשרת שהולכת קדימה.
[01:11:45 - 01:11:56] אוקיי, אז זה הופך לסכום בגלל שאנחנו בתוך הלוג פה, זה היה אמור להיות מכפלות כאן במונה ומכפלות במכנה ואני יכול להפוך את זה לסכום הזה.
[01:11:57 - 01:12:03] אוקיי, עכשיו פה אנחנו הוצאנו החוצה את כל מה שקשור ל-X0.
[01:12:03 - 01:12:13] אחרי זה יסבך אותנו כאן בחישוב הבא שנעשה. אז יש לנו איבר נוסף שאנחנו נצטרך להתייחס אליו בנפרד שקשור ל-X0.
[01:12:15 - 01:12:18] אז כל מה שנשאר לנו כאן אין פה, לא מתעסק יותר ב-X0,
[01:12:19 - 01:12:26] ויש פה עדיין סכום של כל האיברים לא כולל 1, זאת אומרת מ-2 עד ל-T גדול.
[01:12:29 - 01:12:29] עכשיו,
[01:12:29 - 01:12:33] מה שכתוב כאן, יש פה קצת קפיצה אולי על ידי שלב באמצע,
[01:12:34 - 01:12:37] אז שימו לב שאת ה-Q הזה אנחנו יכולים בהתניה להוסיף גם את X0,
[01:12:38 - 01:12:41] כי אמרנו שזה מרקובי, נכון? מרקובי זה אומר שאם אני,
[01:12:41 - 01:12:43] כל מה שאני צריך להתנות זה הצעד הקודם,
[01:12:44 - 01:12:50] אבל אני גם יכול, אם אני רוצה אני יכול להתנות בכל מה שקרה קודם, זה לא ישנה לי את ההתנדבות.
[01:12:51 - 01:12:53] אז פה אני גם יכול להתנות ב-X0,
[01:12:54 - 01:12:57] ואז אני יכול להפעיל על זה את חוק בייס,
[01:12:58 - 01:13:01] אוקיי? אז מה שאתם רואים כאן זה שלושת האיברים של חוק בייס.
[01:13:01 - 01:13:04] זה ה-likelihood,
[01:13:04 - 01:13:05] זה ה-prior,
[01:13:06 - 01:13:09] וזה המכנה של החוק בייס בעצם, מה שנקרא Evidense.
[01:13:10 - 01:13:12] אוקיי? אבל הכל בהינתן X0,
[01:13:13 - 01:13:14] אנחנו משאירים כאן את X0,
[01:13:14 - 01:13:16] וזה בעצם מאפשר לנו להפוך כאן
[01:13:17 - 01:13:21] את ההסתברות. אז עכשיו יש לנו את Q של XT-1 בהינתן XT,
[01:13:21 - 01:13:23] שזה הפוך מאיך ש-Q בעצם בנוי,
[01:13:24 - 01:13:25] אבל גם בהינתן X0.
[01:13:27 - 01:13:29] הסיבה שאנחנו רוצים פה את ה-X0 זה
[01:13:30 - 01:13:32] שלחשב את ה-Q הזה ההפוך, אנחנו לא יודעים.
[01:13:33 - 01:13:34] זה בדיוק מה שאנחנו מנסים לפתור.
[01:13:38 - 01:13:40] זאת אומרת, ההתפלגות הזאת בלי X0 אנחנו לא יודעים לחשב אותה.
[01:13:41 - 01:13:42] זו ההסתברות שהופכת,
[01:13:43 - 01:13:45] שהולכת צעד אחד אחורה מ-T ל-T-1.
[01:13:47 - 01:13:53] אבל מה שאנחנו מקבלים כאן זה שאנחנו יכולים ללמוד אותה על ידי זה שאנחנו,
[01:13:54 - 01:13:57] באובג'קטיב פונקשן שלנו לא יופיע ה-ground truth
[01:13:58 - 01:13:59] של Q,
[01:13:59 - 01:14:02] אלא ה-Q שמופיע כאן שהוא בהינתן X0.
[01:14:03 - 01:14:05] אז זה אנחנו יודעים שזה כן גאוסיאן,
[01:14:05 - 01:14:06] שאנחנו יודעים לחשב אותו.
[01:14:29 - 01:14:38] אז עוד דבר שוב שקורה במעבר מכאן לכאן זה שאנחנו בעצם העלמנו את
[01:14:40 - 01:14:42] האיבר הזה כי כל האיברים כאן מתבטלים,
[01:14:43 - 01:14:47] כי כל איבר שמופיע כאן במכנה הוא גם מופיע במכנה,
[01:14:48 - 01:14:53] סליחה, כל איבר שמופיע במונה גם מופיע במכנה ב-T1 קדימה.
[01:14:54 - 01:14:57] הדבר היחיד שאנחנו נשארים איתו זה XT,
[01:14:58 - 01:15:00] XT גדול בהינתן X0.
[01:15:05 - 01:15:08] אוקיי? אז בסופו של דבר מה שאנחנו מקבלים כאן,
[01:15:09 - 01:15:13] כל האיברים שאנחנו צריכים לחשב זאת אומרת
[01:15:14 - 01:15:21] התפלגויות של Q הם כאלה שמותנות ב-X0 לכן הכל כאן זה גאוסיאנים שאנחנו יודעים לחשב את
[01:15:22 - 01:15:23] התוחלת ואת הווריאנס שלהם.
[01:15:24 - 01:15:26] בהינתן X0 אנחנו יודעים לחשב את
[01:15:27 - 01:15:34] כל מה שצריך לחשב על Q. הדבר היחיד שהוא לא ידוע ב-Q זה ההתפלגות של X0, ההתפלגות ההתחלתית הזאת.
[01:15:35 - 01:15:36] כל השאר זה רק גאוסיאנים.
[01:15:37 - 01:15:39] כמו שאנחנו זוכרים, כל שילוב של גאוסיאנים,
[01:15:40 - 01:15:42] כל התניה,
[01:15:42 - 01:15:45] כל התפלגות שולית, הכל נשאר גאוסיאנים.
[01:15:46 - 01:15:55] הדבר היחיד פה שהוא לא גאוסיאנים זה ההתפלגות ההתחלתית הזאת של X0. אבל אם אנחנו תמיד מתנים על X0 אז כל ההתפלגויות שנשארות לנו הם גאוסיאנים שאנחנו יודעים לחשב בדיוק את
[01:15:56 - 01:15:57] התוחלת ואת הווריאנס שלה.
[01:15:58 - 01:16:02] אז אנחנו נשארים עם אובייקטיב פונקשן שמכיל רק
[01:16:02 - 01:16:04] גאוסיאנים שאנחנו צריכים לחשב
[01:16:05 - 01:16:09] והתוחלת הזאת אוקיי אז אנחנו צריכים לדגום מתוך
[01:16:10 - 01:16:11] הפוסטריאור שלנו
[01:16:13 - 01:16:14] מתוך הקיומים שאנחנו יודעים לחשב
[01:16:15 - 01:16:20] ולעשות אופטימיזציה לפרמטרים תטא של P
[01:16:20 - 01:16:29] אוקיי עכשיו עוד פישוט שאפשר לעשות כאן אז כמו שאמרנו קודם אנחנו מניחים
[01:16:32 - 01:16:39] שהצעד הקטן הזה שאנחנו עושים מ-T ל-T-1 הוא גאוסיאני
[01:16:40 - 01:16:50] ואמרנו גם שה-Q הוא גאוסיאני אז בעצם ה-KL דייברג'נס שיש לנו בכל הסכומים האלה כל אחד מהאיברים האלה הוא KL דייברג'נס בעצם בין
[01:16:50 - 01:16:51] P ל-Q
[01:16:53 - 01:16:58] אז יותר נכון בין Q ל-P כי התוחלת כאן היא על Q
[01:17:00 - 01:17:05] אז ה-KL דייברג'נס האלה הם בעצם ה-KL דייברג'נס בין שני גאוסיאנים
[01:17:06 - 01:17:11] ובגלל שהשונות היא מראש גם אנחנו קובעים אותה
[01:17:12 - 01:17:14] אז כל מה שנשאר לנו זה להשוות את התוחלות
[01:17:15 - 01:17:16] של הגאוסיאנים האלה
[01:17:17 - 01:17:19] אז בקיצור יש משוואה
[01:17:20 - 01:17:23] ברורה של ה-KL דייברג'נס בין שני גאוסיאנים שאנחנו
[01:17:24 - 01:17:25] רוצים ללמוד את התוחלת של אחד מהם
[01:17:26 - 01:17:30] וזאת המשוואה הזאתי זה פשוט השגיאה הריבועית
[01:17:30 - 01:17:36] של בין שתי התוחלות שזאת התוחלת שאנחנו יודעים של Q
[01:17:37 - 01:17:39] וזאת התוחלת שאנחנו רוצים ללמוד
[01:17:39 - 01:17:44] תוחלת שהיא תלויה ב-XT והיא אמורה לתת לנו את התוחלת של XT מינוס 1
[01:17:46 - 01:17:50] והיא גם תלויה ב-T כי עבור כל T אנחנו נקבל
[01:17:51 - 01:17:55] בעצם הכמות רע שאנחנו צריכים להוריד יכולה להיות שונה
[01:17:58 - 01:18:03] אז בעצם זה ה-objective function שנשאר לנו עבור כל ה-Tים זה עבור אחד מה-Tים
[01:18:04 - 01:18:10] צריכים לסכום את זה עבור כל ה-Tים השונים ועבור כל T שונה אנחנו צריכים שהתוחלת שאנחנו מחשבים
[01:18:11 - 01:18:12] תהיה דומה לתוחלת האמיתית
[01:18:14 - 01:18:15] אנחנו מודדים את זה בשגיאה ריבונית
[01:18:16 - 01:18:25] אז מה שהם כתבו שם במאמר בעצם הם אמרו שאוקיי אז יש לנו אם אנחנו משווים בין
[01:18:26 - 01:18:35] תוחלות שזה מה שעשו לפני זה ב-2015 אפשר להגיד שזה בעצם די דומה להשוות בין הרעשים
[01:18:36 - 01:18:41] למה כי בעצם אם XT הוא סכום של X0 ועוד רעש
[01:18:42 - 01:18:45] עם איזה שהם מקדמים
[01:18:46 - 01:18:54] אז אנחנו יכולים להגדיר אז קודם כל אנחנו יודעים שזה נכון שהתוחלת
[01:18:55 - 01:18:59] של XT בהינתן X0 אפשר לכתוב אותו ככה
[01:18:59 - 01:19:03] אז קודם כתבנו את זה בתור קומבינציה ליניארית של XT ו-X0
[01:19:03 - 01:19:09] ובגלל ש-XT הוא סכום של X0 ועוד הרעש אפשר גם לכתוב את זה בתור קומבינציה ליניארית של XT והרעש
[01:19:10 - 01:19:13] הייתי חושב פה על הרעש הזה אפשר להחליף אותו בתור
[01:19:13 - 01:19:16] לכתוב אותו ככה בתור XT פחות
[01:19:18 - 01:19:22] פחות הדבר הזה, פחות X0 כמו המקדם, חלקי המקדם הזה
[01:19:23 - 01:19:27] אז ככה מתקבלת הנוסחה הזאת
[01:19:28 - 01:19:30] אז יש לנו דרך לכתוב את מי הוא בתור
[01:19:30 - 01:19:39] איזושהי קומבינציה של XT שזה הצעד הבא והרעש שהיינו צריכים להוסיף כדי להגיע ל-XT
[01:19:40 - 01:19:48] ולכן אולי כדאי גם ככה לבנות את המודל שלנו של מי הוא תטא,
[01:19:48 - 01:19:52] מי הוא תטא שוב זה המודל שלנו שמנסה לעשות פרדיקציה למי הוא
[01:19:54 - 01:19:55] ואפשר לכתוב אפשר להחליט
[01:19:56 - 01:20:02] עליו איזושהי פרמטרי דאטה שאנחנו רוצים נכון זה יכול להיות איזושהי רשת או משהו כזה אנחנו יכולים להחליט שיש לנו איזושהי רשת
[01:20:02 - 01:20:08] שהיא מוציאה משהו שנקרא לו Epsilon תטא ואז על הרשת הזאת נעשה את החישוב הזה
[01:20:10 - 01:20:12] אוקיי זה גם אפשר לחשוב על זה בתור רשת
[01:20:13 - 01:20:15] שכבה האחרונה של הרשת עושה את הפעולה הזאת
[01:20:16 - 01:20:20] שאנחנו נותר לנו להשתמש ב-XT כי XT זה אינפוט של הרשת הזאת
[01:20:21 - 01:20:26] אז מה אנחנו מקבלים כאן ששתי הנוסחאות האלה נראות אותו דבר בעצם ההבדל היחיד
[01:20:27 - 01:20:34] זה שכאן יש לנו את הרעש האמיתי שהוספנו ופה יש לנו איזשהו Output של רשת שלנו
[01:20:35 - 01:20:38] אז אם בעצם אנחנו נכניס את שני האיברים האלה
[01:20:38 - 01:20:44] במקום התוחלת האמיתית והתוחלת שאנחנו עושים הפרדיקציה שלנו
[01:20:44 - 01:20:49] אז בעצם נקבל שבסופו של דבר יש כל מיני מקדמים שיוצאים החוצה
[01:20:49 - 01:20:54] ואנחנו מקבלים משוואה שמשווה
[01:20:55 - 01:20:59] את השגיאה הריבועית בין הפרדיקציה שלנו לרעש
[01:21:00 - 01:21:02] שזה אמור להיות לכם מוכר מהחלק הקודם של השיעור
[01:21:02 - 01:21:07] שבעצם מה שנשאר לנו זה שהפרדיקציה שאנחנו עושים
[01:21:07 - 01:21:11] אנחנו צריכים שהיא תתקרב לרעש האמיתי מה שאומר שהרשת שלנו
[01:21:11 - 01:21:19] לומדת לחזות את הרעש שנוסף לדאטה כדי לייצר את הדגימה איקסטינג.
[01:21:21 - 01:21:25] אז רק להסביר בעצם למה הגענו,
[01:21:25 - 01:21:37] הגענו לזה שאם אנחנו חושבים על התהליך הזה בתור איזשהו תהליך עם רשת סליחה תהליך עם מודל עם משתנה חבוי הרבה משתנים חבויים
[01:21:37 - 01:21:45] שהפוסטריור שלהם הוא תמיד גאוסיאן שמדעיך קצת את הסיגנל הקודם ומוסיף
[01:21:45 - 01:21:47] איזשהו גאוסיאן מסביב
[01:21:47 - 01:21:51] לסיגנל המודעך הזה
[01:21:53 - 01:21:59] וזה המודל שלנו ואנחנו מאמנים אותו על ידי זה שאנחנו עושים אלבו כמו שאימנו VAE
[01:22:00 - 01:22:06] אז האובג'קטיב פונקשן שלנו יוצא שקול לאובג'קטיב פונקשן שקיבלנו קודם.
[01:22:06 - 01:22:10] זה אומר שבעצם הרשת שאנחנו לומדים צריכה ללמוד לחזות את הרעש
[01:22:11 - 01:22:19] שהוספנו לכל פעם שהגענו לאיזשהו XT מ-X0 אחר.
[01:22:22 - 01:22:25] ההבדל היחיד אולי זה יש פה את כל המקדמים האלה
[01:22:27 - 01:22:32] שפה הם קוראים לזה למדה T ומה שהם אומרים שבעצם ההבדל בין לעשות אלבו,
[01:22:33 - 01:22:37] אימון של אלבו לאימון שראינו קודם של סקורמצ'ינג
[01:22:38 - 01:22:45] זה שפשוט אנחנו בסקורמצ'ינג אנחנו מחליטים שהלמדה T שווה 1 תמיד במקום להשתמש במקדמים האלה שזה אומר משקול אחר
[01:22:46 - 01:22:57] של הלוס שלנו עבור הרעשים השונים, עבור הזמנים השונים, הערכים השונים של T. אתם קוראים לזה במאמר L simple פשוט אנחנו מניחים
[01:22:57 - 01:23:03] שכל המקדמים האלה שווים 1 ולכן אנחנו מקבלים בדיוק את אותו
[01:23:04 - 01:23:09] Objective Function שקיבלנו בחלק הראשון של הקורס שהגענו אליו מ-Score Matching.
[01:23:13 - 01:23:18] אוקיי, איך הרשת הזאת ממומשת? אז גם ככה אתם תממשו את זה, בעצם כל מה שנשאר לנו ללמוד
[01:23:19 - 01:23:26] זה רשת כזאת שמקבלת בתור input את XT שאיך XT הוא נראה הוא X0 ועוד רעש
[01:23:27 - 01:23:31] כל אחד עם מקדמים מסוימים
[01:23:31 - 01:23:34] ו-T שאומר לנו באיזה זמן אנחנו נמצאים
[01:23:35 - 01:23:39] זה מה שמשפיע על הגודל של המקדמים האלה.
[01:23:40 - 01:23:44] אז הרשת הזאת מקבלת את שני האינפוטים האלה וצריכה להוציא את הרעש,
[01:23:45 - 01:23:46] איזושהי פרדיקציה לרעש שהתווסף.
[01:23:47 - 01:23:50] זאת אומרת היא צריכה איכשהו להבין מה מתוך הסיגנל שהיא קיבלה
[01:23:51 - 01:23:54] זה הדאטה זה X0 ומה מתוך הסיגנל שהיא קיבלה זה הרעש.
[01:23:54 - 01:23:57] איך הרשת הזאת ממומשת?
[01:23:58 - 01:24:01] אז בדרך כלל זה רשת נוירונים בצורה שנקראת unit.
[01:24:02 - 01:24:05] זה ארכיטקטורה שמשתמשים בה הרבה כשרוצים
[01:24:08 - 01:24:11] עבור בעיות שבהן האינפוט זה תמונה וגם האאוטפוט זה תמונה באותו גודל.
[01:24:13 - 01:24:13] איך שזה עובד,
[01:24:14 - 01:24:17] אני מקווה שנתקלתם בזה כבר בקורס של Deep Learning,
[01:24:18 - 01:24:19] יש לנו
[01:24:21 - 01:24:24] בהתחלה רשת שהולכת ומתכווצת
[01:24:24 - 01:24:35] יש הרשת בדרך כלל זה מבוסס על קונבולוציות שהולכות ויש לנו איזה שהוא פולינג או סטרייד או משהו שבכל שכבה אנחנו מקבלים תמונה הולכת וקטנה
[01:24:36 - 01:24:48] ואחר כך אנחנו עושים בדיוק את הפעולה ההפוכה קוראים לזה de-convolution לפעמים אנחנו הולכים ופותחים יותר ויותר את הרשת עד שאנחנו מקבלים בחזרה את השכבה באותו גודל כמו התמונה.
[01:24:49 - 01:24:52] בנוסף יש לנו חיבורים ישירים בין,
[01:24:53 - 01:24:55] את זה לא רואים כאן, זה לא החצים האלה,
[01:24:55 - 01:24:59] בין השכבה הזאת יש חיבור לשכבה הזאת
[01:24:59 - 01:25:02] ובין השכבה הזאת יש חיבור לשכבה הזאת.
[01:25:02 - 01:25:08] כל שכבה יש לה חיבור ישיר לשכבה באותו גודל אחרי שהיא עברה את האיווץ וההרחבה הזאת.
[01:25:10 - 01:25:18] בעצם האינטואיציה כאן זה שיש כל מיני סגננים שהם חשובים לשכבה הזאת שאפשר להעביר אותם באופן ישיר ל-output,
[01:25:19 - 01:25:30] אבל יש דברים שצריך להבין אותם יותר גלובליים, צריך לעבור יותר עיבוד של כל התמונה והם צריכים לעבור דרך כל הצוואר הבקבוק הזה כדי לחזור אחורה,
[01:25:31 - 01:25:31] לחזור החוצה.
[01:25:32 - 01:25:35] אוקיי, אז זה היוניט הבסיסי ולזה מתווסף
[01:25:36 - 01:25:44] איזושהי דרך לשנות את הארטפוט כפונקציה של t גם, נכון? אנחנו רוצים שהמודל שלנו יהיה תלוי ב-t,
[01:25:45 - 01:25:55] רגע שעבור זמנים שונים ידע לחזות שיש כמות שונה של רעש ולכן הוא תמיד ידע שחבל לא לתת לרשת.
[01:25:56 - 01:26:11] אז יש כמה דרכים לממש את זה אנחנו נראה לכם בתרגיל אחת מהדרכים לעשות את זה יש פשוט אנחנו עושים איזשהו עיבוד למספר הזה לסקלר הזה ככה שהופך להיות וקטור ואו
[01:26:11 - 01:26:16] מוסיפים את הוקטור הזה בכל שכבה או שהם עושים קונקטנציה של הוקטור הזה בכל שכבה.
[01:26:18 - 01:26:23] אוקיי, אז ככה זה המימוש של הרשת בעצם, זה הדבר היחיד שנלמד בתוך כל המודל הזה,
[01:26:23 - 01:26:26] זה רשת ניורונים שמקבלת תמונה מורעשת,
[01:26:28 - 01:26:30] איזשהו מספר שאומר עד כמה הרעש חזק
[01:26:31 - 01:26:32] בתמונה הזאת,
[01:26:32 - 01:26:33] והארטפוט זה הרעש,
[01:26:34 - 01:26:35] הערכים של הרעש.
[01:26:35 - 01:26:43] אוקיי, אבל זה בסופו של דבר האלגוריתם של האימון ושל הדגימה.
[01:26:45 - 01:26:46] זה יוצא מאוד פשוט,
[01:26:47 - 01:26:49] אז בואו נראה רגע, בואו נתחיל מהאימון.
[01:26:50 - 01:26:51] אוקיי, אז איך נראה האימון?
[01:26:52 - 01:26:55] אנחנו דוגמים תמונה בתוך הדאטה סק שלנו,
[01:26:55 - 01:26:59] אוקיי, Q של X0 זה בעצם ההסתברות של הדאטה,
[01:27:00 - 01:27:00] כמו שאנחנו לא יודעים.
[01:27:01 - 01:27:02] אנחנו דוגמים את התמונה,
[01:27:03 - 01:27:05] אנחנו דוגמים איזשהו זמן,
[01:27:07 - 01:27:07] זאת אומרת
[01:27:08 - 01:27:11] זמן מ-1 עד T, נגיד מ-1 עד 1000,
[01:27:12 - 01:27:15] זמן הכוונה כאן היא בתהליך הזה של ה-diffusion,
[01:27:15 - 01:27:19] כמה, לאיזה מקום בתהליך הזה אנחנו רוצים לקפוץ.
[01:27:21 - 01:27:24] אנחנו דוגמים רעש, ערך של רעש הוא בעצם, זה תמונת רעש,
[01:27:25 - 01:27:29] תמונה שהיא רעש כאוסיאני שכל פיקסל הוא בלתי יותר ריחד בשני.
[01:27:30 - 01:27:32] אוקיי, אז אנחנו דוגמים את שלושת הדברים האלה.
[01:27:33 - 01:27:36] ועכשיו אנחנו מחשבים את ה-objective function שלנו ככה.
[01:27:38 - 01:27:39] זה הרשת שלנו,
[01:27:39 - 01:27:40] Epsilon תטא,
[01:27:41 - 01:27:42] אנחנו נותנים לרשת את האינפוט הזה,
[01:27:44 - 01:27:47] אוקיי, הנגיד כמות הזה הוא X0 שדגמנו כאן,
[01:27:47 - 01:27:49] ועוד הרעש
[01:27:49 - 01:27:50] שדגמנו כאן,
[01:27:51 - 01:27:54] כל אחד כפול איזה שהם מקדמים, שאותם אנחנו מחשבים לפי הנוסחאות שראינו,
[01:27:55 - 01:27:55] והם תלויים ב-T,
[01:27:56 - 01:28:00] אוקיי, זה פונקציה של ה-Beta T הזה שרצינו,
[01:28:00 - 01:28:03] שהחלטנו שזה יהיה הקצב שבו אנחנו מוסיפים את הרעש.
[01:28:04 - 01:28:05] אז זה נותן לנו תמונה סך הכל.
[01:28:06 - 01:28:09] זה אינפוט אחד של הרשת, ואינפוט אחר זה פשוט ה-T.
[01:28:10 - 01:28:13] זה ה-T הזה שדגמנו איתו והשתמשנו בו כדי לחשב את האלפות האלה.
[01:28:14 - 01:28:16] אז זה האינפוטים של הרשת.
[01:28:16 - 01:28:19] Output זה יהיה משהו שאנחנו נרצה שהוא יהיה קרוב
[01:28:20 - 01:28:24] ל-Epsilon האמיתי, זה ה-Epsilon שדגמנו כאן והשתמשנו בו
[01:28:24 - 01:28:27] כדי לייצר את הדוגמה או הרשת.
[01:28:28 - 01:28:31] אנחנו רוצים שה-Output יהיה כמה שיותר קרוב ל-Epsilon הזה,
[01:28:32 - 01:28:34] ואנחנו מודדים את זה פשוט בשגיאה ריבועית,
[01:28:35 - 01:28:36] בנורמה ריבועית.
[01:28:37 - 01:28:40] זאת אומרת, אנחנו פשוט מסתכלים על השגיאה פיקסל-פיקסל,
[01:28:40 - 01:28:42] מרחק הריבועי של כל הפיקסלים,
[01:28:43 - 01:28:45] ואת זה אנחנו גוזרים לפי תטא,
[01:28:45 - 01:28:46] אוקיי, עושים gradient descent.
[01:28:48 - 01:28:53] ככה ממשיכים על כל התמונות כמה פעמים עד שהמודל מתכנס.
[01:28:55 - 01:28:55] אוקיי, אז זה האימון,
[01:28:56 - 01:28:57] מאוד פשוט.
[01:28:58 - 01:29:01] איך, ברגע שסיימנו לאמן, איך אנחנו עושים דגימה.
[01:29:02 - 01:29:08] אז אנחנו, אם אתם זוכרים, אנחנו מניחים שהגענו בסופו של תהליך ה-Defusion,
[01:29:08 - 01:29:13] הדאטה שלנו ייראה כמו גאוסיאן סטנדרטי,
[01:29:14 - 01:29:18] עם תוחלת 0 ו-Covarience I,
[01:29:18 - 01:29:20] אלכסוני עם אחדות באלכסון.
[01:29:21 - 01:29:23] אז אנחנו פשוט דוגמים דוגמה מתוך
[01:29:26 - 01:29:27] ההתפלגות הזאת.
[01:29:28 - 01:29:28] זה ייראה כמו רעש.
[01:29:31 - 01:29:34] ואנחנו חוזרים אחורה בתהליך מ-T גדול עד 1.
[01:29:36 - 01:29:44] ובכל איטרציה אנחנו בעצם צריכים לדגום מ-P של XT מינוס 1 בהינתן XT.
[01:29:46 - 01:29:48] שאמרנו איך זה, איך הדגימה הזאת נראית.
[01:29:49 - 01:29:51] אז כל מה שכתוב כאן
[01:29:51 - 01:29:53] זה הנוסחה של התוחלת.
[01:29:54 - 01:29:57] אמרנו שאנחנו מניחים שהדבר הזה הוא גאוסיאן.
[01:29:58 - 01:29:59] ו...
[01:30:00 - 01:30:01] זה התוחלת של הגאוסיאן.
[01:30:03 - 01:30:07] ואנחנו צריכים להוסיף לתוחלת הזאת איזשהו רעש
[01:30:08 - 01:30:09] שאנחנו דומים.
[01:30:09 - 01:30:13] אז אנחנו בעצם מחשבים את התוחלת בכל איטרציה, הולכים אחורה בזמן,
[01:30:13 - 01:30:15] כל איטרציה אנחנו מחשבים את התוחלת
[01:30:15 - 01:30:16] של הצעד הקודם.
[01:30:18 - 01:30:20] בשביל לחשב את התוחלת אנחנו צריכים להשתמש ברשת שלנו, כן?
[01:30:21 - 01:30:26] פה אנחנו משתמשים ברשת שלמדנו ואנחנו צריכים להכפיל את זה בכל מיני מספרים.
[01:30:26 - 01:30:34] זה נותן לנו תמונה שהיא התוחלת של ה-XT-1 ולזה אנחנו מוסיפים איזשהו רעש
[01:30:35 - 01:30:41] בווריאנס שהולך ומקטן עד שבסופו של דבר אנחנו מקבלים תמונה עם
[01:30:42 - 01:30:43] כמעט בלי רעש
[01:30:44 - 01:30:45] שהיא תהיה ה-X0 שלנו.
[01:30:46 - 01:30:51] בדרך כלל יש פה איזה תנאי שאם זה האיטרציה האחרונה אנחנו בכלל לא מוסיפים את הרעש, אנחנו מחזירים את התוחלת.
[01:30:52 - 01:30:54] כי אנחנו חושבים שבסופו של דבר אנחנו מגיעים לרעש כל כך קטן
[01:30:54 - 01:30:57] שכבר אנחנו לא צריכים ל...
[01:30:58 - 01:31:01] אנחנו קרובים מספיק להתפלגות של X
[01:31:02 - 01:31:08] של X0 והתוחלת שם היא תהיה יותר נקייה מאשר להוסיף עוד קצת רעש כתבל.
[01:31:09 - 01:31:10] אז זהו, אז זה התהליך של הדגימה.
[01:31:11 - 01:31:13] אז ככה אנחנו מאמנים את המודל,
[01:31:13 - 01:31:16] ככה אנחנו משתמשים בו לפחות בשביל לדגום.
[01:31:17 - 01:31:21] בשבוע הבא אנחנו נראה עוד דרכים שאפשר להשתמש בו כדי לעשות עוד דברים חוץ מלדגום.
[01:31:25 - 01:31:35] עוד נקודה למה המודל הזה עובד, יש פה משהו מעניין שה-Xים הגבוהים שהם קרובים ל-XT הם תופסים
[01:31:36 - 01:31:43] איזשהו משהו קצת יותר אולי גלובלי בתמונה,
[01:31:44 - 01:31:46] אוקיי, מה האזורים הבהירים, מה האזורים הכהים,
[01:31:47 - 01:31:49] מה תהיה הצורה של התמונה.
[01:31:49 - 01:31:53] אז אם נסתכל על התהליך הזה בצורה הפוכה לאט לאט באיטרציות האלה אנחנו
[01:31:53 - 01:31:57] כבר יכולים להבין אולי איזה צורה הולך לצאת,
[01:31:58 - 01:32:00] אולי אפילו מה יהיה האובייקט שיש כאן.
[01:32:01 - 01:32:02] אז המידע,
[01:32:02 - 01:32:04] אנחנו חושבים על זה במובן של representation learning,
[01:32:05 - 01:32:08] איפה נמצא המידע הזה, אז הוא נמצא ב-Xים האלה.
[01:32:09 - 01:32:12] ב-Xים האלה נמצא המידע של החדות של התמונה.
[01:32:13 - 01:32:20] של לדאוג לזה שכל הפיקסלים אחד ליד השני הם חלקים, שהטקסטורות נראות טוב,
[01:32:20 - 01:32:23] שיש אדג'ים חדים, שהתמונה נראית חדה.
[01:32:24 - 01:32:27] אז למשל אם נרצה לעשות איזשהו representation learning,
[01:32:27 - 01:32:34] למשל להשתמש בתמונה כדי לזהות מה יש בה,
[01:32:35 - 01:32:41] יכול להיות שפה נמצא מידע יותר גלובלי שהוא מידע על האובייקט,
[01:32:42 - 01:32:47] וכאן זה יותר מידע על פרטים שאולי לפעמים הם פחות מעניינים, יותר חשובים בשביל שהתמונה תיראה טוב,
[01:32:48 - 01:32:52] פחות חשובים בשביל להבין מה הסמנטיקה של התמונה.
[01:32:54 - 01:32:59] אוקיי, זה תוצאות שהציגו במאמר הזה מ-2020,
[01:33:00 - 01:33:06] אז זה כבר היה תוצאות שהן היו בסדר גודל של המודלים שהיו הכי טובים אז, שזה היה גאנז,
[01:33:07 - 01:33:11] זה דאטה מסלב A, זה דאטה מסיפר שוב פעם, זה דגימות
[01:33:12 - 01:33:17] שהם הוציאו מהמודל עם האלגוריתם הזה של סמפלינג,
[01:33:17 - 01:33:19] שהוא בעצם, מה שראינו שזה בעצם אותו תהליך כמו Lenge of in Dynamics,
[01:33:20 - 01:33:27] אז ללכת הפוך במשתנים החבויים האלה זה בעצם יוצא אותו דבר כמו Lenge of in Dynamics.
[01:33:28 - 01:33:31] זה בעצם היה הרעיון במאמר הזה, להגיד שהאימון וגם הדגימה
[01:33:32 - 01:33:37] זה אותו דבר אם אנחנו חושבים על זה בתור משתנים חבויים או בתור סקור מאצ'ינג
[01:33:38 - 01:33:39] עם Lenge of in Dynamics.
[01:33:41 - 01:33:47] אבל יש פה גם תוצאות מספריות שמשוות, אז אאוור זה המודל הזה שלהם,
[01:33:48 - 01:33:49] אלה מאמרים שהם די טובים,
[01:33:49 - 01:33:52] אתם זוכרים דיברנו שבוע שעבר על המדדים האלה,
[01:33:53 - 01:33:58] Inception Score ו-FID זה מדדים שאומרים עד כמה הדגימות נראות טוב,
[01:33:59 - 01:34:00] ו-NLL זה ה-likelihood,
[01:34:01 - 01:34:02] ה- negative log-likelihood,
[01:34:03 - 01:34:11] שאנשים חושבים על זה בתור מודל הסתברותי וכל המודלים האלה שיש פה קטן-שווה זה בגלל שאפשר לחשב רק חסם,
[01:34:12 - 01:34:13] כי זה מודלים עם משתנים חבויים
[01:34:14 - 01:34:16] ואנחנו יכולים לחשב איתם רק חסם,
[01:34:16 - 01:34:18] לעומת למשל פיקסל-CNN,
[01:34:18 - 01:34:20] שזה מודל אוטו-רגרסיב,
[01:34:21 - 01:34:24] ששם אנחנו יכולים לחשב באופן מדויק את ה-likelihood.
[01:34:33 - 01:34:46] בחלק האחרון של השיעור אני רוצה לדבר על הקשר של המודל הזה ל-Continuous Normalizing Flows, זאת אומרת Normalizing Flows כשהזמן הוא רציף,
[01:34:47 - 01:34:49] זאת אומרת אין לנו סדרה של
[01:34:52 - 01:34:57] מספר בדיד של צעדים, אלא אנחנו מניחים שיש לנו כאן תהליך רציף בזמן,
[01:34:58 - 01:35:03] וזה קשור גם למשוואות דיפרנציאליות, רגילות וסטוכסטיות,
[01:35:04 - 01:35:06] ולעוד משהו שנקרא Flow Matching.
[01:35:06 - 01:35:07] אני אעבור על זה ממש מהר,
[01:35:07 - 01:35:14] המטרה כאן היא לא שתבינו את כל הנוסחאות של הפומולציה הזאת,
[01:35:15 - 01:35:17] אלא יותר שתכירו את המושגים האלה,
[01:35:18 - 01:35:23] כי זה משהו שנתקלים בו הרבה כשמתעסקים במודלי דיפיוז'ן,
[01:35:24 - 01:35:27] וגם שתכירו את הכיוון הזה אם תרצו להעמיק בו.
[01:35:29 - 01:35:38] אוקיי, אז רק בואו נגדיר, Stochastic differential equation זה משהו שמתאר בעצם דרך להגדיר תהליך בזמן רציף,
[01:35:39 - 01:35:43] על ידי זה שאנחנו מגדירים את השינוי שיש בכל רגע,
[01:35:43 - 01:35:45] זה השינוי הרגעי שיש בכל רגע ב-X.
[01:35:46 - 01:35:52] X זה איזשהו סיגנל שעובר תהליך, הוא משתנה בזמן,
[01:35:53 - 01:35:59] ואנחנו יכולים להגדיר את השינוי שלו על ידי משוואה דיפרנציאלית.
[01:36:01 - 01:36:08] זאת אומרת, איך השינוי שלו בזמן, ואם המשוואה הדיפרנציאלית היא לא דטרמיניסטית, זאת אומרת יש לה חלק שהוא סטוכסטי,
[01:36:09 - 01:36:11] אז זה נקרא Stochastic differential equation,
[01:36:11 - 01:36:13] זאת אומרת הסיגנל הזה משתנה בזמן,
[01:36:14 - 01:36:19] אבל השינוי בזמן שלו הוא רועש, זאת אומרת שיש אלמנט של רעש,
[01:36:20 - 01:36:22] אנחנו לא יודעים לחזות בדיוק איך הוא ישתנה בזמן.
[01:36:23 - 01:36:27] אז מה שרואים כאן, F זה החלק הדטרמיניסטי של השינוי,
[01:36:28 - 01:36:31] ופה יש עוד איזשהו רעש שמתווסף.
[01:36:33 - 01:36:39] אז אפשר ככה לתאר את ה-forward process בעצם,
[01:36:40 - 01:36:46] תהליך ה-diffusion שקורה בדאטה שלנו. אם אנחנו חושבים עליו בתור איזשהו תהליך רציף שמתחיל
[01:36:47 - 01:36:49] בתור תמונה, אז X מתחיל בתור תמונה,
[01:36:50 - 01:36:51] ויש איזשהו תהליך
[01:36:51 - 01:36:59] שאנחנו יודעים להגיד בכל רגע שהנגזרת שלו היא שווה לאיזושהי פונקציה ועוד רעש.
[01:37:02 - 01:37:04] אבל זה תהליך רציף,
[01:37:04 - 01:37:08] בסופו של דבר כדי לראות לאן הגענו אנחנו צריכים לעשות אינטגרציה
[01:37:09 - 01:37:12] של כל השינויים האלה שקרו,
[01:37:13 - 01:37:14] שהשינויים האלה הם אסטרוכסטיים,
[01:37:15 - 01:37:15] יש באמת אסטרוכסטיים.
[01:37:17 - 01:37:21] פה זו דוגמה של התפלגות חד-ממדית שהיא מתחילה ככה,
[01:37:22 - 01:37:26] והיא בזמן הופכת להיות פשוט גאוסיאן עם מרכז אחד.
[01:37:28 - 01:37:32] אוקיי, אז שוב כאן רואים התפלגויות רב-ממדיות על תמונות,
[01:37:32 - 01:37:34] אבל בהמחשה הזאת רואים התפלגות חד-ממדית.
[01:37:35 - 01:37:38] זה רק להמחשה, זה לא בדיוק קשור אחד-ממדי.
[01:37:38 - 01:37:44] אוקיי, אז אם זה התהליך קדימה,
[01:37:44 - 01:37:50] התהליך אחורה הוא בעצם אפשר להראות שהוא גם SDE,
[01:37:51 - 01:37:53] גם אפשר להגדיר אותו בתור SDE,
[01:37:54 - 01:37:55] ואפשר ממש לחשב אותו,
[01:37:56 - 01:37:58] זאת אומרת אפשר לחשב את החלק,
[01:37:59 - 01:38:05] את הפונקציה הזאת שמגדירה את השינוי הרגעי בכל רגע,
[01:38:06 - 01:38:09] וזה קשור לפונקציה גם של הקדימה,
[01:38:09 - 01:38:10] F זו אותה F שהייתה קודם,
[01:38:11 - 01:38:12] אבל יש פה אלמנט חדש
[01:38:13 - 01:38:16] שמתווסף שזה בדיוק ה-score function.
[01:38:17 - 01:38:23] ה-score function בנקודה T זה באמת בעצם קשור
[01:38:25 - 01:38:28] להתפלגות שאנחנו מקבלים באיזשהו זמן,
[01:38:28 - 01:38:31] ולשינוי שאנחנו צריכים כדי להגדיל
[01:38:32 - 01:38:34] את ההתפלגות הזאת כמה שיותר מהר.
[01:38:35 - 01:38:38] אז ה-SDE בכיוון ההפוך, אם הגדרנו SDE קדימה,
[01:38:39 - 01:38:45] אז ה-SDE שהופך אותנו בחזרה הוא הופך להיות תלוי ב-score function.
[01:38:47 - 01:38:55] אז זה מאוד מזכיר את מה שאנחנו רואים בדיוק את התהליך שאנחנו עושים ב-Difusion Model, נכון?
[01:38:55 - 01:38:58] הכיוון קדימה אנחנו מגדירים את זה פשוט על ידי הוספת רעש,
[01:38:59 - 01:39:03] והכיוון אחורה, כדי לעשות את הכיוון אחורה אנחנו צריכים את ה-score function.
[01:39:05 - 01:39:08] אז בעצם אפשר לחשוב על התהליך הזה,
[01:39:10 - 01:39:15] שהמודל הזה שהוא הגדרנו אותו, אנחנו הגדרנו אותו בזמן בדיד,
[01:39:16 - 01:39:20] אבל אפשר לחשוב עליו בתור תהליך שבעצם קורה בזמן רציף,
[01:39:21 - 01:39:28] ושהחישוב שאנחנו עושים בזמן בדיד זה חישוב נומרי שמקרב את הפתרון שלו.
[01:39:29 - 01:39:30] אז גם כשמגדירים את
[01:39:31 - 01:39:34] המשוואות הדיפרנציאליות האלה בזמן רציף,
[01:39:35 - 01:39:41] הדרך לפתור אותן, אי אפשר לפתור משהו בצורה רציפה, אנחנו חייבים לפתור את זה על ידי איזשהו קירוב,
[01:39:42 - 01:39:54] ואחד הקירובים הכי פשוטים הוא פשוט לעשות בדיוק את הצעדים שאנחנו עושים במודל Difusion הרגיל, זאת אומרת לחלק את הדאטה שלנו לצעדים בדידים,
[01:39:54 - 01:39:57] ולעשות בדיוק את החישוב הזה שאנחנו עושים,
[01:39:58 - 01:40:05] זאת אומרת בכל צעד נחשב את ה-F שלנו, שה-F שלנו פשוט מדעיך את הסיגנל, ועוד הרעש הגאוסיאני,
[01:40:06 - 01:40:18] וזה כשאנחנו רוצים לעשות את הכיוון קדימה, וכשאנחנו רוצים לחזור אחורה אנחנו עושים בדיוק מה שאנחנו רואים כאן, אם אנחנו עושים את זה בזמן בדיד זה יוצא בדיוק אניל לנג'אווין דיינאמיקס,
[01:40:20 - 01:40:27] שחלק מהדבר הזה זה לחשב את ה-score בכל איטרציה וזה בדיוק מה שחסר לנו ואנחנו צריכים את הרשת
[01:40:28 - 01:40:30] כדי לשעריך את זה.
[01:40:32 - 01:40:35] אז מה שזה אומר זה שאנחנו הולכים לחשוב על המודל דיפיוז'ן בתור
[01:40:35 - 01:40:39] איזושהי דרך נומרית לחשב את ההליך,
[01:40:42 - 01:40:46] לחשב פתרון נומרי לתהליך רציף
[01:40:46 - 01:40:53] שממדל בצורת SDE שינוי מהתפלגות אחת להתפלגות אחרת.
[01:40:54 - 01:41:17] אז זה בגדול מה שאתם צריכים לזכור שיש את הדבר הזה, זה פותח פשוט דלת לעוד כל מיני דברים אחרים שאפשר לעשות. למשל אפשר, יש הרבה דרכים לפתור SDE לא רק בדרך הפשוטה הזאת שאנחנו הראינו שאנחנו משתמשים בה
[01:41:18 - 01:41:22] בדיפיוז'ן מודל רגיל ואפשר להשתמש בכל
[01:41:23 - 01:41:29] מחקר והדרכים השונות לעשות את החישובים האלה גם פה והרבה פעמים זה מייעל כל מיני דברים.
[01:41:30 - 01:41:35] למשל אחד מהדברים שקוראים לזה probability flow ODE זה שאנחנו יכולים
[01:41:38 - 01:41:38] להפוך
[01:41:39 - 01:41:39] את
[01:41:42 - 01:41:43] התהליך ההפוך
[01:41:44 - 01:41:48] מ-SDE אפשר להפוך אותו ל-ODE שהוא שקול.
[01:41:48 - 01:41:49] מה זה ODE?
[01:41:49 - 01:42:00] זה אומר שאין לנו ב... אנחנו לא צריכים להוסיף רעש בכל איטרציה בעצם אנחנו מניחים שיש לנו רעש בהתחלה ואחר כך התהליך הוא דטרמיניסטי
[01:42:01 - 01:42:07] הוא עדיין קשור ב-flow, סליחה ב-score אבל הוא דטרמיניסטי
[01:42:08 - 01:42:13] אוקיי? בעצם תחשבו על זה שכל הרעש שיש לנו בדרך אנחנו מכניסים את כל הרעש הזה להתחלה
[01:42:15 - 01:42:17] אנחנו רק בהתחלה מגרילים משהו
[01:42:18 - 01:42:20] ובאותו רגע אנחנו עושים תהליך דטרמיניסטי
[01:42:21 - 01:42:24] אפשר להראות שההתפלגויות שיוצאות מזה הן שקולות
[01:42:25 - 01:42:27] ההתפלגויות השוליות שיוצאות מהתהליך הזה הוא שקול
[01:42:28 - 01:42:33] אז זה למשל משהו שברגע שחושבים על הדבר הזה בתור תהליך רציף
[01:42:33 - 01:42:36] ובתור פתרון של משוואה דיפרנציאלית
[01:42:36 - 01:42:39] אז זה משהו שאנחנו יכולים להשתמש בו.
[01:42:41 - 01:42:45] יש גם איזה המחשה שאני לא יודע כמה זה ברור זה הקווים
[01:42:46 - 01:42:52] הקופצנים האלה זה מה שראינו קודם זה ה-SDE's הרועשים ובעצם אפשר להגדיר
[01:42:54 - 01:43:05] ODE שהוא שקול עבור כל נקודה כאן יהיה מסלול דטרמיניסטי שיביא אותנו לנקודה אחרת בהתפלגות השנייה
[01:43:06 - 01:43:11] ואם אנחנו נעשה את זה עבור אינסוף דגימות מפה
[01:43:11 - 01:43:16] בעצם ההתפלגות השולית שיוצא לנו בכל נקודת זמן כאן תהיה שווה
[01:43:17 - 01:43:21] אם נעשה את התהליך הזה לפי ה-SDE או לפי ה-ODE הדטרמיניסטי.
[01:43:24 - 01:43:33] אז זה פותח דלת לכמה דברים אחד מהדברים שאנחנו יכולים בעצם לחשוב על הדבר הזה בתור normalizing flow
[01:43:35 - 01:43:40] שוב פעם שמוגדר על ידי זה שה-flow עצמו הוא רציף אבל
[01:43:40 - 01:43:43] אנחנו מחשבים אותו רק בזמנים בדידים.
[01:43:47 - 01:43:50] אוקיי אז יש כאן כמה יתרונות,
[01:43:51 - 01:43:57] אחד זה מה שאמרתי, אחד זה זה מאפשר לנו להשתמש בכל מיני שיטות של פתרון משוואות דיפרנציאליות
[01:43:58 - 01:44:03] שאולי הן יותר יעילות מאשר לעשות את האלף איטרציות האלה שאנחנו עושים,
[01:44:04 - 01:44:10] זה מחשב לנו, זה עוזר לנו לעשות חישוב דטרמיניסטי זאת אומרת זה מפריד לנו את החלק הרנדומלי
[01:44:10 - 01:44:18] שהוא רק בהתחלה מהתהליך של הייצור אז זה הופך את זה ליותר דומה ל-VAE רגיל שיש לנו latent variable על אחד
[01:44:18 - 01:44:23] אז יש לנו דקודר של מה-Latent variable בצורה דטרמיניסטית מייצר לנו את התמונה
[01:44:23 - 01:44:26] וזה יכול לאפשר כל מיני דברים אחרים שאפשר לעשות עם
[01:44:27 - 01:44:29] להבין עם לשנות דברים בתוך
[01:44:33 - 01:44:35] ה-Latent representation הזה, סוג של,
[01:44:35 - 01:44:39] יותר קל לעשות עם זה representation learning כי יש לנו רק מקום אחד שבו יש את ה...
[01:44:40 - 01:44:44] משתנה החבוי ולמשל אפשר לעשות כל מיני דברים שקשורים
[01:44:44 - 01:44:48] לאינטרפולציה בין תמונות, דברים שראינו ב-BA.
[01:44:50 - 01:44:58] והדבר האחרון זה שאנחנו יכולים לחשוב על זה כמו שוב כמו flow ולהשתמש בזה כדי לחשב את הלוג-לייטליות בצורה יותר מדויקת,
[01:44:58 - 01:45:01] אני אומר יותר מדויקת כי גם פה הדבר הזה זה קירוב,
[01:45:02 - 01:45:05] זה פתרון מקורב לבעיית ה...
[01:45:05 - 01:45:10] לבעיה הרציפה,
[01:45:11 - 01:45:15] אבל הפתרון הזה משרה לנו דרך לחשב לוג-לייטליות,
[01:45:15 - 01:45:18] בדיוק כמו שחישבנו ב-Normalizing flows.
[01:45:20 - 01:45:28] יש פה דוגמה שהחישובים שעושים עם השיטה הזאת הם יותר טובים מהחסם אלבו שהיה לנו קודם.
[01:45:35 - 01:45:41] כן, אז כל מה שכתוב פה בעצם כבר היה לנו קודם.
[01:45:43 - 01:45:48] אוקיי, דבר אחרון זה עוד מושג שאני רק רוצה שתכירו את המושג הזה, flow-matching,
[01:45:48 - 01:45:51] אז זה בעצם, זה אחד מה...
[01:45:54 - 01:45:59] התחומי מחקר שעובדים בהם עכשיו, ממש בזמן הזה,
[01:45:59 - 01:46:02] הפיתוחים החדשים של מודלי דיפיוז'ן,
[01:46:02 - 01:46:04] זה בעצם הכללה של דיפיוז'ן מודל,
[01:46:05 - 01:46:11] שהיא אומרת, אוקיי, כל מה שיש לנו כאן זה בעצם איזשהו מיפוי מ-X0 ל-XT.
[01:46:14 - 01:46:18] ואנחנו הקדמנו את המיפוי הזה על ידי זה שאנחנו מוסיפים רעש כל פעם,
[01:46:19 - 01:46:22] עד שאנחנו מקבלים XT שהוא רעש מוחלט.
[01:46:23 - 01:46:29] אבל אולי אנחנו רוצים להגדיר את המיפויים האלה בדרך אחרת, יותר כללית, לאו דווקא להוסיף רעש.
[01:46:29 - 01:46:31] אולי אנחנו רוצים ממש להגדיר איזושהי פונקציה
[01:46:32 - 01:46:35] שלוקחת אותנו מנקודה אחת לנקודה אחרת במרחב.
[01:46:36 - 01:46:37] עכשיו יש כאן את ההמחשה הזאתי,
[01:46:38 - 01:46:39] שאם אנחנו מתחילים מכל מיני נקודות,
[01:46:40 - 01:46:42] אנחנו רוצים להגיע לנקודה אחת,
[01:46:43 - 01:46:44] אם נעשה את זה עם דיפיוז'ן,
[01:46:44 - 01:46:46] זה לאו דווקא יהיה הדרך הכי קצרה.
[01:46:47 - 01:46:51] אנחנו כל פעם מוסיפים רעש. תחשבו שאם זה הדאטה שלנו, אנחנו כל פעם מוסיפים לו רעש, אז הוא
[01:46:52 - 01:46:54] ילך באופן רנדומלי לכל הכיוונים,
[01:46:55 - 01:46:59] ומובטח לנו שבסופו של דבר הוא יגיע להתפלגות שרצינו.
[01:47:00 - 01:47:02] אבל לאו דווקא בדרך הכי קצרה.
[01:47:03 - 01:47:09] אם אנחנו חושבים על הדבר הזה רק בתור דרך להגיע מהתפלגות אחת להתפלגות שנייה, יכול להיות שאנחנו יכולים להגדיר דרכים יותר יעילות,
[01:47:09 - 01:47:10] לעשות את זה יותר מהר,
[01:47:11 - 01:47:15] כל עוד ההתפלגויות השוליות ב-X0 ו-XT הם ההתפלגויות הנכונות,
[01:47:16 - 01:47:23] ושאנחנו יכולים לחשב את הסיגנל של השינוי, זאת אומרת את המהירות בכל נקודה שבה זזנו,
[01:47:24 - 01:47:29] כדי לעשות את המצ'ינג הזה. במקום לעשות סקור מצ'ינג קוראים לזה פלור מצ'ינג בעצם,
[01:47:30 - 01:47:32] זה לא בדיוק התפלגויות בכל שלב,
[01:47:33 - 01:47:37] אלא אנחנו מחשבים ממש את המהירות של כל דגימה לאן היא עוברת,
[01:47:37 - 01:47:41] ולזה אנחנו מאמנים את הרשת שלנו לדעת לחזור את הדבר הזה,
[01:47:42 - 01:47:45] ומעבר לזה משתמשים בזה בדיוק באותה דרך שהם משתמשים ב-Defusion Model,
[01:47:46 - 01:47:50] אבל יש לזה פוטנציאל להיות הרבה יותר יעיל בכל מיני מובנים.
[01:47:53 - 01:47:55] אוקיי, אז סיימנו להיום,
[01:47:56 - 01:47:58] אז אני אסכם,
[01:47:58 - 01:48:02] ובעצם בשבוע הבא אנחנו נמשיך לדבר על אספקטים שונים של Defusion Models.
[01:48:03 - 01:48:04] אבל מה שראינו היום שבעצם,
[01:48:05 - 01:48:09] קודם כל ראינו היום דרכים שונות להציג Defusion Model,
[01:48:09 - 01:48:13] כשתראו אנשים שמדברים על Defusion Model, חלקם מדברים על זה במובן של Score Matching,
[01:48:14 - 01:48:27] הרבה מדברים על זה במובן של Laetent Variables ו-Elebo ויותר ויותר אנשים בזמן האחרון מדברים על זה במובנים של SDE, של תהליכים רציפים,
[01:48:28 - 01:48:31] שבעצם אנחנו משתמשים בכל מיני פתרונות נומריים כדי
[01:48:32 - 01:48:36] לשערך דברים בתוך התהליכים הרציפים האלה.
[01:48:37 - 01:48:39] אז ראינו את שלושת האספקטים האלה,
[01:48:40 - 01:48:42] שהם מובילים בסופו של דבר לאותו מודל,
[01:48:44 - 01:48:49] והיתרונות של המודל הזה, קודם כל בסופו של דבר ה-objective function הוא מאוד מאוד פשוט.
[01:48:50 - 01:48:57] אז ראינו שיטת אימון של זה, הרבה יותר פשוטה למשל מ-BAE או מפיקסל CNN או אפילו מ-flow,
[01:48:57 - 01:48:58] אפילו לא היינו צריכים כל פעם ללכת את כל הדרך.
[01:48:59 - 01:49:02] בפיקסל CNN היו צריכים גם כל פעם לפרק את ה...
[01:49:02 - 01:49:06] מלאם אחד מלכת לעשות את זה במקביל אבל היינו צריכים לדעת לפרק את הדאטה.
[01:49:08 - 01:49:14] ב-VE יש לנו את האלבו הזה שיש לנו את התהליך קדימה ואחורה,
[01:49:14 - 01:49:24] סליחה את ה-Q ואת ה-P, אנחנו צריכים לאמן את שניהם פה אנחנו מלכים שה-Q קבוע ובעצם ה-objective function שלנו הוא מאוד מאוד פשוט כל פעם אנחנו מוסיפים רעש לתמונה צריכים לדעת לחזות אותה.
[01:49:24 - 01:49:27] אז זה יתרון אחד.
[01:49:28 - 01:49:32] יתרון שני זה שהתוצאות פשוט יוצאות טובות מאוד.
[01:49:33 - 01:49:43] אז היום התוצאות הכי טובות זה תוצאות עם המודל הזה מה שאומר שזה כנראה לא בטוח שאי אפשר להגיע לתוצאות טובות כאלה גם עם מודלים אחרים אבל זה כנראה אומר שהמודל הזה הוא יחסית קל לאימון
[01:49:46 - 01:49:48] וקל להגיע איתו לתוצאות טובות.
[01:49:49 - 01:49:52] האימון גם הוא יותר יעיל ויותר מהיר
[01:49:52 - 01:49:55] יותר מהר אפשר להגיע לתוצאות טובות.
[01:49:56 - 01:50:01] שני דברים שאנחנו יותר נראה שבוע הבא זה שאפשר
[01:50:03 - 01:50:08] להשתמש בו לא רק בשביל דגימה אלא גם לעשות כל מיני
[01:50:08 - 01:50:11] פרובליסטיק אינפרנס או פרובליסטיק ריזנינג,
[01:50:12 - 01:50:17] כל מיני שאלות כאלה של בהינתן משהו או לחזות את משהו אחר.
[01:50:17 - 01:50:30] ועוד דבר שבאופן אמפירי ראו שהם ממש מגיעים לתוצאות טובות איתו זה שאפשר להרחיב את זה למודלים שהם קונדישיונל, זאת אומרת שהם תלויים בעוד איזשהו אינפוט
[01:50:31 - 01:50:36] והדוגמה הכי בולטת לזה זה המודלים שתלויים באיזשהו טקסט.
[01:50:37 - 01:50:39] אז אנחנו נדבר על זה גם בשבוע הבא,
[01:50:40 - 01:50:45] זה כל המודלי דלי 2 ודומיהם שבהינתן טקסט מייצרים תמונה.
[01:50:48 - 01:50:50] אז זה היתרונות של המודל הזה,
[01:50:50 - 01:50:53] חסרונות זה שהדגימה היא עדיין איטית,
[01:50:53 - 01:50:56] אוקיי? אז זה לא כמו אוטו-רגרסיב מודלס,
[01:50:57 - 01:51:04] אבל זה עדיין יכול להיות משהו כדי להגיע לתוצאות טובות, בדרך כלל עובדים על משהו כמו אלף איטרציות.
[01:51:07 - 01:51:12] יש כל מיני דרכים לשפר את זה, יש הרבה מחקר על איך להאיץ את זה,
[01:51:13 - 01:51:14] הרבה זה סביב הנושא הזה בדיוק של
[01:51:17 - 01:51:22] משוואות דיפרנציאליות, כל מיני דרכים לפתור משוואות דיפרנציאליות כדי להאיץ את הדגימה.
[01:51:23 - 01:51:24] אנחנו לא נדבר על זה,
[01:51:25 - 01:51:30] אבל אתם יכולים להסתכל, יש כל מיני דרכים שמנסות לשפר את הדבר הזה.
[01:51:31 - 01:51:37] עוד חיסרון זה שאין דרך טבעית לממש את הרעיון הזה לדאטה דיסקרטי,
[01:51:38 - 01:51:43] גם משהו שהוא מחקר פעיל עכשיו ויש כל מיני ניסיונות לעשות את זה,
[01:51:44 - 01:51:46] אבל זה לא מגיע בחינם,
[01:51:46 - 01:51:48] צריך לעבוד בשביל זה,
[01:51:49 - 01:51:50] וגם על זה אנחנו לא נדבר.
[01:51:51 - 01:51:53] אז שבוע הבא אנחנו ניפגש לשיעור האחרון,
[01:51:54 - 01:51:56] ונדבר על שתי הנקודות האלה,
[01:51:57 - 01:52:03] וגם נעשה סיכום בעצם של כל הקורס, כל המודלים שראינו.
[01:52:04 - 01:52:05] אז ניפגש ביום ראשון.