[00:00:00 - 00:00:00] טוב,
[00:00:00 - 00:00:01] שלום לכולם.
[00:00:03 - 00:00:04] שלום לכולם.
[00:00:05 - 00:00:09] אז עברנו את החלק הראשון של הקורס.
[00:00:10 - 00:00:10] שלום לכבוד,
[00:00:11 - 00:00:12] מי ששרד.
[00:00:13 - 00:00:21] ועכשיו אנחנו נדבר יותר על נושאים שונים, וכל הנושאים הם ממש מודל שאנחנו נסתכל עליו קצת יותר בפירוט,
[00:00:23 - 00:00:24] כולל מימוש.
[00:00:25 - 00:00:27] חלק מסתכל ממש על הפרטים בפרקידה,
[00:00:27 - 00:00:30] חלק אתם תעשו בתרגילים.
[00:00:32 - 00:00:37] והיום אנחנו נתחיל עם הגישה שנקראת auto-regressive models,
[00:00:39 - 00:00:41] לדבר יותר ספציפית על מודל שנקרא פיקס וCNN,
[00:00:42 - 00:00:47] וזה יהיה מודל שאתם תעבדו עליו בבת בית.
[00:00:48 - 00:00:52] אני לא תסתכלו ממש לממש אותו, כי אנחנו נתחיל מימוש כבר קיים שלו,
[00:00:53 - 00:00:58] שצטרפו לעשות עליו כל מיני תוספות ובדיקות,
[00:00:59 - 00:01:02] וזה יהיה גם הבסיס שאחר כך אנחנו נממש מודלים אחרים,
[00:01:03 - 00:01:04] לתוך הכול.
[00:01:06 - 00:01:08] לא סיימתי לארגן את כל התרגילים,
[00:01:08 - 00:01:11] זה עדיין לא ברור לי איך נראה עד הסוף,
[00:01:12 - 00:01:16] אבל זאת הכוונה, יכול להיות שנראה שיש כמה דברים שיותר מסובכים,
[00:01:16 - 00:01:18] בכל מסובכים זה משנה,
[00:01:19 - 00:01:20] אבל זאת הכוונה.
[00:01:20 - 00:01:23] אז היום כבר בסוף שיעור נסתכל קצת על הקוד עצמו.
[00:01:27 - 00:01:29] קיבלתם כבר חזרה נכון תרגיל אחד,
[00:01:30 - 00:01:33] ויש לכם תרגיל שלישי עכשיו שאתם מוכנים להגיש,
[00:01:34 - 00:01:35] על השבוע בגדול,
[00:01:37 - 00:01:42] והתרגיל הבא יהיה כבר תרגיל שקשור למימוש של מה שאנחנו נראה היום.
[00:01:50 - 00:01:53] שאלות על התרגילים, אם יכולה לצאת אופציה טוב של הקורס,
[00:01:54 - 00:01:55] יש לכם אתם חלק חשבים?
[00:01:59 - 00:02:00] אוקיי, אז היום התוכנית,
[00:02:02 - 00:02:02] זה
[00:02:04 - 00:02:08] תתחיל שוב מחזרה בעצם להחזיר אתכם לתמונה גדולה של מה אנחנו רוצים לעשות,
[00:02:10 - 00:02:17] אני חושב שיש לכם כבר קצת יותר הבנה של חלק מהעקרונות היותר תאורטיים,
[00:02:17 - 00:02:20] אבל זה יהיה לכם יותר קר לראות קצת את התמונה הגדולה,
[00:02:22 - 00:02:28] ואז אנחנו נדבר על הגישה שאנחנו מדברים עליה היום, לאוטו-רגרסיב מודל, נראה אותה בהתחלה בלי מודלים עמוקים,
[00:02:29 - 00:02:30] בלי רשתות עמוקות,
[00:02:31 - 00:02:32] רג'יסטיק רגרשן פשוט,
[00:02:33 - 00:02:37] אחר כך נעשו תזכורת למה זה אומר לממש את זה עם הרשתות הדוגות,
[00:02:38 - 00:02:40] ואז נראה כמה דוגמאות,
[00:02:40 - 00:02:45] נעבור מהר לשלושת הדוגמאות האלה שהם מודלים קודמים,
[00:02:45 - 00:02:49] ונתעכב קצת יותר על המודל עבור פיקסל-CNN,
[00:02:49 - 00:02:53] המודלים האלה הם פיקסל-CNN מ-2015 אני חושב,
[00:02:54 - 00:02:56] אלא כמה שנים לפני.
[00:02:59 - 00:03:01] כולם פה עשו את הקורס ב-deep learning?
[00:03:02 - 00:03:06] דיברנו בשיעור הראשון אבל זה היה בזום ולא כל כך תכוונתי מה מצארכם.
[00:03:07 - 00:03:08] מי עשה את הקורס ב-deep learning?
[00:03:12 - 00:03:13] זה הרוב הגדול.
[00:03:14 - 00:03:17] מישהו עושה עכשיו? אין עכשיו בעצם קורס בדיוק.
[00:03:19 - 00:03:21] מישהו עשה איזשהו החלטה בקורס?
[00:03:24 - 00:03:31] בסדר, טוב אני מקווה שכולם, לא שמנו את זה בתור דרישת דם רשמית,
[00:03:31 - 00:03:33] לא יודע אם זה היה נכון או לא נכון,
[00:03:34 - 00:03:36] הכוונה היא אבל שאתם,
[00:03:37 - 00:03:41] ההנחה היא שאתם תסתדרו עם לממש את הדברים האלה,
[00:03:41 - 00:03:43] אבל נעשה היום את זה חברה לצורה כזאת בשביל מה זה אומר.
[00:03:45 - 00:03:47] אוקיי, אז אנחנו בתזכורת.
[00:03:48 - 00:03:53] מה זה מודלים גנרטיביים? אמרנו שזה מודלים שאנחנו,
[00:03:56 - 00:03:59] שהoutput שלהם הוא בממד גבוה, זה דבר ראשון,
[00:03:59 - 00:04:02] כמו תמונות, כמו אודיו, כמו טקסט,
[00:04:03 - 00:04:05] וגם הם הסתברותיים.
[00:04:05 - 00:04:08] זה בגדול שתי הדרישות שלנו שאנחנו אומרים שהמודל הוא מודל גנרטיבי.
[00:04:09 - 00:04:13] הסתברותיים, זה אומר שהמודל שלנו בעצם ממדל את ההסתברות
[00:04:14 - 00:04:16] באיזשהו אופן,
[00:04:16 - 00:04:18] ועל ידי זה אנחנו יכולים לעשות כל מיני דברים.
[00:04:19 - 00:04:20] בין השאר, גם לייצר
[00:04:21 - 00:04:23] דגימות מהמודל ההסתברותי הזה.
[00:04:25 - 00:04:29] ומודלים שונים הם נמצאים במקומות שונים. יש כאלה שקל לדגום,
[00:04:30 - 00:04:34] אבל קשה, נגיד, לבדוק מה ההסתברות של דוגמה חדשה.
[00:04:35 - 00:04:37] ויש מודלים אחרים שדווקא
[00:04:38 - 00:04:41] בסדר, אמרתי את זה עכשיו. זה קשה לדגום,
[00:04:41 - 00:04:46] אבל קל לבדוק את ההסתברות של נקודה חדשה.
[00:04:47 - 00:04:52] אז יש מודלים שונים, יש להם בעצם משמעות אחרת למילה הזאת,
[00:04:53 - 00:04:55] פורבנמיניסטית. מה זאת אומרת שהמודלים זה דוגמאותי?
[00:04:56 - 00:04:57] אז היום אנחנו נראה דוגמה אחת,
[00:04:58 - 00:05:00] ונמשיך בהמשך לדוגמאות יותר.
[00:05:03 - 00:05:03] לא, למה לדבר על ההתקפה הזאת?
[00:05:04 - 00:05:10] אוקיי, אז מה אנחנו יכולים לעשות עם המודלים האלה, או מה אנחנו רוצים לעשות איתם?
[00:05:11 - 00:05:12] אנחנו רוצים, לפעמים אנחנו
[00:05:14 - 00:05:17] בראש יש לנו איזושהי משימה כמו פסיפיקציה,
[00:05:17 - 00:05:19] אנחנו משתמשים בגישה גנרטיבית כדי לפתור אותה.
[00:05:20 - 00:05:27] למשל, אנחנו לומדים את כל המודל ההסתברותי של תמונות של חפולים לעומת מודל ההסתברותי של תמונות של כלבים, כדי שאחר כך שיהיה לנו תמונה חדשה,
[00:05:28 - 00:05:31] אנחנו פשוט נבדוק תחת איזה מודל יש להסתברות יותר גבוהה,
[00:05:32 - 00:05:36] וזו הקלסיפיקציה שלנו, דרך שנעשה קלסיפיקציה
[00:05:37 - 00:05:38] ותמיד זאת המטרה
[00:05:39 - 00:05:41] מסך אחרת זה לייצר דאטה
[00:05:41 - 00:05:45] למשל בתמונות, הרבה פעמים זה דבר ממש אמנותי, אנחנו רוצים לייצר כל מיני
[00:05:47 - 00:05:49] תמונות של כל מיני דברים יפים ומעניינים
[00:05:50 - 00:05:56] אני לא חייב להיות אמנותי, זה יכול להיות גם שאנחנו רוצים לייצר דוגמאות חדשות כדי שנוכל לאמן איזשהו מודל אחר
[00:05:57 - 00:06:00] או כדי שנבין איך הדאטה הזה בנוי
[00:06:01 - 00:06:01] וכל מיני דברים כאלה
[00:06:02 - 00:06:23] וזו מטרה שנייה, ממש לייצר דאטה. מטרה שלישית, קראנו לה presentation learning זה בעצם להבין משהו על המבנה של הדאטה ולייצג אותו בדרך אחרת. במקום לייצג תמונה בתור אוסף של פיקסלים לייצג אותו בתור איזשהו אוסף של פקטורים או פיצ'רים שהם חבויים,
[00:06:23 - 00:06:25] הם לא ממש רואים אותם מהפיקסלים,
[00:06:26 - 00:06:29] אלא שאם יש לנו מודל גנרטיבי טוב של התמונות
[00:06:30 - 00:06:32] אפשר מתוכו להסיט מה המבנה.
[00:06:34 - 00:06:39] אז אנחנו היום, זה יהיה דוגמה למודל שהוא לא כל כך טוב בשביל המשימה הזאת,
[00:06:40 - 00:06:45] אבל שבוע הבא אנחנו נדבר על מודלים עם משתנים חבועים שהם טובים דווקא יותר במשימה הזאת.
[00:06:48 - 00:06:57] זו דוגמה שאמרנו, אוקיי, אם יש לנו מודל שיש לו הסתברות ואנחנו יודעים להגיד דברים הם בהסתברות גבוהה או נמוכה, אם הפרדיקציה שלנו אנחנו בטוחים לגביה,
[00:06:57 - 00:07:01] להריץ הרבה פרדיקציות, כל פעם זה יהיה אותו דבר או שכל פעם זה יהיה משהו אחר.
[00:07:02 - 00:07:09] זה דברים שיכולים לעזור לנו מאוד לא רק לתת פרדיקציה אלא גם לעשות איזשהם מערכת קבלת החלטות על סמך הפרדיקציה הזאת.
[00:07:09 - 00:07:13] אם אנחנו יודעים שאנחנו מאוד בטוחים במשהו זה יעזור לנו לקבל החלטה
[00:07:13 - 00:07:14] לגבי זה.
[00:07:15 - 00:07:25] ומשהו שהוא קצת יותר כללי ומכיל קצת את הכל או מין גישה לעשות את כל הדברים האלה זה פורמוליסטיק אינפרנס, זה בעצם כל משימה ברגע שהמודל שלנו הוא מודל הסתברותי
[00:07:25 - 00:07:29] אנחנו יכולים להגדיר כל מיני משימות על ידי איזושהי בעיית
[00:07:29 - 00:07:32] אורוביליסטיק אינפרנס, זאת אומרת יש לנו משתנה שאנחנו צריכים
[00:07:33 - 00:07:34] להבין מה ההסתברות שלו
[00:07:35 - 00:07:40] משתנה זה למשל יכול להגיד האם יש לנו קלאס של כלב או קלאס של חתום
[00:07:41 - 00:07:41] או
[00:07:43 - 00:07:49] האם עד כמה התמונה הזאת של הפנים היא של מישהו שהוא במצב רוח טוב או לא טוב
[00:07:50 - 00:07:53] כל מיני דברים שאנחנו מוצאים או מה הפיקסל,
[00:07:54 - 00:07:57] מהחצי התחתון של התמונה ולפי החצי העליון של התמונה
[00:07:58 - 00:08:04] כל מיני דברים כאלה אנחנו יכולים להגדיר בתור יש לנו משתנים שאנחנו לא יודעים אותם ואנחנו צריכים לחשב את ההסתברות אליהם
[00:08:07 - 00:08:09] וזה בעצם בעיה של הסקה
[00:08:09 - 00:08:11] הסתברותית אורגניסטיק אינפרנס
[00:08:11 - 00:08:15] ואז אם יש לנו איזושהי דרך כללית לתור כל מיני בעיות
[00:08:16 - 00:08:20] של הסקה הסתברותית כזאת אנחנו יכולים לפתור הרבה בעיות שונות
[00:08:21 - 00:08:25] אפילו בעיות שכשאימנו את המודל שלנו, לא ידענו שזה מה שאימנים אותנו
[00:08:26 - 00:08:32] זה בעצם הדבר שהכי אולי מראה את הכוח של הגישה הזאת,
[00:08:32 - 00:08:33] גישה למיוחדית
[00:08:35 - 00:08:37] אוקיי, איך אנחנו
[00:08:39 - 00:08:44] לומדים את המודל הזה, אז גם תזכורת למשהו שראינו כבר כמה פעמים,
[00:08:44 - 00:08:47] אנחנו מניחים שיש לנו התפלגות שמייצרת את הדאטה,
[00:08:47 - 00:08:48] קוראים לה פי דאטה
[00:08:49 - 00:08:51] אנחנו מניחים שיש לנו מודל
[00:08:52 - 00:08:53] עם איזושהי התפלגות,
[00:08:54 - 00:08:57] איזה שהם פרמטרים לא ידועים, אנחנו קוראים למודל הזה פי דאטה,
[00:08:58 - 00:09:00] אנחנו רוצים שפי דאטה יהיה קרוב לפי דאטה,
[00:09:03 - 00:09:06] אם הדאטה שלנו היה דו-מימד היינו יכולים לצייר את
[00:09:07 - 00:09:08] הנדרך הזה,
[00:09:08 - 00:09:13] אזורים גבוהים זה אזורים עם הסתברות גבוהה, אזורים מנותקים אזורים עם הסתברות נמוכה
[00:09:14 - 00:09:21] אוקיי, מה המרכיבים שאנחנו צריכים כדי לאמן
[00:09:21 - 00:09:23] מודל נרטיבי או החלטות שצריך לעשות,
[00:09:24 - 00:09:26] אז קודם כל אנחנו צריכים דאטה כדי לעבוד איתו
[00:09:27 - 00:09:30] והדאטה הזה גם צריך להיות
[00:09:31 - 00:09:37] מייצג טוב את הדאטה שקוראים לך, צריכים להציג דוגמאות שמייצגות את המרחב הזה שאנחנו רוצים
[00:09:38 - 00:09:40] עליו ללמוד מודל תרבותי
[00:09:41 - 00:09:44] וגם אנחנו צריכים איזשהו ייצוג טוב שלו,
[00:09:44 - 00:09:45] נדבר על זה קצת היום,
[00:09:46 - 00:09:47] מודל,
[00:09:47 - 00:09:49] אנחנו צריכים להחליט מה המודל ההסתברותי שלנו,
[00:09:49 - 00:09:56] מה הפרמטריזציה שלו ואיזה סוג של מודל זה, אז כבר ראינו דוגמה של גאוסיאנים ותערובת גאוסיאנים
[00:09:56 - 00:10:02] ודיברנו על Aten-Valable מודל באופן כללי ואנחנו בעצם ממשיכים עם ה...
[00:10:02 - 00:10:04] היום אנחנו לא נדבר על גאוסיאנים כל כך,
[00:10:05 - 00:10:10] אבל אנחנו נדבר על משהו... הייתי יכול לכתוב כאן את ברנולי גם,
[00:10:10 - 00:10:13] דיברנו על ברנולי והיום אנחנו נדבר יותר על ברנולי, כי אנחנו נדבר על דאטה דיספקי יותר
[00:10:17 - 00:10:18] אבל גם כשאנחנו מדברים על מודל
[00:10:20 - 00:10:22] שמבוסס על רשתות נוירונים,
[00:10:22 - 00:10:26] הרבה פעמים בסופו של דבר הרשתות נוירונים זה פרמטריזציה
[00:10:27 - 00:10:29] לגאוסיאנים או תערובת גאוסיאנים
[00:10:31 - 00:10:32] או לברנולי, כפי שנראה היום,
[00:10:32 - 00:10:36] ואחר כך יש את השאלה של איך אנחנו מאמנים, זאת אומרת מה ה-objective function שלנו,
[00:10:37 - 00:10:38] שלרוב זה יהיה maximum likelihood,
[00:10:39 - 00:10:40] דיברנו על זה גם כבר,
[00:10:41 - 00:10:42] וגם זה מה שנעשה היום.
[00:10:44 - 00:10:47] בסוף הקורס אנחנו נדבר קצת על שיטה אחרת שנקראת Score Matching,
[00:10:48 - 00:10:49] גם אפשר לחשוב עליה בתור
[00:10:50 - 00:10:55] קירוב של maximum likelihood או איזושהי דרך לשערך את ה-MXLightיות
[00:10:57 - 00:11:02] והמרכיב הרביעי זה איך אנחנו עושים את האופטימיזציה של ה-objective function
[00:11:04 - 00:11:06] יש מקרים כמו היום שזה קל,
[00:11:07 - 00:11:09] אנחנו לא צריכים להסתבך עם כל מיני מרכים כאלה,
[00:11:10 - 00:11:14] ויש מקרים שזה בעיקר כשיש לנו משתנים חבויים
[00:11:15 - 00:11:19] שאנחנו צריכים לעשות והם קצת יותר מתוחכמים, וזה קצת התוארה שדיברנו בשני שיעורים האחרונים,
[00:11:19 - 00:11:22] שזה ה-Varational Influence ו-MCNC
[00:11:23 - 00:11:25] היום לא נדבר על זה, אבל בשבוע הבא נחזור
[00:11:27 - 00:11:28] לשיטות האלה
[00:11:30 - 00:11:36] ובגדול הבעיה, הסיבה שזה קשה זה שפשוט לא קשוט הכול בצורה ישירה, זה שיותר מדי
[00:11:36 - 00:11:36] פרמטרים
[00:11:38 - 00:11:40] ויש לנו את קללת המימד, נכון?
[00:11:40 - 00:11:46] מספר הפרמטרים גדל בצורה אקספוננציאלית, אם אנחנו רוצים תשכית הסתברות לכל אחת מהאפשרויות
[00:11:47 - 00:11:50] של תמונה בגודל N-PXL
[00:11:52 - 00:11:55] אנחנו צריכים להכניס כל מיני הנחות למבנה
[00:11:55 - 00:12:04] אנחנו נדבר קצת על מבנה, אבל דיברנו על conditional independent בתור סוג של המערכות שמורידו אותנו את מספר הפרמטרים
[00:12:07 - 00:12:09] אוקיי,
[00:12:10 - 00:12:12] אז בואו נתחיל לדבר קצת על הדאטה, אז
[00:12:13 - 00:12:16] אנחנו שתי דוגמאות, אני חושב שאנחנו נעבוד איתן
[00:12:18 - 00:12:22] כנראה עד סוף הקורס, זה דאטוסק הזה שנקרא אמניסט
[00:12:23 - 00:12:24] זה דאטוסט של,
[00:12:25 - 00:12:29] אני מניח שאתם מכירים אותו כבר, דאטוסט של תמונות
[00:12:30 - 00:12:31] בקור לבן
[00:12:32 - 00:12:32] של ספרות
[00:12:33 - 00:12:34] מ-0 עד 9
[00:12:37 - 00:12:40] ודאטוסט השני זה דאטה של תמונות טבעיות
[00:12:42 - 00:12:43] שנקרא ImageNet
[00:12:44 - 00:12:46] ואנחנו נעבוד עם גרסה מותנת שלו
[00:12:47 - 00:12:48] Imagement 32
[00:12:49 - 00:12:51] זה גם קשור רק 32 או 32
[00:12:54 - 00:12:57] זאת אומרת, מעני החישוב של כל דבר שנעשה יהיה קצת גדול
[00:12:59 - 00:13:02] יש כמה דרכים לייצג תמונות
[00:13:03 - 00:13:06] אז שתי הדרכים הסטנדרטיות זה דרך דיסקרטית
[00:13:07 - 00:13:10] ככה זה ממש בעצם במחשב,
[00:13:11 - 00:13:13] בדרך כלל ככה נשמעות התמונות
[00:13:14 - 00:13:18] בתור כל פיקסל, בעצם אינטג'ר, מספר
[00:13:22 - 00:13:27] שלם, ציובי, עם 8 ביטס, זאת אומרת זה מספר בין 0 ל-255
[00:13:29 - 00:13:29] דברים שונים
[00:13:30 - 00:13:31] יש לנו 256
[00:13:36 - 00:13:37] ערכים אפשריים
[00:13:38 - 00:13:39] של כל פיקסל
[00:13:40 - 00:13:43] ושיטה אחרת לייצג תמונות זה שכל פיקסל הוא ערך רציף
[00:13:44 - 00:13:46] ואז בדרך כלל מנרמלים את זה בין 0 ל-1
[00:13:50 - 00:13:52] ואם יש לנו תמונת צבעונית אז בעצם כל פיקסל הוא
[00:13:55 - 00:13:56] שלושה ערכים כאלה
[00:13:56 - 00:13:58] זה יכול להיות גם תמונת צבעונית
[00:13:59 - 00:14:02] ייצוג דיסקטי ותמונת צבעונית עם ייצוג רציף
[00:14:04 - 00:14:07] בסדר, היום אנחנו נדבר על הייצוג הדיסקטי
[00:14:08 - 00:14:10] מודלים אוטוריגרסיביים,
[00:14:11 - 00:14:14] פיתחו אותם בהתחלה עם גישה כזאת, זה היה יותר קל
[00:14:15 - 00:14:20] אבל כל מה שאנחנו רואים היום גם אפשר לעשות בייצוג רציף
[00:14:23 - 00:14:24] תודה רבה, תשמעו לזה
[00:14:25 - 00:14:32] אז מודלים, בשביל הייצוג הדיסקרטי בעצם המודלים שדיברנו עליהם
[00:14:33 - 00:14:37] אנחנו ממשיכים לדבר עליהם היום, זה מודל ברנולי ומודל קטגורי
[00:14:38 - 00:14:43] ברנולי זה פשוט אומר שלכל פיקסל יש רק שני ערכים
[00:14:44 - 00:14:48] אז יש לנו פרמטר אחד בעצם שאומר לנו מה ההסתברות
[00:14:49 - 00:14:51] כשהפיקסל יהיה לבן
[00:14:52 - 00:14:55] והאחד פחות ההסתברות זה ההסתברות שהוא יהיה שחור
[00:14:56 - 00:15:02] זה קטגורי, זה שיש לנו, נגיד אם אנחנו לוקחים באמת 256 ערכים לכל פיקסל
[00:15:03 - 00:15:06] ואנחנו צריכים 255 מספרים כאלה שהם כולם
[00:15:08 - 00:15:08] בין 0 ל-1
[00:15:09 - 00:15:10] כשהסכום שלהם שווה 1
[00:15:12 - 00:15:16] הסכום שלהם קטן מ-1 כי אז יש את הערך האחרון שהוא משלים ל-1
[00:15:17 - 00:15:19] אני צריך 255 מספרים כדי
[00:15:23 - 00:15:27] למדל את ההסתברות הקטגורית של כל ההסתברות הקטגורית של חיילים לציון
[00:15:30 - 00:15:36] זה העיתות שאנחנו נדבר עליו היום, ואם אנחנו בשבוע הבא אני חושב שכבר נעבור לייצוג רציף
[00:15:37 - 00:15:39] אז דיברנו כבר על גאוסיאן, זה יכול להיות ייצוג רציף
[00:15:40 - 00:15:42] למרות שהוא ייצוג לא כל כך טוב כי אם הערכים שלנו
[00:15:43 - 00:15:46] הם בין 0 ל-1 ולא יכולים לקטנים מ-0
[00:15:46 - 00:15:49] אז זה לא יכול להיות גדולים מאחד, אז אנחנו לא יכולים בעצם על ידי גאוסיאן
[00:15:50 - 00:15:55] למדל את אותו דבר, גאוסיאן יכול לקבל את כל הארכיב
[00:15:56 - 00:16:00] אבל עדיין הרבה פעמים מקרובים את זה על ידי איזשהו גאוסיאן או איזושהי תערובת של גאוסיאנים
[00:16:01 - 00:16:02] כמו שראינו
[00:16:12 - 00:16:14] עד עכשיו בעצם מה שאנחנו ראינו,
[00:16:14 - 00:16:24] אנחנו לא כל כך נכנסנו ממש למודלים של תמונות אבל בכל הדוגמאות שדיברנו בכיתה כשאמרנו שאנחנו עושים נידול ברנולי או קטגורי אז באמת אמרנו
[00:16:25 - 00:16:28] שקבענו שאנחנו עושים פרמטריזציה מלאה של ההתפלגות הזאת
[00:16:29 - 00:16:31] זאת אומרת אנחנו פשוט שומרים את הפרמטר
[00:16:32 - 00:16:34] אם זה ברנולי אנחנו שומרים את הפרמטר P
[00:16:34 - 00:16:39] אם זה קטגורי אז יש לנו פרמטר שאומר מה ההסתברות של כל אחד מהערכים האפשריים
[00:16:40 - 00:16:43] אם זה גאוסיאן אז יש לנו את הפוחלת פשוט מספר שאומר מה הפוחלת
[00:16:44 - 00:16:47] ועוד איזה מספר שם הוא אמר וריאנס,
[00:16:47 - 00:16:48] זה רב מלמדי, אז זה כמה מספרים
[00:16:49 - 00:16:50] בוקטור במטריצה.
[00:16:51 - 00:16:58] אבל זאת הכוונה, זה ייצוג, זה פרמטריזציה פשוטה, זה פשוט הפרמטריזציה המלאה של הבעיה.
[00:17:00 - 00:17:06] היום אנחנו נדבר על פרמטריזציה קצת יותר שעושה כל מיני הנחות,
[00:17:06 - 00:17:08] אלא היא מבוססת על רשתות עמוקות.
[00:17:09 - 00:17:12] כן, תכף נסביר קצת יותר מה זה,
[00:17:13 - 00:17:14] תכף נסביר קצת יותר מזומנטי
[00:17:24 - 00:17:24] טוב,
[00:17:25 - 00:17:33] אוקיי, מה הבעיה עם פרמטריזציה מלאה? זה מה שאמרנו, פרמטריזציה מלאה אנחנו די מהר,
[00:17:34 - 00:17:36] מספר הפרמטרים גדל,
[00:17:37 - 00:17:40] וגם אם אנחנו עושים ייצוג רציג, גם ייצוג בדיק,
[00:17:41 - 00:17:46] ראינו למשל שבתמונה אם אנחנו יש לנו n פיקסלים,
[00:17:46 - 00:17:49] אנחנו נצטרך שתיים, נגיד שזו תמונה אפילו בינארית,
[00:17:50 - 00:17:52] לא עם 256 ערכים בכל פיקסל,
[00:17:52 - 00:17:53] יש שני ערכים בכל פיקסל,
[00:17:54 - 00:17:57] אז אם יש לנו n פיקסלים אנחנו נצטרך שתיים בפזקת n
[00:17:58 - 00:17:59] ערכים כדי
[00:18:02 - 00:18:07] לתת איזושהי הסתברות קטגורית בכל אחת מהאפשרויות,
[00:18:07 - 00:18:08] יש שתיים בפזקת n אפשרויות.
[00:18:11 - 00:18:14] אז אנחנו צריכים, דיברנו על כל מיני
[00:18:15 - 00:18:18] כל מיני הנחות שאנחנו עשו בדרך,
[00:18:19 - 00:18:20] לשימוש
[00:18:23 - 00:18:26] בכלל השרשרת של ההסתברות, אז כל הסתברות כזאת היא
[00:18:27 - 00:18:28] של n פיקסלים,
[00:18:29 - 00:18:30] יכול לפרק באותו
[00:18:31 - 00:18:33] שרשרת של התפלגויות מותנות כאלה,
[00:18:34 - 00:18:37] שכל אחד מותנה על כל אלה שלפניו,
[00:18:37 - 00:18:38] יינתן איזשהו סידור,
[00:18:39 - 00:18:41] אוקיי, כל סידור הוא סידור חוקי,
[00:18:42 - 00:18:43] זה תמיד נכון,
[00:18:45 - 00:18:45] אבל זה עדיין,
[00:18:46 - 00:18:48] ראינו, זה עדיין לא חוסך לנו את הפרמטר,
[00:18:48 - 00:18:48] כי פה,
[00:18:50 - 00:18:50] מה קורה כאן?
[00:18:52 - 00:18:55] כאן יש לנו הרבה פחות פרמטר, בשביל הפיקסל הראשון יש לנו רק פרמטר אחד,
[00:18:55 - 00:18:56] הפיקסל השני,
[00:18:57 - 00:18:58] יש לנו,
[00:18:59 - 00:19:03] כל ערך של x1 יש לנו פרמטר 1 ל-x2,
[00:19:04 - 00:19:05] נדבר עכשיו דוגמה בינאלי,
[00:19:06 - 00:19:13] אוקיי, זה ברור, אמיתי, כן, אז אם x1 היה שווה 0, אני צריך להגיד מה ההסתברות של x2 שווה 1,
[00:19:14 - 00:19:16] אבל אם x1 היה שווה 1,
[00:19:16 - 00:19:18] אז יכול להיות שההסתברות של x2 היא משהו אחר,
[00:19:19 - 00:19:20] זו הכוונה, שזה מותנה,
[00:19:21 - 00:19:25] אז אני צריך עבור כל ערך של x1 לתת את ההסתברות של x2,
[00:19:25 - 00:19:27] שבוא נקרא שזה רנולי זה רק מספר 1,
[00:19:28 - 00:19:29] אבל האחרון פה,
[00:19:29 - 00:19:31] אז הוא מותנה על כל הקודמים,
[00:19:32 - 00:19:33] על n-1 קודמים,
[00:19:34 - 00:19:36] אז עבור כל קומבינציה שונה של כל הקודמים,
[00:19:37 - 00:19:39] אני צריך לתת מספר עבור הפיקסל האחרון.
[00:19:40 - 00:19:48] אוקיי, אז הטבלה שתארת את הדבר הזה, הגודל שלה יהיה n-1 על 1.
[00:19:48 - 00:19:53] סליחה, כל האפשרויות של n נוסחת 1 זה 2 בחזקת n-1 על 1.
[00:19:54 - 00:19:57] אוקיי, אז לא חסכתי את האקספוננציאליות של הבעיה.
[00:19:59 - 00:20:03] זה הגיוני, אם זה תמיד נכון, אז אני לא אעשה פה שום הנחה,
[00:20:03 - 00:20:06] לא יכול להיות שאני הורדתי את מספר הפרמטרים שמוצעו.
[00:20:08 - 00:20:15] אוקיי, אז איך אפשר בכל זאת להשתמש בדבר הזה כדי לבנות מודל סביר?
[00:20:17 - 00:20:18] אז בעצם דיברנו על שתי אפשרויות.
[00:20:20 - 00:20:22] האפשרות פה משמאל
[00:20:24 - 00:20:26] זו האפשרות שאנחנו עושים, אנחנו מניחים
[00:20:27 - 00:20:31] שיש conditional independence, אי תלות מותנית,
[00:20:32 - 00:20:36] על המשתנים שאנחנו רואים, שבמקרה של תמונה זה פשוט הפיקסלים עצמם.
[00:20:38 - 00:20:43] אוקיי, אז בעצם אנחנו אומרים ההסתברות של כל התמונה שלנו, של כל הפיקסלים,
[00:20:44 - 00:20:45] היא שווה לאיזושהי מכפלה,
[00:20:46 - 00:20:48] גם קודם זו הייתה מכפלה, נכון?
[00:20:48 - 00:20:49] גם הייתה מכפלה כזאת,
[00:20:51 - 00:20:54] אבל כאן המכפלה הזאת, כל xi, כל פיקסל,
[00:20:54 - 00:20:56] הוא לא תלוי בכל הקודמים שלו,
[00:20:57 - 00:20:59] אלא הוא תלוי רק בנגיד k קודמים,
[00:21:00 - 00:21:03] פשוט ידעתי כאן, כן? זה כל ה-xים מ-i-k עד i-k-i.
[00:21:06 - 00:21:13] זה בעצם אומר שאני מניח אי תלות מסוימת של פיקסל, בהינתן ה-k הפיקסלים הקודמים שלו, לפי איזשהו סידור,
[00:21:14 - 00:21:15] הוא לא תלוי בשאר הפיקסלים.
[00:21:18 - 00:21:21] אוקיי, אז זו אופציה אחת שאנחנו יכולים לעשות,
[00:21:22 - 00:21:28] וזה חושף לנו הרבה פרמטרים, כי עכשיו כמה פרמטרים יהיה לנו,
[00:21:28 - 00:21:30] במקום שאני צריך לבדוק את כל האפשרויות
[00:21:31 - 00:21:31] ל-n
[00:21:32 - 00:21:34] מינוס אחד איברים, זה רק k, אני חוסם את זה.
[00:21:35 - 00:21:38] כן, אז עדיין יש לי משהו אקספונציאלי, אבל אני יודע,
[00:21:38 - 00:21:41] זה יכול להיות בגודל קטן, k יכול להיות עם עשרה פיקסלים,
[00:21:42 - 00:21:44] אז זה יהיה לי רק 2 פיקסלים עשר אפשרויות,
[00:21:45 - 00:21:45] את כל קבלה.
[00:21:47 - 00:21:49] כן, אז זה 2 פיקסל עשר, כפול,
[00:21:50 - 00:21:51] במקרה זה b של 2, פיקסל,
[00:21:52 - 00:21:53] זה המימד של כל פיקסל,
[00:21:54 - 00:21:55] כפול n פיקסלים.
[00:21:59 - 00:22:03] b זה 2 או 256, זה כמה ערכים הפיקסל יכול לקבל,
[00:22:04 - 00:22:05] זה המימד כאילו של
[00:22:06 - 00:22:08] מוכר הקבע.
[00:22:12 - 00:22:17] אוקיי, אז הורדנו פה, בעצם ההורדה כאן היא בזה שהחזקה היא ב-k ולא ב-n
[00:22:17 - 00:22:17] מינוס אחד.
[00:22:21 - 00:22:25] טוב, אם אני רוצה שכל פיקסל יעלה תביא בעשרה פיקסלים אחרונים, אז גם הפיקסל האחרון,
[00:22:26 - 00:22:27] שהיו מיליון פיקסלים לפניו,
[00:22:28 - 00:22:29] וצריך להסתכל על כל מיליון האפשרויות,
[00:22:31 - 00:22:33] 2 פסקת מיליון אפשרויות, אלא רק 2 פסקת עשר אפשרויות.
[00:22:36 - 00:22:39] אוקיי, אז זו גישה אחת שאנחנו נדבר עליה קצת יותר בהרחבה היום.
[00:22:43 - 00:22:45] קודם כל, מה הבעיה בגישה הזאתי? ככה, כמו שהיא כתובה עכשיו,
[00:22:46 - 00:22:52] זה שאנחנו בעצם מעווים כאן הרבה מידע, לפעמים אנחנו כן רוצים שהפיקסל יהיה תלוי בכל מה שקרה לפני זה.
[00:22:53 - 00:22:55] אבל אנחנו נראה שיש דרכים, בעצם,
[00:22:56 - 00:23:00] יש דרך לעשות את זה שזה כן יהיה תלוי בכל הפיקסלים הקודמים, אבל עם מספר יותר קטן של פרמר,
[00:23:01 - 00:23:02] זה מה שאנחנו נדבר עליו היום.
[00:23:02 - 00:23:04] יש גם בעניין של הפיקסלים של
[00:23:07 - 00:23:08] לא עכשיו, לא, אתה יכול להחליט,
[00:23:09 - 00:23:12] זה מה שאנחנו נעשה היום, שאנחנו מסדרים את הפיקסלים ככה, הפיקסלים של הסדר הזה,
[00:23:13 - 00:23:14] זאת אומרת שאנחנו מגיעים לפיקסל הזה,
[00:23:15 - 00:23:17] כל אלה זה הפיקסלים שהיו לפניו.
[00:23:18 - 00:23:20] נכון, ואז אם אתה רוצה להגיד לך סופר יותר נורכבי,
[00:23:21 - 00:23:25] אני לא חושב שפיקסל נמצא בעבודה הכי נמידה, נכון,
[00:23:25 - 00:23:27] בסידור הזה נגיד הפיקסל הזה,
[00:23:27 - 00:23:30] אם הוא תלוי בעשרה הקודמים, אז הוא דווקא יהיה תלוי בעשרה האלה.
[00:23:31 - 00:23:31] לא קשור למהרבה.
[00:23:33 - 00:23:36] כן, נכון. אז הסידור הזה מכניס כל מיני דברים מוזרים.
[00:23:36 - 00:23:38] כן, אנחנו נרצה להתגבר על הדבר הזה,
[00:23:39 - 00:23:45] וגם אולי שבאמת כל פיקסל יהיה תלוי בסביבה הקרובה שלו יותר מאשר באזורים יקוקים,
[00:23:45 - 00:23:49] וגם אבל שיהיה תלוי בכמה שיותר, שאולי אפילו איכשהו יהיה תלוי בהכול.
[00:23:51 - 00:23:51] אנחנו נראה,
[00:23:52 - 00:23:54] חשבתי את זה כאילו זה הדרך היחידה,
[00:23:54 - 00:23:56] שזה זה של הדרכים היחידות, אבל יש בעצם,
[00:23:56 - 00:23:57] אפשר לחשוב לזה בתור דרך שלישית,
[00:23:57 - 00:24:01] או איזושהי אדפטציה של הדרך הזאת שתכף נראה לך.
[00:24:04 - 00:24:05] ודרך אחרת שאנחנו לא נדבר עליה היום,
[00:24:06 - 00:24:10] ודיברנו עליה בעיקר בשבועיים-שלושה האחרונים,
[00:24:10 - 00:24:11] ואנחנו נחזור אליה בהמשך,
[00:24:11 - 00:24:18] זה שהפיקויות האלה שאנחנו עושים זה בהינתן משתנה חבוי, חדש,
[00:24:18 - 00:24:19] אנחנו נמצאים משתנה חדש,
[00:24:20 - 00:24:20] אנחנו לא רואים אותו,
[00:24:21 - 00:24:24] במקום שכל פיקסל בהינתן השכנים שלו יהיה תלוי בשאר,
[00:24:24 - 00:24:29] אז כל פיקסל בהינתן איזה משהו מומצא חדש הוא בלתי תלוי בכל השאר.
[00:24:30 - 00:24:31] זה מה שכתוב כאן,
[00:24:32 - 00:24:35] כל פיקסל הוא רק פונקציה של איזשהו משתנה חבוי,
[00:24:37 - 00:24:37] קילו באיינטון.
[00:24:40 - 00:24:48] עכשיו זה גם דרך להוריד את מספר הפרמטרים, כי במקום ה-D בחזקת K, שזה בעצם מספר האפשרויות שיש לי כאן,
[00:24:49 - 00:24:52] אז זה יהיה מספר האפשרויות שיש לזד, אז אם זד הוא גם מיסקרפי,
[00:24:53 - 00:24:55] אז מספר האפשרויות של זד,
[00:24:56 - 00:24:56] נתתי כאן
[00:24:58 - 00:25:00] על המופרכים בין ה-H הוא זד,
[00:25:01 - 00:25:16] אז מספר האפשרויות של זד כפול אותו דבר שמפודד ביתר כמה פרמטרים אני צריך בשביל למדד דבר נולי, את הקטגורית של פיקסל אחד כפול מספר הפרמטרים.
[00:25:18 - 00:25:20] אוקיי, אז האפשרות הזאת אנחנו לא נדבר היום,
[00:25:21 - 00:25:22] ונחזור אליה בהקשר.
[00:25:23 - 00:25:24] אנחנו נתמקד באפשרות הזאת
[00:25:25 - 00:25:27] עם איזושהי אדפטציה שכן מאפשרת להיות תלוי
[00:25:28 - 00:25:34] בכל השאר, להיות תלוי יותר בקרובים וקצת בכל השאר באיזושהי דרך שלא,
[00:25:35 - 00:25:39] לא, בלי פרמטריזציה מלאה, שתסתכל על כל האפשרויות של הפרמטריזציה,
[00:25:40 - 00:25:41] שזה יהיה יותר נגיד,
[00:25:42 - 00:25:44] עדיין לא נסתף מספר אקספוניציה על איזושהי פרמטרים.
[00:25:44 - 00:25:52] אוקיי, איך אנחנו מאמנים?
[00:25:52 - 00:25:54] זהו עדיין, אנחנו עדיין בשלב החזרה,
[00:25:56 - 00:25:58] אנחנו עושים עדיין תזכורת קצת.
[00:25:59 - 00:26:06] אז איך אנחנו מאמנים את המודלים האלה? אז אמרנו אנחנו רוצים למצוא את ה-Teta ככה ש-PTeta יהיה כמה שיותר קוב PData,
[00:26:07 - 00:26:10] ודרך סטנדרטי לעשות עם זה זה מקסימום לייקליהוד.
[00:26:14 - 00:26:23] אז זה שאנחנו מחפשים מתוך כל המודלים האפשריים במשפחה שהגדרנו את ה-Beta, ככה שה-PTeta יהיה כמה שיותר קרוב ל-PData.
[00:26:24 - 00:26:28] אין לנו באמת גישה ל-PData, יש לנו עוד גישה לדגימות מתוך PData.
[00:26:30 - 00:26:38] אבל בעצם כשחפשים מקסימום לייקליהוד זה קצת כמו לעשות מינימום ל-KL Divergence בין ה-PData ל-PTeta.
[00:26:40 - 00:26:43] ראינו פה את הפיתוח הזה שמראה שלעשות מקסימום לייקליהוד,
[00:26:43 - 00:26:47] זה כמו לעשות מינימום בין ה-KL Divergence בין PData ל-PTeta.
[00:26:50 - 00:26:55] כי ה-P Divergence שווה ל-likelihood או ל-מינוס ה-likelihood ועוד איזה שהוא קבוע,
[00:26:56 - 00:26:58] קבוע זה הוא האנטרופיה של ה-Data.
[00:27:02 - 00:27:02] אתם זוכרים את זה?
[00:27:03 - 00:27:09] כן, בעצם הדבר הזה, למה זה יש פה קירוב? כי במקום לסוף את האינטגרל אנחנו יכולים לראות את כל PData
[00:27:10 - 00:27:12] ומסתכלים על דגימות מתוך PData,
[00:27:12 - 00:27:15] זו שיטת מונטק-ארבע שגם דיברנו עליה בהקשרים אחרים,
[00:27:16 - 00:27:18] ואנחנו עושים שיערוך של האינטגרל הזה על ידי
[00:27:19 - 00:27:21] דגימות של ה-training set שלנו,
[00:27:23 - 00:27:25] אז יש לנו פה את הממוצע הזה במקום האינטגרל,
[00:27:26 - 00:27:28] על לוג של PData,
[00:27:29 - 00:27:34] זה בעצם ההסתברות תחת המודל שלנו, או לוג ההסתברות תחת המודל שלנו
[00:27:35 - 00:27:36] של הדוגמה ה-I.
[00:27:38 - 00:27:42] אז זה בעצם ה-objective function שלנו שאנחנו נרצה למזער.
[00:27:43 - 00:27:48] אנחנו נרצה למקסם את לוג ההסתברות, או למזער את מינוס לוג ההסתברות
[00:27:49 - 00:27:50] על כל הדוגמאות שאנחנו רואים.
[00:27:54 - 00:27:56] עוד משמעות שיש לדבר הזה,
[00:27:56 - 00:28:00] זה קצת עוזר להבין מה המספרים שיוצאים,
[00:28:01 - 00:28:02] שמאמנים את הדברים האלה,
[00:28:03 - 00:28:04] שיוצאים להם אבאלואציה.
[00:28:07 - 00:28:10] זה בעיקר תקף כשהמודל הוא דיסקרטי.
[00:28:11 - 00:28:13] אני חושב שהדאטה הוא דיסקרטי,
[00:28:13 - 00:28:15] אנחנו מאמנים את הפיקסלים בצורה דיסקרטית.
[00:28:18 - 00:28:18] אז המשמעות
[00:28:20 - 00:28:20] של הלוג-לייקליות
[00:28:24 - 00:28:30] זה בעצם מספר הביטים שצריך, מספר הביטים האידיאלי
[00:28:31 - 00:28:36] שצריך כדי לשדר את התמונה הזאת למישהו אחר.
[00:28:37 - 00:28:39] אז אם מישהו אחר מכיר את המודל שלנו,
[00:28:39 - 00:28:40] נגיד למדנו איזשהו מודל,
[00:28:42 - 00:28:43] ומישהו אחר מכיר אותו,
[00:28:44 - 00:28:47] אז בכמה ביטים אנחנו נצטרך כדי לשדר את המודל הזה.
[00:28:48 - 00:28:49] אם היה לנו את המודל המושלם,
[00:28:50 - 00:28:50] את pdata,
[00:28:51 - 00:28:55] זאת אומרת ה-Ldivergence היה 0, והיה נשאר לנו רק מה שכתוב כאן ה-Cons,
[00:28:55 - 00:28:58] שזה אינטגרז של pdata, לא פי דאטה,
[00:28:59 - 00:29:00] זה אינטרופיה של pdata.
[00:29:01 - 00:29:02] אז זה בעצם
[00:29:05 - 00:29:08] הקידוד האידיאלי של תמונות,
[00:29:09 - 00:29:13] הוא מסתמך על ההסתברות האמיתית שממנה
[00:29:14 - 00:29:15] התמונות נוצרו
[00:29:18 - 00:29:19] והעורק של הקידוד הזה שווה לאנטרופיה
[00:29:21 - 00:29:22] של ההסתברות הזאת.
[00:29:22 - 00:29:24] שוק הנסה לתורת האינפורמציה?
[00:29:25 - 00:29:27] אוקיי, טוב.
[00:29:27 - 00:29:30] אולי אתם מבינים פחות או יותר מה אני מתכוון.
[00:29:32 - 00:29:34] ברגע שהמודל שלנו הוא לא בדיוק
[00:29:36 - 00:29:39] האנטרופיה, הוא לא בדיוק המודל האמיתי,
[00:29:39 - 00:29:43] אז מה שנשאר לנו כאן זה לא האנטרופיה, זה האנטרופיה פלוס ה-Kale Divergence הזה,
[00:29:45 - 00:29:49] וה-Kale Divergence הוא בעצם אומר לנו כמה ביטים נוספים אנחנו צריכים,
[00:29:50 - 00:29:53] אנחנו משלמים על זה שהמודל שלנו הוא לא מושלם.
[00:29:56 - 00:29:57] בגלל זה גם קוראים לזה קורס אנטרופיה.
[00:29:58 - 00:30:00] זה אנטרופיה בין המודל שלנו למודל האמיתי.
[00:30:01 - 00:30:03] זה אומר כמה ביטים אנחנו נצטרך
[00:30:03 - 00:30:05] למדל דאטה שהגיע מהתפלגות pdata
[00:30:06 - 00:30:09] על ידי קוד שנבנה על ידי pdata.
[00:30:09 - 00:30:19] אוקיי, אז אני אומר את זה כדי שכשאנחנו נראה עוד מעט כל מיני מספרים בסוף השיעור על כל מיני מודלים, מה ה-log-likeיות שהם משיגים,
[00:30:20 - 00:30:21] אז זה קל,
[00:30:22 - 00:30:24] לפעמים קצת קשה להבין את המספרים האלה,
[00:30:25 - 00:30:33] אבל כשחושבים על זה ככה זה קצת עוזר להבין מה המשמעות של זה, אוקיי? זה כמה ביטים צריך כדי לשדר את
[00:30:34 - 00:30:35] כל תמונה.
[00:30:36 - 00:30:38] או אם בממוצע כמה תמונות,
[00:30:38 - 00:30:41] כמה ביטים בממוצע הם נצטרך כדי לשדר לתמונות שלהם.
[00:30:48 - 00:30:52] אוקיי, הדבר הזה נכון רק לייצוג דיסקרטי, לייצוג רציף משמעות של
[00:30:53 - 00:30:54] טרופיה,
[00:30:54 - 00:30:55] דייברסנס,
[00:30:56 - 00:31:00] פורמציה בלית, כל זה, הוא לא בדיוק אותו דבר, זה הכל דיסקרטי יחסי,
[00:31:01 - 00:31:04] זה כבר אין לזה כל כך משמעות ישירה כמו זאת,
[00:31:05 - 00:31:10] אבל כשמספרים עדיין אפשר להשוות במודלים שונים והמשמעות של זה זה
[00:31:12 - 00:31:16] אפשר לבדוק כמה המודל קרוב למודל האמיתי שאתה מצטרף בו, אוקיי?
[00:31:16 - 00:31:19] היום לא נדבר על דאטה רציף, אבל כשנגיע לזה אולי
[00:31:20 - 00:31:23] קצת ננסה להבין מה המשמעות של המספרים שיש שם.
[00:31:25 - 00:31:25] טוב,
[00:31:25 - 00:31:29] אז נחזור רגע לשתי אפשרויות שיש לנו, אז אנחנו נראה היום,
[00:31:29 - 00:31:33] רוצים לתת כאן את התמונות האלה שדיברנו עליהן.
[00:31:35 - 00:31:45] כבר דיברנו בעיקר על הדבר הזה בשבועות האחרונים, היום נדבר על מה שקורה כאן,
[00:31:46 - 00:31:48] אז בעצם לעשות,
[00:31:48 - 00:31:55] היתרון של המודל הזה זה שאנחנו יכולים לחשב את ה-likelihood שלו בצורה ישירה, וקל לעשות אופטימיזציה,
[00:31:56 - 00:32:01] כי בעצם יש לנו גישה לכל מה שמחושב כאן, אוקיי?
[00:32:02 - 00:32:08] המודל הזה הוא של XI בהינתן באיזה שהם X'ים אחרים, אנחנו רואים את כל ה-X'ים האלה, בהינתן דוגמה שיש לנו, אנחנו רואים את הכל,
[00:32:09 - 00:32:10] ואנחנו יכולים לחשב את P,
[00:32:11 - 00:32:13] אנחנו יכולים לחשב את כל אחד מה-P'ים האלה
[00:32:14 - 00:32:15] במכפלה הזאת ולהכפיל את כולם.
[00:32:16 - 00:32:18] זה נותן לנו את ההסתברות,
[00:32:20 - 00:32:23] ה-likelihood של הדאטה שאנחנו רואים,
[00:32:24 - 00:32:26] של המודל על פני הדאטה שאנחנו רואים,
[00:32:27 - 00:32:31] אנחנו נעסוק לזה לוב, לספום על כל הנקודות שיש לנו ולעשות איזו אופטימיזציה.
[00:32:32 - 00:32:38] אז אופטימיזציה אולי זה, לא בטוח שהיא קלה, אנחנו מדברים במודל עצמו, אבל לפחות לחשב את זה,
[00:32:38 - 00:32:41] ויש לנו את כל האינפורמציה שאנחנו צריכים כדי לחשב את זה בצורה ישירה.
[00:32:42 - 00:32:43] לעומת הדבר הזה,
[00:32:44 - 00:32:48] שזה בדיוק הדברים שדיברנו בשבועות האחרונים, זה מסובך, כי אנחנו צריכים לעשות
[00:32:48 - 00:32:50] קודם כל איזשהו פוסטיריו על זה,
[00:32:51 - 00:32:54] אנחנו לא רואים את זה, אנחנו לא יכולים לחשב את זה בצורה ישירה, כי אין לנו את זה.
[00:32:55 - 00:32:56] גם אם יש לנו את ה...
[00:32:57 - 00:33:00] יש לנו איזשהו מודל, אנחנו רוצים לדעת מה ההסתברות שנותן לנו לדאטה,
[00:33:01 - 00:33:02] ‫אין לנו דרך ישירה לעשות את זה.
[00:33:03 - 00:33:05] ‫אנחנו בדרך כלל נחשב בשביל פוסטריאור על זה,
[00:33:06 - 00:33:08] ‫ופוסטריאור על זה לשארך את
[00:33:09 - 00:33:12] ‫ההסתדרות של הדף, אוקיי? ‫זה קצת מסובך, או על ידי דגימות,
[00:33:13 - 00:33:14] ‫או על ידי variation Inference.
[00:33:15 - 00:33:16] ‫במקרים מסוימים,
[00:33:17 - 00:33:21] ‫אנחנו יכולים לעשות את ה-Varation Inference ‫עד הסוף, ‫אבל אפשר להיכנס ב-Ein.
[00:33:22 - 00:33:25] ‫זה קצת יותר מסובך.
[00:33:26 - 00:33:30] ‫דוגמאות של הגישה הזאתי שנראה היום, זה מינאייד,
[00:33:30 - 00:33:33] ‫פיקסלר-NN ופיקסל-CNN, ‫אז אנחנו נדבר עליהם היום.
[00:33:33 - 00:33:37] ‫כולם בעצם מפרקים את הדאטה לייצוג הזה.
[00:33:38 - 00:33:39] ‫המודל בעצם עונה רק ככה.
[00:33:40 - 00:33:45] ‫וזה יהיה שער הכול, אוקיי? ‫המודלים שנראים ככה, GMM כבר ראינו,
[00:33:46 - 00:33:47] ‫Varation Noto-Encorders,
[00:33:49 - 00:33:50] Normalizing Flow, Diffusion Models,
[00:33:51 - 00:33:54] ‫עם כולם אפשר לחשוב עליהם ‫בתור latency כזה אחד,
[00:33:55 - 00:33:59] ‫וכל הפיקסלים מגלהים את הפונקציה של ה-Latence.
[00:33:59 - 00:34:03] ‫לפעמים יש לנו הירארכיה של לגיטנט, ‫כי אם הוא לגיטנט בגיטנט בגיטנט זה לא לגיטנט,
[00:34:05 - 00:34:08] ‫ולפעמים זה לא בדיוק הדרך הראשונית
[00:34:08 - 00:34:12] ‫לחשוב על המודל, אבל כל המודלים האלה, ‫אפשר גם לתאר אותם בתור
[00:34:13 - 00:34:14] ‫מודל שהוא כתוב פעם.
[00:34:16 - 00:34:17] ‫עוד אחד שלא קטעתי כאן זה GAN,
[00:34:18 - 00:34:18] זה גם
[00:34:22 - 00:34:26] מודל שאפשר לתתו אותו ככה, Generative Unversarial Networks.
[00:34:27 - 00:34:31] ‫כנראה לא נספיק לעשות, ‫לגמור את זה ככה במסגרה הזאת,
[00:34:31 - 00:34:32] ‫אנחנו נמצא קצת,
[00:34:33 - 00:34:36] ‫אבל אני משתמש לפחות את ה-Latage עליו כמה מודלים.
[00:34:37 - 00:34:38] ‫אז כל זה זה יהיה בהמשך הקורס,
[00:34:39 - 00:34:45] ‫היום אנחנו בצד הזה, ‫שהכול חג. אוקיי, אין לנו משתנים חבועים, ‫אנחנו רואים את כל הדאטה,
[00:34:45 - 00:34:49] ‫והמטרה שלנו זה למצוא את המודל ‫שמבצעים אותו דבר.
[00:34:57 - 00:35:07] ‫אוקיי, אז בעצם אמרתי שאנחנו עושים כאן, ‫ההנחה שאנחנו עושים זה שכל פיקסל ‫תלוי ב-K הפיקסלים הקודמים, ‫וזה לא הנחה כל כך טובה,
[00:35:08 - 00:35:17] ‫אז עכשיו אנחנו נראה שבעצם ‫אפשר לעשות איזשהו טרייד-אוף כזה, ‫בין כמה אנחנו מצמצמים את ההיסטוריה הזאת ‫שאנחנו תלויים בה,
[00:35:17 - 00:35:21] ‫ואיך אנחנו עושים את הפרמטריזציה במדעת דאטה.
[00:35:21 - 00:35:26] ‫אז הרעיון הוא שאם אנחנו לא עושים ‫פרמטריזציה מלאה, זאת אומרת,
[00:35:26 - 00:35:26] ‫אם אנחנו לא אומרים,
[00:35:27 - 00:35:28] ‫עוברים על כל האפשרויות.
[00:35:29 - 00:35:30] ‫בואו, מקור הבעיה שלנו הייתה שכאן,
[00:35:36 - 00:35:36] ‫כן פה,
[00:35:37 - 00:35:42] ‫היינו צריכים לבדוק את כל האפשרויות, ‫אם אנחנו רוצים לראות טבלה של הדבר הזה, ‫זה היה לנו 2 בחזקת n-1,
[00:35:44 - 00:35:48] ‫כל המלחמת הטבלה שזה 2 בחזקת n-1 אפשרויות,
[00:35:50 - 00:35:51] ‫ופה, אם זה בינארי, זה היה רק,
[00:35:52 - 00:35:54] ‫בנקודת בינארי זה היה...
[00:35:56 - 00:35:59] ‫באים לפני שעוד חמש, חמישה מספרים, ‫על
[00:36:00 - 00:36:01] ‫2 בחזקת n-1 מספרים.
[00:36:02 - 00:36:04] ‫זה היה פשוט לטבלה הזאת ‫היה יותר מדי שורות.
[00:36:06 - 00:36:07] ‫אז אולי אנחנו יכולים...
[00:36:08 - 00:36:09] ‫הרעיון כאן שאולי אנחנו לא צריכים...
[00:36:10 - 00:36:13] ‫אנחנו עדיין רוצים להיות תלויים ‫בכל המשתנים האלה,
[00:36:14 - 00:36:22] ‫אבל לא לתת ממש שורה לכל משתנה, אוקיי? ‫לא להקדיש לכל משתנה שורה, ‫אלא לעשות איזושהי פרמטריזציה משותפת כזאת, כדי להתראות.
[00:36:23 - 00:36:27] ‫אז אם אנחנו עושים פרמטריזציה ‫לא מלאה, ‫לדבר הזה קוראים פרמטריזציה מלאה.
[00:36:28 - 00:36:29] ‫זה פשוט נותן, מסתכל על כל האפשרויות,
[00:36:30 - 00:36:33] ‫כל אפשרות נותן את ההסתברות ‫שנובעת מזה.
[00:36:33 - 00:36:38] ‫אנחנו נגיד, נרצה שההסתברות שלנו ‫תהיה איזושהי פונקציה ‫של כל ה-x'ים האלה,
[00:36:39 - 00:36:45] ‫ולא שורה שונה לכל אחד מה-x האלה.
[00:36:46 - 00:36:51] ‫אוקיי? והפונקציה הזאת תהיה פרמטרים משותפים. ‫אז אני אהיה פחות פרמטר מ-2 עד כת מ-1.
[00:36:52 - 00:36:53] ‫זה יהיה הרעיון.
[00:36:54 - 00:37:00] ‫עדיין אנחנו כנראה נרצה שלפיקסלים ‫שהם איכשהו קרובים ל-x1, ‫יהיה להם יותר משקל.
[00:37:01 - 00:37:04] ‫איכשהו ישביעו יותר על הפונקציה הזאתי,
[00:37:05 - 00:37:06] ‫מאשר הפיקסלים הרבה פיקסלים.
[00:37:09 - 00:37:10] ‫אז זה הרעיון של מה שנעשה היום.
[00:37:18 - 00:37:22] ‫אוקיי, ודוגמה לאיך אפשר לעשות את זה, ‫זה עם Logistic Regression למשל.
[00:37:22 - 00:37:24] ‫אם אנחנו מדברים על הבעיה הבינארית, ‫אנחנו רוצים
[00:37:26 - 00:37:27] להגיד מה ההסתברות
[00:37:33 - 00:37:42] ‫של הפיקסל I. ‫בהינתן כל הפיקסלים שהם קטנים מה-Ifer, ‫אבל אני אשתמש בסיפור הזה,
[00:37:44 - 00:37:46] ‫כל הפיקסלים שבאים לפניו זה איזשהו סיפור.
[00:37:47 - 00:37:49] ‫אז במקום להחזיק את כל הטבלה הזאתי,
[00:37:49 - 00:37:58] ‫כל אחת מהאפשרויות של ה-X קטן מ-I ‫תהיה לי שורה אחרת, ‫אז במקום זה אני אשתמש בפונקציה הזאתי,
[00:37:59 - 00:38:01] ‫אם זה ברנולי אז אני רק צריך
[00:38:01 - 00:38:04] ‫איזושהי הסתברות שהיא מספר בין 0 ל-1.
[00:38:05 - 00:38:06] ‫זה סיגמוד הזה,
[00:38:06 - 00:38:09] ‫הלוג'יסטיק כזה, ‫זו פונקציה שפשוט מנרמלת לי
[00:38:10 - 00:38:12] איזשהו מספר, ‫ככה שהוא יהיה כתבין בין 0 ל-1.
[00:38:12 - 00:38:21] ‫אז כמו שכתוב אותם, ‫אז כמו שכתוב אותם שם, ‫זה 1 ל-E בפסקת
[00:38:21 - 00:38:23] ‫מינוס תטא,
[00:38:24 - 00:38:27] ‫כפול כל ה-Xים הקטנים מ-I.
[00:38:33 - 00:38:34] ‫התשובה פנימית בין
[00:38:34 - 00:38:37] הפרמטרים שלי ‫לכל ה-Xים שקטנים מ-I.
[00:38:39 - 00:38:42] ‫אוקיי, אז תשובו את מספר הפרמטרים שיש פה,
[00:38:43 - 00:38:45] ‫המספר הפרמטרים בתגרה הזאת ‫שהייתה לנו קודם.
[00:38:46 - 00:38:48] ‫בעצם נתתי את הדבר הקודם אחד.
[00:38:49 - 00:38:51] ‫אני לא מניח שיש 250 שישהי תפעול, ‫אז זה
[00:38:52 - 00:38:53] משהו גינארי.
[00:38:53 - 00:38:54] אבל כאן יש,
[00:38:57 - 00:38:58] ‫כן, אני חושב שזה נתן מ-I,
[00:39:00 - 00:39:02] ‫כמה פיקסיטים יש שוב נתנים מ-I,
[00:39:03 - 00:39:05] ‫זה 2 ל-0 ל-1.
[00:39:06 - 00:39:07] אוקיי, אז מה יהיה
[00:39:13 - 00:39:18] ‫שתיים בפסוקת I. ‫I פחות 1. ‫שתיים עושים קטן I פחות 1.
[00:39:20 - 00:39:24] ‫אז כמה פרמטרים יש לי כאן בייצוג הזה, ‫וכמה פרמטרים בשביל הייצוג הזה?
[00:39:26 - 00:39:35] ‫I פחות 1. ‫בו יש פסוק I פחות 1, נכון? ‫כי זה מכפלה פנימית. ‫בדרך כלל אנחנו מניחים כאן ‫שיש עוד גם ביאס כזה.
[00:39:43 - 00:40:03] ‫אני לא יכול ממש להגיד, ‫אוקיי, אם הפיקסלים האלה היו... ‫אם הפיקסלים החוזמים היו ככה וככה וככה, ‫אז הם לגמרי משנה לי את ההסתברות.
[00:40:04 - 00:40:07] ‫אני לא יכול למדל את ההסתברות ‫בצורה מלאה, אוקיי? ‫זו לא פרמטריזציה מלאה.
[00:40:07 - 00:40:12] ‫אני מדדל אותה על ידי ‫איזושהי הנחה כזאת לינארית, ‫יש לי תמות לינארית ‫בין מה שהיה קודם.
[00:40:13 - 00:40:17] ‫ואז אני מפיר לה את הסיגוויד הזה, ‫שנותן לי את המספר הזה ‫בין 0 ל-1,
[00:40:18 - 00:40:19] ‫וזהו, זו הנחה שהיא מגבילה.
[00:40:20 - 00:40:22] ‫אז במקום להגביל את
[00:40:23 - 00:40:27] מספר הפיקסלים בהיסטוריה ‫שאני תלוי בהם, ‫אני מגביל את איך שאני תלוי בפיקסלים.
[00:40:28 - 00:40:30] ‫אני מרשה לעצמי להסתכל על כל הפיקסלים,
[00:40:31 - 00:40:35] ‫אבל לא לעשות כל מה שאני אמצא ‫עם כל הפיקסלים הפופולרינריים.
[00:40:39 - 00:40:42] ‫זה בעצם הכוונה שלי ב-Trade-off בין
[00:40:43 - 00:40:47] ‫Conditional Dependence והפרמטריזציה.
[00:40:52 - 00:40:55] ‫הדוגמה שאנחנו בעצם נתחיל ‫להסתכל עליה הרבה זה M-List,
[00:40:56 - 00:40:56] ‫כמו שאמרתי,
[00:40:59 - 00:41:06] ‫ואנחנו הרבה פעמים נסתכל על גישה ‫על דאטה שהוא אפילו עוד יותר פשוט, ‫זה נקרא בינארייד M-List,
[00:41:07 - 00:41:10] ‫שהפיקסלים האלה הם או 0 או 1 פשוט, ‫או שופר לבן.
[00:41:10 - 00:41:17] ‫-M-List המלא זה 255, 256 ערכים לכל פיקסלים.
[00:41:23 - 00:41:24] ‫אז הדאטה הזה בנוי ככה, יש לו,
[00:41:25 - 00:41:27] ‫כל תמונה היא בגודל 28 על 28,
[00:41:28 - 00:41:29] ‫והיא שחור לבן, אז יש לה רק
[00:41:31 - 00:41:32] ערוץ אחד.
[00:41:33 - 00:41:38] ‫זה אומר שבסך הכול יש 784 פיקסלים, ‫בסך הכול 784 ערכים בכלל.
[00:41:41 - 00:41:49] ‫במקרה הבינאריים יכולים להיות ‫רק 0 או 1. ‫אוקיי, אז המטרה שלנו היא למצוא ‫איזושהי פונקציית התפלגות כזאת, PX,
[00:41:51 - 00:41:52] ‫שהיא בעצם,
[00:41:52 - 00:41:57] ‫זה נותן לנו הסתברות ‫על כל אחד מהערכים האפשריים,
[00:41:57 - 00:42:07] ‫של הוקטור הזה בגודל 784. ‫ואנחנו רוצים שזה יהיה מודל טוב, ‫זאת אומרת, רוצים לדגום מהמודל הזה ‫שזה ייראה כמו ספרה.
[00:42:10 - 00:42:24] ‫אוקיי, אז בואו נסתכל על מודל כזה. ‫אז אמרנו שהדבר הזה תמיד נכון, אוקיי? ‫זה שוב פעם כלל השרשרת, ‫בלי שעשיתי הנחות, אוקיי?
[00:42:24 - 00:42:25] ‫זאת האחרון כאן,
[00:42:26 - 00:42:27] ‫הוא תלוי בכל הקודמים.
[00:42:32 - 00:42:35] ‫אז הפיקסל ה-784 זה הפיקסל האחרון, ‫הוא תלוי בכל הקודמים.
[00:42:36 - 00:42:39] ‫אז לא עשיתי פה הנחות על אי-תלות.
[00:42:41 - 00:42:45] ‫אז זה מה שאמרנו קודם, אוקיי? ‫אם עכשיו בעצם כל התפלגות כזאתי,
[00:42:46 - 00:42:51] ‫היא תהיה לוג'יסטיק כזאתי, ‫כמו מה שכתוב כאן,
[00:42:51 - 00:42:53] ‫כאילו פשוט עם איזשהו תטא.
[00:42:54 - 00:42:56] ‫לכל i יהיה איזשהו צד של פרמטרים, ‫תטא i,
[00:42:57 - 00:42:58] ‫שהוא אומר,
[00:42:59 - 00:43:01] ‫כל פעם ההיסטוריה תהיה בגודל אחר.
[00:43:02 - 00:43:06] ‫כל פעם יהיה i-1 בקיצוריה בהיסטוריה,
[00:43:06 - 00:43:08] ‫אז יהיה לי תטא בגודל אחר.
[00:43:11 - 00:43:19] ‫וזה יהיה הפרמטרים שלי. ‫כל אחד מהפקטורים האלה, ‫במקום שהוא יסתכל על כל האפשרויות, ‫אני אהפוך אותו לפונקציות לוג'יסטיק.
[00:43:20 - 00:43:28] ‫אז מה כתוב כאן? כתוב כאן x2 ‫זו פונקה של x1, ‫ובינתן x1, ‫זה יהיה בעצם האינפוט של הלוג'יסטיק הזה,
[00:43:28 - 00:43:30] ‫והפרמטרים שלו, נתה נקודה פסיק,
[00:43:31 - 00:43:32] ‫זה יהיה תטא.
[00:43:33 - 00:43:37] ‫לאחרון גם הוא יהיה תלוי בצפה יותר גדול,
[00:43:38 - 00:43:39] ‫וזה יהיה התטא שלו,
[00:43:39 - 00:43:41] ‫כדי שזה יהיה וקטור יותר גדול גם.
[00:43:43 - 00:43:44] ‫זה יהיה בגודל של הדבר הזה,
[00:43:45 - 00:43:48] ‫פלוס 1, ואני רוצה גם ‫שיהיה איבר חופשי פה.
[00:43:54 - 00:43:55] ‫אוקיי, אז כן,
[00:43:56 - 00:44:00] זה מה שכתבנו כאן. ‫כתבתי כאן את האחד הזה, ‫כדי שיהיה את האיבר החופשי פה.
[00:44:02 - 00:44:06] ‫הסיגמה הזה זה דרך שכותבים את ה... ‫קוראים לזה סיגמויד או לוג'יט.
[00:44:07 - 00:44:09] זה פשוט ה-1 חלקי 1 ועוד E בחזקה.
[00:44:10 - 00:44:10] ‫מינוס
[00:44:11 - 00:44:12] מה שיש פה.
[00:44:20 - 00:44:21] ‫זה מה שראינו קודם,
[00:44:21 - 00:44:30] ‫וזה מספר הפרמטרים שיוצא, ‫כי לראשון יש רק פרמטר אחד, ‫כי הוא הפרמטר החופשי, ‫הוא לא מתאמן בכלום, ‫לשני שני פרמטרים,
[00:44:31 - 00:44:34] ‫כן הלאה עד לאחרון, ‫שראינו שיש פה N פרמטרים.
[00:44:35 - 00:44:39] ‫מסך הכול זה עצב דרך N בריבוע, ‫כי יש לנו פרמטרים.
[00:44:39 - 00:44:49] ‫כן, זה מה שחסכנו מ-2 בחזקת N. ‫אוקיי, זה משהו שכבר אפשר למדל.
[00:44:55 - 00:44:59] ‫זה בעצם, מה זה אומר המידול הזה? ‫שכל פקטור כזה הוא...
[00:45:00 - 00:45:02] ‫אחרי שאנחנו עושים את החישוב הזה,
[00:45:03 - 00:45:04] ‫התוצאה של החישוב הזה,
[00:45:04 - 00:45:09] ‫זה הפרמטר ברנולי של הטיקסל ה-I.
[00:45:10 - 00:45:12] ‫זאת אומרת, זו ההסתברות ‫שהטיקסל ה-I שווה 1.
[00:45:16 - 00:45:20] ‫ואפשר לחשוב על זה בשתי דרכים.
[00:45:21 - 00:45:25] ‫אחת, שזה מה שאמרתי עכשיו, ‫שזה ההסתברות שטיקסל ה-I שווה 1, שהוא גדולה,
[00:45:27 - 00:45:30] ‫או שזה בעצמו כבר הפרדיקציה שלי ‫לפיקסל ה-I.
[00:45:31 - 00:45:35] ‫אבל אני יכול להגיד ש... ‫למרות שהפיקסל ה-I הוא...
[00:45:36 - 00:45:43] ‫אני יודע שבדאטה הוא 0.1, ‫אולי הפרדיקציה שלי היא שהוא 0.6. ‫זה יותר מעט אפשוב על זה ככה. ‫הפרדיקציה שלי עצמה היא
[00:45:44 - 00:45:48] ‫איזשהו מספר בין 0 ל-1, ‫למרות שמראש הפיקסלנו הוא 0.1.
[00:46:01 - 00:46:08] ‫אז כן, לפעמים זה נוח... ‫כל מיני דברים. ‫זה נוח לעשות את החישוב של הלוס על גבי זה לפעמים.
[00:46:08 - 00:46:13] ‫היום אנחנו לא כל כך ניכנס לזה באמת, ‫אבל זה עוזר.
[00:46:14 - 00:46:23] ‫לפעמים גם ממדלים בסופו של... ‫למרות שהדאטה הגיעה בתור בינארי, ‫עדיין ממדלים אותו כאילו שהוא רציף, ‫בעצם בין 0 ל-1,
[00:46:24 - 00:46:29] ‫ואז הפרדיקציה עושים בעולם הרציף, ‫והלוס עושים בעולם הרציף, ‫למרות שכל ה-Labלים מגיעים רק ב-0.
[00:46:29 - 00:46:31] ‫זאת הכוונה.
[00:46:34 - 00:46:37] ‫אז נראה דוגמאות, ‫מה ההבדל כשאנחנו מסתכלים על תמונה,
[00:46:39 - 00:46:41] ‫באינטרפטציה הזאת, ‫לעומת תמונה על האינטרפטציה הזאת.
[00:46:46 - 00:46:49] ‫אוקיי, אז המודל כזה מיימשו אותו ‫עם סיגמויד,
[00:46:51 - 00:46:53] ‫ויש סיגמויד ולוג'יסטיק,
[00:46:54 - 00:46:55] ‫יש לראות אותו דבר, בעצם.
[00:46:56 - 00:47:04] ‫סיגמויד, סיגמויד זה כל פונקציה ‫שיש לצורה כזאת בשביל S. ‫זה הדוגמה הכי שימושית ‫של סיגמויד בזו.
[00:47:09 - 00:47:13] ‫אז השם שנתנו למודל הזה ‫זה פול אינטרפט סיגמויד בליף מטרפט.
[00:47:15 - 00:47:17] ‫בליף מטרפט זה שם אחר ל-Basian נטרפט.
[00:47:18 - 00:47:19] בסדר?
[00:47:20 - 00:47:23] ‫ברשת יש לנו מודל של משתנים מקריים,
[00:47:23 - 00:47:29] ‫עם מרצים מקוונים שהם דג.
[00:47:35 - 00:47:39] ‫אוקיי, אז נגיד שזה הראשון, ‫אז איך אנחנו עושים אבלואציה?
[00:47:39 - 00:47:40] ‫מה זה אומר אבלואציה?
[00:47:41 - 00:47:42] ‫זה אומר שאם יש לנו...
[00:47:43 - 00:47:47] ‫קודם כול, זה דרך גרפית ‫להראות את המודל הזה, כן? ‫זה בהינתן,
[00:47:48 - 00:47:51] נגיד שיש לנו ארבעה פיקסלים, ‫אז הפיקסל הראשון,
[00:47:51 - 00:47:54] ‫אנחנו עושים לו פרדיקציה, ‫הוא לא תלוי בשום דבר אחר.
[00:47:55 - 00:47:58] ‫הפרדיקציה של הפיקסל השני ‫תהיה תלויה בראשון,
[00:47:59 - 00:48:02] ‫השלישי תהיה תלויה ‫גם בראשון ובשני, ‫והרביעי בשלושת הראשונים.
[00:48:06 - 00:48:10] ‫זו דרך לעשות להראות את הגרף של החישוב הזה.
[00:48:13 - 00:48:15] ‫ומה אנחנו יכולים לעשות עם זה? ‫אז הנה אנחנו רוצים לעשות אבלואציה.
[00:48:16 - 00:48:19] ‫אבלואלואציה זה בהינתן ש... ‫תגיד שלמדנו כבר את המודל,
[00:48:20 - 00:48:21] ‫היא נותנת תמונה חדשה,
[00:48:22 - 00:48:25] ‫מה ההסתברות שלה תחת המודל שלה. ‫באמת איך עושים את זה?
[00:48:34 - 00:48:38] ‫אז בשביל לחשב? ‫כן, אפשר ל...
[00:48:39 - 00:48:42] ‫יש לנו את כל הדאטה, ‫אנחנו מקבלים את הדאטה, ‫אנחנו מקבלים את ארבעת אלף,
[00:48:43 - 00:48:43] ‫אוקיי?
[00:48:44 - 00:48:45] ‫אז אנחנו צריכים לחשב,
[00:48:46 - 00:48:47] ‫נגיד בשביל הראשון,
[00:48:48 - 00:48:51] ‫הראשון אין לנו... ‫הראשון הוא נראה ככה, נראה...
[00:49:00 - 00:49:01] ‫כפי שביטק אחד,
[00:49:04 - 00:49:05] ‫תראות שהוא שווה אחד,
[00:49:06 - 00:49:10] ‫זה שווה לאחד תקן 1 בנספין, מינוס,
[00:49:11 - 00:49:12] ‫רק הטאטא אפס שלו,
[00:49:14 - 00:49:15] ‫טאטא אפס של אחד.
[00:49:15 - 00:49:18] ‫פשוט מספר,
[00:49:19 - 00:49:24] ‫זה 0 ו-1, ‫זו ההסתברות שהוא שווה 1, ‫ההסתברות שהוא שווה 0. ‫אנחנו בודקים אם הוא 1,
[00:49:26 - 00:49:28] ‫אז זה יהיה זה, זה יהיה המספר הזה.
[00:49:29 - 00:49:31] ‫אם הוא שווה 0, זה יהיה 1 פחות זה.
[00:49:32 - 00:49:32] ‫זה יהיה הפקטור הראשון.
[00:49:34 - 00:49:39] ‫אוקיי? כשזה 1 וזה, ‫אם פי 1 שווה 0,
[00:49:40 - 00:49:41] ‫זה פשוט 1 פחות זה.
[00:49:41 - 00:49:49] ‫אפשר לכתוב את זה בקצור, פי x1 יהיה שווה ל...
[00:50:12 - 00:50:14] ‫עוד עד איזה פחות זה.
[00:50:21 - 00:50:22] ‫זה סתם, אנחנו רוצים לממש את זה בקוד.
[00:50:24 - 00:50:25] ‫תמיד רק אחד מהם יגלום,
[00:50:26 - 00:50:32] ‫x1 הוא שווה 1 או 0. ‫אז או שזה יפעל או שזה יפעל.
[00:50:34 - 00:50:36] ‫אוקיי? זו ההסתברות של x1. ‫עכשיו, מה אם x2?
[00:50:39 - 00:50:41] x2, אני צריך לחשב אותו קודם כול.
[00:50:41 - 00:50:44] ‫אם אני צריך לחשב אותו, ‫אני צריך להסתכל על x1 גם.
[00:50:46 - 00:50:49] ‫זה בסדר, כי x1 נתון לי גם. ‫נתונים לכל ה-xים שאני חושב.
[00:50:50 - 00:50:52] ‫אוקיי? אז יהיה בדיוק אותו חישוב כמו קודם.
[00:50:54 - 00:50:58] ‫פחות שתי העונים וכולו על הדבר הזה. ‫עכשיו, יהיה תלוי גם ב-x1.
[00:50:59 - 00:51:00] ‫ב-1 מינוס פרמטר
[00:51:04 - 00:51:04] על
[00:51:04 - 00:51:25] כפול 1 מינוס פרמטר על x1, ‫ועוד 1 מינוס,
[00:51:27 - 00:51:28] ‫במקרה של x2 עד ה-0,
[00:51:31 - 00:51:34] ‫כפול 1 מינוס כל דבר.
[00:51:34 - 00:51:46] ‫אוקיי, זה פי x3.
[00:51:52 - 00:51:53] ‫זה פי x2 בהינתן x1.
[00:51:55 - 00:51:58] פי x3 הוא בהינתן x1 ו-x2.
[00:52:01 - 00:52:03] ‫תוך חישוב, רק שכאן מופיע לי עכשיו x1 ו-x2.
[00:52:05 - 00:52:08] ‫כן, ויש לי את כל הפרמטרים האלה. ‫אני אגיד שכבר למדתי את המודל הזה.
[00:52:09 - 00:52:13] ‫יש לי את כל הפרמטרים האלה, ‫ועכשיו, כשיש לי תמונה חדשה, ‫אני אגיד מה ההסתברות שלה.
[00:52:14 - 00:52:15] ‫אני פשוט עושה את כל החישובים האלה,
[00:52:16 - 00:52:17] ‫ומכפיש אותם.
[00:52:22 - 00:52:22] אוקיי?
[00:52:23 - 00:52:23] זה ברור?
[00:52:27 - 00:52:29] זה מאוד קל לעשות את זה. ‫עכשיו עוד בצורה ישירה.
[00:52:30 - 00:52:32] ‫בדבר אחת, ועכשיו עוד בזה במקביל.
[00:52:33 - 00:52:34] ‫אוקיי, חשוב לזה.
[00:52:35 - 00:52:37] ‫כל החישובים האלה אני חושב במקביל, ‫ואז להכפיל על זה.
[00:52:45 - 00:52:46] ‫אתם לא נראים מרוצים.
[00:52:49 - 00:52:49] ‫ברור?
[00:52:51 - 00:52:56] זה אבלואציה, ‫וגם לזה אנחנו משתמשים ‫בשביל לעשות את האימון. ‫זו המשמעות ש...
[00:52:57 - 00:53:00] ‫זה אחד מהיתרונות של המודל הזה, ‫שאפשר לחשב איזה בצורה יעילה.
[00:53:01 - 00:53:07] ‫וכשנאמן את זה, הרי איך אנחנו מאמנים? ‫אנחנו עושים מקסימום לייקטיות, ‫מחשבים את הלייקטיות וגוזרים. ‫אנחנו עושים את ה-dein descent.
[00:53:08 - 00:53:14] ‫אז זה, קודם כול נעשה את החישוב הזה, ‫ונגזור את זה כדי למצוא את הערכים ‫היותר טובים לתת לכל הפרמטרים שלנו.
[00:53:17 - 00:53:21] ‫אוקיי, אז זה לגבי אבלואציה, ‫שזה קשור גם לאימון.
[00:53:21 - 00:53:24] ‫מה אנחנו רוצים לדגום מתוך המודל הזה?
[00:53:31 - 00:53:32] ‫כן?
[00:53:33 - 00:53:41] ‫אפשר, כן, בתמונה ההתחלה מ-X1 ל-X2. ‫נכון, את זה צריך לעשות אחד-אחד, ‫התפלגות במקביל.
[00:53:42 - 00:53:49] ‫אנחנו פשוט נדגום קודם את X1. ‫עכשיו, יש לנו את ההסתברות של X1, ‫את ההתפלגות של X1, ‫כי X1 לא תלוי בשום דבר אחר,
[00:53:50 - 00:53:52] ‫אז יש לנו את ההתפלגות הזאת. ‫התפלגות בנומולי פשוט.
[00:53:54 - 00:53:58] אוקיי, זה מספר, ‫הדבר הזה מספר בין 0 ל-1, ‫אנחנו דוגמים את המספר הזה.
[00:53:58 - 00:53:58] ‫אוקיי, אנחנו עושים,
[00:53:59 - 00:54:00] ‫אם אולי שאתם יכולים ‫לתפרקוד אפילו.
[00:54:03 - 00:54:04] ‫לא, רק את הביטחון.
[00:54:04 - 00:54:06] ‫אנחנו דוגמים פשוט מ...
[00:54:07 - 00:54:08] ‫איך עושים את זה?
[00:54:08 - 00:54:12] ב-numpey זה רנדום חוויס, אני חושב, ‫מותאים במספר, או רנדום...
[00:54:14 - 00:54:19] ‫אבל זה פשוט רנד גדול מהמספר שהייתה כאן.
[00:54:20 - 00:54:23] ‫אוקיי, ואז אם זה רנד אינט, ‫רנד אינט גדול מהמספר הזה.
[00:54:26 - 00:54:27] לא, רנד גדול מהמספר הזה,
[00:54:27 - 00:54:30] ‫ואז זה נותן לי 1, אם זה גדול ‫המספר הזה, ‫ואפס אם זה קטן מהמספר.
[00:54:33 - 00:54:36] ‫אם זו הספרות של 1, ‫הוא צריך כבוד רנד קטן מהמספר, אוקיי? ‫לא משנה.
[00:54:37 - 00:54:43] אבל יש דרך פשוט, זה מתפר בנולי, ‫שאומר, מה ההסתברות שהערך הזה שווה 1, אוקיי? ‫אני פשוט דוגם מההסתברות הזאת.
[00:54:43 - 00:54:48] ‫יצא לי 1, יצא 1, יצא 0, יצא 0. ‫וזה מה שאני שם ב-X1.
[00:54:48 - 00:54:53] ‫ועכשיו אני רוצה לחשב את X2, ‫ש-X2 הוא צריך לדעת את X1.
[00:54:54 - 00:54:57] ‫לא צריך לדעת שום דבר אחר, ‫אבל יש לי כבר דגימה מ-X1,
[00:54:58 - 00:55:00] ‫אז אני מכניס את X1 למודל הזה.
[00:55:00 - 00:55:01] ‫עוד פעם, יצא לי מספר,
[00:55:02 - 00:55:06] ‫הההסתברות של X2 שווה 1. ‫אני דוגם מההסתברות הזאת.
[00:55:06 - 00:55:08] ‫יצא 1, יצא 1, יצא 0, יצא 0,
[00:55:09 - 00:55:10] ‫זה מה שאני שם ב-X2.
[00:55:10 - 00:55:13] ‫עכשיו אני משתמש ב-X1 ו-X2 ‫כדי לגזום את X שלו.
[00:55:14 - 00:55:16] ‫כך הלאה, עד שאני דוגם את כל ה...
[00:55:18 - 00:55:19] ‫אז זה גם משהו מאוד פשוט.
[00:55:20 - 00:55:22] ‫החיסרון זה שאי אפשר לעשות את זה ‫במקביל.
[00:55:24 - 00:55:29] ‫אז אם יש לי תמונה גדולה ‫ואם יש לי מיליון פיקסלים, ‫אז צריך שתחזור לפעולה הזאת
[00:55:30 - 00:55:30] מיליון פעמים.
[00:55:37 - 00:55:39] ‫זה ברור דגימה ממודל אוטו-רגרסיב.
[00:55:40 - 00:55:43] ‫-אם אתה רואה פה שנייה בהליכה אימונטית,
[00:55:43 - 00:55:45] ‫אמרת שבסוף אתה מפקיר את הסוף זה ה-X.
[00:55:46 - 00:55:47] ‫אז פי X,
[00:55:53 - 00:55:55] ‫בעצם מפקיר את אימון אני רוצה ממש מיליון,
[00:55:57 - 00:55:59] ‫אבל בגדול אני רוצה לעשות אחד פקריים,
[00:55:59 - 00:56:01] ‫לוג E
[00:56:02 - 00:56:04] ‫תטא ל-XI.
[00:56:05 - 00:56:06] ‫כן, כל הדוגמאות שלי?
[00:56:07 - 00:56:09] ‫את זה אני רוצה לגזור לפי תטא.
[00:56:10 - 00:56:12] ‫ולעשות גדלינט מסיימת על תטא.
[00:56:14 - 00:56:15] ‫בדרך כל אחד כזה,
[00:56:16 - 00:56:18] ‫אז לוג של פי של ההסתברות,
[00:56:19 - 00:56:22] ‫זה ה-P של ההסתברות, לא נכון?
[00:56:22 - 00:56:26] ‫כל פי של ההסתברות זה המכפלה ‫שם כל הדברים האלה.
[00:56:27 - 00:56:28] ‫-אה, זה I כן?
[00:56:28 - 00:56:30] ‫לא, I קטנדמר זה מספר הדוגמה.
[00:56:35 - 00:56:37] ‫יש לי J דוגמאות,
[00:56:39 - 00:56:39] ‫כל אחד מהם
[00:56:45 - 00:56:47] ‫גדול, מספר הדוגמאות.
[00:56:51 - 00:56:52] ‫כל דוגמא,
[00:56:53 - 00:56:56] ‫אם תחבור על כל הפיקסלים ולעשות את זה,
[00:56:56 - 00:57:00] ‫בגלל שזה לוג, במקום המכפלה זה ספורם, אוקיי? ‫של לוג,
[00:57:01 - 00:57:03] כמו שכתוב כאן, שזה P
[00:57:08 - 00:57:10] X IJ, נדלי I,
[00:57:10 - 00:57:12] ‫דוגמה ה-J, הפיקסל ה-I,
[00:57:14 - 00:57:14] ‫בהינתן
[00:57:16 - 00:57:18] ‫הדוגמא ה-J היא כל מי שקטן מ-I,
[00:57:20 - 00:57:21] ‫והדטאטא
[00:57:22 - 00:57:28] של I. זה הדבר הזה.
[00:57:31 - 00:57:32] ‫זה ההסתברות של תמונה ספציפית.
[00:57:34 - 00:57:37] ‫התחום של כל הדברים זה לוג ההסתברות ‫של תמונה ספציפית.
[00:57:46 - 00:57:57] ‫כן, אז את זה אני צריך לבדוק פשוט,
[00:57:58 - 00:58:01] ‫אז אני יכול לעשות את כל הדברים האלה במקביל. ‫תמיד יש לי את כל ה-X,
[00:58:03 - 00:58:06] ‫כל איטרציה, יש לי את התטא ‫שאנחנו נוכחים,
[00:58:06 - 00:58:07] ‫את ה-Xים הקודמים,
[00:58:08 - 00:58:09] ‫את ה-Xים שאני רוצה לבדוק.
[00:58:10 - 00:58:14] ‫אני פשוט לוקח תמונות, ‫מאיזשהו מיני-באטש, ‫לספר על כל הפיקסלים,
[00:58:15 - 00:58:18] ‫אני חושב את זה דובר אופי תטא ומאבקן תטא.
[00:58:23 - 00:58:27] ‫הנקודה היא שכל חישוב כזה ‫אני יכול לעשות בבת אחת ‫במקביל בצורה יעילה.
[00:58:30 - 00:58:34] ‫אוקיי, זה סתם דוגמאות של איזשהו מודל ‫שעושה את זה באמת עם סיגמוידס כאלה.
[00:58:35 - 00:58:40] ‫זה בלי רשתות עמוקות, אוקיי? ‫זה רק סיגמוידס כאלה.
[00:58:40 - 00:58:49] ‫זה בעצם פונקציות ליניאריות ‫עם פונקציות סיגמויד ‫על גבי הפונקציה הליניארית.
[00:58:49 - 00:58:52] ‫כל פיקסל הוא בעצם תלוי בצורה ליניארית ‫בפיקסלים הקודמים.
[00:58:55 - 00:59:00] ‫זה דאטוסט שנקרא קלטק 101 סילואץ, ‫כל מיני אצליות כאלה של אובייקטים.
[00:59:02 - 00:59:04] ‫יוצר משהו לא הכי טוב,
[00:59:06 - 00:59:06] משהו יצוי.
[00:59:07 - 00:59:13] ‫אוקיי, שמאל זה הדאטה המלנטיב, ‫ולימין זה הדגימות מתוך המודלים.
[00:59:16 - 00:59:18] ‫אוקיי, אז מה הבעיה? ‫למה זה לא יצא ממש טוב?
[00:59:21 - 00:59:25] ‫הבעיה שיש לנו פה פשוט ליניארית, אוקיי? ‫אנחנו עכשיו רוצים
[00:59:28 - 00:59:31] ‫שפיקסל באמת יהיה תלוי בפיקסלים הקודמים, ‫אבל בצורה קצת יותר מעוקרת.
[00:59:31 - 00:59:34] ‫אנחנו לא רוצים את הפרמטריזציה המלאה,
[00:59:35 - 00:59:43] ‫של זה שאנחנו רואים את כל האפשרויות ‫של מה שהיה קודם, מה תהיה ההסתגרות. ‫אנחנו רוצים שזו תהיה איזושהי פונקציה ‫שמסתכלת על האינפוט ואומרת מה ההסתגרות,
[00:59:44 - 00:59:47] ‫אבל שזו לא יהיה פונקציה ליניארית ‫כאילו פשוטה, אלא משהו קצת יותר מורחב.
[00:59:49 - 00:59:53] ‫אנחנו עושים הסקה של עשר דקות, ‫ואז נדבר על איך בבקשה מסווכת ‫ואם רשת את כל מיני דקות.
[01:00:04 - 01:00:15] ‫-כן. השאלה, סליחה,
[01:00:17 - 01:00:20] ‫אז נדבר לי בכל התקצרים שבאים יחד, ‫זה לא יכול להיות...
[01:00:25 - 01:00:29] ‫-הבעיה אז לא הייתה מספר פרמטרים ‫אז שזה לא היה...
[01:00:29 - 01:00:30] ‫לא היה מזה להסתכל ‫קשה.
[01:00:31 - 01:00:36] ‫קשה. אנחנו התחלנו מזה ‫שאנחנו יכולים למצוא ‫בקריאה של שרשת, נכון?
[01:00:36 - 01:00:38] ‫כפי שלי, ‫שאנחנו יכולים לכתוב אותו ‫בתור פית של פרמטרים.
[01:00:38 - 01:00:39] ‫אה, הבנתי.
[01:00:40 - 01:00:44] ‫כמו שאתה בוחר, פרמטר שחקרת, ‫זה אומר שיש לנו איזשהו דאג בלי מעגלים.
[01:00:45 - 01:00:49] ‫אם יש לך מעגל, זה אומר שבאמת... ‫תחשוב על הדרך שאתה דוגם,
[01:00:49 - 01:00:53] ‫לא על הדרך לדגום. ‫-כן, להכפיא את זה בשביל להגיע ‫להסתברות של...
[01:00:54 - 01:00:57] ‫כן, תחשוב על זה גם אינטואיטיבי ‫שלא הייתה יכול לדגום,
[01:00:57 - 01:01:05] ‫אם בפיקסל הראשון הייתה לי בשני, ‫או בשנייה הייתה לי בראשונה, ‫ואלה הייתם עובדים, ‫אבל פחות לא בדרך, תודה.
[01:01:27 - 01:01:57] ‫-כן, תודה רבה.
[01:01:57 - 01:01:58] ‫-כן, תודה רבה.
[01:02:27 - 01:02:35] ‫-כן, תודה רבה.
[01:02:57 - 01:02:58] ‫-כן, תודה רבה.
[01:03:27 - 01:03:28] ‫-כן, תודה רבה.
[01:03:57 - 01:03:58] ‫-כן, תודה רבה.
[01:04:27 - 01:04:28] ‫-כן, תודה רבה.
[01:04:57 - 01:04:58] ‫-כן, תודה רבה.
[01:05:27 - 01:05:28] ‫-כן, תודה רבה.
[01:05:57 - 01:05:58] ‫-כן, תודה רבה.
[01:06:27 - 01:06:28] ‫-כן, תודה רבה.
[01:06:57 - 01:06:58] ‫-כן, תודה רבה.
[01:07:27 - 01:07:31] ‫אם אפשר להשתמש בו, זה לא...
[01:07:57 - 01:08:00] ‫אם אפשר להשתמש בו, זה לא...
[01:08:27 - 01:08:37] ‫אם אפשר להשתמש בו, זה לא בסדר. ‫אם אפשר להשתמש בו, זה בסדר?
[01:08:57 - 01:09:14] ‫אם אפשר להשתמש בו, זה בסדר? ‫-כן, תודה רבה.
[01:09:27 - 01:09:35] ‫אם אפשר להשתמש בו, זה בסדר? ‫אם אפשר להשתמש בו, זה לא נכון?
[01:09:57 - 01:10:05] ‫אם אפשר להשתמש בו, זה בסדר? ‫אם אפשר להשתמש בו, זה בסדר? ‫אני מודה לך,
[01:10:06 - 01:10:06] ‫שאני מסתמש בו,
[01:10:07 - 01:10:08] ‫מבין התנדות שלנו,
[01:10:09 - 01:10:13] ‫מהמחנה של דאבי, ‫ולכן, פעם כל סרטיים של ה...
[01:10:14 - 01:10:16] ‫לרכז כל דברים ממשיים, ‫התסתכלו על...
[01:10:17 - 01:10:19] ‫קצת הלבות שלנו,
[01:10:20 - 01:10:22] ‫גם פרוש מיגויות בו,
[01:10:23 - 01:10:25] ‫אבל אני באמת מצמד מאשר.
[01:10:25 - 01:10:26] ‫-כן, תודה רבה.
[01:10:55 - 01:10:55] ‫באמת סתם נוסעים להפחיד את העמדה,
[01:11:25 - 01:11:27] ‫יש ענן למרכא...
[01:11:55 - 01:12:24] ‫נתחיל, נבוא עשר דקות פעם כאן.
[01:12:26 - 01:12:26] ‫אז נתחיל.
[01:12:27 - 01:12:28] ‫עכשיו זה גם שוב
[01:12:29 - 01:12:30] ‫שאמור להיות תזכורת לרובכם.
[01:12:31 - 01:12:36] ‫אז בעצם אנחנו לא רוצים שהמודל שלנו ‫יהיה רק לינארי כזה.
[01:12:40 - 01:12:45] ‫יש התלות בין ה-Xים הקודמים ‫להסתברות של ה-X החדש, ‫היא תהיה פשוט לינארית.
[01:12:45 - 01:12:47] ‫פשוט מדי, אנחנו רוצים לסבך את זה קצת.
[01:12:49 - 01:12:52] ‫אנחנו נשתמש ברשתות נוירונים ‫לעשות את זה.
[01:12:53 - 01:13:02] ‫אוקיי, אז תזכור את זה רגע. ‫אז בעצם Deep Learning, השם שנתנו לשימוש ‫ברשתות נוירונים עם הרבה שכבות,
[01:13:03 - 01:13:05] ‫זה פותח מקו של קאסיפיקציה.
[01:13:05 - 01:13:08] ‫אז בעצם הוא נותן איזשהו input, ‫זה עובר דרך הרבה שכבות,
[01:13:09 - 01:13:11] ‫והם נותנים לעשות איזושהי ‫אדיקציה לקלאס.
[01:13:14 - 01:13:18] ‫אבל לרוב, איך שזה ממומש, ‫זה על בסיס משהו די דומי ‫ג'יסטיק רגרשן.
[01:13:19 - 01:13:20] ‫אז אם יש לנו לוג'יסטיק רגרשן,
[01:13:20 - 01:13:24] ‫שאנחנו מאמנים את זה,
[01:13:24 - 01:13:26] ‫כבר את זה אנחנו לא יודעים ‫לפתור בצורה אמלית איתה.
[01:13:27 - 01:13:28] ‫אנחנו מאמנים את זה ‫עם gradient descent.
[01:13:39 - 01:13:43] ‫אנחנו היינו רוצים לעשות אותו דבר, ‫רק שפה תלות שהיא לא ליניארית,
[01:13:44 - 01:13:46] ‫זו תלות שהיא קצת יותר מורכבת.
[01:13:47 - 01:13:52] ‫אז זה הרעיון בדיפריאנינג.
[01:13:54 - 01:13:55] ‫ובעצם,
[01:13:56 - 01:13:58] איך שזה ממומש, ‫יש לנו כל מיני סוגי ארכיטקטורות,
[01:13:59 - 01:13:59] ‫זה דוגמא
[01:14:03 - 01:14:06] ל...ארכיטקטורה שלנו בסוף כלל קונבולוציות, ‫אני מדבר על תקציות גם היום,
[01:14:07 - 01:14:13] ‫אבל בעיקרון יש לנו בדרך כלל כל מיני שכבות, ‫כל שכבה מסתכלת על הארכיב של השכבה הקודמת,
[01:14:14 - 01:14:16] ‫עושה איזשהו חישוב ליניארי,
[01:14:16 - 01:14:18] ‫פלוס משהו לא ליניארי,
[01:14:18 - 01:14:21] ‫זה יותר הכי פשוט ב-value, ‫זה משהו ש...
[01:14:22 - 01:14:24] ‫זה פשוט פונקציה ש...
[01:14:26 - 01:14:27] ‫ככה, זאת אומרת,
[01:14:27 - 01:14:29] ‫עד איזשהו ערך מסוים היא מאפסת,
[01:14:30 - 01:14:35] ‫עם הכניסה לערך שקטן ‫מאיזשהו פשוט מאפסת את זה, ‫אחרת היא פשוט מעבירה אותו.
[01:14:37 - 01:14:39] ‫זה הדבר היחיד שהוא לא ליניארי.
[01:14:40 - 01:14:42] ‫חוץ מהשכבה האחרונה,
[01:14:42 - 01:14:53] ‫שהיא בדרך כלל גם קלסיפיקציה זה ‫סופטמאקס, אוקיי? ‫שסופטמאקס אפשר לחשוב על זה ‫בתור הכללה של לוג'יסטיק רגרשן.
[01:14:54 - 01:14:54] ‫לוג'יסטיק רגרשן,
[01:14:55 - 01:14:58] אם היינו רק שני קלאסים, ‫אז בעצם אפשר להגיד ‫שהארטפוט שלנו יהיה,
[01:15:00 - 01:15:07] ‫פשוט יהיה כתוב כאן במקום טטה כפול X, ‫טטטה כפול השכבה האחרונה, ‫הארטפוט של השכבה האחרונה ברשת,
[01:15:08 - 01:15:12] וזה יהיה, אנחנו נפעיל על זה את הסיגמויד,
[01:15:12 - 01:15:14] ‫כדי שזה יהיה מספר בין אפס לאחד, ‫וזה יהיה ההסתברות
[01:15:15 - 01:15:15] של כלבים,
[01:15:16 - 01:15:18] ‫ואחד פחות זה, ‫זה יהיה ההסתברות של חתולים.
[01:15:19 - 01:15:21] אם יש לנו יותר משתי קבוצות,
[01:15:23 - 01:15:26] ‫אז יש את ההכללה הזאת P, ‫שקוראים לזה SoftM�קס בדרך כלל,
[01:15:27 - 01:15:31] ‫יש את זה פשוט החישוב הזה, ‫אז XI זה הארטפוט בשכבה האחרונה,
[01:15:32 - 01:15:32] ‫של הדוגמה I,
[01:15:33 - 01:15:35] ‫אנחנו מכפילים את זה ‫באיזשהו משהו ליניארי,
[01:15:38 - 01:15:40] ‫ושים את האקספוננט של זה.
[01:15:41 - 01:15:46] ‫יש פרמטר כזה, יש פרמטר אחר ‫לכל אחד מה-K קלאסיס.
[01:15:47 - 01:15:48] ‫לאמניניסט יש לנו,
[01:15:49 - 01:15:50] ‫אם עושים את הסיפיקציה לאמיניסט,
[01:15:51 - 01:15:55] ‫לספורות, יש לנו עשר מתפקות שונות,
[01:15:56 - 01:16:00] ‫יש לנו בשפרה האחרונה ‫עשר קבוצות שונות של תטא,
[01:16:01 - 01:16:04] ‫כל אחד מהם מכפיל בארטפוט של השכבה ‫לפני האחרונה.
[01:16:05 - 01:16:07] ‫אז נקח את האקספוננט של זה,
[01:16:07 - 01:16:08] ‫כדי שזה יהיה מספר חיובי,
[01:16:09 - 01:16:12] ‫ונחלק בסכום כל האקספוננטים ‫של כל ה...
[01:16:12 - 01:16:14] ‫בואו נקח את האקספוננטיות האחרות, ‫של הקלאסים האחרים,
[01:16:15 - 01:16:16] ‫כך שזה יהיה מספר
[01:16:16 - 01:16:25] ‫שהוא יהיה חיובי בין 0 ל-1, ‫שסכום כל המספרים האלה יהיה שווה 1. ‫בעצם השכבה האחרונה ‫של הרשת הזאת תהיה הסתברות קטגורית,
[01:16:27 - 01:16:30] ‫על כל הקטגוריות שיכולות להיות,
[01:16:30 - 01:16:31] ‫כל הקלאסים שיכולים להיות.
[01:16:36 - 01:16:37] ‫הלוס
[01:16:39 - 01:16:41] ‫אז הוא נראה ככה, ‫זה בדיוק בעצם מה שכתבנו פה.
[01:16:42 - 01:16:48] ‫פה במקרה יותר כללי שזה Soft Macs, ‫עם K קלאסים, אוקיי? זה סכום על כל ה...
[01:16:49 - 01:16:50] ‫סלחו לי על
[01:16:53 - 01:17:07] ‫השתלם כאן על שני ערכים אחרים. ‫אז כאן I זה הדוגמה, ו-K זה המחלקה, כן? ‫במקרה שלנו זה אותו דבר, ‫אנחנו יכולים להגיד שהערכים ‫שהפיקסל מקבלים יכול לקבל,
[01:17:07 - 01:17:12] ‫זה בעצם המחלקה שיכול להיות. ‫אם זה פיקסל בינארי או 0 או 1,
[01:17:13 - 01:17:15] ‫אז אנחנו נגיד, ‫יהיה לנו פה שני קלאסים,
[01:17:16 - 01:17:21] ‫ויהיה לנו שני ערכים, ‫או 0 או 1. ‫אם זה יותר, אם זה נגיד 256 ערכים,
[01:17:21 - 01:17:22] ‫אז יהיו לנו...
[01:17:23 - 01:17:27] זה הופך להיות כמו בעיית קלסיפיקציה ‫עם 256 קלאסים.
[01:17:28 - 01:17:29] ואנחנו...
[01:17:29 - 01:17:36] הדרך של... המקסימום לייקיות של האלגוריתם הזה, ‫זה בדיוק אותו דבר כמו לעשות מקסימום לייקיות ‫בבעיית קלסיפיקציות.
[01:17:37 - 01:17:38] ‫זה מה שכתוב כאן.
[01:17:38 - 01:17:39] ‫אנחנו עוברים על כל הדוגמאות,
[01:17:40 - 01:17:43] ‫אנחנו עוברים על כל האפשרויות,
[01:17:44 - 01:17:45] ‫רק אחד מהם דולק,
[01:17:46 - 01:17:51] ‫הפיקסל המסוים, ‫אז רק הדוגמה ה-K שלו דולקת,
[01:17:51 - 01:17:54] ‫רק הקלאס ה-K שלו דולקת, ‫יש לו ערך מסוים,
[01:17:55 - 01:17:59] ‫וזה הערך שאנחנו מכניסים לחישוב.
[01:18:02 - 01:18:03] ‫לוג של הדבר הזה.
[01:18:04 - 01:18:06] ‫בסדר, בדרך זה אנחנו עושים
[01:18:10 - 01:18:13] ‫מדוזרים את זה ומחזרים את זה.
[01:18:17 - 01:18:18] ‫זה ככה גם עושים את הסיפיקציה,
[01:18:19 - 01:18:24] ‫אנחנו עושים את זה לא על כל הדאטה-סט שלנו בבת אחת. ‫-כן, אז אם אנחנו רוצים לאמן מודל על אמניסט,
[01:18:26 - 01:18:27] ‫לא מאמנים על...
[01:18:28 - 01:18:31] ‫לא כל פעם לוקחים את כל הנקודות של אמניסט, ‫את כל התמונות של אמניסט.
[01:18:33 - 01:18:41] ‫ומעדכנים, עושים עדכון אחד של התטא ‫על סמך כל התמונות, ‫אלא עובדים סטוקסטי גרדיט ניסיונט. ‫אותו פעם לוקחים איזשהו סאבסט של תמונות,
[01:18:42 - 01:18:44] ‫ועובדים איתו.
[01:18:44 - 01:18:45] ‫זה פותר לנו כמה בעיות,
[01:18:46 - 01:18:47] ‫זה גם יותר יעיל.
[01:18:49 - 01:18:52] ‫אנחנו צריכים לקחת סטיילת על הכול ‫כדי לעשות רק חישוב קטן.
[01:18:53 - 01:18:58] ‫זה גם קצת מכניס לנו רעש ‫לפעמים הוא עוזב לצאת ‫מכל מיני מינימומים,
[01:18:59 - 01:19:01] ‫מינימומים דוקאליים כאלה שקודמים את זה.
[01:19:04 - 01:19:08] ‫טוב, מי שלא זוכר את הדברים האלה, ‫צריך לעשות חזרה לדיט-לרנינג.
[01:19:09 - 01:19:11] ‫איך שזה ממומש,
[01:19:12 - 01:19:14] ‫זה על ידי ספריות שעושות פוד-טודיף,
[01:19:16 - 01:19:18] ‫כמו טנסור-פלו, פייטורג' וג'אקס.
[01:19:20 - 01:19:22] ‫והרעיון הוא שבעצם כל שכבה,
[01:19:24 - 01:19:26] ‫הדרך שאנחנו בונים את הרשתות האלה,
[01:19:27 - 01:19:32] ‫היא מודולרית, זאת אומרת, ‫אנחנו בונים כל שכבה כזאת בצורה כזאת,
[01:19:32 - 01:19:34] ‫שאנחנו יודעים לחשב את החישוב שהיא עושה,
[01:19:35 - 01:19:36] ‫בהינתן האינפוט של השכבה הקודמת,
[01:19:37 - 01:19:39] ‫אבל גם לגזור, ‫להעביר את הנגזרת אחורה,
[01:19:40 - 01:19:41] ‫מה שנקרא ב-propagation.
[01:19:41 - 01:19:44] ‫זה הכול ממומש כבר בצורה אוטומטית ‫בכל שכבה,
[01:19:44 - 01:19:47] ‫אבל כל מה שאנחנו צריכים לעשות ‫זה להכניס את השכבות האלה בסדר הנכון,
[01:19:48 - 01:19:50] ‫והגזירה תהיה אוטומטית.
[01:19:52 - 01:19:53] ‫יש כאן דוגמה למימוש כזה.
[01:19:54 - 01:19:57] ‫אז לא תצטרכו אף פעם לראות קוד כזה.
[01:19:58 - 01:20:01] ‫פעם היה צריך לכתוב את הקוד הזה,
[01:20:01 - 01:20:03] ‫עכשיו הכול כבר נמצא בתוך הספריות.
[01:20:05 - 01:20:07] ‫זה למשל שכבה ליניארית עם סיגמויד,
[01:20:08 - 01:20:09] ‫אז יש כאן את החישוב ה-forward,
[01:20:11 - 01:20:13] ‫אז צריך להכפיל את ה-X ו-W,
[01:20:14 - 01:20:20] ‫ואז לעשות את ה-A-pump זה בעצם הסיגמויד הזה,
[01:20:21 - 01:20:21] ‫אוקיי?
[01:20:22 - 01:20:25] ויש פה חישוב את הנגזרת של זה, ‫מה יוצא הנגזרת,
[01:20:26 - 01:20:28] ‫ובעצם יש דרך לשרשר את כל הדברים האלה, ‫גם
[01:20:29 - 01:20:32] בחישוב קדימה וגם בחישוב של הנגזרת של זה.
[01:20:34 - 01:20:36] ‫אוקיי, אז זה גם מה שאנחנו רואים ‫שאנחנו נשתמש.
[01:20:37 - 01:20:49] ‫עכשיו בואו נראה דוגמה ראשונה, ‫שזה בעצם כמו ה-FVSBI שראינו קודם, ‫שזה ה-Full-Evisable Sigma-Mov
[01:20:49 - 01:20:51] ‫בניברנט-נטוורק, ‫רק עם שכבה אחת,
[01:20:51 - 01:20:52] ‫בראשית דורונים, שיש לה שכבה אחת כבוי ארתע.
[01:20:53 - 01:20:57] ‫הידן לייר פה זה לא שונה מ-Latent Variable שלנו קודם, אוקיי?
[01:20:57 - 01:20:59] ‫זו שכבה חבויה של רשת ניורונים.
[01:21:00 - 01:21:01] ‫יש איזושהי קונבנציה ‫שלא כולם
[01:21:04 - 01:21:06] ‫מתמשים בה, אבל בדרך כלל כן, ‫כשאומרים hidden,
[01:21:07 - 01:21:12] ‫אז הכוונה לשכבות בתוך הרשתות, ‫בתוך גישוב.
[01:21:12 - 01:21:13] ‫כשאומרים ל-Latent,
[01:21:14 - 01:21:18] ‫אז מתכוונים למשתנה סטוכסטי חבוי, ‫שעושים עליו את ה...
[01:21:21 - 01:21:24] ‫דיברנו בשיעורים הקודמים, ‫שאנחנו נחזור לדבר עליהם ‫בשבועות הבאים,
[01:21:24 - 01:21:28] ‫אבל יש לנו אינפרנס, כיוון כאלה.
[01:21:29 - 01:21:33] ‫אוקיי, אז פה זה לא כזה דבר, ‫זה פשוט שכבה במקום שאנחנו נחשב
[01:21:34 - 01:21:36] ‫שכבה חישוב כאן, או במקום שהוא יהיה לינארי,
[01:21:36 - 01:21:37] ‫תטא כפול X,
[01:21:38 - 01:21:40] ‫אז קודם אנחנו עושים חישוב אחד ‫שמחשב את H,
[01:21:41 - 01:21:43] ‫שזה יהיה תטא כפול X,
[01:21:44 - 01:21:49] ‫אז משהו אחר שמחשב את הארטנוק הזה, ‫שזה יהיה תטא אחר כפול ה-H שיצא.
[01:21:50 - 01:21:53] אוקיי? אנחנו תוקפים את כל ה-X, ‫זה ה-X הראשון שלנו,
[01:21:53 - 01:21:55] ‫אוקיי, שיש לנו 84 פיקסלים.
[01:21:57 - 01:22:00] ‫עכשיו אנחנו מחשבים את H הראשון. ‫ה-H הראשון הוא לא פונקציה של כלום.
[01:22:02 - 01:22:02] אוקיי?
[01:22:02 - 01:22:07] ‫בשקבה הזאת היא לא מקבלת שום אינפוט, ‫יש לה איזשהו פתחול או משהו.
[01:22:08 - 01:22:11] ‫עכשיו יש לנו את X0, השיערוך של...
[01:22:18 - 01:22:21] ‫מה המשמעות של ה-0 הזה כאן? ‫אבל הפיקסל הראשון של ה-X הזה,
[01:22:21 - 01:22:32] ‫הוא יהיה פשוט פונקציה של H1. ‫פה בעצם לא השתנה כלום, ‫ראש שבמקום שיהיה לנו משתנה חופשי אחד,
[01:22:33 - 01:22:36] ‫אומר לנו מה ההסתברות, ‫יש לנו פה איזשהו חישוב מסובך,
[01:22:37 - 01:22:40] ‫קצת מיותר, ‫אבל זה מה שיוצא מהחישוב הזה, ‫זה לא משנה.
[01:22:42 - 01:22:45] ‫הרבה מספרים שבסופו של דבר ‫נותנים לנו מספר אחד, ‫שהוא תמיד יהיה יותר מספר,
[01:22:46 - 01:22:50] ‫זה אומר מה ההסתברות, ‫שהפיקסל הראשון שווה 1. ‫פחות מעניין.
[01:22:51 - 01:22:55] ‫מה שמעניין זה מה שקורה אחר כך, אוקיי? ‫הפיקסל השני, כשאני רוצה לחשוב ‫את הפיקסל השני,
[01:22:55 - 01:22:57] ‫אז הוא תלוי באידן השני,
[01:22:58 - 01:23:01] ‫והאידן השני הוא פונקציה של הפיקסל הראשון,
[01:23:01 - 01:23:02] ‫אז הוא כבר פונקציה,
[01:23:03 - 01:23:06] ‫יוצא שהוא פונקציה לא ליניארית ‫של הפיקסל הראשון.
[01:23:08 - 01:23:09] ‫אתם רואים למה זה לא לינארי?
[01:23:10 - 01:23:16] ‫יש כאן חישוב לינארי, פלוס בתוך כל יוניט כזה, ‫יש איזה משהו לא לינארי,
[01:23:17 - 01:23:17] נגיד רלייו,
[01:23:18 - 01:23:20] ‫ואז יש לנו חישוב לינארי של התוצאה שלי.
[01:23:20 - 01:23:23] ‫וזה כבר יהיה משהו שהוא לא ליניאלי ‫בפיצל הפיצל.
[01:23:24 - 01:23:26] ‫אז הוא יכול לתפוס אולי דברים ‫קצת יותר מעניינים.
[01:23:26 - 01:23:28] ‫ואם נסתכל על הפיצל האחרון,
[01:23:29 - 01:23:32] ‫אז הוא מסתכל על ה-Latent, ‫ה-הידן, סליחה, האחרון,
[01:23:33 - 01:23:35] ‫שהוא פונקציה של כל הפיצלים הקודמים,
[01:23:36 - 01:23:39] ‫אז זה יהיה פונקציה לא ליניארית ‫של כל הפיצלים הקודמים.
[01:23:42 - 01:23:42] בסדר?
[01:23:43 - 01:23:44] כן.
[01:23:47 - 01:23:49] אז זה למשל החלטה שאפשר לעשות,
[01:23:49 - 01:23:51] ‫שזה מודל הזה שנקרא נייל,
[01:23:52 - 01:23:55] ‫מה הגודל של השכבת ביניים הזאתי.
[01:23:57 - 01:23:59] ‫בעצם זה שהגודל של שכבת ביניים הזאתי ‫לא 500, אלא 500,
[01:24:01 - 01:24:01] ‫כפול
[01:24:04 - 01:24:05] 780 ו-ארץ.
[01:24:07 - 01:24:10] ‫זה מודל שנראה, אתה מצייר אותו.
[01:24:20 - 01:24:22] ‫אז יש לנו כאן את ה-X וה-input כאילו,
[01:24:24 - 01:24:25] ‫אז כאן יש לנו את ה-H,
[01:24:28 - 01:24:31] ‫מאוד גדול, כן, ‫הוא יהיה פי 500 יותר גדול ב-H.
[01:24:32 - 01:24:33] ‫כאן יש לנו את הפרדיקציות.
[01:24:35 - 01:24:36] ‫יש לנו פה כל מיני
[01:24:37 - 01:24:38] ‫עובדים כאלה.
[01:24:39 - 01:24:39] צריכים אבל להישמע
[01:24:42 - 01:24:44] להוראות האלה שכל קבוצה של H
[01:24:45 - 01:24:47] ‫היא קובעת את ה-X הבא,
[01:24:48 - 01:24:49] ‫היא רק תלויה ב-Xים הקודמים.
[01:24:54 - 01:24:58] ‫אז המוגד הזה נקרא ‫נאייד, New All Auto-Regressive Density Exthumbation,
[01:24:58 - 01:24:59] ‫מ-2014.
[01:25:00 - 01:25:01] ‫-נראה את המחוז הזה עוד
[01:25:03 - 01:25:06] ‫מ-500 ל-500 תפגעת יותר גדולה.
[01:25:07 - 01:25:11] ‫אז יש כמה החלטות שיש לעשות, ‫אפשר להרחיב את זה, ‫אפשר גם שזה יהיה יותר עמוק משתי שכבות.
[01:25:12 - 01:25:13] ‫בעצם הגדלנו את זה עכשיו,
[01:25:13 - 01:25:16] ‫אפשר לקרוא לשתי שכבות בשכבה אחת מילינארית,
[01:25:17 - 01:25:22] ‫כאילו, עשינו את הצעד הבא, ‫שזה יש לנו שני דברים ליניאריים ‫ומשהו ליניארי אחד באמצע.
[01:25:22 - 01:25:25] ‫אפשר שהדבר הזה יהיה יותר רחב,
[01:25:25 - 01:25:28] ‫ואופציה אחת. ‫אופציה שנייה, זה שיהיה לנו כמה שכבות.
[01:25:32 - 01:25:35] ‫אין משהו שאומר מה כדאי לעשות.
[01:25:38 - 01:25:47] ‫אין משהו בו מקסימום לייקליות ‫בעצם בין כמה אופציות.
[01:25:48 - 01:25:49] ‫ולראות מה עובד הכי טוב.
[01:25:50 - 01:25:55] ‫קצת קשה לעשות גרדיאנט דיסנט דרך זה, ‫כי זו החלטה דיסקרטית כזאתי, ‫האם אני מוסיף או לא מוסיף,
[01:25:56 - 01:26:01] ‫אז אין לך גרדיאנט לדבר הזה, ‫אבל אתה יכול פשוט לבדוק כמה דברים ‫ולראות מה יצא הכי טוב.
[01:26:03 - 01:26:08] ‫זה כמו לעשות מקסימום לייקליות בעצם, ‫אבל בצורה חיפוש טיפש כזה, ‫לא חיפוש טכן.
[01:26:12 - 01:26:15] ‫אוקיי, אז זה תוצאות של נייד של המודל הזה.
[01:26:16 - 01:26:21] ‫אף פעם רואים טוב, אצלי נדמה לי יותר טוב, ‫אבל יש כאן הבדל ‫בין מה שיש לספול ומימין.
[01:26:22 - 01:26:25] ‫כל זה זה דגימות, אוקיי? ‫שוב, למדנו את המודל,
[01:26:25 - 01:26:30] ‫ואז דגמנו. איך דגמנו? ‫כמו שראינו קודם, אנחנו מעבירים...
[01:26:34 - 01:26:38] ‫אנחנו עושים את החישוב הזה, ‫זה נותן לנו את ההסתברות של x0.
[01:26:39 - 01:26:41] ‫אנחנו דוגמים מההסתברות הזאת.
[01:26:42 - 01:26:43] ‫יצא לנו 0 או 1,
[01:26:44 - 01:26:46] ‫אז אנחנו שמים את זה כאן, אוקיי?
[01:26:47 - 01:26:55] ‫ואז אנחנו מפעילים את h2, ‫דוגמים... סליחה, אנחנו מחשבים את h2, ‫מחשבים את x2,
[01:26:56 - 01:26:58] ‫דוגמים ממנו, יצא לנו 0 או 1,
[01:26:59 - 01:27:03] ‫אנחנו נותנים פה בדיוק את החישוב ‫שעשינו קודם, ‫אבל כשעכשיו כל חישוב ‫הוא לא חישוב לינארי,
[01:27:04 - 01:27:06] ‫אלא יש לו שתי שכבות, שתי רמות של חישוב.
[01:27:08 - 01:27:10] ‫וככה אנחנו, עד שאנחנו מגיעים ‫לכל התובבויות.
[01:27:11 - 01:27:12] ‫זה נותן לנו תמונה אחת.
[01:27:13 - 01:27:19] ‫אז אפשר להתחיל עוד פעם. ‫מההתחלה, איפשהו יצא לנו בהסתברות ‫שאנחנו דוגמים משהו אחר,
[01:27:19 - 01:27:21] ‫ואז כל מה שיהיה מתחת יהיה שונה.
[01:27:22 - 01:27:28] ‫יכול להיות שהx0 הראשון יצא לנו עכשיו 0 ‫במקום 1 בפעם הראשונה, ‫אז עכשיו כל החישוב הזה יהיה שונה.
[01:27:29 - 01:27:30] ‫כל התמונה שתצא לנו תראה אחרת.
[01:27:32 - 01:27:38] ‫אז זו דוגמה, אני לא יודע כמה דוגמאות יש פה בעיות. ‫-100 קיימים חזרו על התהליך הזה, ‫בכל פעם יצא משהו אחר.
[01:27:39 - 01:27:42] ‫אז מה שרואים משמאל זה המדמידי, ‫כמו שתיארתי,
[01:27:42 - 01:27:50] ‫אז מה שרואים מימין זה שעשו את התהליך ‫כמו שתיארתי, ‫אבל מה שציירו כאן זה לא את ה-0 או 1 שיצא, ‫אלא את ההסתברות שיצאה.
[01:27:53 - 01:27:55] ‫זה יותר חלק קצת.
[01:27:56 - 01:28:00] ‫למשל פה יש בשלוש הזה, ‫יש כל מיני חורים שיצא 0,
[01:28:01 - 01:28:03] ‫למרות שההסתברות אולי יותר גדולה, ‫שיהיה 1,
[01:28:05 - 01:28:08] ‫אבל גם אם זה הסתברות 90%, 90% שזה יוצא 1,
[01:28:09 - 01:28:10] ‫לפעם ל-10 יוצא 0.
[01:28:11 - 01:28:26] ‫כן, זה יוצא כאן 0. ‫יש כאן איזה חור כזה שחור, ‫למרות שהסתכלים על זה כאן, ‫זה קצת יותר חלק. ‫זה ערך קצת יותר, קצת פחות לבן, ‫כי זה 90% ולא 99% ‫אבל זה עדיין ימר על הלבן. ‫זה יותר נוח להסתכל ‫על המונח החלקה הזאת.
[01:28:29 - 01:28:30] ‫אוקיי, אז זה התוצאות.
[01:28:32 - 01:28:35] ‫זה משהו חשוב, זה מתוך המאמר הזה שלהם, ‫של מייל מ-2011.
[01:28:40 - 01:28:46] ‫אוקיי, אז נקודה למחשבה שהיא... ‫אני חושב שנוח וכדאי לחשוב ככה ‫על מודלים כאלה,
[01:28:47 - 01:28:49] ‫גם אולי יתקשר לנו קצת למודלים ‫שנראה בהמשך.
[01:28:50 - 01:28:52] ‫אפשר לחשוב על מודלים כאלה ‫בתור auto-encoders.
[01:28:54 - 01:28:55] ‫אתם יודעים מה זה auto-encoders?
[01:28:56 - 01:28:58] ‫בקורס לדיפ-לרנינג, ‫דיברתם על auto-encoder?
[01:29:00 - 01:29:02] ‫-auto-encoder זה פשוט מודל שמקבל,
[01:29:03 - 01:29:10] ‫שהinput שלו והoutput וה-label ‫שאיתו אנחנו עושים את ה...
[01:29:11 - 01:29:17] ‫מחשבים את הלוס, אותו דבר, אוקיי? ‫בעצם המטרה שלו זה לעשות auto-encoder, ‫זאת אומרת, לעשות
[01:29:17 - 01:29:18] ‫פרדיקציה לעצמו.
[01:29:19 - 01:29:26] ‫אז אפשר לחשוב על auto-regressive models ‫בתור auto-encoders,
[01:29:27 - 01:29:29] ‫אבל שהoutput הוא מוזל.
[01:29:30 - 01:29:32] ‫בעצם x-time לא יכול להסתכל,
[01:29:34 - 01:29:37] ‫יש פה כל מיני אילוצים על החיבורים,
[01:29:38 - 01:29:41] ‫זאת אומרת שx-time לא יכול להיות תלוי בעצמו,
[01:29:42 - 01:29:44] ‫הוא רק יכול להיות תלוי ‫בכל מי שבא לפניו.
[01:29:45 - 01:29:47] ‫אם אנחנו יכולים לסדר ככה את החיצים,
[01:29:47 - 01:29:48] ‫אז זה חוקי.
[01:29:49 - 01:29:50] ‫עוד דרך לחשוב על זה,
[01:29:52 - 01:29:53] ‫זה בעצם אותו דבר,
[01:29:57 - 01:30:00] ‫זה שהoutput הוא בעצם איזשהו
[01:30:04 - 01:30:06] שיפט של האינפוט, אוקיי?
[01:30:06 - 01:30:09] ‫שוב, אסור שיהיו חצים שעוברים ‫משמאל לימין,
[01:30:10 - 01:30:16] ‫אבל החץ של עצום, זאת אומרת, x-2, ‫אני משווה אותו לאיזושהי הזוזה. ‫זאת אומרת, גם ככה זה מצויר פה,
[01:30:22 - 01:30:26] ‫שהכל כאן מוזז באחד מזה, אוקיי? ‫זה לא תלוי בכלום.
[01:30:27 - 01:30:29] ‫ככה הם חזרו לכאן, אבל...
[01:30:30 - 01:30:33] ‫כבר נראה לי זה היה צורך טוב טוב, ‫אבל לא משנה.
[01:30:33 - 01:30:36] ‫אז הפיקסל הראשון הוא לא תלוי בכלום.
[01:30:37 - 01:30:40] ‫הפיקסל השני תלוי רק בפיקסל הראשון,
[01:30:40 - 01:30:45] ‫הפיקסל השלישי תלוי בקום שבא ‫לפני הפיקסל השלישי,
[01:30:46 - 01:30:46] ‫וככה הלאה.
[01:30:48 - 01:30:50] ‫דרך שהרבה פעמים ‫מממשים דברים כאלה,
[01:30:51 - 01:30:53] ‫זה שמכניסים את האינפוט,
[01:30:54 - 01:30:55] ‫מסתכלים על האינפוט, ‫אבל באיזושהי הזזה.
[01:30:57 - 01:31:00] תזכרו את זה, ואולי כשנדבר על זה, ‫כשנדבר על קוד,
[01:31:00 - 01:31:04] ‫אז זה ילכים קצת יותר ברור ‫במה אני מתכוון.
[01:31:09 - 01:31:16] ‫אוקיי, אז פה יש איזה גרסה יותר עמוקה ‫כזאתי של נייד, שנקראת מייד, ‫והם בעצם
[01:31:19 - 01:31:22] ‫כן הסתכלו על זה בתור, ‫הם ממש אוטו-אנקודר כזה, ‫בתור רשת עמוקה,
[01:31:23 - 01:31:29] ‫שפשוט יש כל מיני אילוצים ‫על המיקום שמותר לשים פץ.
[01:31:30 - 01:31:35] ‫כמו שאמרנו, אני לא רוצה ש... ‫אסור לי ש-X3 יהיה תלוי ב-X3. ‫כי כבר אני אסור לי רק ב-X1 וב-X2.
[01:31:36 - 01:31:40] ‫אסור לי שיהיה לי כאן מסלול מ-X3 ‫לפרדיקציה של X3.
[01:31:43 - 01:31:46] ‫אז בשיטה הזאת, ‫הם קוראו לזה ‫מאסק אוטו-אנקודר.
[01:31:46 - 01:31:51] ‫הם פשוט חישבו כל מיני מפות כאלה ‫של המאסקים,
[01:31:52 - 01:31:53] ‫ומאסק זה פשוט
[01:31:54 - 01:31:57] ‫הפריצות שיש להם, ‫הם הולכים בין 0 או 1. ‫אתהם מכפיל את זה
[01:31:57 - 01:32:01] ‫במשקולות שיש לך,
[01:32:02 - 01:32:04] ‫ובעצם אתה מאפס את החיצים, ‫אתה כאילו מוחק חלק מהחיצים.
[01:32:06 - 01:32:11] ‫וככה הם מאלצים, צריך לבנות את ‫המסכות האלה בצורה חכמה, ‫ככה שבאמת
[01:32:12 - 01:32:18] לא יהיה מסלול מכל פיקסל בעצמו, ‫וגם לא יהיה מסלול מהפיקסלים...
[01:32:19 - 01:32:19] ‫זאת אומרת,
[01:32:20 - 01:32:24] ‫עם כל פיקסל בפרדיקציה יהיה מסלול ‫רק מכל הפיקסלים הקודמים.
[01:32:25 - 01:32:26] ‫יש הרבה דרכים שאפשר לסדר ככה.
[01:32:27 - 01:32:32] ‫את הרשת ככה שזה יקרה. ‫אבל צריך לגרום לזה שבאמת.
[01:32:33 - 01:32:40] ‫זה נעשה בקונפיגורציה הראשונית של הרשת, ‫או שיש איזה דרופ-אאוט ‫שקורית תוך כדי האימון?
[01:32:41 - 01:32:47] ‫זה נעשה, יש להם פה כמה שיטות, ‫כי אני חושב שהם מאמנים ביחד ‫עם כמה מסכות שונות,
[01:32:49 - 01:32:54] ‫אבל הרעיון הזה, בגדול אתה צריך לחשוב ‫על זה כאילו זה לפני זה. יש לך... זה בעצם איך שאתה בונה את הרשת,
[01:32:55 - 01:32:57] ‫אבל כדי שהדברים יהיו יותר יעילים, ‫בונים את הרשת המלאה.
[01:32:57 - 01:33:02] ‫עם כל השכבות המודולריות האלה ‫שראינו שעובדות מראש,
[01:33:03 - 01:33:05] ‫שפשוט בונים מסכות
[01:33:06 - 01:33:09] ‫שמכפילים אותן כל פעם שעושים את החישוב.
[01:33:10 - 01:33:14] ‫אני חושב שהיה להם גם איזה משהו ‫שכל פעם הם מסתכלים על מסכות אחרות,
[01:33:14 - 01:33:16] ‫אבל כל מסכה כזאת צריכה להיות חוקית.
[01:33:27 - 01:33:28] ‫אוקיי.
[01:33:32 - 01:33:34] ‫אז זה ברור, הרעיון פה?
[01:33:37 - 01:33:41] ‫זה לא ספורי למה שהיה פה, נכון? ‫כי פה יש את הוועצים משותפים במקרה ה...
[01:33:42 - 01:33:43] ‫בכל היציאות.
[01:33:44 - 01:33:44] ‫זה לא היה.
[01:33:45 - 01:33:49] ‫כן, אז פה חלק מהוועצים משותפים, ‫הם לא חייבים להיות משותפים, ‫זה תלוי איך אתה בונה את זה, אבל כן.
[01:33:49 - 01:33:50] זה...
[01:33:52 - 01:33:54] כן, מצד אחד אתה יכול להביא לך יותר
[01:33:55 - 01:33:57] ‫יותר משקולות ויותר עמוק.
[01:33:58 - 01:34:00] ‫מצד שני אתה גם קצת משתף בין המשקולות.
[01:34:00 - 01:34:05] ‫אבל מתפלא שאתה בונה את המאסק. ‫אתה יכול לבנות מאסק ‫שזה יהיה בדיוק שקול למה שהיה קודם.
[01:34:06 - 01:34:10] ‫זאת אומרת, יש כאן הבדל ‫שיש פה יותר שכבות, אבל אם היה רק שכבה אחת, ‫היית יכול לבנות את המאסק
[01:34:11 - 01:34:12] ‫שיגרום לזה להיות
[01:34:15 - 01:34:23] בדיוק ככה. ‫בזה בעצם יש פה מאסק, אוקיי? ‫אז זה מחקת ואת הכול, זה מחקת את כל מה שבורקת וזה, ‫מחוספת כאילו שם.
[01:34:24 - 01:34:29] ‫אז היית יכול לבנות בדיוק את המאסק הזה, ‫אבל בעצם אפשר לבנות מאסקים יותר כלליים ‫שעדיין ישמרו את האילוץ הזה,
[01:34:30 - 01:34:33] ‫שכל פיקסל תלוי רק בפיקסלים הקודמים.
[01:34:38 - 01:34:45] ‫אוקיי, עוד, אז זה נקרא Mage. ‫עוד יש כמה מודלים שמסתמכים על Repרנט.
[01:34:46 - 01:34:47] ‫מכירים את זה?
[01:34:48 - 01:34:49] ‫שמעתם את זה בשתי מוחות?
[01:34:50 - 01:34:52] ‫אוקיי, אז פה הרעיון זה שאנחנו שומרים ‫איזשהו מצב
[01:34:53 - 01:34:56] ‫שהוא תלוי כל פעם בכל ההיסטוריה ‫שהייתה עד עכשיו.
[01:34:59 - 01:34:59] ‫אז
[01:35:01 - 01:35:03] יש כמה דרכים לממש את זה, ‫זו דרך פשוטה,
[01:35:03 - 01:35:06] ‫לסטי-אנג זו דרך מאוד פופולרית,
[01:35:07 - 01:35:09] ‫ובעצם כל אנחנו שומעים ‫משהו שנקרא
[01:35:16 - 01:35:21] ‫ההידן זה בעצם ה-State שלנו, אוקיי? ‫אז ההידן הוא בהתחלה ‫הוא לא תלוי בכלום, ‫יש לו איזשהו זכוי.
[01:35:22 - 01:35:23] ‫אז כל פעם אנחנו מעדכנים את ההידן,
[01:35:25 - 01:35:27] ‫ה-State שלנו, על ידי שני דברים, ‫על ידי
[01:35:27 - 01:35:28] ההידן הקודם,
[01:35:29 - 01:35:30] ‫הערכים שהיה לנו קודם,
[01:35:31 - 01:35:32] ‫בפיקסל הקודם,
[01:35:33 - 01:35:35] ‫והפיקסל הנוכחי שאנחנו רואים.
[01:35:36 - 01:35:37] ‫שני הדברים האלה משפיעים
[01:35:38 - 01:35:39] ‫על ה-State שלנו,
[01:35:40 - 01:35:42] ‫והארקוד שלנו תלוי ‫רק בסטייט הנוכחי.
[01:35:43 - 01:35:50] ‫אז ככה אנחנו מקדמים כל פעם. ‫בפיקסל הראשון הוא לא תלוי בכלום, ‫הוא יהיה רק תלוי למה שכתוב כאן, ‫באתחול שלנו של ה-State.
[01:35:51 - 01:35:58] ‫הפיקסל השני הוא יהיה תלוי ‫בעדכון של הירוקים כאן. ‫הירוקים יתעדכנו לפי מה שהיה קודם,
[01:35:58 - 01:36:00] ‫וגם לפי הפיקסל הראשון,
[01:36:01 - 01:36:02] ‫אוקיי? זה יהיה הפיקסל השני.
[01:36:05 - 01:36:10] ‫הפיקסל השלישי הוא, עוד פעם, ‫ה-State יתעדכן, מה שהיה כאן, אז
[01:36:11 - 01:36:15] הפיקסל הראשון איכשהו ישפיע ‫על מה שקורה כאן בדבר הזה,
[01:36:16 - 01:36:17] ‫אבל גם הפיקסל השני ישפיע.
[01:36:17 - 01:36:20] ‫את הפיקסל השלישי תלוי עכשיו ‫גם בראשון וגם בשני.
[01:36:21 - 01:36:23] ‫ככה עד האחרון, ‫שני יהיה תלוי בכולם.
[01:36:23 - 01:36:27] ‫זו דרך אחרת, שוב פעם, ‫לממש את הרעיון הזה ‫שכל פיקסל לא יצטרך על הפיקסל.
[01:36:31 - 01:36:34] ‫אנחנו נראה עוד מעט, ‫יש גם דוגמה של
[01:36:36 - 01:36:38] ‫שימוש בזה בתמונות.
[01:36:39 - 01:36:45] ‫היתרונות בגישה הזאתי זה שזה בעצם ‫קצת יותר כללי,
[01:36:45 - 01:36:53] ‫זה לא מאלץ אותנו ממש לבנות ‫משהו לתמונות בגודל מסוים, ‫אפשר להמשיך לבנות בתמונות ‫כמה שאנחנו רוצים.
[01:36:54 - 01:36:56] ‫אין פה משהו שאומר ‫מה הגודל של התמונה.
[01:36:59 - 01:37:00] ‫ובאופן כללי,
[01:37:01 - 01:37:02] ‫פגישה כזאתי
[01:37:06 - 01:37:15] ‫לקרנט זה משהו שאפשר לממש איתו ‫כל פונקציה, אוקיי? ‫יש כל מיני משפטים על זה ‫שזה כמו רשתות עמוקות באופן כללי.
[01:37:15 - 01:37:17] ‫זה נותן לנו דרך לממש.
[01:37:18 - 01:37:20] ‫כל פונקציה שאפשר לחשב, ‫אפשר לממש ככה.
[01:37:21 - 01:37:23] ‫אבל החסרונות זה שזה, ‫קודם כול זה עדיין,
[01:37:24 - 01:37:29] ‫למרות שזה קצת יותר כללי, ‫זה עדיין דורש איזשהו סדר ‫שאנחנו מניחים מראש על הפיקסלים,
[01:37:30 - 01:37:39] ‫והבעיה העיקרית זה שזה מאוד איטי. ‫זאת אומרת, אנחנו לא יכולים לעשות ‫את מה שעשינו קודם בצורה מגבילית, לחשב את כל ה... ‫אפילו אם אנחנו מקבלים תמונה, ‫אנחנו לא יכולים לחשב את הלייטיות שלה
[01:37:40 - 01:37:42] ‫בצורה מגבילה על כל הפיקסלים.
[01:37:43 - 01:37:48] ‫בעצם, בשביל הפיקסל האחרון, ‫אני צריך לדעת מה ה-State שלו, ‫שהוא תלוי בכל המעברים שאני עושה.
[01:37:49 - 01:37:52] ‫אז הפיקסל האחרון, יש לו באמת חישוב ‫שהוא יכול להיות מאוד מאוד מורכב,
[01:37:52 - 01:37:59] ‫אבל הוא גם תלוי בכל מה שקרה בדרך. ‫אבל אני חייב לעשות את כל החישובים האלה ‫בצורה סדרתית, אחד תחת השנייה.
[01:38:00 - 01:38:01] ‫אז האימון של זה הרבה יותר איטי,
[01:38:02 - 01:38:06] ‫וגם כשעושים Back Propagation, בעצם צריך לעבור ‫דרך כל התהליך הזה עד הסוף,
[01:38:06 - 01:38:10] ‫יש כל מיני בעיות של רדיאנטים הם
[01:38:11 - 01:38:13] ‫בלמים או מפוצצים או כל מיני דברים כאלה.
[01:38:15 - 01:38:19] ‫זה יכול להיות יותר חזק, ‫אבל יותר קשה קצת לאימון.
[01:38:22 - 01:38:22] אז זה דוגמאות,
[01:38:23 - 01:38:28] ‫פה זה לא דגימות מ-0, ‫אלא דגימות מתוך חצי תמונה.
[01:38:29 - 01:38:32] ‫כן, אז בעצם כל הפיקסלים האלה ‫כבר היו נתונים,
[01:38:32 - 01:38:33] חצי מהתמונה הזאת,
[01:38:34 - 01:38:36] ‫מתוך תמונה אמיתית,
[01:38:36 - 01:38:40] ‫ואז פשוט דגמו השלמות אפשריות ‫של התמונה הזאת.
[01:38:41 - 01:38:48] ‫אנחנו רואים שזה מתרפס את מה ‫שבדרך כלל יש בתמונות, ‫שתמונות הן חלקות. ‫הצבע נשמע הרבה בכל דבר, ‫אם יש כל מיני תופעות, ‫אז זה חוזר על עצמו,
[01:38:49 - 01:38:53] ‫כן יש כל מיני תופעות של גלים, ‫כאן יש את הטקסטורה של הדשא, ‫בתמונות קצת קטנות, ‫אז קשה לראות.
[01:38:55 - 01:39:00] ‫וגם הוא מבין שיש איזשהו עד שממשיך, ‫שמתחיל, אז כנראה הוא ימשיך.
[01:39:03 - 01:39:05] ‫אני יודע שיש דברים סימטריות,
[01:39:06 - 01:39:08] ‫אז כמה דברים משלים את ה...
[01:39:08 - 01:39:19] ‫ברוב הדגימות הוא משלים את ה... ‫זה היה גם טרו ופה, ‫אבל גם פה יש הרבה דוגמאות ‫שהוא קצת או יותר משלים את המעגל הזה.
[01:39:20 - 01:39:21] ‫לא תמיד.
[01:39:22 - 01:39:22] אוקיי, אז הוא...
[01:39:23 - 01:39:32] ‫רואים שהוא למד משהו ‫על מבנה של דברים בעולם, ‫וכל זה רק מתוך העיקרון הזה ‫שכל פיקסל פלוי בכל הפיקסלים הקודמים,
[01:39:33 - 01:39:34] ‫עם איזשהו חישוב.
[01:39:35 - 01:39:37] ‫זו דוגמה זאת חישוב שבנוי על איזשהו ריקר,
[01:39:38 - 01:39:41] ‫אבל תכף נראה דוגמה ‫שזה לא חייב להיות כבר.
[01:39:42 - 01:39:47] ‫אבל הסיבה שזה עובד יותר טוב ‫ממה שהיה קודם, ‫זה פשוט שהחישוב הוא יותר מורכב, ‫יש יותר פרמטרים.
[01:39:48 - 01:39:52] ‫אנחנו עדיין, זה לא בפרמטריזציה מלאה ‫שאנחנו מסתכלים על כל האפשרויות,
[01:39:53 - 01:39:59] ‫אבל זה הרבה יותר פרמטרים ‫מסתם לעשות מכפלה ליניארית ‫של כל הפיקסלים הקודמים, ‫באיזשהם...
[01:40:01 - 01:40:02] ‫כמה שכבות של חישוב כזה.
[01:40:03 - 01:40:11] ‫אוקיי, אז מה שרציתי ‫שנסתכל עליו קצת יותר לעומק, ‫יש לנו בערך 40 דקות לזה,
[01:40:12 - 01:40:18] ‫זה מודל שנקרא פיקסל-CNN, ‫CNN זה קונבולוציון נוירו-נטוורקס,
[01:40:18 - 01:40:20] ‫זה באמת מבוסס ‫על הרעיון של קונבולוציה.
[01:40:23 - 01:40:29] ‫הרעיון זה שקונבולוציה זה משהו ‫שהוכח שהוא מאוד שימושי בוויז'ן באופן גדולי,
[01:40:29 - 01:40:31] ‫ובעצם,
[01:40:32 - 01:40:34] ‫השאלה אם אנחנו יכולים להשתמש בזה ‫לא רק כדי לעשות
[01:40:35 - 01:40:39] ‫קלוסיפיקציה נגיד לתמונות, ‫אלא גם למודלים גנרטיביים של תמונות.
[01:40:40 - 01:40:41] ‫שני הדברים שהיינו רוצים,
[01:40:42 - 01:40:43] ‫של התכונות שהיינו רוצים לתפוס
[01:40:44 - 01:40:46] ‫בקונבולוציות זה אחד,
[01:40:48 - 01:41:02] ‫זה ההיררכיה הזאת שתלויה בלוקאליות. ‫זאת אומרת, כשאנחנו מסתובבים ‫קונבולוציות אחת על השנייה, ‫מה שזה עושה בעצם זה שיש תלות מאוד חזקה ‫באזור מסוים בסביבה הקרובה,
[01:41:02 - 01:41:07] ‫בצוקציה פחות חזקה בסביבה יותר חוקה, ‫ולאט לאט זה נחלש,
[01:41:09 - 01:41:12] ‫אבל עדיין אפשר לגנות ככה ‫ממש היררכיה שתלויה בכל התמונה.
[01:41:14 - 01:41:16] ‫יש איזושהי תלות גובלית ‫ויותר ויותר תלות לוקאלית.
[01:41:19 - 01:41:21] ‫יש הרבה דרכים לעשות את זה, ‫אבל קונבולוציות זה,
[01:41:22 - 01:41:23] במשך השנים,
[01:41:24 - 01:41:27] ‫זו הדרך הכי טובה כנראה ‫שהם רצו לעבוד עם תמונות,
[01:41:28 - 01:41:29] ‫זו דרך שתופסת את ה...
[01:41:32 - 01:41:41] ‫המבנה הזה שיש לתמונות, ‫שבאופן לוקאלי הפיקסלים ‫מאוד קשורים אחד לשני, ‫אבל גם יש איזשהו קשר גלובלי ‫בין אזורים שונים של התמונה.
[01:41:41 - 01:41:45] ‫אז זה מה שקראתי כאן, ההיררכיה ‫שמבוססת על לוקאליות.
[01:41:47 - 01:41:50] ‫והתכונה השנייה זה הדרך ‫שבה המשקולות
[01:41:52 - 01:41:54] ‫משותפים בין פיקסלים שונים.
[01:41:55 - 01:42:00] ‫גם ב-RNN וגם ברשת שהיינו קודם, ‫כמו שאמרו פה, ‫אז היה איזשהו שיתוף.
[01:42:01 - 01:42:03] ‫כאן זה דרך מאוד ספציפית ‫לשתף
[01:42:04 - 01:42:05] בין משקולות.
[01:42:06 - 01:42:09] ‫זאת אומרת, ממש אותו חישוב ‫נעשה לכל אזור בתמונה
[01:42:10 - 01:42:12] ‫בצורה מגבילית.
[01:42:13 - 01:42:14] ‫זה הרעיון של קונבוליציות.
[01:42:17 - 01:42:23] ‫צריכים חזרה רק מהירה, ‫מה זה בערך אומר קונבוליציה? ‫אז קונבוליציה זה...
[01:42:25 - 01:42:27] ‫מצביעים את זה ככה, ‫מצביעים בדרך כלל עם כוכבית כזאת.
[01:42:28 - 01:42:36] ‫בעצם יש לנו שתי פונקציות, F ו-G, ‫בדרך כלל אחת מהן זה הסיגנל שלנו, ‫התמונה נגיד, ‫והשנייה זה, אנחנו קוראים לזה הפילטר.
[01:42:37 - 01:42:39] ‫זה מה שאנחנו מפעילים על התמונה,
[01:42:40 - 01:42:48] ‫זה המשקולות שלנו. ‫אמרתי שיש שיתוף של המשקולות, ‫אז המשקולות זה יהיה בעצם ‫איזשהו פילטר שאנחנו מפעילים על התמונה.
[01:42:50 - 01:42:50] ‫הדרך שאנחנו מפעילים את זה,
[01:42:51 - 01:42:51] ‫אנחנו
[01:42:52 - 01:42:55] מכפילים כל הסביבה ‫של איזשהו פיקסל מסוים,
[01:42:55 - 01:42:59] ‫זה החישוב שאנחנו עושים ‫לפיקסל במקום מסוים.
[01:43:00 - 01:43:03] ‫אז כל הסביבה של הפיקסל הזה ‫אנחנו מכפילים במשקולות האלה של הפילטר.
[01:43:04 - 01:43:05] ‫לסוכמים.
[01:43:06 - 01:43:09] ‫אוקיי, יש כאן כל מיני ביזואיזציות ‫בחד-מימד.
[01:43:10 - 01:43:11] ‫אז אם אני לוקח
[01:43:15 - 01:43:19] את זה לפילטר, 1, 0 ומינוס 1, ‫ואני מכפיל את זה בסיגנל הזה.
[01:43:20 - 01:43:21] ‫אז כאן ב-1 כתוב זה,
[01:43:22 - 01:43:24] 0 כתוב זה ומינוס 1 כתוב זה,
[01:43:24 - 01:43:26] ‫אז יש לי את החישוב.
[01:43:27 - 01:43:30] ‫עוד פעם אני מכפיל ‫שלושת המספרים האלה ב-1 ל-0 מינוס 1 וסוכם,
[01:43:31 - 01:43:33] ‫זה נותן לי את הערכים שיש לי.
[01:43:33 - 01:43:35] ‫זה החישוב, זה הפילטר.
[01:43:36 - 01:43:39] ‫זה החישוב, זה מה שזה אומר, ‫להפעיל את הקונבולוציה של זה,
[01:43:40 - 01:43:41] ‫עם הסיגנל הזה.
[01:43:42 - 01:43:42] ‫זה ה-OP.
[01:43:45 - 01:43:53] ‫כן, אז זה תלוי איך אתה מגדיר את זה. ‫כן, פה זה לא כתוב הפוך.
[01:43:55 - 01:44:02] ‫כן, אתה לא איך אתה קורא ל-F, ‫אם F נכון אחר. אתה צודק. ‫הדרך כבר אנחנו נכונן לעשות ‫להגדיר את זה ויש הופכים אחדים.
[01:44:05 - 01:44:08] ‫הנדבר שאצלנו גם ככה הכול נלמד, ‫אז זה לא משנה, כאילו,
[01:44:08 - 01:44:13] ‫אם אתה ממשת את זה ככה או ככה, ‫זה לא כל כך משנה.
[01:44:15 - 01:44:18] ‫סתם פחות חשוב לי מענייננו, ‫אבל לידע כללי,
[01:44:19 - 01:44:24] ‫בקונבולוציות משתמשים בזה אפילו ‫לא ברשתות נלמדות, ‫אלאא זה ממש בעיבוד אותות.
[01:44:25 - 01:44:26] ‫אפשר להשתמש בזה לעשות כל מיני דברים,
[01:44:27 - 01:44:30] ‫אז למשל להחליק, יש לנו איזשהו סיגנל,
[01:44:30 - 01:44:34] ‫אם אנחנו עושים לו קונבולוציה ‫עם משהו שנראה ככה,
[01:44:35 - 01:44:39] ‫או אפילו עם משהו שהוא פשוט שטוח,
[01:44:40 - 01:44:41] ‫זה מרקע שנראה פטורסיאני כזה,
[01:44:42 - 01:44:49] ‫ובעצם מה שזה אומר ‫שכל פיקסל יהיה ממוצע של כל הסביבה ‫של הפיקסל המקורי.
[01:44:50 - 01:44:52] ‫נגיד שהיה לנו פיקסל כזה, ‫שהיהיה לו פתאום כל מיני חורים,
[01:44:53 - 01:44:54] קפיצות כאלה,
[01:44:54 - 01:44:57] ‫ואנחנו נתחיל, נעשה קונבולוציה ‫של זה עם זה,
[01:44:57 - 01:45:00] ‫אנחנו נקבל איזושהי סיגנל קצת יותר חלק ‫למה שקופש.
[01:45:01 - 01:45:04] ‫אנחנו משתמשים בזה רוב פעם בשביל להחליק, ‫אז יש ממש פילטרים,
[01:45:04 - 01:45:07] ‫כל מיני סוגים של פילטרים ‫שאפשר להשתמש בהם להחליק,
[01:45:08 - 01:45:14] ‫אבל אפשר גם הפוך, ‫ממש להשתמש בזה ל-edge detection כזה, ‫לזה לזהות מתי יש קפוצות,
[01:45:15 - 01:45:18] מתי יש שינוי גדול. ‫אז אם אנחנו עושים קונבולוציה ‫עם פילטר כזה,
[01:45:19 - 01:45:21] ‫יש לו מינוס אחד ואחד,
[01:45:22 - 01:45:27] ‫ובעצם כל פעם שיש ערכים זהים ‫אחד ליד השני,
[01:45:28 - 01:45:33] ‫אז הם יתבטלו, ‫אחד מהם אנחנו נכפיל במינוס אחד, ‫ואת השכן שלו נכפיל באחד,
[01:45:33 - 01:45:35] ‫הם יתבטלו. ‫אבל אם יש ערכים שונים,
[01:45:36 - 01:45:38] ‫אז הם כבר לא יתבטלו, ‫ואז אנחנו נזהה ‫שהייתה כאן איזושהי קפיצה.
[01:45:40 - 01:45:45] ‫אז פה יש לנו סיגנל של תיאור. ‫אבל פה סיגנל אנחנו קודם, ‫יש פה קפיצה כאן, קפיצה כאן, כאן וכאן, ‫יש ארבע קפיצות,
[01:45:46 - 01:45:51] ‫אנחנו ממש מגבלים ערכים 0 בכל המקומות, ‫חוץ מבארבע הנקודות האלה שהייתה קפיצה,
[01:45:52 - 01:45:55] ‫שזו התוצאה של הקונבולוציות.
[01:45:59 - 01:46:02] ‫אוקיי, בדו-מימד גם אפשר להפעיל, ‫אז בדו-מימד, אז העיקרון הוא אותו עיקרון,
[01:46:03 - 01:46:07] ‫רק שעכשיו הפילטר שלנו מוגדר בתור משהו,
[01:46:07 - 01:46:10] ‫אז זה הפילטר, לא ראינו אותנו, ‫הפילטר 3 על 3 לדוגמה הזאת,
[01:46:11 - 01:46:13] ‫הוא מוגדר בתור משהו דו-מימדי.
[01:46:14 - 01:46:19] ‫קודם זו הייתה פונקציה חד-מימדית, ‫עכשיו זה יהיה משהו דו-מימדי, ‫שאנחנו מפעילים אותו על תמונה דו-מימדית,
[01:46:20 - 01:46:23] ‫וה-output הוא גם תמונה, אוקיי? ‫אז ה-output, הפיקסל הזה,
[01:46:23 - 01:46:28] ‫הוא יהיה פשוט הקומבינציה הלינארית ‫של כל הפיקסלים בסביבה של הפיקסל כאן במקור.
[01:46:28 - 01:46:32] ‫יש איזו שאלה מה אנחנו עושים ‫אם אנחנו יוצאים החוצה,
[01:46:32 - 01:46:37] ‫אבל אני מתעלם מזה רגע, ‫אז נגיד הפיקסל הזה, ‫הוא יהיה פשוט הסביבה של 3 על 3,
[01:46:38 - 01:46:48] ‫זה הגודל של הפילטר שבחרנו. ‫שאנחנו נעשה מכפלה לינארית ‫של ה-3 על 3 עם הערכים בתמונה,
[01:46:49 - 01:46:51] ‫לסכום את הכול, ‫וזה יהיה הערך של הפיקסלים.
[01:46:52 - 01:46:54] ‫אז זו המשמעות של קונבולוציה דו-מימדית.
[01:46:55 - 01:46:59] ‫וגם את אותם דברים שיש בכלל ‫למעט אפשר לעשות פה. ‫נחליט, למצוא אדג'ים,
[01:47:01 - 01:47:02] ‫כל מיני דברים כאלה.
[01:47:02 - 01:47:06] ‫מה עושים עם פיקסלים כשהם חורגים? ‫אז יש כל מיני שיטות, לפעמים...
[01:47:07 - 01:47:10] ‫מניחים שזה אפס פשוט, ‫לפעמים מניחים שזה פיקסל
[01:47:12 - 01:47:14] ‫אותו ערך כמו הפיקסל הכי קרוב,
[01:47:15 - 01:47:17] ‫לפעמים מניחים שזה ציקלי,
[01:47:17 - 01:47:18] ‫שזה חוזר אחורה פה.
[01:47:19 - 01:47:21] ‫באפליקציה יש כל מיני...
[01:47:23 - 01:47:24] ‫יש כל מיני יתרונות ‫לכל אחת מהנחות האלה.
[01:47:27 - 01:47:37] ‫איך זה מומש ברשתות? ‫אז בעצם הקונבנציה זה שיש לנו תמונה. ‫התמונה היא, נגיד, גודל 32-32, 32,
[01:47:37 - 01:47:38] ‫אז בתמונה צבעונית,
[01:47:39 - 01:47:41] ‫אנחנו מפעילים עליה איזשהו פילטר,
[01:47:41 - 01:47:47] ‫אז נגיד הפילטר לגבימה הזאתי הוא 5x5, ‫אבל גם לא יש שלושה ערוצים.
[01:47:48 - 01:47:55] ‫אפשר לחשוב על זה בתור שלושה פילטרים ‫שהם בגודל 5x5. ‫כל אחד מהם אנחנו מפעילים על שלושת ה...
[01:47:56 - 01:47:59] ‫אנחנו מפעילים לא רק את הסביבה של 5x5, ‫את כל הטיקסלים, אלא גם
[01:48:00 - 01:48:02] ‫כל שלושת הערוצים שיש שם.
[01:48:03 - 01:48:05] ‫וזה נותן לנו בעצם שלושה,
[01:48:06 - 01:48:07] ‫אני חושב שאמור להיות איתנו.
[01:48:19 - 01:48:28] ‫כן, אז ה-5x5 ו-3, ‫אז בגלל שכאן יש שלושה ערוצים, ‫מופיע, אז אני מפעיל את ה-5. ‫כל פיקסל, אני מפעיל אותו ב-5 בסביבה של 5x5,
[01:48:29 - 01:48:32] ‫ושלושה, יש לי ערך שונה ‫לכל אחד מהצבעים.
[01:48:32 - 01:48:38] ‫אם צריך שיהיו לי 5x5, 5x5, 3 ‫מספרים בפילטר הזה,
[01:48:40 - 01:48:41] ‫וזה נותן לי
[01:48:43 - 01:48:44] תמונה חדשה.
[01:48:46 - 01:48:50] שוב, יש כל מיני תנאים ‫לאיך אני מתייחס ל-output,
[01:48:51 - 01:48:54] ‫במקרה הזה, הדוגמה הזאתי, ‫זה ירד מ-32 ל-28,
[01:48:55 - 01:48:57] ‫כי אני לא מרשה לפיקסל ‫לחרוג מהחוצה.
[01:49:00 - 01:49:02] במקרים שלנו אנחנו נרצה ‫שהכול יישאר באותו גודל,
[01:49:02 - 01:49:03] ‫ואנחנו כן נרשם בו,
[01:49:04 - 01:49:10] ‫ואנחנו יכולים לשים הרבה פילטרים כאלה, ‫כמו שיש לי ברשתות ‫הרבה נוירונים שונים,
[01:49:10 - 01:49:15] ‫אז אני יכול שיהיו לי הרבה פילטרים כאלה. ‫כל אחד מהם ייצור לי תמונה חדשה,
[01:49:16 - 01:49:19] ‫ועכשיו בעצם מה שזה אומר, ‫שהלייר השני שלי יהיה,
[01:49:20 - 01:49:24] ‫הוא לייר שמורכב, אם קודם היו לי שלושה ערוצים, ‫עכשיו יש לי תמונה עם חמישה ערוצים, נגיד,
[01:49:25 - 01:49:26] ‫ועד זה אני יכול לעשות עוד פעם קונבולוציה.
[01:49:29 - 01:49:30] אוקיי,
[01:49:31 - 01:49:33] ‫זה היה תזכורת קטנה ‫לרשתות קונבולוציות.
[01:49:39 - 01:49:43] ‫אז מה עושים בפיקסל CNN? ‫אז בפיקסל CNN הכול אותו דבר.
[01:49:44 - 01:49:49] ‫אנחנו רוצים להשתמש במה שעשינו קודם, ‫זאת אומרת, אנחנו רוצים למדל את הפיקסל ה-I,
[01:49:50 - 01:49:52] ‫שהוא יהיה רק פונקציה ‫של כל הפיקסלים הקודמים,
[01:49:53 - 01:49:59] ‫כי אנחנו רוצים שזה יהיה חוקי, ‫שזה יהיה פירוג של כלל השרשרת חוקית.
[01:49:59 - 01:50:01] ‫אנחנו רוצים לממש את זה ‫על ידי קונבולוציות.
[01:50:01 - 01:50:11] ‫זאת אומרת, בעצם להשתמש ביתרונות ‫שיש בקונבולוציות. ‫אחד, שזה דרך יעילה שם, ‫לשתף משקולות בין פיקסלים שונים.
[01:50:12 - 01:50:13] ‫ושתיים, זה
[01:50:18 - 01:50:22] ‫ברגע שאנחנו עושים היררכיה כזאת, ‫של הרבה שכבות של קונבולוציה,
[01:50:22 - 01:50:27] ‫זה באופן אוטומטי עושה לנו את ההיררכיה הזאת, ‫של דברים לוקאליים ‫מקבלים משקל יחסית גבוה,
[01:50:27 - 01:50:31] ‫אבל עדיין יש איזושהי השפעה ‫של כל התמונה.
[01:50:32 - 01:50:33] ‫בצלמית ארבע שכבות יש לנו,
[01:50:33 - 01:50:35] ‫פחות אזור יחסית גדול,
[01:50:36 - 01:50:36] ‫על כל פיקסל.
[01:50:37 - 01:50:39] ‫אנחנו רוצים לממש את הדבר הזה
[01:50:39 - 01:50:43] ‫עם קונבולוציות, ‫אבל לשמר את התכונה הזאת ‫שכל פיקסל רק
[01:50:44 - 01:50:48] ‫תלוי בפיקסלים הקודמים. ‫אסור לנו שהפיקסל הזה ‫יהיה תלוי בפיקסלים שבהם נבחרנו.
[01:50:49 - 01:50:55] ‫אז אם אני סתם מפעיל קונבולוציה, ‫זה מה שיקרה, נכון? ‫כי אני מסתכל חזרה על מה ש...
[01:50:58 - 01:50:58] ‫הדגיף הזה,
[01:50:59 - 01:51:02] ‫כל פיקסל כאן, ‫תלוי בכל הסביבה שלו,
[01:51:02 - 01:51:04] ‫זה תלוי גם בפיקסלים שאחר כך.
[01:51:05 - 01:51:07] ‫ועוד יותר, אם יש היררכיה כזאת,
[01:51:07 - 01:51:09] ‫זה אומר שהפיקסל הבא, ‫הסביבה בעצם גדלה.
[01:51:11 - 01:51:15] ‫הפיקסל הזה תלוי בפיקסל השכן, ‫שהוא בעצמו היה כבר תלוי ‫בפיקסל השכן שלו,
[01:51:15 - 01:51:18] ‫כל שכבה שאנחנו הולכים וגדלים, ‫זה בעצם היה תלוי ביותר ויותר פיקסל.
[01:51:20 - 01:51:21] ‫אז אני רוצה שזה יקרה,
[01:51:21 - 01:51:23] ‫אבל רק לכיוון אחד,
[01:51:24 - 01:51:25] לא לעתיד.
[01:51:25 - 01:51:30] ‫אם נסדר את הפיקסלים על בסדר, ‫הוא רק לפיקסלים בעבר ‫ולא לפיקסלים בעתיד.
[01:51:34 - 01:51:46] ‫אין לי משהו להגיד? אה, גם כאן, לא אמרתי את זה, ‫אבל זה בעצם השיתוף של המשקולות, ‫זה בגמה שרואים כאן. ‫כל פיקסל כאן הוא אותה פונקציה ‫של הפיקסלים השכנים שלו. ‫הפיקסלים השכנים שלו הם שונים,
[01:51:46 - 01:51:48] ‫אבל אני מכפיל אותם באותן משקולות, ‫באותו פילטר.
[01:51:49 - 01:51:53] זאת הכוונה של השיתוף של המשקולות שהוא יעיל
[01:51:55 - 01:52:04] ואנחנו רוצים להשתמש בו עכשיו רק בתוך הדבר הזה. בעצם אנחנו רוצים לפתח קונבולוציות שרק
[01:52:05 - 01:52:06] רשתות קונבולוציות,
[01:52:06 - 01:52:10] סטאקס כאלה של קונבולוציות שרק בסופו של דבר יפה שכל פיקסל רק תלוי
[01:52:11 - 01:52:12] בפיקסלים הקודמים
[01:52:13 - 01:52:15] קוראים לזה גם לפעמים causal revolutions
[01:52:16 - 01:52:18] אם חושבים על הדבר הזה בתור זמן
[01:52:18 - 01:52:20] כמו שזה לא באמת זמן בפיקסלים
[01:52:21 - 01:52:26] אבל הרבה פעמים עדיין מדברים על זה במונחים של זמן ואז זה בעצם אפשר להגיד קונבולוציה סיבתית
[01:52:29 - 01:52:30] אוקיי יש גם את העניין של צבע
[01:52:32 - 01:52:33] איך עושים את זה בצבע, אז
[01:52:34 - 01:52:39] בעצם יש פה דוגמה לאיך אפשר לעשות את זה, אז קונטקס זה בעצם כל הפיקסלים הקודמים
[01:52:40 - 01:52:42] עכשיו יש לי את הפיקסל הנוכחי, יש לי את ה-RG וה-B שלו
[01:52:43 - 01:52:46] אז אחת מהדרכים לממש את זה זה לעשות סטאקס כזה
[01:52:47 - 01:52:50] שאני עושה קונבולוציה אבל אני עושה איזשהו masking
[01:52:51 - 01:52:55] על הקונבולוציה בצורה כזאת שהשכבה הראשונה
[01:53:00 - 01:53:05] יש לי את כל הפיקסלים הקודמים ואת הפיקסל הנוכחי והחישוב שאני עושה זה ככה
[01:53:06 - 01:53:10] ה-Red בפיקסל הנוכחי הוא תלוי בקונטקסט רק בפיקסלים הקודמים
[01:53:11 - 01:53:13] ה-G מותר לו להיות תלוי ב-Red
[01:53:14 - 01:53:16] של הפיקסל הנוכחי ובכל הקונטקסט
[01:53:17 - 01:53:20] וה-B מותר לו להיות תלוי ב-G וה-R
[01:53:22 - 01:53:24] של הפיקסל הנוכחי ובכל הפיקסלים הקודמים
[01:53:26 - 01:53:28] ה-B של הפיקסל הנוכחי לא נכנס לשום דבר
[01:53:29 - 01:53:30] רק הוא ייכנס לפיקסל הבא,
[01:53:31 - 01:53:33] הוא ייכנס לקונטקסט של הפיקסל הבא.
[01:53:33 - 01:53:37] וברגע שעשיתי את זה, אז כל השכבות שבאות מעל
[01:53:38 - 01:53:43] אני כבר לא, אני יכול, אם כן יכול להוסיף כאן את המשקולת
[01:53:43 - 01:53:45] ‫הזאת מהפיקסל לעצמו.
[01:53:48 - 01:53:50] ‫ה-G הזה הוא כבר לא תלוי ב-G הזה,
[01:53:50 - 01:53:53] ‫אבל אני יכול עכשיו פרטיין להיות תלוי בו, ‫הוא כבר לא יהיה תלוי בו.
[01:53:54 - 01:54:00] ‫יש כל מיני דרכים לעשות את המאסקינג האלה, ‫אבל זו השיטה שאנחנו נתבסס עליה.
[01:54:02 - 01:54:03] ‫זה ברור המיסוך הזה?
[01:54:05 - 01:54:06] ‫בעצם יוצא שעכשיו הפיקסל הזה
[01:54:07 - 01:54:09] ‫הוא לא תלוי בעצמו. ‫ה-R לא תלוי ב-R.
[01:54:10 - 01:54:15] ‫אין פה שום ערך שראה את עצמו, ‫כאילו באינטרנט.
[01:54:15 - 01:54:18] ‫הערך הזה, ה-R, ‫הוא רק ראה את הקונטקסט בתולמין.
[01:54:19 - 01:54:22] ‫הערך הזה ראה את ה-R של הפיקסל ‫הפיקסל מסוגלית ואת כל התולמין,
[01:54:23 - 01:54:26] ‫וב-B ראה את G, R ואת כל הקונטקסטים.
[01:54:28 - 01:54:32] ‫אז אנחנו מקיימים את התנאים ‫של כלל השרשרת.
[01:54:34 - 01:54:37] ‫בעצם יש לנו איזשהו סידור, סידרנו את כל הפיקסלים, ‫וכל פיקסל מסוגל לפי RGB,
[01:54:38 - 01:54:45] ‫זה כל הסידור של המשתנים שלנו, ‫וכל הפירוק הזה של כלל השרשרת מתקיים.
[01:54:51 - 01:54:58] ‫אוקיי, אז אם אנחנו מסתכלים על זה ‫בתור קונבולוציות, אז בעצם יש ככה ‫גם עניין ה-RGV, ‫אפילו שאנחנו מסתכלים עליו ‫רק על ערך אחד,
[01:54:59 - 01:55:05] ‫אז אנחנו צריכים להכפיל ‫את המשקולות של הקונבולוציה ‫בפסיכה הזאת.
[01:55:06 - 01:55:15] ‫אוקיי, מה זה אומר? ‫זה אומר שאנחנו מניחים שזה כבר בלי, ‫זה כבר כאילו אחרי ההיפוך, ‫אנחנו כבר הופכים לזה, ‫אז בעצם הפיקסל הזה,
[01:55:16 - 01:55:20] ‫אנחנו נכפיל אותו בפילטר הזה, ‫בפילטר שהוא 5x5,
[01:55:21 - 01:55:23] ‫אבל כל הערכים האלה,
[01:55:24 - 01:55:25] ‫כל המשקולות שפה,
[01:55:26 - 01:55:28] זה יהיה להם איזשהו ערך,
[01:55:29 - 01:55:30] ‫כל המשקולות של זה, הפיקסל עצמו,
[01:55:31 - 01:55:33] ‫וכל הפיקסלים שבאים אחריו בסידור,
[01:55:34 - 01:55:35] ‫אנחנו תמיד נכפיל אותם ב-0.
[01:55:37 - 01:55:40] ‫אז כל קונבולוציה שאנחנו נפיל, ‫לפני שאנחנו מפעילים את הקונבולוציה,
[01:55:41 - 01:55:44] ‫אנחנו נכפיל את הפילטרים במסכה הזאת.
[01:55:45 - 01:55:55] ‫אפשר להגיד שזה בעצם יותר קטן, ‫ואז הכול מוזז.
[01:55:56 - 01:55:59] כן, אז כשממשים את זה באמת, ‫אפשר לעשות את זה כבר.
[01:55:59 - 01:56:02] ‫כשמציגים את זה יותר קל, ‫לחשוב על זה קל.
[01:56:04 - 01:56:09] ‫כאילו, מספר השורות אפשר להגיד, ‫זה מספר המוביינטיקה.
[01:56:12 - 01:56:16] ‫זה מובן מה אנחנו עושים בעצם, ‫אנחנו מפיינים פונקציות,
[01:56:19 - 01:56:21] ‫פגישתות קונבולוציה רביעיות, ‫רק שאנחנו עכשיו נכפיל
[01:56:22 - 01:56:23] את המאסט הזה.
[01:56:25 - 01:56:30] ‫אז זה יהיה המאסט שאנחנו נכפיל בעצם ב-Layיר הראשון,
[01:56:30 - 01:56:32] ‫ב-Layיר השני התוספת זה שכאן יש אחד.
[01:56:33 - 01:56:35] ‫-כן, זה לא הבדל.
[01:56:35 - 01:56:37] ‫אנחנו מותר לנו כבר להסתכל על עצמנו.
[01:56:40 - 01:56:40] ‫כן.
[01:56:49 - 01:56:52] ‫אתה יכול, אבל אתה מפספס קצת את ה...
[01:56:52 - 01:56:52] ‫את אלה.
[01:56:53 - 01:56:55] ‫זאת אומרת, אז יש לך כל פיקסל.
[01:56:57 - 01:56:59] ‫אז תכף אנחנו נראה שגם זה לא כל כך טוב,
[01:56:59 - 01:57:02] ‫אבל מה שאתה אומר זה עוד פחות, ‫במובן הזה ש...
[01:57:04 - 01:57:05] ‫אם יש לנו הונאה,
[01:57:07 - 01:57:09] ‫מה שיוצא זה שהפיקסל הזה ‫יהיה תלוי רק
[01:57:10 - 01:57:11] בפיקסלים האלה.
[01:57:15 - 01:57:17] ‫אוקיי, בפיקסלים שמעליו ומשמאלו.
[01:57:18 - 01:57:20] ‫אנחנו רוצים, אבל זה לא בדיוק סידור,
[01:57:21 - 01:57:22] ‫זה כאילו קצת מפספס...
[01:57:26 - 01:57:30] ‫זה מפספס כמה פיקסלים. ‫לא סידרת את זה בצורה כזאת ‫שאתה מסתכל על כל הפיקסלים שהיו לפניך.
[01:57:34 - 01:57:40] ‫אחר כך אתה ישר קופץ, ‫בפיקסל הזה אתה מפספס את ה...
[01:57:40 - 01:57:45] ‫היית יכול כאילו להגיד שהסידור שלך ‫הוא על אחסונים כאלה, ‫אבל גם אתה מפספס את זה בכל פעם.
[01:57:48 - 01:57:53] ‫אתה רוצה כאילו, מה הדבר הכי יעיל? ‫אתה משתמש בכל מה שאתה יכול עד עכשיו, ‫שעדיין הוא חוקי.
[01:57:56 - 01:58:00] ‫אוקיי, אבל גם פה יש איזושהי בעיה, ‫קוראים לזה בליינד ספוט.
[01:58:01 - 01:58:03] ‫בעצם יוצא שאתה...
[01:58:04 - 01:58:08] ‫לא משתמש בכל האלכסון השמאלי הזה. ‫זאת אומרת, הפיקסל הזה,
[01:58:09 - 01:58:10] ‫אחרי שאתה מפעיל כמה פעמים
[01:58:12 - 01:58:13] ‫את הקונבולוציה הזאת,
[01:58:14 - 01:58:15] ‫היית רוצה,
[01:58:15 - 01:58:17] באופן כללי, ‫כשאתה מפעיל כמה פעמים קונבולוציה,
[01:58:18 - 01:58:22] ‫אז מה שקורה, נגיד שכל הפיקסלים ‫הוא אותו בודד,
[01:58:22 - 01:58:26] ‫אז אחרי פעם אחת, ‫הפיקסל הזה הוא כבר תלוי בסביבה הזאת.
[01:58:27 - 01:58:28] ‫אחרי שהפעת פעמיים,
[01:58:28 - 01:58:32] ‫הפיקסל הזה תלוי בפיקסל שליד,
[01:58:33 - 01:58:38] ‫שהוא כבר לא תלוי בסביבה הזאתי שלו, ‫זאת אומרת, ‫שאתה תלוי בסביבה כזאתי. ‫אחרי שלוש פעמים,
[01:58:39 - 01:58:40] ‫מה שלישית הייתה כבר תלוי,
[01:58:40 - 01:58:46] ‫קוראים לזה פילד או ויו. ‫הפילד או ויו שלך, האזור שאתה משתמש בו ‫כדי לחשב את הערך כאן,
[01:58:46 - 01:58:49] ‫הולך וגדל, כמו שאתה שם ‫עוד שכבות של קונבולוציה.
[01:58:50 - 01:58:52] ‫עכשיו, כשאנחנו משתמשים ‫במאסקים האלה שלנו,
[01:58:53 - 01:58:53] ‫היינו רוצים
[01:58:55 - 01:59:01] שזה גם יגדל, אבל ישמר את הסדר. ‫זאת אומרת, שהפיקסל הראשון
[01:59:01 - 01:59:03] ‫יהיה תלוי
[01:59:06 - 01:59:07] ‫בכזה דבר,
[01:59:09 - 01:59:09] ‫בשכבה הראשונה.
[01:59:10 - 01:59:14] ‫השכבה השנייה תהיה תלויה בכזה דבר.
[01:59:19 - 01:59:21] ‫השכבה השלישית בכזה דבר.
[01:59:23 - 01:59:25] ‫הופ, לא, לא, עוד אחד שביעי הסוף.
[01:59:26 - 01:59:26] ‫אבל שתמיד יהיה,
[01:59:28 - 01:59:30] ‫שתמיד, ‫שאף פעם לא ניגע במה שיש אחר כך.
[01:59:30 - 01:59:32] ‫אבל שכן יתרחב לכל מה שהיה לפני,
[01:59:33 - 01:59:34] ‫כל האפשרויות שהיו לפני.
[01:59:35 - 01:59:37] ‫אבל זה לא המצב, אם אנחנו משתמשים ‫במאסקים שהיו לנו קודם.
[01:59:38 - 01:59:42] ‫מה שקורה זה שיש לנו מין בליינדספורט ‫כזה אלכסוני שהולך לגבי.
[01:59:43 - 01:59:44] ‫אבל האזור הזה,
[01:59:44 - 01:59:50] אני הולך לידי למה, ‫כי בעצם הפיקסל שהיה פה,
[01:59:51 - 01:59:53] ‫הוא גם לא יסתכל על השכן, על איזה מימינו.
[01:59:54 - 01:59:56] ‫אף אחד מהפיקסלים ‫לא יסתכלו על השכנים מימינם,
[01:59:57 - 02:00:00] ‫אז בעצם גדלים רק בצורה ‫אחסונית טובה.
[02:00:01 - 02:00:04] ‫אבל האדור הזה, אנחנו מאבדים על זה.
[02:00:05 - 02:00:06] ‫אז יש להם פה ז'טריק,
[02:00:07 - 02:00:08] ‫במאמר
[02:00:10 - 02:00:17] ‫לפצל את הקונבולוציות ‫לשתי סוגי קונבולוציות, ‫אחת ורטיקל ואחת וריזונטל,
[02:00:18 - 02:00:19] ‫שבעצם כן משנה לזה.
[02:00:19 - 02:00:21] ‫אנחנו נכנסים פה קצת ‫לפרטים טכניים של המימוש.
[02:00:25 - 02:00:26] ‫איך עושים את זה?
[02:00:26 - 02:00:31] ‫אנחנו נסתכל גם על הקוד, ‫אני מסכים לראות את זה בדיוק,
[02:00:32 - 02:00:38] ‫אבל הרעיון הוא כזה, ‫בעצם מפצלים את זה ‫לפילטר ורטיקלי,
[02:00:39 - 02:00:41] ‫זה נראה ככה, ‫מאסק בעצם ורטיקלי,
[02:00:42 - 02:00:44] ‫למאסק אוריזונטלי,
[02:00:45 - 02:00:47] ‫שהוא רק מסתכל על,
[02:00:47 - 02:00:53] ‫לא רק מאסק וריזונטלי, ‫זה קונבולוציה אוריזונטלית, ‫שרק מסתכלת על השורה הנוחותית.
[02:00:54 - 02:00:56] ‫אוקיי, ועוד פעם יש לנו הפרדה ‫בין השכבה הראשונה,
[02:00:56 - 02:00:57] ‫עם השקועות האחרות,
[02:00:57 - 02:01:00] ‫שכבר הראשונה אנחנו מסתכלים ‫על כל השורות שמעלינו,
[02:01:01 - 02:01:03] ‫לא כולל השורה שלנו, ‫שזו שורה אמצעית,
[02:01:04 - 02:01:06] ‫ואת שני הפיקסלים,
[02:01:07 - 02:01:09] ‫כל הפיקסלים שהם משמאלנו, ‫זה לא חייב להיות חמש וחמש,
[02:01:10 - 02:01:12] ‫כל הפיקסלים שהם משמאלנו ‫בשורה הנוחותית.
[02:01:13 - 02:01:14] ‫זאת אומרת, אם נסכום את זה ואת זה,
[02:01:15 - 02:01:19] ‫קונבולוציה זה פעולה ליניארית, ‫ואני יכול לעשות אותה פעם אחת ככה, ‫אבל פעם שנייה, ‫ואז נסכום את זה.
[02:01:20 - 02:01:24] אם נספום את זה וזה נקבל קונבולציה בעצם שהיא שקולה למה שהיה לנו קודם
[02:01:29 - 02:01:30] בשכבות האחרות
[02:01:31 - 02:01:33] אני מסתכל על כל השורות
[02:01:35 - 02:01:38] סליחה, על כל השורות מעליי כולל השורה שלי, וההבדל הוא שכאן אני
[02:01:38 - 02:01:41] מכליל גם בשורה שלי את מה שבא מקדימה
[02:01:43 - 02:01:45] אבל אני לא מפעיל את זה בשכבה הראשונה הזאת
[02:01:45 - 02:01:48] ואת מה שהיה לנו קודם
[02:01:49 - 02:01:56] על השורה שלי, את כל השארה עם שמאל, כולל הערך שלי
[02:01:56 - 02:01:57] של הפיקסל-I
[02:01:58 - 02:02:01] ותכף נראה איך מפעילים את זה, אז בעצם מפעילים,
[02:02:01 - 02:02:04] בשכבה הראשונה מפעילים את זה ומוסיפים את זה
[02:02:05 - 02:02:07] ואז בשכבות הבאות
[02:02:07 - 02:02:11] בעצם שומרים איזשהו stack שהוא רק של ה-verticals שמפעילים את זה אחד על השני
[02:02:12 - 02:02:15] ואת זה כל פעם מוסיפים פעם אחת על התוצאה
[02:02:15 - 02:02:20] וזה מאפשר בעצם לדבר הזה לגדול בדיוק כמו שרצים
[02:02:22 - 02:02:23] בפעם הראשונה זה נשאר אותו דבר,
[02:02:24 - 02:02:25] בשכבה הראשונה זה נשאר אותו דבר,
[02:02:26 - 02:02:28] השכבה השנייה והלאה, הפיקסל הזה
[02:02:29 - 02:02:31] מסתכל על כל הדברים שאתה
[02:02:32 - 02:02:34] יכול לשנות עליו מיד הסוף, אבל זה יהיה קצת ממינוי,
[02:02:35 - 02:02:39] בפעם הבאה זה יהיה עוד פעם ימינה עד שזה יגיע בסוף והכל
[02:02:41 - 02:02:44] זה עדיין יוצא חוקי וזה יעיל ויוצא
[02:02:44 - 02:02:47] שזה מכיל את כל הקונטקסט שרצינו להכיל,
[02:02:47 - 02:02:48] כל פעם אפשר לעשות את זה.
[02:02:52 - 02:02:57] אוקיי, אז בואו נסתכל קצת על תוצאות ואז יהיה לנו איזה עשר דקות להסתכל קצת על הקוד
[02:03:00 - 02:03:05] אז אוקיי, זו הייתה הפעם הראשונה בערך שהציגו תוצאות של
[02:03:08 - 02:03:09] קנרציה של תמונות
[02:03:11 - 02:03:12] שהם לא הם מיסטיקה,
[02:03:12 - 02:03:15] אז אין מסתכלים קודם היה בנייד כבר היה תוצאות די טובות
[02:03:19 - 02:03:32] ‫אז פה גם הייתה תוצאות טובות באמליסט אבל הדבר הראשון שהתחילו להראות תוצאות באימג'נט זה 2014-2014,
[02:03:33 - 02:03:37] כל הזמן התחילו גם לפתח מודל אחר שמתארגן,
[02:03:38 - 02:03:39] ‫שגם התחיל להראות תוצאות טובות.
[02:03:40 - 02:03:43] ‫בערך ב-2017 כבר היו תוצאות ממש טובות,
[02:03:46 - 02:03:48] ‫בעיקר בהתחלה זה היה יותר על פנים,
[02:03:50 - 02:03:57] ‫ובסביבות 2014-2020 התחילו להיות תוצאות ממש טובות ‫על אימג'נט מלא.
[02:03:58 - 02:03:59] ‫לא יודע אם אתם רואים, אני אספרו אחר כך,
[02:04:00 - 02:04:05] ‫אבל זה נראה כזה יחסית, כאילו הסטטיסטיקה של הצבעים, ‫והכול נראה טוב, יש אג'ים, יש כל מיני טקסטורות,
[02:04:06 - 02:04:09] ‫ואין פה בדיוק דברים שאפשר להבין מהם.
[02:04:13 - 02:04:17] ‫זה משהו שאני אמרתי, כמו שמאיין ומדבר, ‫או איזה סלע או משהו כזה,
[02:04:17 - 02:04:18] ‫אין פה שום דבר ברור.
[02:04:19 - 02:04:19] ‫אבל זה תופס,
[02:04:21 - 02:04:24] תופס משהו, בהחלט משהו ‫שלא הצליחו לעשות קודם,
[02:04:25 - 02:04:27] ‫אבל זה עדיין לא ממש מודל של תמונות,
[02:04:28 - 02:04:35] ‫הוא מצליח לפתוס את כל המבנה של מה יש בתמונה, ‫בצורה שאדם
[02:04:36 - 02:04:38] ‫את הדברים הסמנטיים שיש בתחומים.
[02:04:43 - 02:04:47] ‫זה מתוך המאמר שלהם, ‫גם יש כאן השוואה של הלוג לייטליות,
[02:04:48 - 02:04:52] ‫שמה שיצא להם, ‫השוואה למודלים אחרים.
[02:04:53 - 02:04:55] ‫אז פה זה דוגמה, זה על אמיסט,
[02:04:57 - 02:04:57] ‫אוקיי?
[02:04:57 - 02:04:58] ‫זה מספרים,
[02:04:59 - 02:05:02] ‫זה NLL, זה negative log לייטליות,
[02:05:03 - 02:05:04] ‫שזה מה שאמרנו קודם,
[02:05:04 - 02:05:10] ‫זה בעצם אורך הקידוד הזה. ‫בגלל שזה דרך דיסקרטי, ‫אז הדבר הזה אומר כמה ביטים צריך
[02:05:11 - 02:05:15] ‫כדי לשדר תמונה של nx בממוצע.
[02:05:18 - 02:05:27] ‫למרות שכאן אני רואה שכתוב ‫פשוט זה בנאצ, אוקיי? זה לא ביטים, אוקיי? ‫הלוג שהם עשו שם זה לוג בבסיס סעיג,
[02:05:27 - 02:05:31] ‫זה לא לוג בבסיס 2. ‫זה לא בדיוק ביטים, אבל זה בערך, אוקיי?
[02:05:32 - 02:05:36] ‫אז יש פה כמה מודלים שדיברנו עליהם.
[02:05:38 - 02:05:40] ‫אתם מכירים, יש נאיד,
[02:05:41 - 02:05:45] ‫עוד כל מיני גרסאות של נאיד ‫שהן נסתכלו על סידורים שונים של הפיקסלים.
[02:05:46 - 02:05:59] ‫אז נגיד נאיד צריך 88 פיקסלים בגרסה, ‫בגילה 88 נאצ כדי לשדר תמונת אמיסט בממוצע,
[02:05:59 - 02:06:04] ‫בגרס המשוחררות, 85-85, 84. ‫יש פה את מייד
[02:06:05 - 02:06:07] שראינו קודם, ‫שאתם יודעים מה עשינו האלה שעשינו.
[02:06:09 - 02:06:10] ‫זה יותר טוב,
[02:06:10 - 02:06:12] יותר טוב ממייד, ‫אבל לא יותר טוב מהשיטעון שלו,
[02:06:13 - 02:06:15] ‫מייד 86. יש פה מודל
[02:06:19 - 02:06:19] דרו,
[02:06:20 - 02:06:21] זה מודל שהוא,
[02:06:22 - 02:06:28] נדבר עליו בשבוע הבא, אולי לא עליו, ‫אבל זה מודל שמבוסס על VCE. ‫כתוב גם קטן, שווה,
[02:06:28 - 02:06:29] ‫מ-80.
[02:06:30 - 02:06:31] אם אתם זוכרים,
[02:06:31 - 02:06:36] ‫בלישה של וריאשיונל, ‫היא וריאשיונל הטרנקודר, ‫של וריאשיונל אינפרנס,
[02:06:37 - 02:06:40] ‫אנחנו לא יכולים לקשר בייקולוגם, ‫אנחנו יכולים לקשר רק חסם על הלייטים.
[02:06:40 - 02:06:44] ‫אנחנו יודעים שזה קטן מ-80, ‫אבל לא יודעים לקשר כמה זה בדיוק.
[02:06:47 - 02:06:49] ‫וזה מה שיוצא ב...
[02:06:49 - 02:06:50] יש כאן את פיקסל-CNL,
[02:06:51 - 02:06:53] ‫זה עכשיו ב-81.
[02:06:55 - 02:06:58] ויש כאן, מה שאתם רואים כאן,
[02:06:58 - 02:07:00] ‫זה קוראים לזה פיקסל-RNN,
[02:07:01 - 02:07:05] ‫זה בעצם פיקסל-CNN ‫עם כל הצרקים האלה של קונבולוציות, ‫אבל בנוסף יש גם איזשהו סטייט,
[02:07:06 - 02:07:10] ‫שהוא ריקרנט כזה, עם איזשהו LSTM, שעובר,
[02:07:10 - 02:07:16] ‫אז בעצם זה אומר שהאימון יהיה יותר איטי, ‫כי הם צריכים לאמן את הכול בצורה סדרתית,
[02:07:17 - 02:07:17] ‫ולא בצורה
[02:07:20 - 02:07:20] מגבילית.
[02:07:22 - 02:07:25] ‫אז הוא מסיתוצות יותר טובות, ‫אבל זה הרבה יותר יקר לאימון.
[02:07:28 - 02:07:29] ‫כן, הספרים האלה ברורים.
[02:07:31 - 02:07:40] ‫לא אמרתי, בעצם ברגע שיש לנו ‫את כל הקונבולוציות האלה, ‫אנחנו יכולים פשוט לאמן ככה ‫את המודל שלנו בצורה, ‫כמו שעשינו קודם, ‫אנחנו מפעילים את הקונבולוציות,
[02:07:40 - 02:07:44] ‫וצאים לנו בעצם ה-output בכל פיקסל, ‫וצאה לנו ההסתברות,
[02:07:45 - 02:07:50] ‫איזה בינארי, ההסתברות שזה יהיה 1, ‫בכל אחד מהפיקסלים,
[02:07:51 - 02:07:52] ‫וזהו, אנחנו יכולים לדגום מזה פשוט,
[02:07:53 - 02:07:54] ‫אם אנחנו יכולים לייצר תמונה,
[02:07:55 - 02:07:58] ‫או לחשב את המכפלה של המספרים האלה ‫כדי לחשב את הלייקליקס.
[02:08:00 - 02:08:05] ‫בדיוק כמו קודם. ‫פשוט אנחנו ממשים את הכול עם קונבולוציות ‫שאנחנו מפעילים בבת אחת על הפועל,
[02:08:06 - 02:08:09] ‫עם כל המאסטים האלה, ‫זה היתרון ביעילות שלי.
[02:08:14 - 02:08:22] ‫אוקיי, ויש פה עוד דוגמאות, ‫אז כמו שאמרתי, זה היה קודם יותר ‫המודלים הראשונים ‫שהצליחו לייצר דאטה על...
[02:08:23 - 02:08:25] ‫אני חושב שזה היה הממש הראשון ‫על אימג'נט,
[02:08:26 - 02:08:29] ‫באימג'נט לא ישבו לאף אחד ‫כמו אירולוגים ראשונים, אז זה היה...
[02:08:30 - 02:08:32] ‫פה הם עשו כל זה כאין ביטים,
[02:08:34 - 02:08:36] ‫וזה חלקי ה-Dimensiones, ‫חלקי מספר הפיקסלים.
[02:08:37 - 02:08:39] ‫זה מספר יותר קטן ממי שהכולם.
[02:08:40 - 02:08:46] ‫לפעמים צריך לתת כאן יותר ביטים ‫כדי לשדר תמונת אימג'נט
[02:08:46 - 02:08:48] ‫למפי תמונת M-ליסט.
[02:08:49 - 02:08:57] ‫זה גם ביטים וזה גם חלקי מספר הביטחוניים, ‫שזה אלף עשרה, ארבעה פה,
[02:08:58 - 02:09:00] ‫ואני יוצא כאן, למטה הלכים
[02:09:03 - 02:09:04] ‫אבל זה מסוימת יותר גדולה.
[02:09:05 - 02:09:06] ‫אז צריכים כאן
[02:09:08 - 02:09:09] ‫בדור גודל של שלוש וחצי ארבעה
[02:09:11 - 02:09:11] ‫ביטים ל-Tixel.
[02:09:14 - 02:09:14] ופה
[02:09:15 - 02:09:19] ‫זה CFAR-PEN, אתם מכירים את הדאטה-סט הזה?
[02:09:20 - 02:09:24] ‫זה גם דאטה-סט שהוא גם לא יודע ‫שלושים ושתיים ושתיים, ‫אבל הרבה יותר קטן מאימג'נט.
[02:09:26 - 02:09:30] ‫הוא קצת בעתיק, ‫יש פה מספר חצי קטן של תמונות, ‫אז די מהר עושים שם אוברפיטינג.
[02:09:32 - 02:09:36] ‫אז הם הראו פה בגמות שלא מפועל ‫השיבו לכל מיני מחירים.
[02:09:37 - 02:09:39] ‫יש פה משהו שהוא נוסס על GMM's,
[02:09:40 - 02:09:43] ‫גם עם איזה מפרשת עמוקה כזאת.
[02:09:44 - 02:09:44] ‫יש פה...
[02:09:46 - 02:09:49] ‫נייס, אנחנו נדבר על זה קצת, ‫זה משהו שמבוסס על
[02:09:51 - 02:09:52] ‫Normalizing flows.
[02:09:53 - 02:09:56] ‫יש פה גם איזה גרסר, ‫יש ריקטור של דפיוז'ן, ‫אני לא יודע מה זה בדיוק.
[02:10:07 - 02:10:11] כן, גם פה וגם פה אתם רואים, ‫יש את המספרים של הטסט ‫ואת המספרים של הטועים.
[02:10:13 - 02:10:19] ‫זה לא בלייקטיות על התמונות שהיו באימון,
[02:10:20 - 02:10:23] ‫אבל לא בלייקטיות על התמונות שהיו בטסט.
[02:10:24 - 02:10:28] ‫יכול להיות מצב שאנחנו ממש רואים ‫אוברפיקינג.
[02:10:29 - 02:10:30] ‫אם המודל ממש לומד רק
[02:10:31 - 02:10:36] דברים שהם בטריינג, ‫בלי יכולת להכליל, ‫אז יהיה לו לייקטיות מאוד טוב
[02:10:37 - 02:10:40] על הטריינינג, ‫אבל הלייקטיות יהיה גרוע על הטסט.
[02:10:41 - 02:10:42] ‫הנגטיב הבאק יהיה הרבה יותר גבוה
[02:10:43 - 02:10:43] ‫על הטסט.
[02:10:45 - 02:10:47] ‫הם רואים שפה הוא קצת יותר גבוה, ‫אבל לא בטח תמונות.
[02:10:48 - 02:10:48] ‫פה אולי זה לא...
[02:10:49 - 02:10:50] ‫גם פה זה לא בטח תמונות.
[02:10:52 - 02:10:53] אוקיי,
[02:10:54 - 02:10:57] אז יכול להיות שתעשו ב... ‫אני אראה עכשיו את הקוד,
[02:10:57 - 02:10:59] ‫נסתכל עליו כמה דקות,
[02:10:59 - 02:11:05] ‫ויכול להיות שתצטרכו חלק מהתרגיל ‫אולי לממש את ה...
[02:11:06 - 02:11:10] ‫לעשות את החישובים האלה, של מה הלייקטיות, ‫של כל מיני מרסאות שונות של המודל.
[02:11:13 - 02:11:18] ‫אוקיי, יש כאן איזה סיכום, ‫אולי לפני שאני אדבר על זה, ‫סיכום על המודלים של אוטו-רגרסיב.
[02:11:19 - 02:11:19] ‫אז
[02:11:20 - 02:11:22] מה טוב בהם? ‫קל מאוד לדגום מהם.
[02:11:23 - 02:11:26] ‫האלגוריתם מאוד פשוט, ‫אתם פשוט דוגמים פיקסל-פיקסל.
[02:11:27 - 02:11:29] ‫החיסרון זה שזה יכול לקחת יותר הרבה זמן.
[02:11:31 - 02:11:34] ‫יתרון שני זה שקל לחשב את ההסתברות שלהם,
[02:11:36 - 02:11:36] ‫אוקיי, זה מה ש...
[02:11:37 - 02:11:39] ‫שזה פשוט המכפלה של הדברים האלה,
[02:11:40 - 02:11:40] אוקיי?
[02:11:41 - 02:11:44] ‫ובאופן אידיאלי, ‫אם אפשר לעשות את הדברים האלה במקביל,
[02:11:45 - 02:11:48] ‫אז זה עוד יותר קל, ‫אפשר לעשות את זה בצורה יעילה, ‫וזה מאוד עוזר לאימון.
[02:11:49 - 02:11:53] ‫כי באימון בעצם אנחנו צריכים ‫לחשב את הדבר הזה הרבה פעמים, ‫על הרבה דוגמאות,
[02:11:54 - 02:11:55] ‫כל פעם גם לגזור את זה.
[02:11:55 - 02:11:58] ‫אם אפשר לעשות את זה בצורה יעילה, ‫אז זה מאוד עוזר לאימון.
[02:11:59 - 02:12:04] ‫מתי אי אפשר לעשות את זה בצורה יעילה ‫כשאנחנו עושים, כשיש לנו איזשהו RNN, איזשהו LSTN, ‫שצריך להתעדכן כל פעם
[02:12:05 - 02:12:06] ‫בין הפיקסלים.
[02:12:08 - 02:12:17] ‫אנחנו דיברנו על דוגמאות דיסקרטיות, ‫זאת אומרת שהארטפוד היה בסדר ‫התפלגות ברנולי או קטגורית, ‫אבל אפשר לעשות את אותו דבר ‫על דאטה שהוא רציף.
[02:12:18 - 02:12:20] ‫למשל, אפשר שהארטפוד יהיה גאוסיאן,
[02:12:21 - 02:12:26] ‫שהתוחלת שלו היא פי פונקציה ‫של כל ה-Xים הקודמים,
[02:12:26 - 02:12:29] ‫גם יכול להיות איזושהי רשת עמוקה ‫שנותנת לנו את התוחלת,
[02:12:29 - 02:12:34] ‫והקובריאנס זה גם איזושהי רשת עמוקה ‫שנותנת לנו את הקובריאנס.
[02:12:34 - 02:12:37] ‫זה יכול להיות גאוסיאנים, ‫אולי תערובת של גאוסיאנים,
[02:12:37 - 02:12:43] ‫שכל התוחלות וכל הקובריאנסים ‫הם אאוטפוטים של איזושהי רשת עמוקה ‫שהם יוצאים בו.
[02:12:44 - 02:12:49] ‫אז כל מה שאמרנו עכשיו, ‫הוא לא באמת חייב להיות רק על דיסקרטי, ‫אבל צריך להיות ‫הדוגמה דיסקרטית, כי זה יותר פשוט להבין,
[02:12:49 - 02:12:52] ‫וזה גם איך שפיתחו את המודלים האלה מראש.
[02:12:53 - 02:12:55] ‫יש, אני חושב, מודל שנקרא ‫פיקסל CNN פלוס,
[02:12:56 - 02:12:59] ‫שהוא פחות או יותר ממש פיקסל CNN פלוס פלוס.
[02:13:00 - 02:13:03] ‫חיסרון גדול של המודלים האלה,
[02:13:04 - 02:13:07] ‫מעבר לזה שזה יכול לקחת זמן לדגום מהם,
[02:13:08 - 02:13:11] ‫שהם לא כל כך חוזרים לנו ‫לאחת מהמשימות שרצינו,
[02:13:11 - 02:13:14] ‫של מודלים גנרטיביים, ‫זה מה שקראנו representation learning,
[02:13:15 - 02:13:23] ‫כי אין לנו בתוך כל התהליך הזה ‫איזשהו ייצוג שהוא מועמד טבעי ‫להיות הייצוג של התמונה, ‫איזה משהו שתופס לתמונה בצורה גלובלית.
[02:13:24 - 02:13:28] ‫ובעצם אנחנו סידרנו את הפיקסלים האלה ‫סתם באיזושהי צורה יחסית שריבותית,
[02:13:28 - 02:13:34] ‫כל פיקסל תלוי בקודמי, ‫יש לנו כל מיני ערכים ‫שאנחנו מחשבים באמצע, ‫אבל אין איזה משהו שהוא,
[02:13:34 - 02:13:39] ‫חושבים שאולי הוא תופס משהו יותר עמוק ‫על התמונה בצורה גלובלית.
[02:13:40 - 02:13:44] ‫יכול להיות שאיזשהו ערך ‫בפיקסל האחרון שתלוי בכולם,
[02:13:45 - 02:13:53] ‫כן איכשהו תופס דברים כאלה, ‫אבל אין ממש מועמד טבעי ‫במודלים אחרים, ‫יש לנו בדרך כלל יותר תקווה
[02:13:54 - 02:13:59] ‫נצפות שדבר כזה יתפוס באמת ‫משהו שהוא מעניין.
[02:14:01 - 02:14:03] ‫אוקיי, שאלות לפני שאני עובר על ה...
[02:14:05 - 02:14:05] ‫קוד?
[02:14:08 - 02:14:13] ‫אין לנו הרבה זמן, אני רק אראה כזה ‫שתהיה לכם טעימה של איך זה נראה, ‫ואז אני אפרסם את זה.
[02:14:17 - 02:14:22] ‫כן, אז זה קולאב כזה ‫שהוא ממש טוטוריאל, אתם רואים?
[02:14:22 - 02:14:24] ‫פעם צריכים לדבר קצת נאום.
[02:14:52 - 02:14:54] ‫אז
[02:14:59 - 02:15:04] אוקיי, זה ממש טוטוריאל מסביר נחמד ‫גם כל דברים שאמרנו עליהם, ‫אנחנו רוצים לראות את הדבר הזה, ‫ויש פה כל מיני
[02:15:11 - 02:15:12] ‫ויזואליזציות נחמדות.
[02:15:13 - 02:15:16] ‫אז קודם כול זה בנוי על דורץ', על פייטורץ'.
[02:15:23 - 02:15:38] ‫והדוגמה שיש פה זה אמניסט, ‫אז הוא מוריד את הדאטה סט של אמניסט.
[02:15:41 - 02:15:43] ‫כן, מראה דוגמאות, זה הדאטה סט עצמו.
[02:15:52 - 02:15:54] ‫עכשיו נדבר על כל העניין הזה ‫של המאסקינג,
[02:15:55 - 02:15:56] ‫מה שדיברנו עד עכשיו.
[02:15:57 - 02:15:58] ‫אז יש פה את המימוש של
[02:16:04 - 02:16:04] ‫השימוש במאסקינג.
[02:16:05 - 02:16:08] ‫לעשות קונבולוציה ועם המאסקינג, ‫אז יש כאן את הקרנל עצמו.
[02:16:10 - 02:16:16] ‫קרנל זה בעצם הפילטר שאנחנו מפעילים עליו, ‫מפעילים את אותה קונבולוציה, ‫אז יש כאן את ההגדרה של הגודל שלו.
[02:16:17 - 02:16:20] ‫זה כל מיני דברים שיש כבר בפייטורץ',
[02:16:20 - 02:16:25] ‫של בונה שכבה של קונבולוציה ‫עם גודל קרנל שאנחנו רוצים.
[02:16:27 - 02:16:29] ‫מה אנחנו רוצים שיקרה מחוץ לתמונה,
[02:16:30 - 02:16:33] ‫ובנוסף, אנחנו מגדירים את המאסק,
[02:16:34 - 02:16:38] ‫אוקיי? ‫מאותו איזה שהוא משתנה, ‫למרות שהוא לא משתנה מלמד, ‫אבל הוא משתנה של המודל.
[02:16:39 - 02:16:47] ‫וכשאנחנו מפעילים את ה-forward אז אנחנו לא סתם ‫עושים את הקונבולוציה, ‫אנחנו קודם מכפילים את ה-weights של הקונבולוציה, ‫שזה הפילטר הזה,
[02:16:48 - 02:16:48] ‫מכפילים את זה במאסק.
[02:16:50 - 02:16:52] ‫אם המאס שלנו יהיה חוקי, ‫אז זה בעצם
[02:16:55 - 02:16:59] ‫גורום למודל שלנו להיות סיבתי כזה, ‫שכל פיקסל, לא רק תורידו ‫פיקסל עם התזמין.
[02:17:01 - 02:17:05] ‫ועכשיו יש כאן כמה מיני מימושים ‫של מאסקים שונים, ‫בדיוק עם מה שדיברנו עליו.
[02:17:07 - 02:17:11] ‫אז יש פה את ההפרדה ‫בין ורטיקל לאוריזונטל.
[02:17:12 - 02:17:13] ‫אז בוורטיקל
[02:17:14 - 02:17:19] יש כאן אתם שתי אופציות, ‫בשביל האיטרציה, ‫בשביל השכבה הראשונה והשכבות האחרות.
[02:17:19 - 02:17:25] ‫בשכבה הראשונה אנחנו גם רוצים לעשות ‫לאפס את השורה האמצעית,
[02:17:27 - 02:17:29] ‫ואחר כך לא. יש פה פלאג כזה,
[02:17:30 - 02:17:33] ‫אבל תמיד אנחנו רוצים ‫לאפס את כל השורות
[02:17:34 - 02:17:36] ‫שהם אחרי חצי מהקרנל סייט.
[02:17:38 - 02:17:39] ‫זה מה שכתוב כאן, אנחנו
[02:17:39 - 02:17:42] מאפסים את זה, ‫מאפסים את זה ביחדות, ואז מאפסים.
[02:17:45 - 02:17:48] ‫והקרנל של האוריזונטל ‫הוא רק בגודל אחד, אוקיי?
[02:17:49 - 02:17:50] ‫הוא רק שורה אחת,
[02:17:51 - 02:17:53] ‫וגם אותו דבר, ‫חצי מהשורה הזאת אנחנו מאפסים,
[02:17:54 - 02:17:55] ‫כולל האמצע.
[02:17:58 - 02:18:02] ‫תמיד את כל מה שבאחרי האמצע, ‫וכולל האמצע גם באיטרציה הרשמית.
[02:18:05 - 02:18:07] ‫ויש פה איזו דרך לעשות ויזואליזציה ‫למה קורה,
[02:18:08 - 02:18:12] ‫על ידי זה שהם פשוט מחשבים את ה... ‫בגלל שאפשר לחשב את הגרדיאנט בצורה אוטומטית,
[02:18:13 - 02:18:20] ‫אז אפשר פשוט לדעת ‫האם פיקסל אחד, באיזה פיקסל הוא תלוי, ‫על ידי חישוב גרדיאנט. ‫פשוט מחשבים מה הגרדיאנט ‫של כל הפיקסלים האחרים,
[02:18:20 - 02:18:22] ‫כפונקציה של פיקסל אחד.
[02:18:23 - 02:18:29] ‫ואז אם יש איזשהו גרדיאנט, ‫זה אומר שהפיקסל הזה תלוי, ‫החישוב שלו תלוי בפיקסלים האחרים.
[02:18:30 - 02:18:31] ‫איפה יש את כל הויזואליזציה,
[02:18:32 - 02:18:35] ‫הדרך לעשות ויזואליזציה ‫על הגרדיאנטים האלה.
[02:18:36 - 02:18:37] ‫לא ניכנס לפה.
[02:18:38 - 02:18:46] ‫ואז זה מראה, אוקיי, ‫אם אני מפעיל פעם אחת ‫את האוריזונטל בגודל 3 על 3,
[02:18:46 - 02:18:49] ‫ובעצם זה אומר שהאוריזונטל, ‫יש לו רק פיקסל אחד משמאל,
[02:18:49 - 02:18:50] ‫תסתכל עליו,
[02:18:51 - 02:18:52] ‫אז הוא רק תלוי בפיקסל אחד משמאל,
[02:18:53 - 02:19:01] ‫הוורטיקל תלוי בשלושת הפיקסלים ‫שמעליו, וזה אומר שיש גרדיאנט, ‫כל שאר הפיקסלים, הגרדיאנט הוא 0, ‫הם לא משפיעים על הפיקסל האדום,
[02:19:01 - 02:19:03] ‫רק שלושת אלהם משפיעים על האדום.
[02:19:05 - 02:19:07] ‫ועכשיו כשמחברים את שניהם,
[02:19:07 - 02:19:09] אוקיי? זה בדיוק מה שרצינו בשביל כלום.
[02:19:13 - 02:19:15] טוב, עכשיו יש כמה שכבות שמפקידים,
[02:19:16 - 02:19:19] אז אחרי שמפקידים שתי שכבות, זה בעצם התלות.
[02:19:20 - 02:19:25] פה זה גם הערך ממש של הגרדיאנט, כאן זה דרך פינאריק, אבל האם הוא שונה מ-0?
[02:19:26 - 02:19:30] אז זה המאסק שיוצא, אחרי השכבה השלישית, זה גדל,
[02:19:31 - 02:19:32] והראשית זה גדל עוד יותר,
[02:19:33 - 02:19:34] ובשמישית זה גדל עוד יותר.
[02:19:38 - 02:19:50] אוקיי, יש פה עוד כל מיני דברים שלא נכנסנו, יש עוד כל מיני טריקים במאמר שלא נכנסנו אליהם, אחד זה איזשהו גייטינג כזה בתוך הקונבולוציות, משהו שדומה קצת ל-STM,
[02:19:51 - 02:19:51] תוך קונבולוציות,
[02:19:52 - 02:19:54] חשוב כרגע.
[02:19:56 - 02:20:05] המטרה כאן זה שבעצם זה הכל יהיה יותר יעיל, שבפחות פרמטרים יהיה דרך להסתכל בצורה יעילה על יותר פיקסלים בקונפקסט.
[02:20:06 - 02:20:07] זו שיטה אחת, זו שיטה
[02:20:07 - 02:20:09] עוד שיטה שנקראת DILATION פה,
[02:20:13 - 02:20:19] שבעצם גם בונים פילטרים שהם לא מסתכלים על השכבה הקרובה, אלא מדלגים כל פעם על טיקסל אחד,
[02:20:19 - 02:20:22] זו היררכיה כזאת שקופצת יותר מהר.
[02:20:24 - 02:20:26] ועדיין, מפעילים את כל המאסקים באותה צורה.
[02:20:26 - 02:20:28] אז זה הטריקים האלה,
[02:20:29 - 02:20:31] ופה יש את המודל עצמו,
[02:20:33 - 02:20:34] אני לא יודע אם אני יכול להריץ את זה עכשיו.
[02:20:37 - 02:20:42] זה מספיק לרוץ.
[02:20:43 - 02:20:47] זה פשוט בנוי את ה-VSTAC וה-HSTAC,
[02:20:47 - 02:20:52] זה ה-Layר הזה של הסטאקינג כמו שאמרנו, אה, לא הראתי לכם.
[02:21:08 - 02:21:16] טוב, אני לא רואה איך זה, אתם תצטרכו לראות את זה לבד, יש את הרעיון הזה של איך ה-Vertical וה-Oריזונטל מחוברים.
[02:21:20 - 02:21:23] אפשר לראות את זה בוויזואליזציות שהם עשו כאן, בעצם.
[02:21:24 - 02:21:24] איך עשו...
[02:21:37 - 02:21:42] כן, כשעושים כאן כמה שכבות של קונבולוציות, זה הוויזואליזציות שראינו קודם.
[02:21:43 - 02:21:46] אז בעצם מפעילים, יש משהו שקוראים לו vertical image,
[02:21:47 - 02:21:50] שהוא פשוט stack של ה-Vertical Involution, זה אחד על השנייה.
[02:21:51 - 02:21:57] ואז בסופו של דבר מה שאנחנו מראים זה שאנחנו מפעילים את ה-Horizontal Involution על ה-Vertical הנוכחי.
[02:21:57 - 02:22:01] יש לנו stack של Vertיקל שהולך וגדל, וכל כך אנחנו מפעילים הוריזונטל
[02:22:02 - 02:22:02] אחד רק.
[02:22:03 - 02:22:06] אצלו לנו להפעיל את ה-Horizontal כמה פעמים ולעשות stack-ing שלו.
[02:22:07 - 02:22:11] זה הדרך היעילה שלהם לעשות את זה. בסופו של דבר, ההסבר פה הוא מאוד מאוד מאוד.
[02:22:15 - 02:22:15] בסדר.
[02:22:23 - 02:22:28] אז יש פה דוגמא, זה המודל עצמו עם כל הדברים בתוכו כבר,
[02:22:29 - 02:22:30] ופונקציה שנקראת Calc-Lightlyhood,
[02:22:31 - 02:22:33] פונקציה שנקראת Sample,
[02:22:34 - 02:22:35] שמייצרת את הדגימות.
[02:22:37 - 02:22:42] וגם את כל האופטימיזציות שצריכים בשביל האימון.
[02:22:55 - 02:22:58] הפיקסל האמצעי בתמונה, למה הוא תלוי, זה דרך בדיוק ש...
[02:22:59 - 02:23:03] כמו מה שאנחנו עושים את הדבר הזה לכל פיקסל, לראות שכל פיקסל לא תלוי בפיקסלים האחרים.
[02:23:04 - 02:23:10] ועד שתקן את האימון. פה עכשיו הוא יטען, אני חושב, משהו שהוא כבר קיים,
[02:23:11 - 02:23:13] שהוא כבר אומן,
[02:23:14 - 02:23:14] ועד אפשר
[02:23:34 - 02:23:40] ‫אחר כך. זה עובד לי מספיק, זה כבר הזמן.
[02:24:00 - 02:24:03] שיחקתי עם זה, זה לא עובד עכשיו, אבל בעיקרון הוא,
[02:24:03 - 02:24:06] אם לא משנים כלום, הוא אמור לטעון את המודל שהוא כבר אימן,
[02:24:07 - 02:24:10] ‫אז אפשר לדגום ממנו, ‫לחשב איתו לייקליות של תמונות חדשות,
[02:24:10 - 02:24:11] כל מיני דברים כאלה.
[02:24:11 - 02:24:19] אבל תרגיל איכשהו יהיה מבוסס על הקוד הזה, ‫נשלח לכם את הקוד הזה, ‫עם עוד כל מיני דברים לממש עליו.
[02:24:21 - 02:24:26] ויכול להיות, אנחנו קצת בסמסטר מקוצר, ‫אבל יכול להיות שמה שאני עושה זה לחבר את
[02:24:28 - 02:24:29] התרגיל הזה כבר עם המודל הבא,
[02:24:29 - 02:24:33] ‫אולי גם בשבוע הבא כבר נדבר על BAE's,
[02:24:34 - 02:24:37] ‫אז אולי כבר התרגיל יהיה גם לממש BAE's,
[02:24:37 - 02:24:38] ‫על אותו קוד,
[02:24:39 - 02:24:41] ‫ולהשוות ביניהם במה שאתה לא חושב.
[02:24:43 - 02:24:45] טוב, אז זהו, לרעיון.