[00:00:00 - 00:00:07] ‫טוב, נתחיל.
[00:00:30 - 00:00:42] ‫אתם רואים את התמונה בזום, ‫אבל זה נעשה אותה מצלמה ‫בדיוק לפני ה... מאחורי המקרן. ‫זה מקרן קצת מצטיע? ‫-מקרן קצת מצטיע. ‫-אני ממליץ לכם לבוא ולא...
[00:00:43 - 00:00:46] ‫לא להסתמך על ההחלטות, ‫אני לא בטוח עד כמה זה יהיה איכותי.
[00:01:00 - 00:01:21] ‫טוב, אז ברוכים הבאים, שלום לכולם. ‫נפגשנו כבר אולי, אז נראיתם,
[00:01:22 - 00:01:27] ‫דיברתי על הרבה ריבועים שחורים, ‫אבל לא בטוח אם דיברתי כבר ומי לא.
[00:01:27 - 00:01:36] ‫אז טוב שאנחנו נפגשים סוף סוף. ‫היה לכם כבר תרגיל אחד ‫עם הרבה אינטגרלים והשלמות לריבוע,
[00:01:37 - 00:01:37] ‫עבר בשלום.
[00:01:40 - 00:01:41] ‫ככה ככה?
[00:01:42 - 00:01:49] ‫אוקיי, אז קיבלתם כבר תרגיל שני ‫שיש בו גם דברים דומים.
[00:01:49 - 00:01:51] ‫חלק אפילו ממש זהה,
[00:01:52 - 00:01:56] אם תשימו לב, ‫אבל פשוט בפריימינג קצת אחר,
[00:01:56 - 00:01:58] ‫שקשור למה שאנחנו נדבר עליו היום, ‫שזה...
[00:01:59 - 00:02:00] ‫אנחנו נדבר על זה מלושני נושאים.
[00:02:02 - 00:02:05] ‫מבחינה טכנית אנחנו נדבר ‫בעיקר על בייז'ן אינפורנס, ‫סטטיסטיקה בייזיאנית,
[00:02:06 - 00:02:09] ‫אבל הכול בתוך המוטיבציה ‫של להכניס,
[00:02:11 - 00:02:13] לבנות מודלים,
[00:02:13 - 00:02:15] ‫שהם מה שנקרא משתנים חבויים.
[00:02:16 - 00:02:19] ‫אני מקווה שעד סוף השיעור ‫אתם כבר תבינו את הקשר ביניהם,
[00:02:20 - 00:02:20] ‫עם שני הדברים האלה,
[00:02:21 - 00:02:27] ‫ובשיעור הבא אנחנו בעצם כבר נתחיל ‫לעבוד על מודלים ממש, ‫עדיין לא מודלים עמוקים,
[00:02:29 - 00:02:34] ‫אבל מודלים שיצטרכו כל מיני ‫אלגוריתמים יותר מסובכים ‫בשביל ללמוד אותם.
[00:02:39 - 00:02:40] ‫-תשמעו את ההצעה למודלים.
[00:02:41 - 00:02:41] ‫מה?
[00:02:43 - 00:02:47] את השקפים לא, אבל הרוב על הלוח היום, ‫אז אני לא העליתי את זה.
[00:02:48 - 00:02:49] ‫אני לא יודע אם הרוב, אבל...
[00:02:51 - 00:03:08] ‫אוקיי, אז מה שנדבר היום, ‫נעשה את זה חזרה על מה שהיה לנו פעם שעברה, ‫היא עברה הרבה זמן מאז.
[00:03:10 - 00:03:20] ‫ואז בעצם אנחנו נתחיל את ה... ‫מה שאני רוצה שהיום יהיה... ‫הנושא בגדול של השיעור זה מבוא למודלים ממשתנים חבויים.
[00:03:21 - 00:03:27] ‫ואז אנחנו נדבר קצת על כמה דברים ‫יותר טכניים, ‫אחד זה מקסימום לייטליות, ‫שכולכם כבר אמורים להכיר,
[00:03:28 - 00:03:32] ‫שתיים, זה מדבר קצת יותר לעומק,
[00:03:33 - 00:03:36] ‫וגם כבר הייתם בעצם בתרגיל, ‫מודלים ליניאריים גאוסיאניים,
[00:03:38 - 00:03:43] ‫ואז אנחנו נדבר על כל מיני דברים ‫שקשורים לסטטיסטיקה ‫והעסקה בייזיאנית.
[00:03:47 - 00:03:47] אוקיי,
[00:03:48 - 00:03:50] אז תזכורת למה שהיה לנו פעם שעברה,
[00:03:50 - 00:03:52] ‫הגדרנו מה זה, סליחה, ‫מודלים גנרטיביים.
[00:03:53 - 00:03:56] ‫אז אמרנו, נתנו כמה דוגמאות כאלה ‫של כל מיני דברים,
[00:03:57 - 00:04:05] ‫ואמרנו שבגדול אפשר אולי להגדיר את זה ‫עם שתי התכונות האלה, ‫אחד זה שהאוטפוט הפלט של המודל שלנו ‫הוא לא סקלר אחד,
[00:04:06 - 00:04:17] ‫אם זה איזשהו סקור, ‫או אם זה איזה קלאס אנחנו נמצאים, ‫אלא משהו שהוא בממד הרבה יותר גבוה, ‫כמו תמונות או טקסט או סיגנל של אודיו,
[00:04:18 - 00:04:25] ‫והשני זה שאנחנו ניגשים לכל העניין הזה ‫בצורה הסתברותית, זאת אומרת, ‫אנחנו ממודלים את ההתפלגות של הדאטה,
[00:04:26 - 00:04:31] ‫ולא מנסים לחזות איזה משהו אחד ‫שהוא נכון לעומת משהו אחר שלא נכון.
[00:04:33 - 00:04:34] אוקיי,
[00:04:36 - 00:04:39] מה אפשר לעשות עם המודלים האלה?
[00:04:39 - 00:04:46] אז בעצם כבר פגשתם את זה ‫בקורס של Machine Learning, בבוא למשין Learning,
[00:04:47 - 00:04:47] ‫שאפשר,
[00:04:48 - 00:04:54] זה דרך לפתור כל מיני בעיות בצורה ישירה, ‫למשל Classifier, נכון? ‫אם אנחנו עושים את זה לסווג ‫בין שני קלאסים שונים,
[00:04:55 - 00:04:59] ‫אנחנו יכולים ללמוד מודל של הקלאס הראשון, ‫מודל של הקלאס השני,
[00:04:59 - 00:05:04] ‫ואז לפי זה, כשיש לנו דאטה חדש, ‫להחליט דאטה חדש ‫מאיזה מודל יותר הגיוני שהוא הגיע.
[00:05:05 - 00:05:07] נכון? ‫אז זה היה מה שקראנו בג'נרטיב, Classifier.
[00:05:08 - 00:05:14] אז זה דבר אחד. דבר שני, אנחנו יכולים, ‫זה הדוגמאות שראינו קודם, לייצר דאטה.
[00:05:14 - 00:05:17] נכון, אם יש לנו מודל הסתברותי, ‫אנחנו יכולים לדגום מתוך המודל הזה,
[00:05:18 - 00:05:23] ‫ובעצם לייצר דגימות חדשות ‫מההתפלגות שאנחנו חושבים ‫שההתפלגות הנכונה שמייצרת את הדאטה, ‫אז אנחנו יכולים
[00:05:24 - 00:05:25] להתייחס לזה כמו דאטה חדש,
[00:05:26 - 00:05:28] ‫שזה יכול להיות שימושי ‫לכל מיני דברים.
[00:05:29 - 00:05:33] נקודה שלישית שקצת דיברנו עליה ‫פעם שעברה, זה מה שנקרא ‫רפרזנטיישן לרנינג,
[00:05:34 - 00:05:38] ‫ללמוד ייצוג חדש של הדאטה ‫כדי לאפשר לעשות משהו אחר עם ה...
[00:05:39 - 00:05:53] ‫לעשות משהו אחר אחר כך עם הייצוג הזה. ‫למשל, אנחנו רוצים לא לייצג את הדאטה ‫בתור תמונות, בתור הפיקסלים של התמונות, ‫אלא להעביר אותו לאיזשהו ייצוג אחר, ‫שיהיה לנו יותר נוח ללמוד ‫על גבי זה קלאסיפיירים ‫או משהו בסגנון הזה.
[00:05:53 - 00:05:55] ‫זה נקרא רפרזנטיישן לרנינג.
[00:05:56 - 00:06:00] ‫עוד נקודה שאמרנו, שהמודלים האלה, ‫בגלל שאנחנו תופסים את ההסתברות של הדאטה,
[00:06:01 - 00:06:07] ‫הם נותנים לנו גם דרך לעשות פרדיקציה, ‫אבל גם איזשהו מדד של uncertainty,
[00:06:08 - 00:06:12] ‫זאת אומרת, איזשהו עד כמה אני יודע שאני יודע ‫ומה אני חושב שאני לא יודע,
[00:06:12 - 00:06:23] ‫וזה מאוד חשוב כשאנחנו רוצים ‫להשתמש בפרדיקציות האלה ‫כדי לא רק להגיד משהו עליהן, ‫אלא אחר כך לקחת החלטות. ‫למשל, אם יש כמה פרדיקציות ‫של כמה גורמים שונים,
[00:06:23 - 00:06:30] ‫אנחנו צריכים לקבל איזושהי החלטה, ‫אנחנו רוצים לדעת ‫לא רק מה הכי סביר שיקרה, ‫אלא באיזו סבירות ‫אנחנו חושבים שאנחנו צודקים.
[00:06:32 - 00:06:33] ‫אבל זה עוזר לתחום הזה.
[00:06:33 - 00:06:38] ופה הנקודה האחרונה היא אולי משהו מאוד כללי כזה שהוא רלוונטי להכל,
[00:06:38 - 00:06:41] זה שבעצם ברגע שיש לנו מודל כזה שהוא הסתברותי אנחנו יכולים
[00:06:41 - 00:06:45] להשתמש בו בתור איזושהי קומפוננטה בתוך איזושהי מערכת
[00:06:46 - 00:06:50] קבלת החלטות אפשר לקרוא לזה, או הסקה סטטיסטית באופן כללי,
[00:06:50 - 00:06:54] זה משהו שמאפשר לנו להכניס את הכל בתוך מערכת כללית
[00:06:54 - 00:06:59] כזאת ולהתייחס לכל בתור איזושהי מערכת הסתברותית שמאפשרת לנו לקבל
[00:06:59 - 00:07:03] החלטות ולעשות כל מיני חיזויים על כל מיני
[00:07:03 - 00:07:04] אלמנטים שונים של הדאטה.
[00:07:06 - 00:07:07] אתם שומעים טוב או שיש מזגן חזק מדי?
[00:07:09 - 00:07:10] נעים לכם?
[00:07:11 - 00:07:13] תודה רבה. אני גם חה וגם רב הוא סגן חזק מדי.
[00:07:30 - 00:07:31] על מודל דיסקרימנטיבי.
[00:07:33 - 00:07:37] לקלסיפיקציה עצמה אפשר להראות שלפעמים יש לזה יתרון.
[00:07:37 - 00:07:38] למשל,
[00:07:41 - 00:07:44] יכול להיות שזה יותר יעיל, אפשר עם פחות דאטה
[00:07:46 - 00:07:48] ללמוד את אותו דבר, להגיע לאותם ביצועים
[00:07:49 - 00:07:50] במקרים מסוימים.
[00:07:51 - 00:07:52] יכול להיות שזה יותר עמיד,
[00:07:53 - 00:07:55] זה יכול להיות יותר עמיד לשינויים.
[00:07:56 - 00:07:57] למשל, אני לא יודע אם אתם מכירים,
[00:07:58 - 00:07:59] אני חושב שדיברנו על זה קצת בקורס של Machine Learning,
[00:08:00 - 00:08:01] שאם אנחנו,
[00:08:01 - 00:08:04] אולי זה יותר רלוונטי לדיפ-לרנינג,
[00:08:04 - 00:08:08] קלאסיפיירים שנלמדים לסווג תמונות בצורה דיסקרימנטיבית,
[00:08:09 - 00:08:12] מאוד קל לעבוד עליהם.
[00:08:12 - 00:08:16] משנים איזה פיקסל קטן, אפשר למצוא איזשהו סיגנל שאפילו העין לא רואה בכלל,
[00:08:17 - 00:08:19] וזה גורם לקלאסיפייר להשתגע לגמרי.
[00:08:20 - 00:08:25] כשהמודל הוא גנרטיבי זה יותר קשה קצת, כי הוא מודל שצריך ממש לדעת איך תמונה אמורה להיראות,
[00:08:26 - 00:08:28] ולכן קצת יותר קשה לעבוד עליו
[00:08:29 - 00:08:29] במובן הזה.
[00:08:29 - 00:08:32] מודל דיסקרימנטיבי הוא מחפש את הקיצורי דרך כדי למצוא
[00:08:33 - 00:08:36] מה אפשר לעשות כדי לעשות קלאסיפיקציה כמו שצריך,
[00:08:36 - 00:08:38] ולכן הוא מוותר על הרבה מהדאטה.
[00:08:39 - 00:08:41] במובן הזה, זה מודלים שהם יותר יעילים מבחינת הדאטה.
[00:08:43 - 00:08:43] אבל לא תמיד.
[00:08:44 - 00:08:49] מצד שני, זה הרבה יותר קשה לאמן אותם, כי אתה מאמן אותם לעשות תחזית לפקוע על התמונה,
[00:08:50 - 00:08:56] לעשות בדרך כלל זה עם מקסימום לייטליות, למצוא את הפרמטרים הנכונים שהכי קרובים להתפנגות של התמונה,
[00:08:57 - 00:08:59] שזה משהו שהוא הרבה יותר מורכב הרבה פעמים מלעשות קלאסיפיקציה.
[00:09:06 - 00:09:06] אוקיי,
[00:09:07 - 00:09:09] אז איך אנחנו לומדים את המודלים האלה?
[00:09:12 - 00:09:17] אז ההנחות הבסיסיות הן כאלה, אנחנו מניחים שהדאטה שאנחנו ניגשים אליו נוצר על ידי איזושהי התפלגות,
[00:09:17 - 00:09:20] אנחנו מניחים שהעולם הוא גם, יש בו את ה...
[00:09:21 - 00:09:23] מדבר בשפה הזאת של הסתברויות,
[00:09:24 - 00:09:29] ויש הסתברות שהתפלגות שאנחנו קוראים לה p-data, שהיא ייצרה את הדוגמאות, את הדאטה שלנו,
[00:09:30 - 00:09:34] ואנחנו מחפשים פרמטרים תטא ככה שהמודל שלנו,
[00:09:34 - 00:09:36] שהוא יהיה p-tata,
[00:09:36 - 00:09:38] יהיה כמה שיותר קרוב ל-p-data,
[00:09:39 - 00:09:40] זאת המשימה שלנו.
[00:09:43 - 00:09:49] אז אפשר, אם הדאטה שלנו דו-ממדי אפשר לצייר את p-data או את p-tata בצורה הזאת,
[00:09:50 - 00:09:51] אוקיי? זו התפלגות
[00:09:52 - 00:09:55] על פני שני הממדים האלה.
[00:09:59 - 00:10:01] תטא זה לא הממדים של ההתפלגות,
[00:10:01 - 00:10:06] כן? זה לא הממדים פה של הדומיין של ההתפלגות.
[00:10:06 - 00:10:07] x פה הוא דו-ממדי,
[00:10:08 - 00:10:10] יש לי הסתברות התפלגות על x
[00:10:10 - 00:10:16] שהפרמטרים שלה הם תטא, אוקיי? אז נגיד שלמדתי את זה, נגיד שזה דו-ממדי, x הוא דו-ממדי,
[00:10:16 - 00:10:17] זה יכול להיות שההתפלגות נראית ככה.
[00:10:18 - 00:10:19] מה זה אומר? זה אומר שיש אזורים,
[00:10:20 - 00:10:20] זאת אומרת,
[00:10:20 - 00:10:24] xים מסוימים שמבחינתי יש להם הסתברות גבוהה
[00:10:24 - 00:10:26] ו-xים אחרים שיש להם הסתברות נמוכה.
[00:10:27 - 00:10:29] הייתי רוצה מתוך,
[00:10:29 - 00:10:31] אם ה-xים שלי זה בעצם תמונות,
[00:10:32 - 00:10:32] אוקיי? אז
[00:10:34 - 00:10:39] זה לא באמת דו-ממדי, זה הרבה פיקסלים, נגיד תמונות מיליון פיקסלים, זה מיליון ממדים,
[00:10:40 - 00:10:45] אז הייתי רוצה שבתוך המרחב המיליון-ממדי הזה יהיו אזורים עם הסתברות גבוהה ואזורים עם הסתברות נמוכה.
[00:10:46 - 00:10:49] והאזורים עם הסתברות גבוהה הייתי רוצה שהם יתאימו ל...
[00:10:49 - 00:10:50] אתם רואים את זה,
[00:10:51 - 00:10:54] תמונות שמבחינתי הן תמונות סבירות
[00:10:56 - 00:11:04] והאזורים עם ההצטברות הנמוכה מבחינתי זה משהו שהוא לא תמונה, האזור המרחב בדאטה שמבחינתי אני לא מחשיב אותו בתור תמונה
[00:11:05 - 00:11:11] וזה מאוד תלוי בדאטה שאימנתי אותו עליו, זאת אומרת אם אימנתי רק על
[00:11:12 - 00:11:13] תמונות של כלבים
[00:11:13 - 00:11:17] אז הייתי מצפה שפה או תמיד באזורים של הפיקים יהיו לי רק תמונות של כלבים
[00:11:18 - 00:11:23] ובאזורים אחרים יהיה גם דברים שנראים כמו רעש מוחלט אבל גם תמונות אולי של חתולים מבחינתי הן
[00:11:24 - 00:11:26] לא תמונות שהייתי רוצה שהמודל שלי יתפוס.
[00:11:29 - 00:11:33] זה תלוי בדאטה כבר, מה נמצא בהסתברות גבוהה ומה נמצא בהסתברות הממוחכת.
[00:11:35 - 00:11:39] אוקיי, אז זה בגדול ציור סכמטי כזה של מה שאנחנו לא רוצים.
[00:11:42 - 00:11:47] אוקיי, באמת יש לנו דיפ גם בשם של הקורס ואנחנו
[00:11:47 - 00:11:56] נגיע אליו רק בחלק השני של הקורס, בהתחלה אנחנו לא נתפס לזה, אבל בגלל שההסתברויות האלה הן יהיו כאלה מורכבות
[00:11:57 - 00:12:04] אז דרך נוחה למדל אותן, לתפוס אותן, זה כשהמודל הזה שמייצר את ההתפלגות של הדאטה
[00:12:05 - 00:12:07] הוא רשת נוורונים עמוקה.
[00:12:08 - 00:12:10] אוקיי, אז את זה אנחנו נראה בחלק השני של הקורס.
[00:12:11 - 00:12:16] אנחנו נצטרך את זה כדי שנצליח ללמוד בצורה יעילה מודלים כאלה מורכבים.
[00:12:17 - 00:12:22] אבל בינתיים אנחנו נעבוד עם מודלים יותר פשוטים
[00:12:23 - 00:12:30] ואנחנו מתחילים עם גאוסיאן, אוקיי? המודלים הפשוטים הם, מה זה אומר פשוטים? זה אומר שהם מודלים שאנחנו יכולים לעבוד איתם בצורה ישירה
[00:12:33 - 00:12:36] וכשהמודל שלנו גאוסיאן אנחנו ממש יכולים לעבוד עם זה בצורה אנליטית לגמרי.
[00:12:37 - 00:12:40] זה טוב לתרגילי בית שצריך לפתור בצורה אנליטית
[00:12:41 - 00:12:45] וזה טוב גם באמת כשרוצים לדעת שאנחנו מוצאים את הפתרון
[00:12:45 - 00:12:49] כמו שצריך ולא מסתבכים עם איזה אלגוריתם שאנחנו לא יודעים מה קורה איתו.
[00:12:50 - 00:12:51] ואנחנו בהמשך אנחנו נראה,
[00:12:52 - 00:12:53] אנחנו נתחיל לסבך את המודלים שלנו,
[00:12:54 - 00:13:06] עדיין בשלב הראשון של הקורס במרחב כזה שאנחנו יכולים לעבוד איתם בצורה ידנית נקרא לזה, אוקיי? זה לא יהיה אנליטית אבל עדיין אנחנו נוכל לעשות את החישובים בעצמנו.
[00:13:09 - 00:13:14] ובהמשך אנחנו פשוט נסתמך על מודלים עמוקים
[00:13:14 - 00:13:17] וגרדיאנט דיסנט וכל מה שבא עם Deep Learning.
[00:13:19 - 00:13:21] אוקיי, אז מה אנחנו יודעים על הגאוסיאן?
[00:13:22 - 00:13:28] זה היה לכם גם בשיעור בבית וגם עשינו קצת בשיעור שעבר,
[00:13:28 - 00:13:31] נכון? אז קודם כל הגאוסיאן זה הנוסחה שלו,
[00:13:32 - 00:13:32] זה
[00:13:35 - 00:13:36] פונקציית הצפיפות,
[00:13:36 - 00:13:37] הגאוסיאן,
[00:13:37 - 00:13:38] יש לנו אקספונט,
[00:13:38 - 00:13:42] והדבר הזה שנמצא באקספונט אנחנו קוראים לו מנוביס דיסטנס,
[00:13:42 - 00:13:44] זה בעצם סוג של מרחק בין X
[00:13:45 - 00:13:46] לאיזשהו מרכז,
[00:13:46 - 00:13:49] מיו, שמיו במקרה הזה הוא גם התוחלת,
[00:13:50 - 00:13:51] הוא גם ה-Mode,
[00:13:51 - 00:13:53] מוד זה הנקודה עם ההסתדרות הכי גבוהה,
[00:13:54 - 00:13:57] והוא גם התוחלת במקרה של גאוסיאן,
[00:13:57 - 00:13:59] גם חד-למדי וגם רב-למדי,
[00:13:59 - 00:14:05] אז זה מה שמופיע פה ומפונה, ויש לנו כאן משהו שהוא מין מקדם נרמול, הוא לא תלוי ב-X,
[00:14:05 - 00:14:06] נכון? אז זה לא כזה משנה לנו,
[00:14:07 - 00:14:09] כי X משתנה הדבר הזה נשאר תמיד קבוע,
[00:14:10 - 00:14:13] אבל הוא דואג לזה שהאינטגרל של כל הדבר הזה יהיה שווה אחד, נכון?
[00:14:13 - 00:14:14] אז אנחנו לא צריכים
[00:14:15 - 00:14:16] להסתדרות בזה.
[00:14:17 - 00:14:17] אוקיי,
[00:14:18 - 00:14:19] וכל מיני תכונות שראינו,
[00:14:19 - 00:14:20] חלק עשינו בשיעור,
[00:14:20 - 00:14:21] חלק אתם הוכחתם,
[00:14:22 - 00:14:26] שאם אנחנו עושים טרנספורמציה אפינית של גאוסיאן X,
[00:14:27 - 00:14:28] אז זה נשאר גאוסיאן.
[00:14:30 - 00:14:33] אם אנחנו מסתכלים על גאוסיאן רב-למדי, אז
[00:14:36 - 00:14:38] ההתפלגות השולית על חלק מהממדים
[00:14:39 - 00:14:42] היא גם גאוסיאן, זה מימד יותר נמוך.
[00:14:43 - 00:14:49] ואם אנחנו בהתפלגות רב-למדית של גאוסיאנים, ואנחנו מסתכלים על התפלגות מותנית,
[00:14:50 - 00:14:51] אז ההתפלגות המותנית הזאת היא גם גאוסיאנית.
[00:14:53 - 00:14:54] אוקיי?
[00:14:56 - 00:14:58] עוד משהו ש...
[00:15:00 - 00:15:01] תגיד לי עוד דבר שאני רוצה להגיד.
[00:15:04 - 00:15:07] אה, סיימתי היה לי קצת דרך, קצת הסתבכתי איתם בשיעור, נכון? אז
[00:15:08 - 00:15:11] אפשר להסתכל על ה... עדיין כאילו בדו-ממד,
[00:15:12 - 00:15:13] אבל זה בעצם, אני מצייר את מה שמצויר פה,
[00:15:14 - 00:15:15] רק בהסתכלות מלמעלה,
[00:15:15 - 00:15:16] אוקיי? זה קווי הגובה.
[00:15:18 - 00:15:18] זאת אומרת,
[00:15:19 - 00:15:24] האזור הזה הוא האזור עם ההסתברות, עם הצפיפות של ההסתברות הכי גבוהה, נכון? לפחות שאני מתרחק מהמרכז,
[00:15:24 - 00:15:25] הצפיפות יורדת.
[00:15:25 - 00:15:28] אוקיי? אז בגאוסיאן קווי הגובה הם תמיד נראים ככה אליפסות.
[00:15:30 - 00:15:34] ומשהו מיוחד שאמרנו זה שהצירים ה...
[00:15:35 - 00:15:37] זה נקרא הצירים העיקריים של ה...
[00:15:38 - 00:15:39] של האליפסה הזאת,
[00:15:40 - 00:15:41] הכיוונים האלה,
[00:15:41 - 00:15:42] יש שני וקטורים,
[00:15:42 - 00:15:42] אוקיי?
[00:15:43 - 00:15:43] כל בתיור שלי הוא דו-ממד,
[00:15:44 - 00:15:45] אבל הוא מתעקף גם לממדים יותר גבוהים.
[00:15:46 - 00:15:47] אז יש לי פה שני וקטורים,
[00:15:48 - 00:15:50] כל וקטור הוא בדו-ממד,
[00:15:51 - 00:15:54] אז זה בדיוק הווקטורים העצמיים של מטריצת הקוווריאנס,
[00:15:55 - 00:15:55] סיגמה.
[00:15:56 - 00:15:58] סיגמה היא מטריצת הקווריאנס, היא בגודל,
[00:15:59 - 00:16:02] כאן המימד מצומן בתור k, הוא בגודל k על k,
[00:16:04 - 00:16:07] אז יש לנו k וקטורים עצמיים,
[00:16:08 - 00:16:09] שכל אחד הוא בממד k.
[00:16:10 - 00:16:12] אם זה שתיים, אז היו לי שני וקטורים עצמיים במימד שתיים,
[00:16:13 - 00:16:16] זה מה שציירתי כאן, אבל גם במימד יותר גבוה היו לי k וקטורים עצמיים במימד k.
[00:16:18 - 00:16:18] אוקיי, אז זה משהו...
[00:16:29 - 00:16:32] אז הווקטורים האלה הם הווקטורים העצמיים,
[00:16:33 - 00:16:34] זאת אומרת,
[00:16:34 - 00:16:35] קודם כל פה,
[00:16:35 - 00:16:39] מבחינה גיאומטרית זה הצירים העיקריים של האליפסה,
[00:16:41 - 00:16:46] ומבחינה איך שהם קשורים למודל שלנו הם הווקטורים העצמיים של סיגמה.
[00:16:49 - 00:16:51] אוקיי, אז מה זה אומר שאם אני מסתכל על הוקטור העצמי הכי גדול,
[00:16:52 - 00:16:54] זה הכיוון שההסתברות קטנה הכי לאט.
[00:16:56 - 00:16:59] אם אני נמצא כאן בנקודה עם ההסתברות הכי גבוהה,
[00:16:59 - 00:17:05] אם אני אתקדם כאן אז ההסתברות תלך ותקטן בצורה יותר איטית מאשר אם אני אלך בכיוון הזה.
[00:17:06 - 00:17:09] אוקיי, אז הערך העצמי בעצם אומר מה הגודל כאן,
[00:17:09 - 00:17:16] הוקטור העצמי צחצר אותו בצורה אחידה, אבל הגודל כאן מתאים בעצם לערך העצמי של הוקטור העצמי.
[00:17:21 - 00:17:22] אוקיי,
[00:17:22 - 00:17:27] גם אם נסתכל סתם איזושהי אינטואיציה גרפית כזו, זה טוב שיהיה לגאוסיאנים,
[00:17:27 - 00:17:29] אז אם נסתכל בעצם על ההתפלגות השולית,
[00:17:30 - 00:17:33] אז אם אני קורא לדבר הזה x1,
[00:17:34 - 00:17:35] לדבר הזה x2,
[00:17:36 - 00:17:39] אני יכול להסתכל על ההתפלגות השולית, זאת אומרת מה זה התפלגות השולית, זה איך
[00:17:39 - 00:17:42] אם אני עושה אינטגרציה של כל x2,
[00:17:43 - 00:17:45] איך שזה ההטלה שיוצאת לי על x1,
[00:17:46 - 00:17:46] אוקיי?
[00:17:47 - 00:17:51] אז ההטלה הזאת היא גם תהיה באוסיאן חד-ממדי,
[00:17:52 - 00:17:53] איך שציירתי את זה, כי זה דיוק ממדי,
[00:17:53 - 00:17:54] הרדתי לחד-ממדי,
[00:17:55 - 00:18:01] ואם זה היה ממדים k והייתי עושה התפלגות שולית של חצי מהממדים האלה,
[00:18:01 - 00:18:04] אז היה לי, שוב, באוסיאן רב-ממדי בהתפלגות יותר ממוכה.
[00:18:05 - 00:18:15] אז זה נקודה אחת, והתפלגות מותנית זה נגיד עבור ערך מסוים של x2, נגיד אני רוצה את ההתפלגות של x1 בהינתן שx2 שווה 5,
[00:18:17 - 00:18:20] אז בעצם להסתכל על האזור הזה,
[00:18:20 - 00:18:24] אני חותך על החתך הזה של האליפסה אדומה-מדי,
[00:18:25 - 00:18:28] וזה גם באוסיאן, אוקיי? אז הדאטה כאן
[00:18:31 - 00:18:31] גם יהיה באוסיאן.
[00:18:36 - 00:18:37] זאת אומרת, האזור הזה יהיה לו הסתברות יותר גבוהה,
[00:18:38 - 00:18:41] זה ילך ויידאך, זה בדיוק מתאים לגאוסיאן. זה מה שהוכחנו בעצם בשיעור שעברנו.
[00:18:42 - 00:18:44] כן, בדרך כלל לא אהבתי לראות את זה.
[00:18:45 - 00:18:47] ומה שאתם עשיתם בשיעור זה ה...
[00:18:48 - 00:18:50] סליחה, בתרגיל.
[00:18:53 - 00:18:58] עשיתם בעצם שני... לא, רגע. עשיתם את ה-afine transform, או שעשיתם סכום של גאוסיאן, אם אני עכשיו מבולבל?
[00:18:59 - 00:18:59] מה?
[00:19:00 - 00:19:01] את ה-afine transform.
[00:19:01 - 00:19:04] אוקיי, אבל אני חושב ש...
[00:19:04 - 00:19:08] זאת אומרת, הסוף שלכם בעצם גם הראה שסכום של גאוסיאנים,
[00:19:09 - 00:19:12] כן, עשיתם גם של סכום, עשיתם את שניהם, עשיתם עוד תכונה,
[00:19:12 - 00:19:13] שאם יש לי...
[00:19:14 - 00:19:14] איך הוא גדול?
[00:19:16 - 00:19:17] הרוח פה קצת מוצומצם.
[00:19:19 - 00:19:21] לא, אני אשאיר את זה.
[00:19:22 - 00:19:23] אם יש לי
[00:19:26 - 00:19:28] x שהוא גאוסיאן,
[00:19:28 - 00:19:29] כן, מתפלא,
[00:19:30 - 00:19:32] לא יודע שהוא מי הוא בסיגמר,
[00:19:33 - 00:19:38] ויש לי y ששווה ל-x ועוד
[00:19:40 - 00:19:41] אחרון הזה שם, לא זוכר,
[00:19:41 - 00:19:42] אתא או משהו כזה,
[00:19:43 - 00:19:45] x ועוד אתא, שאתא הוא גם
[00:19:48 - 00:19:48] התפלג
[00:19:49 - 00:19:50] גאוסיאן,
[00:19:50 - 00:19:52] גאוסיאן שתיים וסיגמר שתיים,
[00:19:53 - 00:19:54] אז y הוא גם גאוסיאנים,
[00:19:54 - 00:19:56] נכון? גם את זה הוכחתם בתרגיל,
[00:19:57 - 00:19:57] בתרגיל האחרון,
[00:19:58 - 00:20:01] אוקיי? אז זה עוד תכונה. זאת אומרת, סכום של גאוסיאנים,
[00:20:01 - 00:20:03] אני חושב שאולי בעצם היה לכם את זה גם פה, עוד איזה a.
[00:20:04 - 00:20:12] ה-mue שלה היה אפס, כן, זה קצת פישוט, אבל באופן כללי זה נכון גם ככה.
[00:20:15 - 00:20:18] זה בעצם הלינארי, אז אתם יודעים שזה גאוסיאן מקודם,
[00:20:19 - 00:20:21] והוספתם לזה עוד גאוסיאן, וזה נשאר גאוסיאן.
[00:20:23 - 00:20:28] אוקיי? אז עוד דרך, אוכפתם את זה כבר, אבל דרך גרפית להסתכל על זה,
[00:20:28 - 00:20:31] נסתבך פה עם הצבעים.
[00:20:43 - 00:20:49] בעצם הסכום של הגאוסיאנים,
[00:20:52 - 00:20:54] אפשר לחשוב על זה גם בתור סוג של התפלגות מותנית.
[00:20:55 - 00:20:58] אוקיי? זה כל ההסתברויות, ככה ש...
[00:20:59 - 00:21:03] ההסתברות ש-x ועוד y שווים לאיזשהו ערך,
[00:21:04 - 00:21:10] זה בעצם כל המקומות שבהם הערכים של x ועוד y שווים לאיזשהו ערך, אז אפשר גם לחשוב על זה בתור איזשהו חיתוך כאן,
[00:21:11 - 00:21:13] ולכן זה גם יהיה גאוסיאני,
[00:21:13 - 00:21:16] אוקיי? זו אינטואיציה גרפית כזאת של מה שהוכחתם.
[00:21:17 - 00:21:18] אוקיי? באופן כללי,
[00:21:19 - 00:21:22] גאוסיאנים ודברים לינאריים וגאוסיאנים,
[00:21:24 - 00:21:28] להלתבה זה שהכל נשאר גאוסיאני ושאפשר לפתור הכל אנליטית.
[00:21:28 - 00:21:35] יש לי מודל מאוד מורכב, שמורכב מפעולות לינאריות ומשתנים גאוסיאנים,
[00:21:35 - 00:21:36] הכל נשאר גאוסיאני.
[00:21:37 - 00:21:39] ההתפלגות המשותפת, ההתפלגות מותנית, ההתפלגות שולית,
[00:21:40 - 00:21:41] הכל נשאר גאוסיאני.
[00:21:42 - 00:21:43] אוקיי.
[00:21:44 - 00:21:46] טוב, בואו נמשיך.
[00:21:49 - 00:21:54] זה גם עדיין היה בפעם שעברה, מה אנחנו צריכים בשביל ללמוד מודל גנרטיבי?
[00:21:55 - 00:21:57] אנחנו צריכים דאטה קודם כל, שמייצג לנו את המרחב
[00:21:58 - 00:22:00] ההסתברות הזאת שאנחנו רוצים לתפוס.
[00:22:02 - 00:22:06] אנחנו צריכים להכפיל איזה מודל אנחנו עובדים, אז למשל גאוסיאן,
[00:22:06 - 00:22:10] ראינו עכשיו, אנחנו נדבר בשבוע הבא, נדבר על תערובת של גאוסיאנים,
[00:22:11 - 00:22:21] ואנחנו מתחילים גם היום, גם תערובת של גאוסיאנים, זה סוג של משתנה עם מודלים משתנה מקרי חבוי.
[00:22:21 - 00:22:23] אם משתנה חבוי זה.
[00:22:23 - 00:22:27] וגם חלק מהמודלים העמוקים שנעשה,
[00:22:27 - 00:22:30] הם יהיו תחת ההגדרה הזאת של לייטנט פאריגונס.
[00:22:33 - 00:22:35] אז זה המודל שאנחנו רוצים לעבוד איתו,
[00:22:35 - 00:22:39] ואז אנחנו צריכים להחליט איך אנחנו מאמנים את המודל, זה ה-objective function שלנו,
[00:22:40 - 00:22:42] אנחנו כבר היום נדבר על מקסימום לייקליות,
[00:22:42 - 00:22:44] זה יהיה רוב הקורס בעצם על מקסימום לייקליות,
[00:22:45 - 00:22:49] ובשלב מסוים אנחנו נדבר על כל מיני קירובים של מקסימום לייקליות,
[00:22:49 - 00:22:50] וגם על score-matching,
[00:22:50 - 00:22:52] שפשוטה קצת שונה.
[00:22:53 - 00:22:56] במובנים מסוימים גם אפשר לחשוב עליה בתור איזשהו קירוב למקסימום לייקליות.
[00:22:58 - 00:23:02] והדבר הרביעי זה איך אנחנו עושים את האופטימיזציה עצמה,
[00:23:02 - 00:23:06] במקרה של גרסיאן שוב אפשר לפתור הכל בצורה אנליטית, אז אין לנו את השלב הרביעי הזה,
[00:23:07 - 00:23:12] אבל משבוע הבא יהיה לנו בעצם אלגוריתמים שאנחנו לא יכולים לפתור בצורה אנליטית,
[00:23:13 - 00:23:17] אנחנו צריכים איזשהו תהליך של אופטימיזציה, איזשהו אלגוריתם איטרטיבי
[00:23:18 - 00:23:18] שעושה אופטימיזציה.
[00:23:20 - 00:23:24] אוקיי, בהמשך נדבר גם על דברים שהם יותר מורכבים.
[00:23:25 - 00:23:38] אוקיי, מה האתגר פה? למה זה קורס שלהם הדבר הזה? אז בעצם מה שדיברנו שבוע, פעם שעברה, זה שיש לנו את מה שקראנו כללת המימד, נכון? אם אנחנו רוצים פשוט מודל
[00:23:39 - 00:23:40] שתופס את כל האפשרויות
[00:23:41 - 00:23:43] על דאטה שהוא בממד גבוה,
[00:23:43 - 00:23:44] אנחנו די מהר,
[00:23:45 - 00:23:50] מספר האפשרויות האלה יגדל בצורה אקספוננציאלית ואין לנו שום סיכוי לתפוס את ההסתברות המלאה על כל האפשרויות האלה,
[00:23:51 - 00:23:54] ואנחנו צריכים איזושהי דרך לייעל את הדבר הזה.
[00:23:55 - 00:23:56] בעצם לתפוס עם פחות פרמטרים,
[00:23:57 - 00:23:59] לתפוס את כל האפשרויות אבל עם פחות פרמטרים.
[00:24:01 - 00:24:07] אז על זה בעצם הקורס, ואנחנו מחפשים איזשהו מודל שיהיה יותר יעיל,
[00:24:08 - 00:24:10] אבל שיאפשר לנו גם לעשות את הדברים האלה בצורה יעילה.
[00:24:12 - 00:24:12] אופטימיזציה.
[00:24:13 - 00:24:19] אחת מהשיטות שדיברנו עליהן באופן כללי זה conditional independence,
[00:24:19 - 00:24:22] להכניס איזשהו מבנה שאומר אוקיי המשתנה הזה הוא,
[00:24:22 - 00:24:25] יש לי אומנם הרבה משתנים אבל לא כולם תלויים בכולם בצורה מלאה,
[00:24:26 - 00:24:28] אלא אני רוצה להניח כל מיני אי תלויות
[00:24:28 - 00:24:36] בתוך המודל שלי וככה אני נפטר מכמות הפרמטרים ולאט לאט יכול לצמצם אותם למספר שאני יכול לעבוד איתו.
[00:24:40 - 00:24:44] כשאני עשיתי תואר ראשון אז יש קורס, עשיתי קורס שלם בדבר הזה,
[00:24:44 - 00:24:49] התחום הזה באופן כללי נקרא בודלים גרפיים הסתברותיים,
[00:24:49 - 00:24:51] יש ספרים על זה וזה תחום שלם,
[00:24:52 - 00:24:57] איך לבנות את הדבר הזה בדרך כלל אנחנו חושבים על זה בתור איזשהו גרף
[00:24:58 - 00:25:01] ואיך לבנות את זה בצורה שהיא תהיה יעילה ומועילה.
[00:25:03 - 00:25:15] זה תחום שלם, אנחנו פחות ניכנס לזה אבל אנחנו גם בעצם נשתמש בתכונה הזאת שאנחנו נצמצם את מספר הפרמטרים על ידי זה שאנחנו נניח אי תלויות.
[00:25:16 - 00:25:19] אוקיי, אז בואו נדבר על זה,
[00:25:20 - 00:25:26] כל זה היה שיעור שעבר אבל עכשיו בואו נתחיל לדבר. נגיד שאנחנו רוצים פשוט
[00:25:27 - 00:25:29] ללמוד גאוסיין של תמונות,
[00:25:30 - 00:25:31] יש לנו תמונה,
[00:25:33 - 00:25:35] יש לנו הרבה תמונות, אמרתי שגאוסיין הכל טוב,
[00:25:36 - 00:25:37] אין לנו בעיות,
[00:25:38 - 00:25:43] אז למה לא ללמוד פשוט גאוסיין שתופס את ההתפלגות של התמונות?
[00:25:46 - 00:25:48] זה הרבה מימדים,
[00:25:49 - 00:25:51] וספציפית מה הבעיה עם גאוסיין?
[00:25:59 - 00:26:02] איפה אנחנו נתקלים בבעיה כשאנחנו עוסקים לגאוסיין?
[00:26:07 - 00:26:12] נכון אז הקובעה אם x נגיד הוא תמונה בגודל מיליון פיקסלים
[00:26:14 - 00:26:15] אז מי הוא זה אולי לא מורה,
[00:26:15 - 00:26:19] זה יהיה לנו ותוחלת של מי הוא זה יהיה מיליון, אנחנו יכולים ללמוד את זה, תכף נראה איך אנחנו עושים את זה
[00:26:21 - 00:26:23] מתוכנים אבל איך אנחנו יכולים מתוך הרבה xים
[00:26:24 - 00:26:25] למצוא את ה-new,
[00:26:26 - 00:26:30] ובעצם עיקרון יש לנו גם שיטה למצוא את הסיגמה הכי טובה,
[00:26:31 - 00:26:33] רק הבעיה שהסיגמה זה תהיה בגודל מיליון על מיליון,
[00:26:34 - 00:26:35] אז כבר בתמונות,
[00:26:36 - 00:26:38] בגודל מיליון אנחנו כנראה לא נצליח
[00:26:40 - 00:26:41] לתפוס את זה בצורה מלאה.
[00:26:41 - 00:26:43] בצורה מלאה.
[00:26:44 - 00:26:46] כן, נצטרך לפשט את המודל הזה.
[00:26:47 - 00:26:49] אוקיי,
[00:26:49 - 00:26:51] אז זאת סיבה אחת שאנחנו לא רוצים ללמוד
[00:26:52 - 00:26:55] גאופיין תמונות,
[00:26:56 - 00:26:57] אבל יש עוד סיבה, מה הסיבה השנייה?
[00:27:05 - 00:27:07] נסגרנו ממש עליין, אני לא בטוח שזה לאומי.
[00:27:08 - 00:27:12] נכון, אז זה בדיוק בסיבה השנייה,
[00:27:12 - 00:27:19] שגאופיין זה מודל נוח לעבוד איתו אבל הוא רע מאוד לתמונות, הוא בבירור לא מודל של תמונה,
[00:27:20 - 00:27:22] אוקיי, זה השתי הסיבות שכתבתי כאן.
[00:27:25 - 00:27:26] אני רק אדגיר את זה בעצם,
[00:27:27 - 00:27:30] גאופיין, בדיוק לממד, גאופיין נראה ככה, נכון?
[00:27:30 - 00:27:36] ואפילו בהדגמה הפשוטה הזאת שעשיתי לכם אני חושב שההסתברות של תמונות או משהו מסובך נראה ככה, אוקיי?
[00:27:37 - 00:27:38] זה לא גאופיין,
[00:27:39 - 00:27:42] אוקיי? זה בדו-ממד, שבו יש לנו מיליון ממדים,
[00:27:43 - 00:27:53] אנחנו רוצים איזושהי התפלגות כזאת שתופסת את כל הדברים, יש פתאום אזור מסוים עם הרבה דברים, יש אזור אחר שהוא יותר רחב אולי, אבל יש בו הסתברות יותר נמוכה,
[00:27:54 - 00:28:00] יש הרבה פיקים בכל מיני מקומות מוזרים, גאופיין, לא משנה מה הממד שלו, תמיד כאילו יהיה
[00:28:02 - 00:28:04] בצורה זאת, נכון? תמיד זה יהיה פעמון כזה,
[00:28:05 - 00:28:06] אנחנו לא יכולים לדמיין איך הפעמון הזה נראה
[00:28:06 - 00:28:09] בממד גבוה אבל זה יהיה פעמון,
[00:28:09 - 00:28:16] זאת אומרת יש איזו התפלגות שהיא יוני מודלית, זאת אומרת יש לה מוד אחד של נקודה אחת של מקסימום ומשם הכל יורד,
[00:28:17 - 00:28:17] אוקיי?
[00:28:17 - 00:28:21] זה די ברור שתמונות אנחנו לא יכולים לתפוס עם מודל כזה פשוט.
[00:28:23 - 00:28:24] אנחנו רוצים מודלים יותר,
[00:28:25 - 00:28:29] גם מצד אחד יותר מורכב מגאופיין אבל גם מצד שני שיהיה בו פחות פרמטרים,
[00:28:29 - 00:28:35] כי אפילו עם גאופיין אנחנו לא יכולים באופן ישיר ללמוד על תמונות בממד גבוה.
[00:28:37 - 00:28:38] אוקיי.
[00:28:44 - 00:28:50] אז אנחנו רוצים לעשות איזה משהו כזה, טריקים שראינו שבוע שעבר, נכון? אנחנו במקום לתפוס את כל ההסתגרות,
[00:28:50 - 00:28:54] כל ההתפלגות על כל הממדים שלנו,
[00:28:54 - 00:28:56] נגיד לפרק את זה עם כלל השרשרת,
[00:28:57 - 00:29:02] אנחנו יכולים לפרק את זה ככה, ואז לעשות כל מיני הנחות,
[00:29:04 - 00:29:05] אפשר לקרוא להן מרקוביות, הנחות שאמורות, אוקיי,
[00:29:06 - 00:29:07] הפיקסל ה-N,
[00:29:07 - 00:29:08] פיקסל 3,
[00:29:09 - 00:29:10] כבר תלוי בפיקסל 2,
[00:29:11 - 00:29:13] או בהינתן פיקסל 2, הוא כבר לא תלוי בפיקסל 1.
[00:29:14 - 00:29:19] אז אנחנו יכולים כל פיקסל להגיד בהינתן חלק מהפיקסלים,
[00:29:19 - 00:29:21] אנחנו לא צריכים לדעת את כל שאר הפיקסלים,
[00:29:22 - 00:29:23] לעשות כל מיני הנחות כאלה.
[00:29:25 - 00:29:29] איך הייתם בונים כזה דבר לתמונה, יש לכם איזה רעיון?
[00:29:30 - 00:29:34] איזה אי-תלויות הייתם מניחים בשביל תמונה? נתתי כבר פחות או יותר לכיוון.
[00:29:36 - 00:29:47] אמרתם משהו?
[00:29:52 - 00:29:54] כן אז להתעלם מפיקסלים רחוקים נגיד
[00:29:55 - 00:29:55] אז
[00:29:57 - 00:30:01] באמת אפשר לחשוב נגיד על תמונה אנחנו מסדרים אותה ככה יש לנו הרבה פיקסלים
[00:30:36 - 00:30:42] בפיקסלים שכבר הוא תלוי בו כי אז הוא עושה לנו מעגל בתוך המדגרף
[00:30:43 - 00:30:48] אנחנו צריכים שהדבר הזה יהיה תלוי לאיזשהו תהליך של ייצור פיקסל אחרי פיקסל
[00:30:49 - 00:30:56] אנחנו צריכים למצוא איזשהו סדר של הפיקסלים בתמונות נגיד אולי תמיד הפיקסל הזה נוצר ראשון ואז הפיקסל הזה נוצר
[00:30:57 - 00:30:58] בינתן הקודם
[00:30:58 - 00:31:03] ואז ככה יש לנו איזושהי סביבה אולי שהיא אם היינו פשוט פורסים ככה את הכל ככה
[00:31:03 - 00:31:08] שהפיקסל האחרון כאן תלוי בכל שאר הפיקסלים לא היינו חוסכים בכלל פרמטרים
[00:31:08 - 00:31:15] אבל אם בתוך כל אחד מהשלבים הזה היינו אומרים אוקיי בהינתן איזושהי סביבה הוא כבר לא תלוי במה שיש יותר רחוק
[00:31:16 - 00:31:24] אז כבר אנחנו מתחילים לייעל את המודלים שלנו ואנחנו יכולים כבר לעבוד עם זה זה מודל שאפשר לעבוד איתו
[00:31:25 - 00:31:33] ויש מודל כזה אז אנחנו נראה אותו בהמשך מודל שלא רק שהוא כזה כמו שאמרתי אלא בכל נקודה בתמונה
[00:31:33 - 00:31:42] זה בדיוק אותו מודל שעובד אז כל נקודה הפיקסל הזה הוא תלוי באיזשהו שכנים באיזשהו גודל שנמצאים בפינה
[00:31:43 - 00:31:43] שמאלית עליונה
[00:31:45 - 00:31:48] וזה בדיוק אותו מודל אוקיי אז יש לי את המודל הזה
[00:31:48 - 00:31:51] ולעבור הפיקסל הזה זה בדיוק אותו מודל רק כשמסתכל על
[00:31:53 - 00:31:55] פיקסלים אחרים
[00:31:57 - 00:32:02] אז כל פיקסל ככה ואז אנחנו יכולים פשוט לפי הסדר לייצר ככה את כל הפיקסלים
[00:32:02 - 00:32:04] בהתחלה יש לנו כל מיני תנאי קצה שהוא לא רואה את ה...
[00:32:06 - 00:32:11] המודל לא רואה את מה שיש מעל אבל לא נורא, יש כל מיני דרכים להתמודד.
[00:32:12 - 00:32:14] אז זה נקרא מודל אוטו-רגרסיבי,
[00:32:14 - 00:32:15] כזה שהוא סנוורטי,
[00:32:16 - 00:32:21] מודל אוטו-רגרסיבי ואנחנו נראה, זה יהיה המודל הראשון שאתם תממשו
[00:32:22 - 00:32:26] ונדבר עליו כשאנחנו נעבור למודלים העמוקים.
[00:32:27 - 00:32:30] אז כל דבר כזה יהיה בעצם רשת נוירונים שמסתכלת על הפיקסלים הקודמים,
[00:32:32 - 00:32:35] ובהינתן כל הפיקסלים הקודמים, נותן לנו התפלגות לפיקסל הבא.
[00:32:37 - 00:32:42] זה לא נורא, זה פשוט כל פעם להסתכל על הרבה דאטה ולפי זה לתת לנו התפלגות על פיקסל אחד.
[00:32:43 - 00:32:50] זה יכול להיות מודל מאוד מורכב עם הרבה פרמטרים אבל עדיין זה לא יגיע לסיבוכיות שאנחנו צריכים בשביל ממש לתפוס את כל הדאטה בלי לעשות את ההנחות האלה.
[00:32:51 - 00:32:55] וזה מודל כבר די טוב, שאפשר לייצר איתו תמונות שנראות די טוב.
[00:32:57 - 00:32:58] מה הבעיה עם המודל הזה?
[00:32:58 - 00:33:00] יש כמה בעיות,
[00:33:00 - 00:33:01] שוב אנחנו נדבר עליו,
[00:33:02 - 00:33:10] על המימוש שלו בהמשך אבל באופן עקרוני זה לא נראה כמו מודל כזה נכון של הדאטה.
[00:33:11 - 00:33:12] היינו רוצים איכשהו שהמודל
[00:33:13 - 00:33:15] שלנו הוא איכשהו יהיה קשור למודל
[00:33:16 - 00:33:19] שיש בו משהו הגיוני של איך דאטה, איך התמונות נוצרות,
[00:33:20 - 00:33:23] לאו דווקא ממש פיזיקלי אבל לפחות קונספטואלית,
[00:33:24 - 00:33:26] מה גורם לתמונות להיראות כמו שהן נראות,
[00:33:27 - 00:33:28] היינו רוצים משהו שהוא קצת יותר,
[00:33:29 - 00:33:31] יש בו קצת יותר היגיון ופה אנחנו סתם מסדרים
[00:33:31 - 00:33:37] את הפיקסלים בפי איזשהו סדר ומחליטים שהפיקסל הזה תלוי בזה אבל זה לא תלוי בזה.
[00:33:38 - 00:33:40] יש פה משהו קצת שרירותי,
[00:33:41 - 00:33:46] אז זו נקודה אחת שבגלל פחות אוהבים את המודלים מהסוג הזה,
[00:33:47 - 00:33:51] נקודה שנייה זה שלוקח די הרבה זמן לדגום מתוך המודל הזה,
[00:33:52 - 00:33:57] אנחנו רציתם בעצם לדגום פיקסל-פיקסל, דווקא לאמן אותו אפשר לאמן בצורה מאוד יעילה,
[00:33:58 - 00:34:01] אנחנו נראה את זה אבל לדגום מתוכו אחר כך זה מאוד איטי,
[00:34:01 - 00:34:05] צריכים פיקסל-פיקסל לעבור ויש לנו מיליון פיקסלים, אז צריכים בשביל לייצר תמונה,
[00:34:06 - 00:34:09] אנחנו צריכים לולאה עם מיליון איתרציות רק בשביל לייצר תמונה אחת.
[00:34:11 - 00:34:17] אוקיי, אז זה, אבל זו שיטה שבעצם זו השיטה הראשונה שהצליחו איתה לייצר תמונות שכבר שנראו די טוב.
[00:34:24 - 00:34:30] שיטה אחרת שאנחנו נדבר עליה היום זה שיטה של משתנה
[00:34:31 - 00:34:41] חבוי, משתנה חבוי, בעצם אנחנו מתייחסים לכל הפיקסלים האלה בתור משתנים שאנחנו רואים אותה, הדאטה שלנו זה התמונות,
[00:34:42 - 00:34:44] אנחנו רואים את כל הפיקסלים האלה,
[00:34:45 - 00:34:46] אבל פה הטריק,
[00:34:47 - 00:34:49] אני מחכתי פה את זה במקום לצייר עוד דבר חדש.
[00:34:54 - 00:34:56] לדבר ולמחוק את זה גם.
[00:35:01 - 00:35:09] פה הטריק הוא במקום להניח איזשהו מבנה של אי-תלות על המשתנים שאנחנו רואים,
[00:35:11 - 00:35:16] פשוט להוסיף משתנים ולהניח שיש איזשהו מבנה של אי-תלות שתלוי במשתנים שאנחנו לא רואים.
[00:35:17 - 00:35:20] זה הרעיון ב-Latent Variable Models.
[00:35:24 - 00:35:26] אז אנחנו מניחים שיש לנו עוד משתנה,
[00:35:26 - 00:35:28] נגיד פה איזה משתנה,
[00:35:28 - 00:35:31] נמצא אותו בניגוד, אולי כמה משתנים,
[00:35:32 - 00:35:35] שהוא חבוי, אנחנו לא רואים אותו בדאטה. הדאטה שלנו מורכב מהרבה תמונות,
[00:35:36 - 00:35:37] אנחנו רואים את כל המשתנים האלה,
[00:35:38 - 00:35:41] אנחנו לא רואים, לכל תמונה יש גם משתנה כזה שאנחנו לא רואים.
[00:35:44 - 00:35:46] אוקיי? אז ברגע שיש לנו את המשתנה הזה אנחנו יכולים
[00:35:46 - 00:35:49] להשתמש בו כדי לעשות כל מיני הנחות אי-תלות.
[00:35:49 - 00:35:50] אז למשל,
[00:35:51 - 00:35:52] מה אנחנו יכולים להגיד?
[00:35:53 - 00:35:54] אנחנו יכולים
[00:35:54 - 00:36:04] להניח את ההנחה שהיא שקולה לגרף הזה, אנחנו קצת לא דיברנו על איך הגרפים מייצגים את האי-תלות,
[00:36:05 - 00:36:06] ממש בקצרה דיברנו על זה בשיעור שעבר, אבל
[00:36:07 - 00:36:12] זה אינטואיטיבי, אז אני מקווה שתצליחו להבין, אבל נגיד אם אני מצייר את הגרף הזה,
[00:36:13 - 00:36:15] מרוחצים לכל הנקודות,
[00:36:18 - 00:36:19] אתה בסוף,
[00:36:20 - 00:36:21] אז מה זה אומר?
[00:36:25 - 00:36:31] נכון, אבל מה שרלוונטי כאן זה מה החוסר,
[00:36:31 - 00:36:33] הכל תלוי בהכל תמיד, באופן עקרוני,
[00:36:34 - 00:36:35] אבל מה לא תלוי במה?
[00:36:36 - 00:36:37] בהינתן מה?
[00:36:38 - 00:36:39] זה אי-תלות מותנית,
[00:36:41 - 00:36:42] מה שהמשחק כאן זה אי-תלות מותנית,
[00:36:44 - 00:36:46] בעצם מה שהגרף הזה אומר
[00:36:47 - 00:36:52] זה שהפיקסל הזה לא תלוי בכל שאר הפיקסלים,
[00:36:52 - 00:36:54] בהינתן המשתנה החבוי,
[00:36:55 - 00:36:57] אוקיי? אז המשתנה החבוי הוא קובע
[00:36:58 - 00:36:59] איך יראו,
[00:36:59 - 00:37:03] בעצם הוא קובע את ההחלטות הגלובליות נעשות פה,
[00:37:04 - 00:37:06] אחר כך יש לכל הפיקסל איזשהו חופש
[00:37:06 - 00:37:11] מסוים אבל לא גדול מדי, אוקיי? אז למשל אם זה תמונה של פנים,
[00:37:13 - 00:37:16] כל הדברים שהיינו חושבים שהם חשובים בפנים,
[00:37:16 - 00:37:18] הם נקבעים במשתנה הזה,
[00:37:18 - 00:37:20] ציירתי כאן משתנה אחד, אז זה יכול להיות קודם כל כמה משתנים,
[00:37:21 - 00:37:26] או שאתה יכול להיות משתנה גם עם מימד די גבוה, נגיד מימד אלף, אוקיי? יש לנו מיליון פיקסלים, אבל כאן יש לנו
[00:37:26 - 00:37:27] משתנה ממימד אלף,
[00:37:28 - 00:37:35] ונגיד שחלק ממנו יהיה הצבע של השיער, האורך של השיער, צבע העיניים, זגל או אישה, אם יש משקפיים,
[00:37:35 - 00:37:36] אין משקפיים,
[00:37:37 - 00:37:38] אם יש זקן, אין זקן,
[00:37:39 - 00:37:41] כל הדברים האלה בגדול נקבעים כאן,
[00:37:42 - 00:37:45] ואז כל מה שנשאר לכל פיקסל, יש איזשהו קצת משחק,
[00:37:45 - 00:37:50] קצת הבדלים קטנים בגבוה, או קצת רעש נוסף בתמונה הזאת,
[00:37:51 - 00:37:55] אז בעצם זה הופך להיות הייצוג העיקרי שלנו של הדאטה,
[00:37:56 - 00:37:56] פה הכל נקבע,
[00:37:57 - 00:38:05] כאן יהיה לנו איזשהו מודל הסתברותי שיהיה מאוד מורכב, ואחר כך זה פשוט ממנו התמונה נוצרת,
[00:38:05 - 00:38:10] אוקיי? אפשר לחשוב על זה גם באיזושהי דרך על איזשהו סוג של הורדת מימד של התמונה,
[00:38:10 - 00:38:13] אוקיי? אנחנו מעבירים את הבעיה למשהו בממד קצת יותר נמוך,
[00:38:13 - 00:38:16] אנחנו יכולים עדיין, כשיכולים לעבוד איתו בצורה יותר מורכבת,
[00:38:16 - 00:38:20] ואז בהינתן זה אנחנו מניחים איזושהי מפה כזאת של חוסר תלויות,
[00:38:21 - 00:38:26] והתמונה שלנו נוצרת ככה מתוך המשתנה החדולה.
[00:38:43 - 00:38:48] ‫לא, אז יש פה, הדבר הזה באופן כללי, כשיש לנו גרפים כאלה,
[00:38:48 - 00:38:49] ‫הוא מקום נורמלי.
[00:38:51 - 00:38:54] ‫הדוגמה שהיתנו בשיעור שעבר הייתה לנו, לא יודע, איזה משהו נדבר נגיד?
[00:39:05 - 00:39:06] ‫אוקיי? אם יש לנו גרף שנראה ככה,
[00:39:07 - 00:39:09] אוקיי?
[00:39:10 - 00:39:10] ‫אז נגיד,
[00:39:11 - 00:39:17] ‫המשתנה הזה הוא תלוי בעצם בשני אלה, ‫אבל בהינתן שני אלה,
[00:39:18 - 00:39:19] ‫הוא לא תלוי בשני אלה.
[00:39:20 - 00:39:20] ‫אוקיי?
[00:39:22 - 00:39:22] ‫-תלויין שני אלה?
[00:39:23 - 00:39:25] ‫כן, בהינתן ההורים שלו,
[00:39:26 - 00:39:27] ‫הוא לא תלוי בכל השאר.
[00:39:27 - 00:39:29] ‫בגבי זה המבנה הכללי שבגבי.
[00:39:30 - 00:39:31] ‫כן, אבל איזה גם אם זה חבוי דתי.
[00:39:32 - 00:39:33] ‫איזה מה?
[00:39:33 - 00:39:34] חבוי.
[00:39:34 - 00:39:37] ‫איזה לא רואים אותו?
[00:39:38 - 00:39:39] ‫אז קודם מצב כללי,
[00:39:40 - 00:39:43] ‫עכשיו אתה יכול להחליט בדאטה שלך, ‫איפה אתה מוסיף משתנה חבוי.
[00:39:44 - 00:39:47] ‫הדאטה שלנו הוא נראה כבר ככה, ‫יש לנו הרבה משתנים שזה הפיצרים.
[00:39:48 - 00:39:49] ‫אנחנו נוסיף משתנים חבויים,
[00:39:50 - 00:39:53] ‫ואנחנו יכולים להניח כל מיני הנחות ‫במודל שלנו, ‫מה תלוי ומה.
[00:40:04 - 00:40:08] ‫אתה לא יכול להוסיף משתנה ‫ולהגיד שהוא לא יהיה חבוי.
[00:40:08 - 00:40:10] ‫אתה לא רואה? אני נוסף את אותו, ‫אז הוא לא נמצא בדאטה.
[00:40:11 - 00:40:13] ‫המשתנה שאתה מוסיף הוא חבוי, נכון?
[00:40:14 - 00:40:17] ‫ואז אתה צריך להחליט ‫אבל איך הוא מתחבר לשאר המשתנים.
[00:40:18 - 00:40:19] ‫הייתי יכול להגיד
[00:40:20 - 00:40:22] ‫שיש לי משתנה חבוי, ‫אבל שעדיין הכל תלוי בהכול,
[00:40:23 - 00:40:24] ‫אז זה לא חוסך לי כלום.
[00:40:24 - 00:40:31] ‫אחת מהסיבות העיקריות ‫להוסיף משתנה חבוי זה כדי להשתמש בו, ‫כדי לבנות את המבנה הזה ‫של החוסר תלות מותנית.
[00:40:38 - 00:40:47] ‫אוקיי, אפשר לשאול את זה ‫במימד אחד, זה יותר ברור ‫אם יש לי איזשהו דאטה ‫שנראה ככה, פי, ‫שהוא רצף של נקודות.
[00:40:49 - 00:40:52] ‫צריך לציין קודם ככה, ‫הדאטה שלי הוא כזה דבר,
[00:40:53 - 00:40:57] ‫רצף של טיקסלים, נכון? ‫רצף של נגיד שזה משפט, ‫רצף של מילים.
[00:40:58 - 00:41:04] ‫אני יכול להניח שיש לי פה ‫איזשהו דאטה במימד 5, ‫שאני צריך ללמוד את כל התלויות ‫של אחד בשני, הכל תלוי אחד בשני.
[00:41:05 - 00:41:07] ‫מודל אחד יותר פשוט זה כזה מודל, ‫שאומר, אוקיי,
[00:41:08 - 00:41:10] ‫זה מודל מרקובי כזה.
[00:41:10 - 00:41:15] ‫מה המודל הזה אומר? ‫שאני מפרק את זה לפי כבר השרשרת.
[00:41:16 - 00:41:20] ‫אבל אני מתעלה, כשאני מנסה לייצר ‫את הפיקסל הזה,
[00:41:21 - 00:41:25] ‫את המשתנה הזה, אני אומר ‫שבהינתן המשתנה הקודם ‫הוא בלתי תלוי במשתנה הראשון.
[00:41:27 - 00:41:31] ‫למה שמודל? ‫הוא בהינתן החורה שלו, ‫בלתי תלוי בשאר הפיקסלים.
[00:41:33 - 00:41:36] בסדר? וככה. ‫אז זה מפשש לי את המודל, ‫אבל זה אולי לא מספיק טוב.
[00:41:38 - 00:41:41] ‫דרך אחרת זה להגיד, אוקיי, ‫נוסף לי כאן משתנה חדש.
[00:41:43 - 00:41:45] ‫אני יכול להתתמשך את המשתנים שלי,
[00:41:46 - 00:41:49] ‫ובמקום להניח את ההנחה הזאתי, ‫אני מניח שנוסף לי כאן משתנה חדש,
[00:41:51 - 00:41:51] ‫ועכשיו
[00:41:52 - 00:41:53] המודל שלי נראה ככה.
[00:41:56 - 00:41:59] ‫זה מבנה אחר.
[00:42:00 - 00:42:01] זה מבנה אחד
[00:42:01 - 00:42:02] של חוסר תלויות,
[00:42:03 - 00:42:05] ‫וזה מבנה אחר. ‫עכשיו אני צריך ללמוד
[00:42:06 - 00:42:07] מודל אחר, ‫מודל כזה שומר, בהינתן
[00:42:08 - 00:42:10] קודם כול מה ההתפלגות של המשתנה החבוי הזה,
[00:42:11 - 00:42:16] ‫ואז בהינתן המשתנה החבוי, ‫מה ההתפלגות של כל אחד מהם. ‫כל חץ כזה אני יכול ללמוד בעצם בנפרד.
[00:42:17 - 00:42:19] ‫כמו שפה כל חץ אני יכול ללמוד בנפרד.
[00:42:21 - 00:42:22] בסדר? אבל זה פשוט מבנה אחר.
[00:42:26 - 00:42:26] בסדר?
[00:42:28 - 00:42:35] תחשבו על זה קצת, ‫אנחנו עדיין לא ממש נכנסים לזה, ‫זו הקדמה לעניין הזה, אנחנו עוד כמה שערים, ‫נראה ממש דוגמאות של המודלים.
[00:42:36 - 00:42:37] ‫ומה שאני רוצה שכן תבינו,
[00:42:38 - 00:42:41] ‫פחות ממש את המבנה של הגרף, ‫זה
[00:42:42 - 00:42:46] את המטרה של להוסיף משתנה חבוי. ‫המטרה של להוסיף משתנה חבוי
[00:42:46 - 00:42:48] זה דרך לפשט את המודל,
[00:42:49 - 00:42:55] ‫שהיא עוזרת לנו גם, היא יותר טבעית במובן הזה, ‫היא יותר קשה, ‫כי יש לנו משתנה חבוי שאנחנו לא רואים,
[00:42:56 - 00:43:01] ‫אבל יותר טבעית כי היא מאפשרת לנו ‫לבנות איזשהו מודל שהמבנה שלו הוא יותר,
[00:43:02 - 00:43:04] אנחנו חושבים שהוא יותר נכון ‫לתופס את הדאטה יותר טוב,
[00:43:04 - 00:43:07] ‫ולכן גם יש סיכוי שיהיה יותר יעיל ללמוד אותו.
[00:43:08 - 00:43:13] למשל כמו במודל הזה של התמונות, אנחנו מניחים אוקיי, יש איזשהו, איך התמונה הזאת נוצרה,
[00:43:13 - 00:43:18] יש איזשהו משתנה שמגדיר איזה בן אדם יש שם,
[00:43:19 - 00:43:20] מה צבע העור שלו,
[00:43:20 - 00:43:22] איך העיניים שלו נראות, איך השיער שלו נראה,
[00:43:23 - 00:43:24] וזה מייצר לי את התמונה,
[00:43:25 - 00:43:29] אוקיי? אז זה בעצם, אני אומר, ככה אני רוצה לבנות את המודל שלי גם,
[00:43:29 - 00:43:31] אני רוצה שיהיה איזה משהו שיגיד,
[00:43:31 - 00:43:33] אוקיי, יש הסתברות של חצי-חצי שזה יהיה גבר או אישה,
[00:43:34 - 00:43:37] יש הסתברות שכזאתי שהשיער שלו יהיה מטומטל, יש הסתברות כזאת,
[00:43:38 - 00:43:40] נבנה ככה את המודל, ובהינתן כל הדברים האלה,
[00:43:41 - 00:43:46] התמונה נוצרת. אז אני רוצה ללמוד את כל השלבים האלה בתהליך הזה.
[00:43:46 - 00:43:48] המודל שלי ממש יתפוס את כל השלבים האלה בתהליך.
[00:43:49 - 00:43:54] זה נשמע כמו משהו יותר סביר מאשר שהתמונה נוצרה על ידי זה שיש את הפיקסל הראשון, ואז הפיקסל השני הוא
[00:43:54 - 00:43:59] לפי הפיקסל הראשון, הוא ממשיך אותו, ואז הפיקסל השלישי ממשיך אותו, וככה הלאה נוצרת התמונה.
[00:43:59 - 00:44:03] זה בעצם ההבדל בין הגישה הזאתי לגישה הזאתי
[00:44:29 - 00:44:36] אז ההחצים האלה באמת יצטרכו להיות כנראה לא את המודל בכל אחד מהחצים האלה
[00:44:36 - 00:44:41] נגיד זה יהיה חצי שיגיד איך נראה הפיקסל מספר 54 בהינתן ה...
[00:44:43 - 00:44:54] גם הכללי בין הפיקסל של ידו כן אז יכול למשל כן מה שאתה אומר זה גם יכול להיות נגיד זה יכול להיות תלוי בזה וגם בשלושת אלה של ידו
[00:44:54 - 00:44:56] בדיוק כן זה גם משהו שקוראים
[00:44:57 - 00:44:59] אפשר לחשוב על כל מיני מודלים
[00:45:13 - 00:45:18] אז כמו שאמרתי יש לזה אינטרפטציה קצת יותר טבעית, הגיונית
[00:45:21 - 00:45:24] קצת קשה בדיוק לכמת את זה אבל יש סיבות להאמין
[00:45:24 - 00:45:27] שזה גורם לזה שהמודלים יהיו יותר טובים
[00:45:31 - 00:45:33] אז זאת דרך בעצם
[00:45:36 - 00:45:42] אנחנו נראה בדיוק איך משבוע הבא לפתור את הבעיות שהיה לנו קודם עם גאוסיאן
[00:45:42 - 00:45:44] אז בעצם אנחנו יכולים לבנות ככה
[00:45:45 - 00:45:47] מודל שהוא לא גאוסיאני,
[00:45:47 - 00:45:51] זאת אומרת הוא יהיה יותר מורכב מגאוסיאן מצד אחד, מצד שני הוא יהיה עדיין יעיל
[00:45:52 - 00:45:54] אנחנו נוכל לחשב אותו בצורה יעילה
[00:45:55 - 00:45:59] לא יהיה לו מספר הפרמטרים שלו, לא יהיה לו קוואריאנס מלא על מיליון על מיליון
[00:46:00 - 00:46:01] אוקיי, איך?
[00:46:01 - 00:46:03] אם נגיד יהיה לנו כאן נגיד משהו בגודל,
[00:46:04 - 00:46:06] משתנה מקרה בגודל 100,
[00:46:07 - 00:46:09] נגיד אנחנו יכולים ללמוד גאוסיאן עליו,
[00:46:10 - 00:46:11] אז הוא יהיה בגודל
[00:46:11 - 00:46:13] מעל 100 הגאוסיאן, אז זה סביר,
[00:46:14 - 00:46:19] ועכשיו בהינתן מה שיצא פה אנחנו נלמד את החצים האלה שנגיד
[00:46:19 - 00:46:22] כל אחד מהם אולי יהיה קשר ביניהם באמת
[00:46:22 - 00:46:26] אבל כל אחד מהם יהיה רק נגיד גאוסיאן
[00:46:27 - 00:46:31] עם תוחלת אחת וווריאנס לגודל 1
[00:46:32 - 00:46:32] אוקיי?
[00:46:33 - 00:46:35] הוא מסתכל, גאוסיאן מותנה.
[00:46:35 - 00:46:37] כן, המשתנה החבוי הרבה פעמים קוראים Z,
[00:46:38 - 00:46:43] אז פה כל פיקסל יהיה איזשהו מודל של P-XI בהינתן Z,
[00:46:45 - 00:46:49] אז יהיה איזשהו גאוסיאן שהתוחלת והווריאנס שלו יהיו תלויים ב-Z
[00:46:52 - 00:46:55] בסדר, ואז זה לא יהיה פה משהו שהוא בגודל של קובריאנס מלא
[00:46:58 - 00:47:00] ולכן יהיה לנו מודל שהוא לא גאוסיאן
[00:47:03 - 00:47:04] והוא יותר יעיל,
[00:47:05 - 00:47:05] יש לו פחות פרמטרים,
[00:47:06 - 00:47:06] כן.
[00:47:11 - 00:47:11] הגודל?
[00:47:13 - 00:47:15] כן, הוא, כן.
[00:47:16 - 00:47:16] אנחנו
[00:47:22 - 00:47:28] הרבה פעמים פשוט מספיק לכתוב את זה בתור משתנה מקרי אחד עם איזשהו מימד,
[00:47:29 - 00:47:37] כאילו לא להכניס בתוכו מבנה, אבל לפעמים אתה רוצה שזה יהיה משהו ממש עם איזשהו מבנה, משהו היררכי, נגיד שיש Z1, Z2,
[00:47:37 - 00:47:39] זאת אומרת, Z3, Z4, ואז זה מייצר את התמונה.
[00:47:40 - 00:47:42] יש גם מודלים שהם ממש היררכיים כאלה,
[00:47:43 - 00:47:45] יש להם הרבה דרגות של משתנים חבויים.
[00:47:48 - 00:47:50] כן, אין כל כך, השאלה גם מה רוצים לעשות,
[00:47:50 - 00:47:55] הראינו כל מיני דברים, למשל, שאנחנו רוצים לעשות עם מודלים גנרטיביים, אם אתה רק רוצה לייצר,
[00:47:56 - 00:47:59] לפעמים מעדיף לך מבנה מסוים, אם אתה רוצה,
[00:47:59 - 00:48:06] מה שקראה מרוב פרזנטיישן לרנינג, אתה רוצה ללמוד איזשהו ייצוג טוב של הדאטה, אז המשתנה המקרה הזה, למשל, יכול להיות הייצוג של הדאטה שלך.
[00:48:08 - 00:48:09] תהיו אולי,
[00:48:09 - 00:48:13] פה, כמו שאמרנו, הוא אומר לך כבר מראש כל מיני דברים שמעניינים לך בתמונה,
[00:48:14 - 00:48:15] אז אם אתה רוצה ללמוד קלאסיפייר שהוא,
[00:48:17 - 00:48:17] לא יודע אם זה גבר או אישה,
[00:48:18 - 00:48:22] ויכול להיות שהאינפוט שלו זה Z ולא התמונה.
[00:48:25 - 00:48:28] אז זה תלוי גם במה שאתה רוצה לעשות אחר כך עם המודל.
[00:48:33 - 00:48:34] היה לי עוד משהו להגיד. אה,
[00:48:34 - 00:48:37] מה קורה עם המודל הזה, אם המודל של Z הוא גאוסיאן?
[00:48:39 - 00:48:39] ZP של Z
[00:48:47 - 00:48:58] אם P של Z הוא גאוסיאן בעצמו, אוקיי?
[00:48:59 - 00:49:05] ונגיד שהדבר הזה הוא איזושהי התוחלת של X תהיה פונקציה ליניארית ב-Z.
[00:49:08 - 00:49:09] אז מה יקרה במצב כזה?
[00:49:14 - 00:49:15] טוב או לא טוב?
[00:49:18 - 00:49:21] אם התוחלת של XI היא איזושהי פונקציה ליניארית של Z.
[00:49:25 - 00:49:26] ושהוא גם יהיה גאוסיאן.
[00:49:28 - 00:49:33] אז זה לא טוב, למה? כי מה שיקרה, כמו שאמרתי לכם קודם, אם הכל פה זה גאוסיאנים ודברים ליניאריים,
[00:49:34 - 00:49:37] אז אנחנו נשארים גאוסיאנים. אז גם ההסתברות של X
[00:49:38 - 00:49:38] היא גם גאוסיאן.
[00:49:39 - 00:49:43] אם ההסתברות של כל XI הוא גאוסיאן, אז סך הכל יש התפלגות גאוסיאנית על התמונה.
[00:49:45 - 00:49:46] אוקיי? זה לא...
[00:49:47 - 00:49:49] זה לא טוב, כנראה, כי אנחנו רוצים,
[00:49:49 - 00:49:52] המטרה שלנו כאן היא לבנות איזושהי התפלגות שהיא יותר מורכבת.
[00:49:53 - 00:49:57] אז אנחנו נרצה שהדבר הזה לא יהיה פונקציה ליניארית של Z.
[00:49:58 - 00:50:00] לא יהיה פשוט גאוסיאן שהתוחלת לא תהיה פונקציה ליניארית של Z,
[00:50:01 - 00:50:03] אלא משהו קצת יותר מורכב, למשל איזה רשת נוירונים.
[00:50:07 - 00:50:07] טוב,
[00:50:10 - 00:50:11] נגיע לזה בהמשך.
[00:50:13 - 00:50:15] אז זה שבוע הבא,
[00:50:15 - 00:50:21] היום מה שאנחנו נתחיל לדבר זה בעצם באופן קונספטואלי, מה זה אומר שיש לנו משתנה כזה חבוי?
[00:50:22 - 00:50:24] איך אנחנו ניגשים לאימון של כזה דבר?
[00:50:29 - 00:50:29] טוב,
[00:50:30 - 00:50:31] אז
[00:50:33 - 00:50:38] אנחנו רוצים לאמן דאטה באופן כללי, נכון,
[00:50:39 - 00:50:44] תשכחו רגע מהמשתנים החבויים וזה, אוקיי? אנחנו עושים קצת חזרה עכשיו עוד פעם על
[00:50:44 - 00:50:47] תחום שלא עשינו פעם שעברה,
[00:50:47 - 00:50:49] יש לנו מודל הסתברותי, אנחנו רוצים לאמן אותו,
[00:50:50 - 00:50:52] אוקיי? מה זה אומר לאמן מודל הסתברותי? אז שוב,
[00:50:53 - 00:50:57] אנחנו מניחים שיש לנו איזשהו מודל עם פרמטרים לא ידועים,
[00:50:58 - 00:50:58] קוראים להם תטא,
[00:50:59 - 00:51:04] אנחנו מניחים שיש לנו דאטה שנוצר מאיזושהי התפלגות של Tp דאטה,
[00:51:05 - 00:51:08] אנחנו מחפשים את התטא ככה ששניהם יהיו הכי קרובים,
[00:51:09 - 00:51:11] איך עושים את זה?
[00:51:11 - 00:51:16] כפי שהוא מכיר שיטה, השיטה הסטנדרטית זה מקסימום לייקליות,
[00:51:16 - 00:51:17] אוקיי?
[00:51:18 - 00:51:20] מקסימום לייקליות, אתם זוכרים איך זה נראה?
[00:51:21 - 00:51:23] בואו נעשה איזה שתי דוגמאות,
[00:51:23 - 00:51:26] מקסימום לייקליות לברנולי, להטלת מטבע,
[00:51:27 - 00:51:30] אוקיי? אז מה הפרמטר שלי כאן? זה ההסתברות שהמטבע יוצא-הדס,
[00:51:32 - 00:51:34] נגיד שיש לי איזשהו דאטה,
[00:51:34 - 00:51:37] הדאטה של איזה D,
[00:51:38 - 00:51:40] הרבה הטלות מטבע, נגיד שיצא לי
[00:51:40 - 00:51:41] ‫אחד,
[00:51:41 - 00:51:42] ‫אחד, אחד, אקסטנדרפוינטים,
[00:51:43 - 00:51:43] ‫אחד,
[00:51:44 - 00:51:44] ‫אחד,
[00:51:47 - 00:51:48] ‫שבע,
[00:51:49 - 00:51:49] ‫שבע,
[00:51:49 - 00:51:53] ‫זה היה עשר הטלות שעשיתי.
[00:51:54 - 00:51:55] מה היה המקסימום לייקליות?
[00:51:55 - 00:51:56] ‫את הפרמטר תטא?
[00:51:57 - 00:51:58] ‫שישי.
[00:51:58 - 00:51:58] כן?
[00:51:59 - 00:52:01] אתם רואים את זה? מה הפונקציית הלייקליות?
[00:52:03 - 00:52:07] ‫תשישי ותשישי ותשישי, מה?
[00:52:08 - 00:52:09] ‫תשישי ותשישי.
[00:52:09 - 00:52:12] ‫כן, אז אתם נקרא לזה פה תטא, ‫אז תטא בשלישית,
[00:52:13 - 00:52:17] ‫בוא נקרא לזה באופן כללי, ‫תטא בחזקת n1, ‫וכמה פעמים יוצא לי 1?
[00:52:18 - 00:52:24] ‫כפול 1 מינוס תטא בחזקת n גדול, ‫נגיד פחות n1.
[00:52:25 - 00:52:28] ‫אוקיי? זה מספר הטלות, 10, בוא נקרא לזה, פחות שתי פשוט.
[00:52:30 - 00:52:32] ‫בהינתן תטא ההסתברות, מה זה וייקליות? ‫בהינתן תטא,
[00:52:33 - 00:52:35] מה ההסתברות לראות את הדאטה שאני רואה?
[00:52:36 - 00:52:36] נכון?
[00:52:37 - 00:52:38] ‫בהינתן תטא, מה ההסתברות לראות את ה...
[00:52:39 - 00:52:39] ‫הבדלה הזאת,
[00:52:40 - 00:52:44] ‫זה שלוש פעמים תטא ושבע פעמים חל במסתברות.
[00:52:45 - 00:52:47] ‫אני אומר פעמים, אבל זה חזקות.
[00:52:48 - 00:52:49] ‫שלוש פעמים שאני מכפיל את תטא,
[00:52:50 - 00:52:53] ‫שלוש פעמים שאני מכפיל... ‫שבע פעמים שאני מכפיל את 1 מינוס תטא.
[00:52:53 - 00:52:56] ‫זה ה-likelihood, ‫אנחנו רוצים למצוא את התטא ‫שממקסם את זה,
[00:52:57 - 00:52:58] ‫נכון? זה מקסימום likelihood,
[00:52:59 - 00:53:01] ‫זה איך עובדים עם לוג, זה יותר נוח,
[00:53:01 - 00:53:02] ‫אז זה לוג-likelihood,
[00:53:04 - 00:53:04] נכון?
[00:53:06 - 00:53:08] לוג-likelihood,
[00:53:08 - 00:53:19] ‫תטא שווה לוג-תטא, שווה לוג-תטא, נכון?
[00:53:22 - 00:53:22] ‫עוד
[00:53:24 - 00:53:26] n כול-n1,
[00:53:26 - 00:53:31] ‫שווה לוג-t1 כנוס תטא.
[00:53:33 - 00:53:35] ‫אני רוצה לגזור את זה
[00:53:35 - 00:53:38] ‫זה קול-t,
[00:53:40 - 00:53:41] ‫מה העובדות של זה?
[00:53:45 - 00:53:49] ‫n1 בקול-t, נכון? ‫אם נגזרת של לוג זה 1 פחית k או 3 פחית לוג,
[00:53:50 - 00:53:54] ‫ועוד n פחות n פחית k,
[00:53:56 - 00:54:00] מה שיש פה, פחות כמו לנגזרת הפנימית אבל, ‫מהנגזרת הפנימית?
[00:54:01 - 00:54:02] ‫מינוס אישיות, נכון?
[00:54:03 - 00:54:04] ‫אז ככה,
[00:54:05 - 00:54:07] ‫אנחנו רוצים לדעת מתי זה שווה ל-0.
[00:54:10 - 00:54:11] ‫זה כש...
[00:54:14 - 00:54:15] ‫נכון? מתי זה שווה ל-0?
[00:54:16 - 00:54:27] ‫כשהעובדים פה פשוט, נכון? ‫אז n1 בגבול פלוגוס תטא שווה ל-n אחות n
[00:54:30 - 00:54:31] ‫בגבול ו...
[00:54:35 - 00:54:48] ‫אז יש לנו 1 מינוס תטא, ‫אז יש לנו 1 מינוס תטא, ‫וגם פה נכון מינוס תטא. ‫אנחנו נשארים פה עם n1 שווה n
[00:54:49 - 00:54:51] ‫תטא שווה
[00:54:52 - 00:54:53] ככה k k,
[00:54:55 - 00:54:55] ‫אוקיי?
[00:54:55 - 00:55:02] ‫אז זה המקסימום רטיון במקרה של מרמוזיקי, ‫זה אומר מרמוזיקי, שלוש קרקעי s,
[00:55:03 - 00:55:03] ‫כן, שלוש קרקעי s,
[00:55:05 - 00:55:17] ‫לא, אני חושב שיש לנו תשע מספרים. ‫-אה, אוקיי, זה ברור?
[00:55:23 - 00:55:25] ‫אוקיי, אז זה מקסימום לייטליות,
[00:55:26 - 00:55:27] ‫זה היה קרקע.
[00:55:31 - 00:55:33] ‫זה עובד לכם שם עם האור והחושך?
[00:55:33 - 00:55:34] ‫אתם רואים?
[00:55:34 - 00:55:37] בסדר, בסדר, תנאים קשיים.
[00:55:38 - 00:55:40] ‫אוקיי, עכשיו מה קורה בגרסיאן?
[00:55:40 - 00:55:41] ‫מה הפרמטרים שלנו?
[00:55:52 - 00:55:53] ‫מה הפרמטרים של גרסיאן?
[00:55:55 - 00:55:59] ‫כן, יש לנו את התוחלת ואת סיגמא, ‫מי וסיגמא, אז
[00:56:00 - 00:56:02] ‫קודם כול, מה הלוג-לייק להיות?
[00:56:04 - 00:56:19] ‫אוקיי, הלוג-לייקלי יו, ‫אז אני צריך לעשות לוג של הדבר הזה.
[00:56:20 - 00:56:20] אוקיי, ויש לי,
[00:56:21 - 00:56:24] ‫בהינתן שאני ראיתי הרבה נקודות, ‫הרבה איקסים,
[00:56:25 - 00:56:28] ‫אז יהיה לי סכום על כל האיקסים.
[00:56:35 - 00:56:41] ‫איקס איי שבין 1 עד n. ‫אחרי שימו לב שכל איקס פה ‫שאני כותב זה וקטור,
[00:56:42 - 00:56:45] ‫אוקיי? זה לא האיקסים שאתם רואים כאן, ‫כאן זה הממדים של איקס,
[00:56:45 - 00:56:47] ‫הממדים השונים של הוקטור חד של איקס.
[00:56:49 - 00:56:51] ‫כאן זה יש לי איקס איי מ-1 עד n,
[00:56:51 - 00:56:54] ‫אני אעשה את האיי למעלה, ‫זה נלך כאילו יותר ברור.
[00:56:55 - 00:56:55] זה לא אותו דבר.
[00:56:57 - 00:56:59] אה... של לוג של הדבר הזה.
[00:57:01 - 00:57:03] אוקיי? מה זה לוג של הדבר הזה? ‫אז האקספוננט הוא פשוט,
[00:57:04 - 00:57:07] ‫אותו דבר רגע מי אקספוננט, נכון? ‫אז יש לי מינוס חצי
[00:57:09 - 00:57:13] איקס מינוס מיור, ‫אקס איי מינוס מיור,
[00:57:16 - 00:57:17] ‫אם לא מינוס 1,
[00:57:18 - 00:57:21] אקס מינוס מיור,
[00:57:22 - 00:57:25] ‫אקס פוסט, אוקיי? זה מה שכתוב ב-X פוננט.
[00:57:26 - 00:57:29] ‫עוד מה שכתוב במונה... במכנה,
[00:57:30 - 00:57:32] ‫אז אפשר לעשות פחות, כי זה במכנה.
[00:57:33 - 00:57:34] פחות
[00:57:34 - 00:57:36] ‫שורש זה בעצם כמו חצי,
[00:57:36 - 00:57:37] ‫יש לי כאן חצי...
[00:57:42 - 00:57:44] ‫בוא נתחיל ממה להשיג לו דווקא, ‫חצי לוג
[00:57:47 - 00:57:48] ‫דטרמיננטה וסיגמא,
[00:57:51 - 00:57:51] ‫אוקיי?
[00:57:52 - 00:57:53] ‫ומה נשאר לי?
[00:57:54 - 00:57:59] ‫פחות חצי לוג
[00:58:01 - 00:58:02] שני פאי,
[00:58:02 - 00:58:07] ‫חזקת K, אז אפשר לחזקת K חלקי שתיים, ‫לוג שני פאי, בסדר?
[00:58:07 - 00:58:11] ‫אז זה הלוג של פונקציה הזאתי, ‫יבואו הרבה איקסים.
[00:58:12 - 00:58:16] ‫זה הלוג של מכפלה של הדבר הזה ‫עבור הרבה איקסים, ‫אז המכפלה הופכת לסכום
[00:58:17 - 00:58:18] ‫על כל האיקסים האלה של כל הדבר הזה.
[00:58:20 - 00:58:20] בסדר?
[00:58:21 - 00:58:24] אוקיי, עכשיו אנחנו רוצים לגזור את הדבר הזה ‫לפי מיור פעם אחת ולפי סיגמא.
[00:58:26 - 00:58:29] ‫אנחנו משלמים הכול בכתיב וקטורי ונבט רציוני פה, אוקיי?
[00:58:30 - 00:58:33] ‫אז קודם כול, האיבר הזה ‫הוא לא תלוי בכלל ב...
[00:58:34 - 00:58:38] ‫לא בסיגמא ולא במיור, נכון? ‫זה לא מעניין אותנו.
[00:58:39 - 00:58:42] ‫אז בואו נתחיל בלפי מיור.
[00:58:44 - 00:58:46] ‫נגזור את זה לפי מיור, אז
[00:58:47 - 00:58:49] ‫גם פה אין לנו את מיור, נכון? ‫אז זה רק האיבר הזה.
[00:58:49 - 00:58:51] ‫איך נראית הנגזרת של הדבר הזה?
[00:58:52 - 00:58:54] ‫סבירת יכולה להיכנס פה לתוך הספורם, ‫אז נשאר לי כמה ספורם.
[00:58:56 - 00:58:59] ‫איך נראית נגזרת?
[00:58:59 - 00:59:01] ‫כזאתי של תבנית ריבועית.
[00:59:01 - 00:59:03] ‫זוכרים? סתם את זה לא תרגיע עכשיו, אני חושב?
[00:59:06 - 00:59:06] ‫אה?
[00:59:07 - 00:59:13] ‫לא, דווקא אין את הסיגמא, ‫יהיה לנו בעצם רק את הסוגריים האלה.
[00:59:14 - 00:59:18] ‫זה קצת כמו ריבוע הדבר הזה, ‫זה בריבוע, אוקיי? ‫אבל יש בו עוד איזשהו סיגמא,
[00:59:19 - 00:59:21] ‫כי כשאנחנו חוזרים לפי מיור, ‫נשאר לנו רק
[00:59:22 - 00:59:23] פעמיים כפול
[00:59:24 - 00:59:26] ‫מה שיש לנו פה בסוגריים.
[00:59:26 - 00:59:30] ‫אוקיי, אז איזשהו שינוי,
[00:59:31 - 00:59:32] ‫מקבלים,
[00:59:35 - 00:59:37] ‫כן, פעמיים נפלים פה עם החצי,
[00:59:37 - 00:59:41] ‫שאר לנו מינוס xi פחות מיור.
[00:59:45 - 00:59:48] ‫אוקיי, ואם אנחנו רוצים לשאול ‫מתי הדבר הזה שווה 0?
[00:59:50 - 00:59:51] ‫מתי זה שווה 0?
[00:59:56 - 00:59:59] ‫כן, יש לנו פה את הסכום,
[01:00:02 - 01:00:06] ‫מינוס פה לא משנה, נכון? ‫כן, זה בדיוק כמו שראינו בסקלר 1,
[01:00:07 - 01:00:10] ‫נכון? הסכום של xi שווה ל-n כפול מיור,
[01:00:12 - 01:00:20] ‫אז מיור, כובע מוקצומטרי, ‫או לפעמים קוראים לזה, ‫שווה ל-1 חלקי n בסכום xi.
[01:00:23 - 01:00:23] בסדר?
[01:00:23 - 01:00:26] ‫-לאיפה נראה לנו סיגמא? ‫-מה? ‫-לאיפה נראה לנו סיגמא?
[01:00:27 - 01:00:30] ‫בגזירה הוא נראה לנו פשוט.
[01:00:31 - 01:00:40] ‫הנגזרת של זה... ‫רגע, אולי הנגזרת כן תראה לנו סיגמא? ‫-זה נראה לי שני סיגמאו מנוסטת, לא? ‫כן, יכול להיות שיש פה... שכחתי עכשיו.
[01:00:42 - 01:00:45] ‫יש משהו שנקרא כתוב כמה?
[01:00:46 - 01:00:47] ‫מטריקס קוטבוק?
[01:00:54 - 01:00:59] ‫אוקיי, קפסום מטריקס קוטבוק? ‫זה משהו מאוניברסיטה ענת בקנדה,
[01:01:00 - 01:01:02] ‫שיש פה פשוט כל החוקים של נגזרות,
[01:01:03 - 01:01:06] ‫של מטריצות, לגשת הורים וקולג'.
[01:01:07 - 01:01:11] ‫עכשיו לא בטוח איך הסיגמא מופיע בנגזרת, ‫אבל זה לא כל כך משנה, ‫גם אם נשאר פה הסיגמא,
[01:01:12 - 01:01:14] ‫מה שיאפס את זה זה כשהסוגריים האלה מתאפסים.
[01:01:20 - 01:01:22] ‫כן, יכול להיות שזה פשוט נשאר ככה, סיגמאו מנוספת.
[01:01:24 - 01:01:29] ‫אני לא זוכר, אני אסתכל אחר כך ‫בהפסקה ואני אגיד לכם.
[01:01:31 - 01:01:35] ‫אוקיי, אני לא זוכר לגבי זה, ‫אבל בכל מקרה הסוגריים האלה ‫צריכים להיות שווים לאפס,
[01:01:36 - 01:01:37] ‫ואנחנו מקבלים את
[01:01:40 - 01:01:41] ‫המקסימום לייקליות של מיוט.
[01:01:42 - 01:01:45] אוקיי, מה לגבי המקסימום לייקליות ‫לפי סיגמאן?
[01:01:46 - 01:01:49] ‫אנחנו נבדור את זה
[01:01:53 - 01:01:56] ‫יותר קל לגזור לפי סיגמא מינוס אחד,
[01:01:57 - 01:01:58] ‫אבל לפי סיגמא.
[01:02:01 - 01:02:06] ‫אז פה שוב, המטריקס קוקבוק, ‫אם אנחנו מדברים על הדבר הזה, ‫גוזרים לפי הסיגמא מינוס אחד, ‫אתם יודעים מה יוצא?
[01:02:15 - 01:02:24] ‫זה יוצא פשוט, ‫זה יוצא מה שיש מסביב לסיגמא,
[01:02:27 - 01:02:29] ‫אבל במכפלה חיצונית, אני לא יכול ‫לפנימי.
[01:02:30 - 01:02:34] ‫אחד מהכללי האצבע, ‫כשגוזרים לפי נגזרון ולפי וקטורים,
[01:02:35 - 01:02:36] ‫זה שהמימדים צריכים לצאת נכון.
[01:02:37 - 01:02:42] ‫אז כשאנחנו רוצים שהנגזרת לפי סיגמא, ‫זו מטריצה,
[01:02:42 - 01:02:44] ‫צריכה להיות מטריצה בעצמה.
[01:02:46 - 01:02:47] ‫בגודל של מה שאנחנו גוזרים לפי זה.
[01:02:48 - 01:02:51] ‫אז פה זה מסתדר. ‫יש כאן מכפלה חיצונית,
[01:02:53 - 01:02:55] ‫כל אחד יבוא אוקיי, ‫אז זה יהיה K או K,
[01:02:55 - 01:02:56] ‫אז זה יהיה נספור.
[01:02:58 - 01:03:00] ‫אז זה האיבר הראשון, ‫אבל יש לי כאן עוד איבר,
[01:03:01 - 01:03:04] ‫וזה עוד משהו שנראה יחסית מורכב, ‫אבל דווקא לגזור שלו היא מאוד פשוטה.
[01:03:06 - 01:03:07] ‫הנגזרת של לוב
[01:03:09 - 01:03:10] ‫בטרמיננטה,
[01:03:12 - 01:03:13] ‫שוב, זה מעמד טקסטוק, זה לוב
[01:03:14 - 01:03:18] ‫עם הטרמיננטה של X, ‫נגזרת שזה לפי X,
[01:03:20 - 01:03:22] ‫שווה ל-X מינוס 1,
[01:03:24 - 01:03:24] ‫אז פה עוד,
[01:03:26 - 01:03:27] ‫אז זה בסימטר, זה לא חוזר שונה.
[01:03:28 - 01:03:30] ‫אוקיי, אז זה עוד קל, אז
[01:03:31 - 01:03:32] ‫כאילו אין לנו בדיוק X,
[01:03:33 - 01:03:39] ‫לא חוזרים בשביל זה, ‫כי יש נכוונות לפי סיגמא מינוס 1, ‫כאן יש לנו סיגמא, אבל הדבר הזה גם שווה ל...
[01:03:40 - 01:03:43] ‫לוב של...
[01:03:47 - 01:03:49] ‫אחד חלקי,
[01:03:50 - 01:03:51] ‫אצלנו לנו מינוס 1,
[01:03:53 - 01:03:57] ‫כן, אז מינוס חצי של הדבר הזה, ‫שזה שווה גם פלוס חצי
[01:03:58 - 01:04:01] ‫של לוב של סיגמא או נוספת בלב.
[01:04:05 - 01:04:08] בסדר? פה הדטרמיננטה של האופצי,
[01:04:08 - 01:04:11] ‫זה שווה לאופצי של הדבר הזה, ‫במה שעשיתי כאן,
[01:04:11 - 01:04:14] ‫וכאן זה פשוט רק מנוסחה של לוב,
[01:04:16 - 01:04:18] ‫לוב של מה שבו המכנה,
[01:04:18 - 01:04:19] ‫זה לא מינוס של הלומינות.
[01:04:21 - 01:04:28] בסדר, וכל מיני טריקים כאלה, ‫אני לא רוצה להתקרב על זה יותר מדי, אבל בקיצור, ‫מה שיוצא כאן זה שזה שווה ל...
[01:04:31 - 01:04:34] ‫אני יכול גם לומר תגיד שכולי, פלוס חצי לוב,
[01:04:34 - 01:04:42] ‫של סיגמא או פלוס חצי של סיגמא.
[01:04:44 - 01:04:50] ‫זה היה המובטר של זה, ‫ואנחנו רוצים לשאול מתי זה שווה לאפס.
[01:04:54 - 01:04:58] ‫מה אנחנו מקבלים? חצי, כבר מעניין אותנו?
[01:04:59 - 01:05:00] ‫אנחנו נראים שסיגמא של רב
[01:05:04 - 01:05:05] סיגמא
[01:05:07 - 01:05:07] עוד
[01:05:07 - 01:05:08] אני צריך לראות כאן צופי את K
[01:05:12 - 01:05:12] טוב?
[01:05:13 - 01:05:15] טוב, אז זה טרמינטה של סיגמא לא עוד. סליחה
[01:05:18 - 01:05:24] אה אבל זה נמצא בתוך הצפון של זה. כן, אוקיי בסדר, אז יש לי כאן
[01:05:25 - 01:05:28] אין כפול סיגמא
[01:05:29 - 01:05:30] שווה ל...
[01:05:34 - 01:05:48] ‫סיגמא זה גם נמצא בתוך הצפון הזה של ה...
[01:05:50 - 01:05:52] ‫אבל הוא לא תלוי ב-XI.
[01:05:56 - 01:05:58] ‫אותו מספר כל אחד מהדברים ב-B, ‫אז אני יכול
[01:05:59 - 01:06:01] לדבר על זה אין כפול סיגמא הזה,
[01:06:01 - 01:06:03] ולכן אני מקבל שוב פעם משהו שנראה
[01:06:04 - 01:06:06] יחסית אלף סביר, שסיגמא שווה
[01:06:08 - 01:06:09] ‫בתחת חלקיהן
[01:06:10 - 01:06:12] ‫של המכפלות החיצוניות,
[01:06:13 - 01:06:14] ‫לכל ה-XI,
[01:06:19 - 01:06:20] ‫במודחק שלהם אנחנו
[01:06:25 - 01:06:25] בסדר?
[01:06:28 - 01:06:28] בסדר?
[01:06:28 - 01:06:29] בסדר.
[01:06:30 - 01:06:31] זה מקסימום, אולי יש לי עוד
[01:06:31 - 01:06:35] ‫שמאוד דומה למה שכבר ראיתם, ‫אבל הכול עכשיו בכתיב מטריציוני ווקטורי.
[01:06:42 - 01:06:45] ‫זהו, האיבר הזה זה הנגזרת שלו.
[01:07:01 - 01:07:16] ‫-בסדר?
[01:07:19 - 01:07:23] ‫אוקיי, אז למה בכלל מקסימום ‫לא הכיו זה דבר טוב?
[01:07:25 - 01:07:27] ‫כמו שאמרנו, זה היה לנו גם שיעור שעבר,
[01:07:27 - 01:07:28] ‫אנחנו אנחנו רוצים...
[01:07:31 - 01:07:39] ‫למצוא איזושהי התפלגות ‫שהיא קרובה לפי דאטה, אוקיי? ‫אז תרשים על זה מה שהוא מראה, ‫בעצם זה מרחב של
[01:07:41 - 01:07:43] ‫עבור כל תטא שלנו.
[01:07:45 - 01:07:48] ‫כל התטאים האפשריים שאני יכול לבחור,
[01:07:48 - 01:07:52] ‫אוקיי? כל נקודה כאן זה בעצם ‫התפלגות אחרת עם תטא אחר,
[01:07:53 - 01:07:55] ‫ויש איזושהי התפלגות ‫שהיא ייצרה את הדאטה עצמו,
[01:07:56 - 01:07:56] ‫פי דאטה.
[01:07:57 - 01:07:58] ‫אוקיי, היא ייצרה את כל התמונות האלה.
[01:08:01 - 01:08:05] ‫ואני רוצה למצוא את... ‫לפתוח כל התטאים האפשריים, ‫את התטא,
[01:08:05 - 01:08:07] ‫שהוא איכשהו הכי קרוב לפי דאטה, נכון?
[01:08:08 - 01:08:09] ‫גם שאנחנו רוצים,
[01:08:10 - 01:08:11] ‫זו המטרה שלנו,
[01:08:12 - 01:08:12] ‫אנחנו רוצים כמה שיותר...
[01:08:13 - 01:08:19] ‫אנחנו רוצים את פי דאטה, ‫אם אנחנו לא יכולים... ‫אם במקרה פי דאטה נמצא בתוך המודל שלנו,
[01:08:21 - 01:08:22] ‫אז אנחנו נגיע אולי למרחק אפס,
[01:08:23 - 01:08:27] ‫אבל כנראה שאנחנו לא בדיוק ‫מצאנו את המבנה של המודל הנכון,
[01:08:28 - 01:08:31] ‫לכן אנחנו רק נוכל ‫למצוא את התקליטה ‫שהכי קרוב בצליגה.
[01:08:32 - 01:08:32] ‫אוקיי?
[01:08:33 - 01:08:37] ‫אז אם זה מה שאנחנו רוצים, ‫אז בעצם מה שאנחנו רוצים להגיד, זה
[01:08:37 - 01:08:39] ‫לצמצם כאן איזשהו מרחק.
[01:08:39 - 01:08:42] ‫איזה מרחק אנחנו רוצים, ‫אתם יכולים לעבוד איתו?
[01:08:47 - 01:08:48] ‫אתם מכירים מרחק בין התפלגויות?
[01:08:50 - 01:08:54] ‫זה מרחק למשתמשים בעובד,
[01:08:55 - 01:08:57] ‫ובעצם אנחנו נראה תכף למה
[01:08:58 - 01:09:00] ‫לעשות את זה, ‫זה שקול למקסימום לייקליות,
[01:09:01 - 01:09:05] ‫הוא מצדיק את המקסימום לייקליות ‫בתור
[01:09:07 - 01:09:09] ‫שיטה שממזערת מרחק
[01:09:10 - 01:09:16] דייברג'ינס, ‫הייתי יכול להגיד את זה גם הפוך, ‫הסיבה שמשתמשים בקל דייברג'ינס, ‫כי זה די אקסלגיוני.
[01:09:17 - 01:09:19] ‫שקול לעשות מקסימום לייקליות.
[01:09:20 - 01:09:25] ‫שקול זה ככה, אוקיי? ‫קליל דייברג'ינס זה מדד מרחק ‫התפלגויות של גודלות,
[01:09:26 - 01:09:30] ‫ובעצם מה שאני רואה עכשיו זה ‫שאם אנחנו נמזער את ה-קל דייברג'ינס ‫בין פי טלטה לפי דאטה,
[01:09:32 - 01:09:36] ‫מה שאנחנו עושים זה בעצם תוכל שקול ‫למקסימום לייקליות.
[01:09:37 - 01:09:40] ‫זו הצדקה ללמה אנחנו רוצים ‫לנסות את המקסימום לייקליות.
[01:09:41 - 01:09:42] ‫בואו נראה את זה.
[01:09:49 - 01:09:49] ‫כל מסוכן?
[01:09:52 - 01:09:55] ‫אז זה הזמנים שלנו, לא כזה טוב, ‫אז אולי אני...
[01:09:56 - 01:09:58] ‫יש לי את זה. בואו נראה, כן.
[01:09:59 - 01:10:04] ‫אוקיי, בואו נעבור על זה פשוט ככה. ‫אני מעדיף בדרך כלל לכתוב, ‫כי אז יש לכם קצת זמן לראות מה קורה,
[01:10:05 - 01:10:07] ‫זה יותר קל להישאר, נראה לי, בעניינים.
[01:10:08 - 01:10:12] ‫אוקיי, פה יש לי את זה מוכן. ‫אז ה-KL דייברג'ינס
[01:10:13 - 01:10:15] ‫בין דאטה לפי דאטה, אוקיי?
[01:10:15 - 01:10:16] ‫זה, נסתכלו על השורה הזוונה,
[01:10:17 - 01:10:19] ‫השורה הראשונה, ‫זו ההגדרה של KL דייברג'ינס.
[01:10:20 - 01:10:25] אוקיי, זה האינטגרל, ופי דאטה, ‫כפול לוג פי דאטה, אוקיי,
[01:10:26 - 01:10:27] ‫ההתפלגות השנייה.
[01:10:28 - 01:10:30] יש לי קלט דייברג'ינס ‫בין שתי התפלגויות,
[01:10:30 - 01:10:32] ‫כאן כבר כתב לי לפי דאטה וקליטנטה,
[01:10:33 - 01:10:33] ‫זו הנוסחה הזו.
[01:10:34 - 01:10:36] אוקיי, אתם זוכרים אותי, ‫דיברנו על זה באיסטוברות.
[01:10:38 - 01:10:41] מישהו לא נתקל בזה? מישהו נתקל בזה כבר?
[01:10:42 - 01:10:43] ‫-כן?
[01:10:47 - 01:10:49] ‫תיזכרו בזה, אם אתם לא בטוחים.
[01:10:50 - 01:10:54] ‫אז זה מדד למרחק. ‫זה לא בדיוק מרחק, כי זה לא סימטרי.
[01:10:56 - 01:10:59] ‫אם אני אחליף בין פי דאטה ולפי דאטה, ‫אני אקבל משהו אחר,
[01:11:00 - 01:11:01] ‫אבל כמו זה דייברג'ינס.
[01:11:01 - 01:11:02] זה משהו שהוא
[01:11:03 - 01:11:08] ‫עם עצמו מרחק, זה אפס תמיד חיובי, ‫וזה אפס רק אם הם שווים.
[01:11:09 - 01:11:09] אוקיי?
[01:11:10 - 01:11:13] ‫אוקיי, אז זו ההגדרה. ‫עכשיו,
[01:11:14 - 01:11:16] הדבר הזה, אפשר לכתוב אותו ככה,
[01:11:18 - 01:11:23] ‫בתור פי תפריט פשוט ‫את שני החלקים פה בלוב.
[01:11:24 - 01:11:24] ‫אז יש לי כאן
[01:11:25 - 01:11:27] אותו דבר רק עם המונה.
[01:11:29 - 01:11:34] ‫ופה כתבתי פשוט בווד קונסט. ‫מה הקונסט הזה שווה? ‫הוא שווה למינוס
[01:11:35 - 01:11:37] ‫באינטגרד של פי דאטה.
[01:11:37 - 01:11:58] ‫מה זה הדבר הזה, דרך אגב? ‫קודם כול, זה קרה מודפי דפונס, ‫והמובן הזה שזה לא טבעי בטבע. ‫זה לא משפיע על הדבר הזה.
[01:12:00 - 01:12:00] ‫אתם יודעים מה זה המסחרות?
[01:12:04 - 01:12:04] ‫אתם מכירים את זה משהו?
[01:12:05 - 01:12:06] ‫לא כתבתי את זה בצורה מאוד מעורה.
[01:12:08 - 01:12:11] ‫זו אינטגרד של פי לא פי.
[01:12:13 - 01:12:13] ‫מה?
[01:12:13 - 01:12:14] ‫אנטרופיה.
[01:12:15 - 01:12:16] ‫בדיוק, אז אין אטרופיה של הדאטה,
[01:12:17 - 01:12:19] ‫של ההתפגות שיצרה את הדאטה.
[01:12:20 - 01:12:26] ‫אבל זה מבחינתנו לא כל כך מעניין, ‫כי זה פגוע, לא משנה איזה תטא אני אבחר, ‫הדבר הזה יקשר אותו דבר, ‫זה אנטרופיה של איך שיצרה את הדאטה.
[01:12:28 - 01:12:28] ‫בגללו?
[01:12:29 - 01:12:36] ‫ומבחינת מספור ה-KL Divergence, ‫אנחנו רוצים למצוא את התטא ‫שימזער לנו את המרחק הזה, ‫תוך מעט לנו מהדבר הזה,
[01:12:36 - 01:12:36] ‫זה משהו שלך,
[01:12:38 - 01:12:40] ‫תמיד יישאר שם, ‫לא משנה איזה תטא אני אבחר.
[01:12:41 - 01:12:41] ‫אוקיי,
[01:12:42 - 01:12:44] ‫אז נשארנו עם האיבר הזה.
[01:12:45 - 01:12:48] ‫האיבר הזה הוא אינטגרל, הוא פי דאטה,
[01:12:49 - 01:12:49] ‫אוקיי?
[01:12:50 - 01:12:52] ‫אנחנו יכולים לשארך אותו במקום,
[01:12:53 - 01:12:55] ‫אפשר עכשיו לדבר הזה בתור תוחלת
[01:12:56 - 01:12:57] ‫של הפונקציה הזאת, נכון?
[01:12:58 - 01:13:02] ‫אין איזושהי פונקציה של פי דאטה, ‫כאילו פי דאטה מייצרת את ה-X'ים,
[01:13:03 - 01:13:06] ‫אז התוחלת של זה, של הפונקציה הזאת.
[01:13:06 - 01:13:09] ‫אני יכול לשלח את התוחלת הזאת ‫על ידי ממוצע.
[01:13:10 - 01:13:14] ‫כאן זה ממוצע שה-X'ים מגיעים ל-P דאטה.
[01:13:17 - 01:13:19] ‫אני מחשב את הממוצע של הפונקציה הזאת, ‫שזה לא פי דאטה.
[01:13:20 - 01:13:25] ‫אם היה לי אינסוף תוגמאות
[01:13:26 - 01:13:27] ‫שמגיעות ל-P דאטה,
[01:13:27 - 01:13:29] ‫הביטוי הזה והביטוי הזה היו שווים.
[01:13:31 - 01:13:32] ‫הניתובים האלה היו שווים.
[01:13:34 - 01:13:35] ‫ככל שיש לי יותר דוגמאות, הם מתקרבים אותן אישה.
[01:13:35 - 01:13:37] ‫כן, זה חוק המספרים מסוימים.
[01:13:38 - 01:13:41] ‫זה התוחלת של דוארים.
[01:13:41 - 01:13:43] ‫אוקיי, אז הממוצע הזה יבוא,
[01:13:43 - 01:13:46] ‫כאילו ממוצע של לא פי דאטה,
[01:13:46 - 01:13:48] ‫כאשר הדוגמאות מגיעות מ-P דאטה.
[01:13:49 - 01:13:54] ‫איך אני יכול לעשות את זה? ‫איך אני יכול לייצר דוגמאות ‫שמגיעות מ-P דאטה מההתפלגות של P דאטה?
[01:14:00 - 01:14:04] ‫זה פשוט הדאטוסט שאני אוסף, ‫זו ההנחה שלי,
[01:14:04 - 01:14:07] ‫שאני בעולם מביא לי דוגמאות ‫שמגיעות מ-P דאטה.
[01:14:08 - 01:14:09] ‫אם אני אוסף הרבה דוגמאות,
[01:14:10 - 01:14:12] ‫אני יכול להניח שיש לי ‫הרבה דוגמאות שהגיעו מ-P דאטה.
[01:14:13 - 01:14:15] ‫אם אני אחשב עליהם את הדבר הזה ‫ואמצא את השטע
[01:14:16 - 01:14:17] ‫שאני מזער את זה,
[01:14:18 - 01:14:22] ‫אז אני בקבוצת ה-K לדבר ‫אני צריך להחליט חובה לפי שודד.
[01:14:23 - 01:14:25] ‫אבל אני מזער את הדבר הזה, ‫יש פה מינוס.
[01:14:26 - 01:14:31] ‫זה שקול למקסם את הדבר הזה ‫בלי המינוס ובלי הקבוע הזה,
[01:14:31 - 01:14:32] ‫שזה בדיוק המקסימום האחרון.
[01:14:33 - 01:14:36] ‫לא, זה לייקליות של תאטה,
[01:14:36 - 01:14:38] ‫ממוצע על פני כל הדגימות שלי.
[01:14:40 - 01:14:44] ‫אז מקסימום לייקליות זה שקול לגמרי ‫למינימום דיורג'נס
[01:14:45 - 01:14:50] ‫עם ה-P של הדאטה, אוקיי? ‫בין ה-P של הדאטה ל-P של תאטה.
[01:14:54 - 01:14:56] ‫טוב, נעשה איזה הפסקה ונמשיך אחר כך.
[01:14:59 - 01:14:59] ‫עשר דקות.
[01:15:02 - 01:15:07] ‫-תודה רבה.
[01:15:32 - 01:15:33] ‫-תודה רבה.
[01:16:02 - 01:16:03] ‫-תודה רבה.
[01:16:32 - 01:16:33] ‫-תודה רבה.
[01:17:02 - 01:17:03] ‫-תודה רבה.
[01:17:32 - 01:17:33] ‫-תודה רבה.
[01:18:02 - 01:18:03] ‫-תודה רבה.
[01:18:32 - 01:18:33] ‫-תודה רבה.
[01:19:02 - 01:19:03] ‫-תודה רבה.
[01:19:32 - 01:19:33] ‫-תודה רבה.
[01:20:02 - 01:20:03] ‫-תודה רבה.
[01:20:32 - 01:20:33] ‫-תודה רבה.
[01:21:02 - 01:21:03] ‫-תודה רבה.
[01:21:32 - 01:21:33] ‫-תודה רבה.
[01:22:02 - 01:22:03] ‫-תודה רבה.
[01:22:32 - 01:22:33] ‫-תודה רבה.
[01:23:02 - 01:23:03] ‫-תודה רבה.
[01:23:32 - 01:23:33] ‫-תודה רבה.
[01:24:02 - 01:24:03] ‫-תודה רבה.
[01:24:32 - 01:24:42] ‫אז חשבתי שאנחנו גם בזהבי שמי, נכון?
[01:24:44 - 01:24:51] ‫-נו, זה גם בגלל שאם איך קיבלתי ‫את העבודה בקיבורים, ‫קיבלתי מילי כמו נטובר, ‫אז קיבלתי מעצמתו.
[01:24:51 - 01:24:53] ‫אם תוכנית עליה, אז כאילו,
[01:24:55 - 01:24:56] ‫כאילו, ברור ש...
[01:24:57 - 01:25:00] ‫אבל אפשר לפסוק את דעתי ‫בלי ללמד בדרך כלל...
[01:25:01 - 01:25:17] ‫אז דיברנו על... ‫נזכרנו קצת מה זה מקסימום ריקליות, נכון?
[01:25:19 - 01:25:21] ‫והראנו דוגמאות.
[01:25:21 - 01:25:25] ‫נזכרנו מקסימום ריקליות ‫לברנולי ולגאוסיאנים,
[01:25:25 - 01:25:27] ‫ואיך עושים את זה ‫עם מטריצות ווקטורים.
[01:25:28 - 01:25:31] ‫אוקיי, אבל גם הסברנו למה ‫מקסימום ריקליות זה טוב.
[01:25:32 - 01:25:35] ‫לעשות מקסימום ריקליות זה שקול ‫למזער
[01:25:36 - 01:25:39] את המרחק בין ה-p של הדאטה
[01:25:40 - 01:25:43] ‫ל-p של צטא שאנחנו מחפשים, ‫במובן של כאלה דייברדנס.
[01:25:45 - 01:25:46] ‫אני לא יודע למה זה מאלהב ‫ככה כל הזמן,
[01:25:47 - 01:25:48] ‫שזה יפסיק באיזשהו שלב.
[01:25:50 - 01:25:53] ‫אוקיי, עכשיו עוד מודל שאנחנו רוצים ‫להתעסק איתו,
[01:25:54 - 01:25:56] ‫זה מודלים ליניאריים גאוסיאניים.
[01:25:58 - 01:26:00] ‫כמו שאמרנו קודם, ראיתם כבר בתרגיל,
[01:26:03 - 01:26:05] ‫וראיתם שהם...
[01:26:06 - 01:26:08] ‫הכול נשאר שם גאוסיאן.
[01:26:09 - 01:26:19] ‫אוקיי, אז נגיד שיש לנו את הבעיה הזאתי, ‫וואי הוא איזושהי מטריצה 8 ‫אפולי תטא שלנו, פרמטר שלנו, ‫ועוד איזשהו מטרי תטא,
[01:26:20 - 01:26:25] ‫שהוא מתפלג עם גאוסיאן, ‫למשל עם התפלגות כזאת. ‫מה זה אומר ה-cobariance הזה?
[01:26:26 - 01:26:30] ‫זה אומר שה-cobarance הזה הוא אלכסוני, ‫שיש לו פשוט סיגמא את כל האלכסון,
[01:26:31 - 01:26:34] ‫וזה אומר שאין לנו תלויות ‫בין
[01:26:35 - 01:26:37] הממדים השונים של ETA, ‫אוקיי? כל כך זה וקטורים,
[01:26:38 - 01:26:39] ‫איזו מטריצה.
[01:26:40 - 01:26:43] ‫אוקיי, אז אין לנו תלויות ‫בין הממדים השונים של ETA,
[01:26:44 - 01:26:46] ‫וכולם יש להם את אותה התפלגות.
[01:26:48 - 01:26:53] אוקיי, לעשות מקסימום לייפניות למודל כזה, ‫זאת אומרת, למצוא את התטא, מיינתן איזשהו Y,
[01:26:54 - 01:26:54] למצוא את התטא
[01:26:55 - 01:26:59] ‫עבורו ההתפלגות של Y, ‫הההסתברות של Y היא הכי גבוהה, ‫זה מקסימום לייפניות.
[01:27:00 - 01:27:04] זה שקול לכמה דברים. ‫זה מקרה כללי שהוא שקול ‫לכל מיני דברים שאתם אולי מכירים,
[01:27:05 - 01:27:08] ‫וזה משהו שהוא מאוד מאוד שימושי, אוקיי? ‫אז למשל,
[01:27:08 - 01:27:10] רגרסיה ליניארית,
[01:27:11 - 01:27:14] ‫רגילה כמו שפתרנו בשיא-לרנינג,
[01:27:15 - 01:27:18] ‫אפשר לכתוב את זה, ‫זה מקרה פרטי של הדבר הזה,
[01:27:18 - 01:27:20] ‫מה משתנה? ש-A,
[01:27:21 - 01:27:22] המגרית הזאת A,
[01:27:22 - 01:27:24] ‫היא בעצם כל שורה ב-A,
[01:27:25 - 01:27:29] ‫זה וקטור של X'ים, אוקיי? ‫זו נקודה.
[01:27:31 - 01:27:34] ‫באגרסיה ליניארית יש לנו קשר בין X ל-Y, ‫שהוא ליניארי, נכון?
[01:27:35 - 01:27:37] ‫יש לנו דאטה שמורכב מהרבה X'ים,
[01:27:37 - 01:27:39] ‫וה-Y'ים שלהם, נכון?
[01:27:39 - 01:27:40] ‫אז במקרה,
[01:27:41 - 01:27:43] ‫מיחד מימד לאחד מימד,
[01:27:43 - 01:27:45] ‫יש לנו הרבה נקודות כאלה, נכון?
[01:27:46 - 01:27:49] ‫אז יש לנו X לממד אחד, ‫ולכל X יש את ה-Y שלנו.
[01:27:50 - 01:27:51] אנחנו מחפשים איזשהו
[01:27:52 - 01:28:01] איזשהו ישר שהכי קרוב ‫באיזשהו מובן לדאטה, ‫בדרך כלל מחפשים הכי ישר שהכי קרוב ‫במובן של שגיאה ריבועית.
[01:28:02 - 01:28:06] ‫אוקיי? אז הבעיה הזאת היא בדיוק שקולה ‫שנמצוא מקסימום לייט ליוד,
[01:28:06 - 01:28:08] ‫את הדבר הזה, ‫ש-A,
[01:28:08 - 01:28:10] ‫המקרה הזה, אם X הוא בממד אחד, A,
[01:28:11 - 01:28:16] ‫זה יהיה פשוט וקטור של כל ה-X'ים, ‫X1 עד Xn.
[01:28:17 - 01:28:19] אוקיי? את זה אנחנו נכפיל בתטא.
[01:28:21 - 01:28:30] ‫ונשווה את זה ל-Y, ‫שיהיה פשוט הווקטור של כל ה-Y'ים שלנו.
[01:28:33 - 01:28:38] ‫אם אנחנו מניחים שנוסף לנו רעש גאוסיאני, ‫ושבגללו אנחנו רואים את ה-Y,
[01:28:39 - 01:28:45] ‫שהוא לא בדיוק A כפול תטא, ‫אלא Y אחר, נכון? ‫אנחנו לא בדיוק רואים את ה-X הזה, ‫האיבר שלו הוא לא בדיוק
[01:28:46 - 01:28:49] על הישר, ‫הוא לא בדיוק A כפול תטא, ‫הוא קצת שונה.
[01:28:49 - 01:28:54] ‫אוקיי? אז זה בדיוק הרעש הזה ‫שאנחנו רואים פעם. ‫אז דרך החלטה לכתוב את הדבר הזה,
[01:28:55 - 01:28:58] ‫זה ש-Y מתפלל לפי גאוציאן,
[01:29:00 - 01:29:01] ‫שהתוחלת שלו היא A תטא,
[01:29:06 - 01:29:11] ‫והשונות שלו היא Sigma I Sigma.
[01:29:12 - 01:29:15] אוקיי? זה אותו דבר הזה, אפשר לכתוב את כל הדבר הזה.
[01:29:17 - 01:29:17] בסדר?
[01:29:18 - 01:29:20] ‫ובעצם מה זה אומר? ‫עכשיו, אם אני...
[01:29:21 - 01:29:24] זה בעצם אומר שאיך הדאטה שלי נוצר, ‫איך ה-Y יוצרו,
[01:29:25 - 01:29:26] ‫יש איזושהי תוחלת
[01:29:27 - 01:29:28] ‫שהיא A כפול תטא,
[01:29:29 - 01:29:31] ‫אם ה-A הזה זה בעצם X'ים שונים,
[01:29:32 - 01:29:39] ‫שבעצם זה יהיה עבור כל X, ‫ה-X כפול התטא שלו, ‫זה בדיוק המקום שלו, ‫לפי איזשהו תטא אמיתי שייצר את הדאטה,
[01:29:39 - 01:29:44] ‫ועוד רעש, ובגלל זה הנקודות הן לא בדיוק על הישר, ‫אלא הן מורשות, אוקיי?
[01:29:44 - 01:29:46] ‫ולעשות לזה מקסימום עקביות, זה בדיוק שקול
[01:29:46 - 01:29:50] ‫לפתור את השקיעה הריבועית ‫של רגרסיה לינארית.
[01:29:51 - 01:29:57] ‫אבל זה לא חייב להיות רגרסיה לינארית, ‫אפשר בעצם עם רגרסיה לינארית ‫אפשר לפתור בעיות גם יותר מורכבות, ‫למשל
[01:29:58 - 01:30:01] רגרסיה פולינומיאלית. ‫עם כל שורה ב-A,
[01:30:03 - 01:30:05] ‫זה תהיה בעצם ה-X ו-X בריבוע,
[01:30:07 - 01:30:07] ‫מול X בשלישית,
[01:30:08 - 01:30:10] ‫עד XK, איזשהו מימד K,
[01:30:11 - 01:30:16] ‫אז בעצם אנחנו מחפשים את המקדמים ‫של הפולינומים של X ‫שמסבירים הכי טוב את הדאטה.
[01:30:17 - 01:30:23] ‫אז הבעיה הזאת היא מאוד פשוטה, ‫אבל היא כבר מאוד כללית. ‫אפשר לפתור את הבעיות,
[01:30:25 - 01:30:26] ‫הן מאוד מורכבות,
[01:30:28 - 01:30:30] ‫איך אנחנו בונים את ה-A הזה.
[01:30:31 - 01:30:37] ‫אז מה זה אומר לעשות ‫מקסימום לייקליות פה? ‫איך אנחנו פותרים את זה? ‫אתם זוכרים את הפתרון של...
[01:30:38 - 01:30:41] ‫לצד בדיוק את הפתרון ‫של רגרסיה לינארית בעצם?
[01:30:45 - 01:30:46] בואו נראה את זה.
[01:30:47 - 01:30:53] ‫אז הלייקליות פה זה פשוט הגאוסיאן הזה, ‫אז הלוג של הלייקליות
[01:30:54 - 01:30:54] ‫זה יהיה
[01:30:58 - 01:30:59] ‫הקספוננט
[01:31:01 - 01:31:03] ‫הגאוסיאן, איך הוא נראה? ‫הוא נראה
[01:31:05 - 01:31:09] ‫מינוס חצי X חפות מיוב,
[01:31:10 - 01:31:12] ‫אין לי פה מיוב, יש לי בעצם A תטא.
[01:31:14 - 01:31:16] ‫לא קראתי לזה X, קראתי לזה Y.
[01:31:17 - 01:31:19] ‫טרנספוס,
[01:31:21 - 01:31:25] ‫קובריאנס שלי, או הבטיח של הקובריאנס, ‫זה כאילו I
[01:31:27 - 01:31:29] עוד חלקי סיגמה בריבוע,
[01:31:29 - 01:31:33] ‫בדרך כלל פשוט לא כותבים את זה כאן, ‫אנחנו כותבים פה שני סיגמה בריבוע,
[01:31:37 - 01:31:38] ‫Y תוך חוז A תטא.
[01:31:39 - 01:31:43] ‫נכון, ויש לי עוד
[01:31:44 - 01:31:53] ‫עוד איברים שגדולים ב-coוריאנס ‫ובאיבר נרמול, נצטלם ממנו כרגע, ‫כי נחפש רק את התוכלת של הדבר הזה.
[01:31:54 - 01:31:55] ‫אני רוצה לגזור את זה לפי תטא.
[01:32:03 - 01:32:04] ‫אז מה אני אקבל?
[01:32:05 - 01:32:13] ‫אני אקבל מינוס אחד חלקי סיגמה בריבוע,
[01:32:14 - 01:32:14] ‫של
[01:32:18 - 01:32:19] Y פחות A תטא,
[01:32:21 - 01:32:23] ‫או לנגזרת הפנימית של המינוס,
[01:32:23 - 01:32:28] ‫לנגזרת הפנימית זה המינוס וה-A.
[01:32:29 - 01:32:32] ‫נגזרת הפנימית לפי תטא, ‫יש לי פה עוד A.
[01:32:34 - 01:32:37] ‫אז הוא בצד השני עם טרנספוז, ‫אבל אני חושב שעדיף לכתוב ככה,
[01:32:39 - 01:32:41] ‫סיגמה בריבוע A טרנספוז.
[01:32:43 - 01:32:43] ‫סתובב?
[01:32:44 - 01:32:46] שוב, אם מישהו מסתבך עם הדברים האלה, ‫אני אסתכל במטריק ספור-בוק.
[01:32:47 - 01:32:51] ‫אני גוזר לפי הדבר הזה, ‫אז אני קודם כול, יש לי פה איבר בריבוע,
[01:32:52 - 01:32:56] ‫אז זה פעמיים האיבר הזה, ‫ואני נגזרת הפנימית לפי תטא, ‫שמה שכתוב כאן זה פשוט מינוס A.
[01:32:58 - 01:32:59] זה A הזה,
[01:33:00 - 01:33:02] ‫והשאלה מתי זה שווה ל-0,
[01:33:02 - 01:33:09] ‫אוקיי? אז הסיגמה פה לא כל כך מעניין אותי, ‫זה מוצא לי ש-A טרנספוז Y,
[01:33:11 - 01:33:16] ‫אם זה ככה, A טרנספוז A בטא שווה ל-A טרנספוז Y,
[01:33:18 - 01:33:23] ‫ובטא שווה ל-A טרנספוז A זה מינוס אחד,
[01:33:25 - 01:33:26] ‫A טרנספוז Y.
[01:33:28 - 01:33:31] אוקיי, אם מישהו זוכר, ‫זה גם הפתרון של רגעי סגימנט.
[01:33:31 - 01:33:34] ‫אוקיי? והמקסימום לייטליות ‫של הבעיה הזאתי.
[01:33:36 - 01:33:38] ‫אוקיי, אנחנו נחזור לזה גם בהמשך.
[01:33:39 - 01:33:40] ‫כן. ‫-מה קורה אם זה לא הפיך?
[01:33:41 - 01:33:44] ‫אז במקרים שלנו אנחנו יכולים להניח ‫שזה הפיך.
[01:33:45 - 01:33:46] ‫אם זה לא הפיך,
[01:33:47 - 01:33:53] ‫אז יש כמה פתרונות, ‫לפעמים אפשר לעשות את זה הפוך, ‫זאת אומרת A טרנספוז A לא הפיך, ‫אבל A, A טרנספוז יכול להיות הפיך,
[01:33:54 - 01:33:57] ‫ואחרת,
[01:33:57 - 01:34:00] אפשר לחשב בזה,
[01:34:00 - 01:34:01] ‫עוד איזה משהו שנקרא פסודו-אינגרס.
[01:34:03 - 01:34:08] ‫לפעמים זה פשוט אומר ש... ‫אני לא רוצה להיכנס לזה, ‫לפעמים זה אומר שאין לזה פתרון יחיד.
[01:34:10 - 01:34:13] ‫יש לזה כל מיני משמעויות,
[01:34:16 - 01:34:19] ‫אבל בוא ניכנס לזה עכשיו, ‫זה לא ככה אנחנו לא מספיקים לתנאי.
[01:34:20 - 01:34:21] אוקיי.
[01:34:24 - 01:34:25] טוב,
[01:34:26 - 01:34:26] שאלות על זה?
[01:34:27 - 01:34:36] ‫מודל לינארי גרסיאני, אוקיי? ‫אנחנו, כמו שאמרתי, אנחנו לא רוצים, ‫בדרך כלל, לא נרצה שהמודל שלנו יהיה כזה, ‫אבל כדאי שתבינו את המודל הזה טוב,
[01:34:36 - 01:34:40] ‫כי אנחנו עוד מעט תהיינו ‫למודלים לא לינאריים גרסיאניים,
[01:34:40 - 01:34:43] ‫וצריך להבין את זה, ‫איך זה ביחס למודל לינארי גרסיאני.
[01:34:45 - 01:34:45] אוקיי,
[01:34:46 - 01:34:47] תכף נקשר את זה
[01:34:50 - 01:34:50] למשתנים חבריים.
[01:34:52 - 01:34:53] אוקיי, אז
[01:34:57 - 01:35:01] כנראה אנחנו נגלוש קצת לשבוע הבא, ‫אבל משתנים חבויים, אני רוצה,
[01:35:03 - 01:35:07] ‫בשביל להסביר משתנים חבויים, ‫אנחנו צריכים להסביר קצת סטטיסטיקה בייזיאנית,
[01:35:08 - 01:35:15] אוקיי? ‫כדי שיהיה לכם תמונה טובה של מה המשמעות ‫של מודלים משתני חבוי.
[01:35:16 - 01:35:23] אז אני רוצה להסביר קצת את זה. ‫אוקיי, אז עכשיו בעצם מה שעשינו, ‫שבעצם עשינו רק מקסימום לייקליות,
[01:35:24 - 01:35:28] ‫זה נקרא גישה קלאסית ‫או גישה פרקוונטיסטית,
[01:35:29 - 01:35:31] ‫אוקיי? שזה ההפך מגישה בייזיאנית.
[01:35:32 - 01:35:36] בעצם מה שהיא מניחה, ‫היא מניחה שיש איזשהו הפרמטר, ‫מה שקראנו לו הפרמטר, תטא,
[01:35:36 - 01:35:38] ‫זה איזשהו משהו שיש את הפרמטר הנכון.
[01:35:40 - 01:35:41] אוקיי, יש איזשהו פרמטר נכון,
[01:35:42 - 01:35:44] ‫ואנחנו מחפשים משער איך ש...
[01:35:45 - 01:35:47] ככה שהוא יהיה הכי טוב ‫ביחס לפרמטר הנכון.
[01:35:48 - 01:35:49] אוקיי, זה
[01:35:50 - 01:35:52] בדרך להסתכל על ההתפלגויות,
[01:35:53 - 01:35:57] ‫אומר, אוקיי, אנחנו... יש את הפרמטר הנכון ‫של ההתפלגות שלנו,
[01:35:57 - 01:36:01] ‫אולי אנחנו לא יכולים למצוא אותו, ‫אולי אנחנו מחפשים בכלל איזה ‫משפחה אחרת של התפלגויות,
[01:36:02 - 01:36:04] ‫אבל יש איזשהו תטא שהוא התטא הנכון.
[01:36:07 - 01:36:13] כשמשתמשים במשתנה חבוי, ‫אנחנו קצת יותר מתקרבים ‫לגישה שנקראת בייזיאנית.
[01:36:14 - 01:36:16] זה מסביר בעצם מה זה גישה בייזיאנית.
[01:36:17 - 01:36:19] זה מה שאנחנו נעשה היום,
[01:36:19 - 01:36:23] ‫ואחר כך אנחנו נסביר ‫איך בעצם זה קשור למשתנים חבויים בצורה...
[01:36:24 - 01:36:33] ‫בקיצור, אני לא חושב שאני אספיק ‫היום שתבינו את זה עד הסוף, ‫אז נתחיל היום, ובשבוע הבא אני מקווה ‫שאתם תבינו את הקשר ‫בין סטטיסטיקה בייזיאנית למשתנים חבויים.
[01:36:35 - 01:36:39] ‫אז התמונה היום של משתנים חבויים ‫אולי תהיה לכם קצת מבולגנת, ‫ואני מקווה שבשבוע הבא נסדר אותה.
[01:36:40 - 01:36:43] אבל עכשיו, בעצם עד סוף השיעור, ‫אני רוצה שנדבר על סטטיסטיקה בייזיאנית,
[01:36:44 - 01:36:48] ‫שאפשר לחשוב על זה בצורה בלתי תלויה ‫בכלל ממשתנים חבויים, אוקיי? ‫מה זה סטטיסטיקה בייזיאנית?
[01:36:51 - 01:36:51] ‫אוקיי, אז
[01:36:52 - 01:36:53] קודם כול מה זה ה...
[01:36:54 - 01:36:57] ‫אפשר לחשוב על זה ‫בתור ממש פילוסופיה בייזיאנית, אוקיי?
[01:36:57 - 01:37:03] ‫זו נקודת הסתכלות על העולם ההסתברותי,
[01:37:03 - 01:37:07] ‫בשביל סטטיסטיקאים זה עניין מאוד חשוב ‫אם אתה בייזיאני או לא.
[01:37:08 - 01:37:10] יש אנשים שרוב הקריירה שלהם זה
[01:37:11 - 01:37:14] ‫להראות שהצד השני טועה ושהם צודקים.
[01:37:15 - 01:37:20] זה זה זה ממש חלוקה די גס... די
[01:37:23 - 01:37:25] חשובה בעולם הסטטיסטיקה.
[01:37:26 - 01:37:30] אוקיי, אז מה הגישה הבייזיאנית? ‫בניגוד למה שאמרנו קודם, ‫שאם יש לנו איזשהו משתנה,
[01:37:31 - 01:37:32] אם יש לנו פרמטר לא ידוע,
[01:37:33 - 01:37:40] ‫אז עדיין זה לא אומר שהוא איזשהו משתנה מקרי ‫או משהו כזה, זה אומר שפשוט ‫אנחנו לא יודעים מה הערך שלו, יש לו איזשהו ערך אמיתי,
[01:37:40 - 01:37:41] ‫אנחנו לא יודעים מה הערך שלו.
[01:37:42 - 01:37:43] ‫אבל בגישה הבייזיאנית,
[01:37:43 - 01:37:46] ‫כל דבר שאנחנו לא יודעים ‫זה בעצם משתנה מקרי,
[01:37:47 - 01:37:49] ‫שיש לו איזושהי התפלגות.
[01:37:51 - 01:37:53] ‫אוקיי, אז כל משהו שהוא לא ידוע ‫הוא משתנה מקרי,
[01:37:55 - 01:37:59] ‫ותמיד אנחנו שומרים איזושהי התפלגות ‫על המשתנה המקרי הזה.
[01:38:00 - 01:38:04] אז בהתחלה אנחנו לא יודעים עליו כלום, אולי,
[01:38:04 - 01:38:09] ‫אז עדיין אנחנו צריכים להניח ‫איזושהי התפלגות כזאת ‫שאומרת שאנחנו לא יודעים עליו כלום,
[01:38:09 - 01:38:12] לאט לאט אנחנו יודעים אולי שהוא קרוב לאיזשהו ערך
[01:38:13 - 01:38:17] חמש אנחנו צריכים שההתפלגות הזאת היא לאט לאט יותר קרובה לתן הסתברות יותר גבוהה לחמש
[01:38:18 - 01:38:20] הסתברות יותר נמוכה ככל שמתרחקים מחמש
[01:38:21 - 01:38:27] אבל תמיד צריכה להיות לנו איזושהי התפלגות שמתארת את כמה אנחנו מה אנחנו יודעים על המשתנה הזה
[01:38:28 - 01:38:30] וקוראים לזה לפעמים האמונה ביליף
[01:38:34 - 01:38:39] והנקודה השלישית זה קצת אמרתי את זה כבר בעצם אנחנו זה לא שיש לנו איזושהי התפלגות
[01:38:39 - 01:38:51] אנחנו תמיד צריכים לעדכן את ההתפלגות הזאתי על סמך מה שאנחנו רואים, אם קיבלנו פתאום דאטה חדש או איזושהי אובזרבציה חדשה אנחנו צריכים להשתמש באובזרבציה הזאת כדי לעדכן את כל ההתפלגויות שיש לנו
[01:38:51 - 01:38:53] על הדברים שאנחנו יודעים
[01:38:54 - 01:38:58] זה נקרא גישה בייזיאנית כי העדכון הזה בעצם
[01:38:59 - 01:39:01] נעשה עם חוק בייס, אוקיי? כי בעצם
[01:39:09 - 01:39:15] אם הדבר הזה שאני לא יודע זה תטא, אוקיי?
[01:39:16 - 01:39:17] אז בעצם בהינתן
[01:39:19 - 01:39:24] מה שתמיד מחפש זה ההסתברות על תטא בהינתן כל הדאטה שראיתי עד עכשיו
[01:39:27 - 01:39:29] אז יש לי, זה מורכב מההסתברות
[01:39:29 - 01:39:38] איזושהי הסתברות פריאורית התפלגות שהנחתי מראש עוד לפני שראיתי את הדאטה על תטא
[01:39:39 - 01:39:44] וההתפלגות הלייקליות
[01:39:45 - 01:39:49] חלקי זה משהו שמנרמה לי את הכל כמו שצריך
[01:39:53 - 01:39:53] בסדר?
[01:39:54 - 01:40:02] אז אנחנו משתמשים בזה כל פעם לעדכן, פתאום נוסף לנו דאטה חדש, ראינו משהו חדש, אז אנחנו צריכים להשתמש בנוסחה הזאת כדי לעדכן את ההתפלגות שלנו על תטא
[01:40:04 - 01:40:07] בסדר? אם אין לנו דאטה בהתחלה אז ה-sterיאור הזה יהיה פשוט
[01:40:08 - 01:40:10] ה-prime, זה יהיה פשוט פי של תטא
[01:40:13 - 01:40:15] אוקיי, אז זו הייתה גישה הבייזיאנית באופן כללי
[01:40:17 - 01:40:19] ויש לי יתרונות וחסרונות, שתכף נראה
[01:40:21 - 01:40:22] לפחות את היתרונות
[01:40:23 - 01:40:30] היתרונות זה שבעצם אנחנו לא בניגוד למקסימום לייקליות שהתחייבנו לתטא שמצאנו,
[01:40:31 - 01:40:36] נכון? עשינו את הממוצע הזה על כל הדוגמאות שראינו, זה יצא המקסימום לייקליות של התוחלת
[01:40:37 - 01:40:39] זהו, מעכשיו אנחנו מתייחסים לדבר הזה בתור
[01:40:42 - 01:40:47] הפרמטר, מה שמחליף את הערך הלא ידוע של הפרמטר,
[01:40:47 - 01:40:52] פה אנחנו תמיד משמרים איזושהי התפלגות, אז אנחנו יודעים מתי ההתפלגות הזאת היא מאוד
[01:40:53 - 01:40:55] צרה, זאת אומרת היא מאוד בטוחה במה שהיא חושבת,
[01:40:56 - 01:41:00] ומתי ההתפלגות הזאת היא רחבה, אז יש לנו איזשהו מידע על מה אנחנו יודעים ומה אנחנו לא יודעים
[01:41:01 - 01:41:06] ואנחנו כל הזמן עובדים איתו, אוקיי? אז זה יתרון.
[01:41:06 - 01:41:07] של הגישה הבייזיאנית.
[01:41:08 - 01:41:12] עוד יתרון שתכף נראה זה שאנחנו יכולים בעצם להגדיר מה זה
[01:41:13 - 01:41:14] לשערך אופטימלי
[01:41:15 - 01:41:16] באיזשהו מובן,
[01:41:17 - 01:41:19] מה שאי אפשר לעשות בגישה קלאסית,
[01:41:19 - 01:41:20] תכף נראה את זה.
[01:41:21 - 01:41:26] החסרונות העיקריים של הגישה הזאת שאנחנו צריכים להניח תמיד איזשהו פריור, תמיד אנחנו צריכים להתחיל מאיזושהי התפלגות,
[01:41:28 - 01:41:34] ושלפעמים אנחנו, אם אנחנו לא יודעים כלום אז אנחנו נצטרך לחשוב על איזה משהו קצת שרירותי שיגדיר את ההתפלגות הזאת,
[01:41:35 - 01:41:38] ואיך שנגדיר את זה יכול מאוד לשנות את מה שיצא בסוף.
[01:41:39 - 01:41:46] זאת איזושהי החלטה שהיא לכאורה שודאותית של מה הפריור שלנו יכולה מאוד להשפיע על מה תהיה תוצאה בסוף.
[01:41:47 - 01:41:48] אז זה חיסרון של השיטה הזאת.
[01:41:50 - 01:41:58] ועוד חיסרון זה שלפעמים נכון שאנחנו יכולים לחשב דברים ותראו שאני אתן לכם את המשארך האופטימלי תמיד שהכי טוב מכל המשארכים האחרים,
[01:41:58 - 01:42:01] אבל הרבה פעמים אי אפשר לחשב אותם בפועל.
[01:42:01 - 01:42:09] קשה לחשב אותם בפועל, זה יפה שיש לנו ביטוי שאומר לנו מה השארך האופטימלי, אבל אם אי אפשר לשארך אותו,
[01:42:10 - 01:42:11] אז זה לא עוזר לנו הרבה,
[01:42:11 - 01:42:13] צריכים בסופו של דבר לעשות כל מיני כירוגים.
[01:42:14 - 01:42:16] אוקיי, אז זה יתרונות ואחרי יתרונות.
[01:42:18 - 01:42:18] בואו נראה,
[01:42:19 - 01:42:20] כן.
[01:42:21 - 01:42:23] אתה לא פשוט נגיד להניח שזה אינטורי ומתמורמי,
[01:42:23 - 01:42:26] ואז אתה לא מצליח להניח עליו את זה?
[01:42:26 - 01:42:28] זה עדיין משפיע,
[01:42:28 - 01:42:30] זה משנה אם אתה מניח עליו שהוא אוניפורמי או שהוא לא אוניפורמי.
[01:42:31 - 01:42:33] אבל אם הוא אוניפורמי זה לא משקיע כמו בעיה יחדינית?
[01:42:34 - 01:42:35] זה לא שקול, לא, זה לא שקול.
[01:42:36 - 01:42:36] כי אתה עדיין,
[01:42:38 - 01:42:40] יש מקרה אחד שזה שקול, ותכף אנחנו נראה.
[01:42:41 - 01:42:44] אם כל מה שמחפש אותך זה המקסימום של ההתפלגות הזאת,
[01:42:45 - 01:42:48] אז אם אתה מחפש את המקסימום של ההתפלגות הזאת,
[01:42:48 - 01:42:50] זאת אומרת, אתה תטע שממקסן את ההתפלגות הזאת.
[01:42:51 - 01:42:55] ופה אתה מניח שזה אחיד, אז זה שקול למקסימום לייטיות, למקסימום של זה.
[01:42:56 - 01:43:00] אבל הגישה הביזיאנית היא אומרת, לא, אתה תמיד צריך לשמור את כל ההתפלגות
[01:43:01 - 01:43:06] כדי לעשות אחר כך החלטות חכמות או כדי להבין מה המשמעות של זה על משתנים אחרים,
[01:43:06 - 01:43:10] אתה תמיד צריך להסתכל על כל ההתפלגות של הפוסטריאור ואז זה לא אותו דבר.
[01:43:14 - 01:43:17] אוקיי, בוא נראה רגע מה זה אומר לגבי
[01:43:20 - 01:43:23] השיעור של פרמטר של ברנולי שראינו קודם.
[01:43:26 - 01:43:29] אני גם לא מנסה להפוך דקה
[01:43:40 - 01:43:41] אני מנסה להפוך, עדיף.
[01:43:43 - 01:43:46] זכותי להתכנסה כפרק כדי שאני יכולה לכתוב לי לכבוד את המפלג.
[01:43:47 - 01:43:51] אוקיי, אז בואו נדבר על העניין הזה של הפגישה הקלאסית.
[01:43:56 - 01:44:10] איך אנחנו יכולים להגדיר אותי מייליורס בגישה הקלאסית?
[01:44:16 - 01:44:19] אז אמרנו שאנחנו רוצים מקסימום להתמודד טוב, זה המדחק הכי טוב
[01:44:22 - 01:44:23] מההתפלגות של הדאטה.
[01:44:24 - 01:44:26] נגיד שמעניין אותנו לעשות שיעורך לתטה הזה,
[01:44:26 - 01:44:29] איך אנחנו יודעים שמצאנו את התטה בצורה אופטימלית,
[01:44:30 - 01:44:31] מצאנו את השיעורך אופטימלי לתטה.
[01:44:33 - 01:44:38] אחת מהדרכים שאפשר לחשוב עליהם זה פשוט להגדיר איזשהו מרחק בין התטה שמצאנו לתטה האופטימלי.
[01:44:39 - 01:44:45] יש מדד שנקרא, או קריטריון כזה, שנקרא MSE,
[01:44:45 - 01:44:47] מינט סקוורד טרו,
[01:44:48 - 01:44:53] שהוא פשוט מוגדר ככה, זה הדוחלת.
[01:44:54 - 01:44:57] כל הדאטה סט שהיו יכולים להיוצר
[01:44:59 - 01:45:02] של השגיאה הריבועית של התטה שמצאתי,
[01:45:02 - 01:45:05] נסמר לזה שהתטה קובה הוא לפונקציה של הדאטה,
[01:45:06 - 01:45:08] אם אני מנצה בדאטה מסוים,
[01:45:08 - 01:45:10] נגיד הממוצע בשביל התוחלת,
[01:45:10 - 01:45:15] אז יצא לי ממוצע אחד, אם נגרלתי ויצא לי דאטה קצת אחר, אז הממוצע יצא קצת אחר,
[01:45:17 - 01:45:21] אז זה התטה קובה D. מה המרחק שלו מהתטה האמיתי?
[01:45:22 - 01:45:24] והגישה הקלאסית יש לנו איזשהו תטה אמיתי.
[01:45:26 - 01:45:28] אוקיי? אז אנחנו מלחצים את הטכנוכלית,
[01:45:29 - 01:45:31] זה MSE. אז מה
[01:45:32 - 01:45:35] אופטימלי מתחילת הקריטריון הזה?
[01:45:36 - 01:45:39] איך המשלח הזה צריך להיות כדי שהוא יהיה אופטימלי?
[01:45:40 - 01:45:40] אפס.
[01:45:40 - 01:45:42] בוא נסתכל רגע על
[01:45:42 - 01:45:51] הטלת מטבע שוב. אז
[01:45:52 - 01:45:52] תארנו,
[01:45:54 - 01:45:56] אתם זוכרים מה היה הטלת הטטה
[01:45:57 - 01:45:59] מרקסימום לייקליות של הטלת מטבע?
[01:46:02 - 01:46:08] זה היה N1 LKN. נכון? כמה פעמים יצא לנו הד לעומת כמה פעמים יצא לנו את זה.
[01:46:08 - 01:46:12] בוא נגיד שאני אתן לכם שני משרכים אחרים,
[01:46:12 - 01:46:13] אלגוריתמים אחרים.
[01:46:14 - 01:46:14] בוא נשארך,
[01:46:14 - 01:46:17] אתם לא חושבים איזה נוסחה וזה יכול להיות גם איזשהו אלגוריתם.
[01:46:17 - 01:46:20] נגיד אחד זה חצי של הדבר הזה.
[01:46:21 - 01:46:37] בוא נקרא לזה קטע כובע שתיים וחצי של N1 חלקי N. אז אם יצאו לי שלוש מתוך עשר זה יהיה X נקודה חמש עשרה נקודה חמש עשרה נקודה חמש עשרה ועוד נשארך
[01:46:39 - 01:46:42] שהוא פשוט תמיד אומר חצי
[01:46:42 - 01:46:46] ‫לא משנה מה יצא בדעת שם, אוקיי?
[01:46:47 - 01:46:50] ‫אז עכשיו אני רוצה לשאול איזה משערך הוא יותר טוב ‫מבחינת הפריטריון הזה שאתם מסתכלים.
[01:46:52 - 01:46:53] ‫איזה משערך יותר טוב?
[01:46:54 - 01:46:55] ‫מה פתאום?
[01:47:03 - 01:47:03] מה?
[01:47:05 - 01:47:06] זה תלוי בדאטה אמרת?
[01:47:07 - 01:47:11] זה תלוי בדאטה, נכון? כי זה משנה בדאטה, אבל מה שיותר גרוע פה זה שמתתלמיד גם בתטא.
[01:47:11 - 01:47:13] ‫יש איזשהו תטא שהוא תטא נכון?
[01:47:15 - 01:47:16] ‫אנחנו לא יודעים אותו.
[01:47:19 - 01:47:21] ‫בפיקונקציה של תטא ה-MSC הזה יהיה,
[01:47:22 - 01:47:25] ‫יכול להיות שכל שרק אחר יהיה יותר טוב.
[01:47:26 - 01:47:29] ‫הדרך יותר נכונה להסתכל על הדבר הזה, ‫זה בתור איזשהו גרע.
[01:47:30 - 01:47:34] זה תטא, זה בין 0 ל-1, זה רק התטאים האפשריים בו,
[01:47:35 - 01:47:36] ‫וזה ה-MSC.
[01:47:36 - 01:47:42] ‫אוקיי, אז עבור תטא המקסימום ל-IQ,
[01:47:43 - 01:47:45] ‫מה תהיה השגיאה לכל אחד ‫מהקטות האפשריות?
[01:47:47 - 01:47:48] ‫תהיה איזושהי שגיאה,
[01:47:48 - 01:47:50] ‫לאחר כך שאתם תראו בתרגיל שה-MSC,
[01:47:51 - 01:47:56] ‫במוסדות אפשר לפרק אותה, ‫היא בעצם מוקדת מהביאס של המשערך,
[01:47:57 - 01:48:02] ‫אין איזה ביאס שמשערך, ומה התוחלת שלו, אוקיי?
[01:48:02 - 01:48:03] ‫התוחלת שמשערך עצמו,
[01:48:04 - 01:48:05] ‫זו לא התוחלת של השגיאה הליבואית,
[01:48:05 - 01:48:06] ‫והתוחלת של המשערך,
[01:48:07 - 01:48:08] ‫ועוד הווריאנס,
[01:48:09 - 01:48:10] ‫כמה הוא רחוק מהתוחלת
[01:48:12 - 01:48:13] ‫בשגיאה הליבואית, אוקיי?
[01:48:14 - 01:48:17] ‫אז המשערך של מסימום ל-IQ, ‫הביאס שלו הוא 0,
[01:48:18 - 01:48:19] ‫שהוא גם משהו שכבר ראיתם בהזדמנות,
[01:48:20 - 01:48:24] ‫אבל פחות הייתה תגובה, כן?
[01:48:25 - 01:48:26] ‫לא כל המקסימום ל-IQ הוא ביאס שלו,
[01:48:27 - 01:48:28] ‫באס, במקרה הזה, כן?
[01:48:29 - 01:48:31] ‫אז בעצם מה שכתוב כאן נשאר רק את הווריאנס.
[01:48:32 - 01:48:35] ‫והווריאנס, למקרה זה, הוא משהו שהוא קבוע תמיד,
[01:48:36 - 01:48:38] ‫זה תלוי ב-N גדול.
[01:48:38 - 01:48:40] ‫אבל זה משהו קבוע וזה לא משנה, ‫עבור כל תטא
[01:48:41 - 01:48:44] ‫תהיה את אותה השגיאה, אוקיי? ‫אז תטא ml
[01:48:46 - 01:48:49] נראה ככה, תראו אותי שהיא שגיאה, ‫שהיא אותה שגיאה עבור כל התטות.
[01:48:49 - 01:48:49] ‫זה לא משנה.
[01:48:51 - 01:48:53] מה לגבי תטא 2,
[01:48:53 - 01:48:55] ‫שהוא חצי מהדבר הזה?
[01:48:56 - 01:48:58] ‫הוא חישוב יחסית מורכב, ‫לא ניכנס לזה, אבל
[01:48:59 - 01:49:01] ‫לפחות אנחנו יודעים שבאפס,
[01:49:01 - 01:49:04] ‫חצי מאפס זה אפס, ‫אז הביאה בדיוק באותו מקום.
[01:49:05 - 01:49:05] ‫נכון?
[01:49:06 - 01:49:10] ‫ככל שאנחנו נתרחק, ‫השגיאה תגדל, אוקיי?
[01:49:11 - 01:49:12] ‫כנראה משהו כזה.
[01:49:14 - 01:49:16] ‫אוקיי, זה השגיאה של תטא 2, אז
[01:49:16 - 01:49:27] ‫במקרה הזה די ברור ‫שתטא ml יותר טוב מטטא 2. ‫אבל כל תטא, ‫תטא ml הוא יותר טוב מטטא 2. ‫אבל מה קורה אם נסתכל על תטא 3?
[01:49:30 - 01:49:31] ‫מה יהיה ה-C שלו?
[01:49:34 - 01:49:45] ‫-0 וחצי וכאילו בשער כמו צודק על זה? ‫-נכון, אז פה, וחצי הוא צודק בול, נכון? ‫אם תטא באמת היה חצי והוא אמר חצי, ‫אז זה צדק, זאת תהיה שגיאה יותר טובה ‫מהמוקצות רייקר.
[01:49:48 - 01:49:53] ‫וכל שמתרחקים, ‫זה פשוט יהיה שגיאה ריבועית ‫מהמרחק, זה יראה משהו כזה.
[01:49:57 - 01:50:04] ‫זה פטא 3. ‫זה יותר טוב, פטא 3 הוא פטא ml.
[01:50:05 - 01:50:15] ‫זו בדיוק בעיה בגישה הקלאסית, ‫אנחנו אין דרך להגדיר ‫אופטימליות של המשער ערך, ‫במובן הזה של מה הכי טוב, ‫מה הכי קרוב לטטו.
[01:50:15 - 01:50:18] ‫כי אם אנחנו אומרים שיש את הטטא נכון ‫ואנחנו לא יודעים אותו,
[01:50:18 - 01:50:20] ‫אז גם מה הקריטריון הזה ‫אנחנו לא יכולים לחשב בזה.
[01:50:22 - 01:50:25] ‫מה אפשר לעשות? גישה בייזיאנית, זאת אומרת,
[01:50:25 - 01:50:27] ‫נסתכל על כל הטטו האפשריות,
[01:50:28 - 01:50:31] ‫ונעשה איזשהו ממוצע כזה ‫של כל הטט האפשרי, כל הטט האפשרי,
[01:50:32 - 01:50:39] ‫ומי שיהיה הכי טוב בממוצע ‫על כל הטט האפשרי, ‫הוא נותן לו משלב הכי טוב.
[01:50:40 - 01:50:43] ‫אבל למה סתם עשויות ממוצע? ‫אין משמעות. אם יש טטו אחד שהוא באמת
[01:50:44 - 01:50:50] ‫הוא הטט האמיתי, ‫או רוב הסיכויים שהוא יהיה הטט האמיתי, ‫או שיש יותר סיכוי שהטטו ‫יהיה באיזשהו
[01:50:50 - 01:50:53] ערך אחד מאשר לעומת ערך אחר,
[01:50:54 - 01:50:57] ‫אבל לא סתם עשויות ממוצע, ‫אז עכשיו הממוצע הזה יהיה משוקלט.
[01:50:57 - 01:51:00] ‫בגישה בייזיאנית, זאת אומרת, ‫את הממוצע הזה נעשה לפי
[01:51:00 - 01:51:04] ‫הפרייו שלנו לא טט. ‫צריכים להניח משהו על טטא ‫כדי לעשות את הממוצע הזה כמו שצריך.
[01:51:05 - 01:51:08] ‫צריכים להניח שיש איזושהי, ‫נגיד, התפלגות אחידה כמו לטטא.
[01:51:08 - 01:51:10] ‫אבל נגיד שרוב הסיכויים שהטטות...
[01:51:11 - 01:51:16] ‫אנחנו יודעים מראש ‫שרוב המטבעות הם הוגנים, ‫אבל יש מדי פעם ‫איזשהו זנב של מטבעות שהם לא הוגנים.
[01:51:16 - 01:51:20] ‫אנחנו רוצים דווקא יותר ‫שזה יהיה טוב באזור הזה,
[01:51:20 - 01:51:22] ‫יותר חשוב לנו מאשר שזה יהיה טוב ‫באזורים אחרים.
[01:51:23 - 01:51:24] ‫זה בדיוק הפרייו על טטא.
[01:51:25 - 01:51:37] ‫אוקיי, אז יש משהו שנקרא BMSC. ‫פה אין דרך להגדיר ‫אופיימלית בגישה הקלאסית,
[01:51:38 - 01:51:39] ‫אבל בגישה בייזיאנית
[01:51:45 - 01:51:48] ‫יש משהו שנקרא BMSC, זה בייז'ן LSE,
[01:51:51 - 01:51:52] ‫שהוא מוגדר
[01:51:53 - 01:51:59] ‫תוחלת על פני כל הטטות האפשריות ‫של ה-MSC.
[01:52:00 - 01:52:04] ‫עכשיו, MSC הוא בעצם לא עוד פונקציה ‫של טטה חובה,
[01:52:04 - 01:52:05] ‫זה גם של טטה,
[01:52:06 - 01:52:07] ‫טטה משתומם.
[01:52:08 - 01:52:15] ‫פה טטה היה זה משהו ידוע, ‫כי הוא היה צריך להיכנס ‫לתור הפרמטר של מהטטרים.
[01:52:18 - 01:52:20] ‫כן, הדבר הזה בעצם שווה ל...
[01:52:20 - 01:52:23] ‫אבל אז ככה תוחלת על פני ת'טא,
[01:52:25 - 01:52:27] ‫יש פה איזשהו פריו,
[01:52:28 - 01:52:28] ‫מציית ת'טא,
[01:52:30 - 01:52:33] ‫בכל ההסתברות של...
[01:52:34 - 01:52:37] ‫זה מה שכתוב כאן, ‫זה מה שכתוב כאן וכאן,
[01:52:38 - 01:52:39] ‫כפי שלפי,
[01:52:40 - 01:52:43] ‫כי הוא נותן ת'קטוריה לבנואן ראיין,
[01:52:48 - 01:52:49] ‫זה מבחינת ההגדרה של תוחלת.
[01:52:50 - 01:52:57] ‫פה נותנת אותו דבר, רק שעכשיו ההסתברות ‫ההסתברות לייצר דאטה היא כמויה בת'טא,
[01:52:58 - 01:52:59] ‫אם ת'טא הוא משתמש שהכול ישתנות,
[01:53:01 - 01:53:03] ‫אם יש לכם D ולנותן ת'טא,
[01:53:20 - 01:53:23] ‫אוקיי, אז זה ה-BMSC. ‫אז עכשיו כשיש לנו קריטריון כזה,
[01:53:24 - 01:53:28] ‫אנחנו יכולים להשתמש בו ‫כדי למצוא את המשרך האופטימלי.
[01:53:31 - 01:53:32] ‫אוקיי, אז גם מה שנעשה עכשיו.
[01:53:34 - 01:53:36] ‫ברור מה הכתרה לזה אומר?
[01:53:41 - 01:53:45] כן. ‫-הדיאזן זה כאילו ההסתברות לדאטה ‫עם עבר נתון ת'טא?
[01:53:46 - 01:53:47] ‫למעלה מספיק.
[01:53:48 - 01:53:49] ‫זה מה שנקרא להקליאות.
[01:53:50 - 01:53:58] ‫אוקיי, אז בואו, אז אנחנו, ‫ברגע שיש לנו קריטריון כזה ‫שהוא מוגדר היטב,
[01:53:59 - 01:54:05] ‫אנחנו יכולים למצוא את המשרף, ‫לתת הכובע שממקסם אותו.
[01:54:05 - 01:54:06] אוקיי?
[01:54:06 - 01:54:11] ‫הייתי יכול להגיד לכם גם קודם, ‫בואו נעשה את זה פה, ‫אבל מה שיוצא לנו זה משהו ‫שתלוי בת'טא.
[01:54:11 - 01:54:13] אוקיי? כאן זה יוצא משהו שהוא לא תלוי בת'טא.
[01:54:16 - 01:54:17] בואו נראה.
[01:54:20 - 01:54:25] ‫אוקיי, אז אני רוצה...
[01:54:28 - 01:54:34] ‫אז אני אמשיך את זה כאן, ‫אז אפשר לכתוב את ה-NC בצורה אחרת,
[01:54:35 - 01:54:38] ‫להפוך פה את הסדר של האינטגרל. ‫אני אעשה קודם אינטגרל על V,
[01:54:39 - 01:54:43] ‫ואז על פיימנט גם החוק. ‫הדבר הזה, ואפשר לחשוב עליו אותו
[01:54:44 - 01:54:48] ‫בכלל השרשרת של ההתברגות המשותפת ‫בין גיל לדאטא,
[01:54:49 - 01:54:53] ‫בואו נשנה את הסדר של קארציה שתכוון. ‫זה יהיה פה אינטגרל
[01:54:54 - 01:54:54] ‫של פי
[01:54:55 - 01:54:56] של V,
[01:54:59 - 01:55:01] ‫כאן אינטגרל של פי של
[01:55:02 - 01:55:03] ‫ת'טא בין גיל לדאטא.
[01:55:05 - 01:55:06] ‫השגיאה הזו...
[01:55:18 - 01:55:21] ‫זאת אומרת, ‫האינטגרליה של השרשרת, ‫האינטגרציה.
[01:55:23 - 01:55:24] ‫עכשיו הדבר הזה
[01:55:26 - 01:55:28] ‫נקרא J של
[01:55:34 - 01:55:37] ‫מה כתוב כאן? ‫כתוב התוחלת
[01:55:38 - 01:55:41] של J בין פי ל-D.
[01:55:43 - 01:55:47] ‫אחזור לנושאי הרבה פעמים, ‫כל פעם יצא מידת העולם, ‫אני אצא מידת הסט אחר.
[01:55:48 - 01:55:51] ‫אני מתנשא את המשרך הזה, ‫אני מתנשא את השגיאה, ‫אני חושב את האינטגרל הזה,
[01:55:52 - 01:55:54] ‫קומציני J אחר, ‫אם אני מנסה לציין סט אחר פעמים ואמצע,
[01:55:55 - 01:55:58] ‫זה התוחלת של ה-J. ‫עכשיו,
[01:55:59 - 01:56:02] אני מחפש את הת'כובע, ‫את האלגוריתם הזה.
[01:56:04 - 01:56:08] ‫למה זה אלגוריתם? ‫כי בינתי נאטה אני מחזיר ‫לאיזשהו משהו ארוך של ת'כובע.
[01:56:08 - 01:56:10] ‫אני מחפש את האלגוריתם
[01:56:10 - 01:56:13] ‫שהוא יהיה אופטימלי מתוך התוחלת.
[01:56:14 - 01:56:17] ‫עכשיו, אם אני אמצא משהו שהוא אופטימלי ‫בכל דיל,
[01:56:18 - 01:56:22] ‫זה שקול, זה בטוח יהיה אופטימלי ‫גם בתופעת.
[01:56:23 - 01:56:28] ‫כאן אני, מה אני עושה? ‫אני ממשקל כל אחד מהדיל ‫לצורה של הבחרת,
[01:56:28 - 01:56:29] ‫לפי ההסתמנות של D,
[01:56:30 - 01:56:32] ‫ואם יש לי כאן משהו שמורכח לי ‫שהוא אופטימלי לכל D,
[01:56:33 - 01:56:36] ‫לא משנה איך אני עושה את המשקול הזה, ‫אז סיפור של דבר עובד אופטימלי.
[01:56:37 - 01:56:40] ‫אז אני פשוט אמצא את ה...
[01:56:41 - 01:56:42] ‫לתת הכובע האופטימלי,
[01:56:45 - 01:56:48] ‫ממזערת J בבונגולרי.
[01:56:48 - 01:57:03] ‫אז J בעצם הוא אינטגל של פט קרובה,
[01:57:04 - 01:57:06] ‫הוא אינטגרל של פי של פטא.
[01:57:18 - 01:57:23] ‫כן, עכשיו, אני לא כתבתי פה D יותר,
[01:57:24 - 01:57:26] ‫כי עכשיו אני אומר, עבור D מסוים,
[01:57:26 - 01:57:28] ‫אני מחפש את ה-output של האלגוריתם
[01:57:30 - 01:57:31] ‫האופטימלי.
[01:57:31 - 01:57:36] ‫אני הולך להתייחס לזה בתור וקטור, ‫זה הוקטור שהאלגוריתם הזה צריך להוציא,
[01:57:36 - 01:57:40] ‫ומה ה-output האופטימלי שהוא יכול להוציא. ‫אז אני פשוט אגזור את D, J
[01:57:43 - 01:57:44] ‫לת פי קטא קודם.
[01:57:49 - 01:57:52] ‫אני יכול להכניס את הנגזרת פה ‫לתוב חמיים בקינטגלאט.
[01:57:53 - 01:57:54] ‫אני מקבל
[01:57:55 - 01:57:55] E של
[01:57:56 - 01:58:05] T נקודם G. ‫כן, הנגזרת של זה. ‫שוב, זה נורמה בריבו של וקטור, ‫אז אפשר לחשוב את זה בטוב
[01:58:05 - 01:58:07] ‫בסדריים האלה בכל זמן.
[01:58:08 - 01:58:11] ‫אז יש לי פה פעמיים מה שכתוב כאן בסדריים,
[01:58:13 - 01:58:17] ‫שני תטא קודם אחוז תטא.
[01:58:19 - 01:58:23] ‫עדיין בתוך האינטגרל הזה, אוקיי?
[01:58:23 - 01:58:25] ‫שוב, אני שואל מתי זה שווה לאפס.
[01:58:28 - 01:58:30] ‫אפשר להפריד את זה ‫לשני אינטגרלים שונים,
[01:58:31 - 01:58:32] ‫יש איתנו אינטגרל של פי
[01:58:33 - 01:58:37] ‫תטא בלי שתיים, ‫אז אני לא מתקדם ממנו, תטא קובה,
[01:58:39 - 01:58:42] ‫מתי זה שווה לאינטגרל של פי
[01:58:43 - 01:58:44] ‫תטא בין F&D
[01:58:45 - 01:58:47] ‫של תטא בין פי תטא.
[01:58:49 - 01:58:53] ‫אוקיי, מה כתוב כאן? ‫כי יש לי אינטגרל שהוא לא תלוי בכלל בטטא קובה.
[01:58:54 - 01:58:56] ‫אני יכול להוציא את טטא קובה החוצה.
[01:58:56 - 01:58:58] ‫אז מה נשאר לי? ‫מה זה האינטגרל הזה?
[01:59:00 - 01:59:03] ‫אחד, פשוט אינטגרל ההתפלגות על B, על תטא?
[01:59:04 - 01:59:06] ‫זה פשוט יהיה אחד, ‫אם אפשר לקנטט את הכובע.
[01:59:08 - 01:59:09] ‫זה מה כתוב כאן?
[01:59:18 - 01:59:23] ‫מה זה הביטוי הזה?
[01:59:23 - 01:59:27] ‫תוכלת של כמעט?
[01:59:28 - 01:59:30] ‫תטא בינתנדיגי.
[01:59:40 - 01:59:44] ‫זה הפתרון האופטימלי, ‫אם זה מה שאני רוצה למזער,
[01:59:44 - 01:59:49] ‫פשוט צריך למצוא את התוכלת ‫של תטא בינתנדיגי. ‫פי עצם אומר שזה לא תלוי בתטא.
[01:59:52 - 01:59:56] ‫לא תלוי בתטא. ‫כן, אני יכול, עבור כל דאטה סל ‫שתיתנו לי,
[01:59:57 - 01:59:58] ‫זה הדבר האופטימלי לעשות.
[01:59:58 - 01:59:59] ‫קוראים לזה גם
[02:00:04 - 02:00:06] ‫טטא mtst
[02:00:08 - 02:00:12] למינימום mse ddb או בין רשומה,
[02:00:12 - 02:00:13] ‫קצת לבלבל.
[02:00:14 - 02:00:21] ‫ככה קודם נמתן את תטא mmse, ‫זה בעצם המשערך שממזער ‫את ה-basian תטא בינתן לי.
[02:00:24 - 02:00:27] ‫אז לא משנה, זה הדבר האופטימלי ‫לעשות תמיד.
[02:00:28 - 02:00:30] ‫לא משנה איזה משערך תיתנו לי,
[02:00:31 - 02:00:34] ‫משערך שיכול להחזיר ‫את התוכלת של תטא בינתן לי,
[02:00:35 - 02:00:36] ‫תמיד יהיו הפתימות.
[02:00:39 - 02:00:43] ‫זה הקריטריון שבעניין אותנו, ‫-basian bc,
[02:00:44 - 02:00:45] ‫ושזה הפרעיון באמת נכון, שתף.
[02:00:51 - 02:00:54] ‫אם התנאים האלה נכונים, ‫לא משנה איזה משערך תתמציאו,
[02:00:55 - 02:00:57] ‫אם גם בגמרי שלושה משערכים, ‫אבל אתם יכולים,
[02:00:58 - 02:01:02] ‫לא יודע, ‫מכל הדרשת אירוני, ‫עם אלף שכבות וקונקולוציות ורזמנטס,
[02:01:03 - 02:01:05] ‫לא משנה מה, ‫טנשן, פרנספורמרס,
[02:01:06 - 02:01:06] ‫זה יהיה יותר טוב.
[02:01:08 - 02:01:08] ‫אוקיי?
[02:01:09 - 02:01:12] זה יכול להיות קשה לחישוב, ‫אבל מה זה היה? ‫זו תוכלת של הפוסטיריות, כן?
[02:01:12 - 02:01:15] ‫בעצם לחשב את הפוסטיריות, ‫שנתתה באינטרנגלית.
[02:01:18 - 02:01:22] ‫אם אתה חושב את הפוסטיריות ‫בתוכלת שלו, ‫או אולי אפשר לחשב את הפוסטיריות,
[02:01:24 - 02:01:26] ‫אבל זה תמיד יהיה...
[02:01:26 - 02:01:43] ‫אוקיי, בואו נראה מה זה אומר ‫לגבי הטלת מטבע.
[02:01:44 - 02:01:47] ‫נכון? אז בהטלת מטבע ראינו ‫מה המקסימום לייפיות, ‫המשערך המקסימום לייפיות שלו,
[02:01:48 - 02:01:51] ‫הרגיל, אבל מה יהיה עם שערך ה-BMSC,
[02:01:53 - 02:01:54] ‫המשערך MMC?
[02:01:55 - 02:02:01] ‫אז אנחנו צריכים קודם כול ‫להגיד איזשהו פריור ‫על ההטיה של המטבע, על תטא, נכון?
[02:02:01 - 02:02:05] ‫נניח שתטא הוא אוניפורמי, אוקיי? ‫שהתטא מתפלג
[02:02:07 - 02:02:09] ‫התפלגות אוניפורמית בין 0 ל-1,
[02:02:10 - 02:02:16] ‫זה לא יכול להיות ערכים שונים ‫מהקטע הזה בין 0 ל-1.
[02:02:16 - 02:02:22] ‫אז נגיד שאנחנו לא יודעים ‫על איזשהו תטא ‫שהוא יותר סביר מהשני,
[02:02:22 - 02:02:26] ‫ואנחנו רוצים פשוט למצוא את התטא,
[02:02:27 - 02:02:32] ‫אנחנו מניחים שהתטא הוא ‫תשתמשות אותה ההסתברות ‫להיות בכל הטיה שיכולה להיות,
[02:02:32 - 02:02:34] ‫אוקיי? ‫זו המשמעות של הפריור זה.
[02:02:35 - 02:02:39] ‫אז עכשיו אנחנו צריכים לחשב ‫את התוחלת של הפוסטיריור, נכון? ‫בשביל לחשב את ה-MCE,
[02:02:40 - 02:02:41] ‫הפוסטיריור שלנו,
[02:02:43 - 02:02:46] מה זה? ‫זה הסתברות של תטא בהינתן D,
[02:02:48 - 02:02:51] ‫שזה שווה ל-Prior, כפול.
[02:02:52 - 02:02:52] ‫-Lightlihood,
[02:02:54 - 02:02:55] אוקיי,
[02:02:57 - 02:02:58] ‫כדי שהוא יקדם נרמוז.
[02:02:58 - 02:03:00] ‫אז מה זה במקרה שלנו?
[02:03:01 - 02:03:01] ‫מה זה הפריור?
[02:03:04 - 02:03:06] ‫זה הדבר הזה, נכון? ‫אז עבור כל ערך של תטא,
[02:03:07 - 02:03:08] ‫פשוט יהיה שווה 1,
[02:03:11 - 02:03:13] ‫כל עוד אנחנו בין 0 ל-1.
[02:03:15 - 02:03:19] ‫נגיד מראש שאנחנו רק מסתכלים ‫על הערכים בין 0 ל-1, ‫אז פה פשוט יהיה כתוב 1,
[02:03:20 - 02:03:21] ‫כפול, מה זה ה-Lightlihood?
[02:03:22 - 02:03:24] ‫כתבנו את זה קודם.
[02:03:25 - 02:03:26] ‫בהינתן איזשהו תטא,
[02:03:27 - 02:03:28] ‫מה ההסתברות לראות
[02:03:29 - 02:03:30] ‫איזשהו דאטה שאנחנו רואים?
[02:03:33 - 02:03:36] ‫-M�סימום ל-Lightlihood. ‫-לא מקסימום, ל-Lightlihood.
[02:03:37 - 02:03:38] מה ה-Lightlihood?
[02:03:41 - 02:03:47] זה מה שראינו קודם, ‫זה פשוט יהיה, בהינתן תטא, זה יהיה תטא כפול מספר הפעמים ‫שזה יצא העץ בדאטה,
[02:03:48 - 02:03:50] כפול 1,
[02:03:52 - 02:03:54] ‫1 מינוס תטא, זה חזקת
[02:03:55 - 02:03:56] ‫ספר הפעמים שהיה...
[02:04:01 - 02:04:03] ‫אוקיי, ומה זה? ‫זה יכול להיות משהו שקשה לחשב,
[02:04:04 - 02:04:06] ‫אבל זה לא כזה חשוב כרגע,
[02:04:06 - 02:04:08] ‫כי זה איזשהו מקדם נרמול,
[02:04:09 - 02:04:12] ‫שצריך לגרום לדבר הזה, ‫לאינטגרל על הדבר הזה, להיות אחד.
[02:04:14 - 02:04:18] ‫אוקיי, עכשיו שימו לב, ‫אנחנו מסתכלים על הדבר הזה ‫בתור התפלגות על תטא.
[02:04:19 - 02:04:26] ‫קודם לא היינו צריכים כמו דבר הזה ‫שחשבנו על זה בתור לייקיות, ‫בגלל ההתפלגות על התוצאות ‫של הטלת מטבע.
[02:04:26 - 02:04:42] ‫היינו צריכים שעבור כל התוצאות האפשריות ‫הסכום יהיה שווה 1. ‫עכשיו אנחנו מסתכלים על זה ‫בתור התפלגות על כל ההטיות האפשריות, ‫המשתנה המקרה שלנו זה תטא. ‫אנחנו צריכים שאינטגרל על תטא יהיה שווה 1. ‫בגלל זה אנחנו צריכים, ‫יש פנות פה מקדם אחרת.
[02:04:42 - 02:04:50] ‫אוקיי, אז עכשיו מה התוחלת ‫של ההתפלגות הזאת?
[02:04:51 - 02:05:00] ‫קצת מסובך, נחשב, ‫אבל בואו נעשה את זה פה, ‫אבל המשפחה הזו של התפלגויות ‫שנראות ככה נקראת התפלגות דריכלה.
[02:05:06 - 02:05:10] ‫אפשר להסתכל בויקיפדיה ‫ולראות מה זה ההתפלגות הזאת,
[02:05:11 - 02:05:14] ‫זו המשפחה הזאת, ‫ומה יוצא התוחלת של ההתפלגות הזאת?
[02:05:19 - 02:05:22] ‫זה מה שאנחנו עושים, נכון? ‫תוחלת של זה, תוחלת של זה,
[02:05:24 - 02:05:24] ‫זה יוצא
[02:05:26 - 02:05:28] ‫שמוגדרת עם מינוס 1 עם פה ועניין.
[02:05:33 - 02:05:33] ‫זה יוצא ככה.
[02:05:37 - 02:05:39] ‫קצת נוזר, אבל גם שזה יוצא.
[02:05:41 - 02:05:43] ‫אם אנחנו מניחים שלא ידענו כלום ‫על המוגביע,
[02:05:44 - 02:05:45] ‫הדבר האופטימלי לעשות
[02:05:47 - 02:05:51] ‫הוא לא מה שעשינו קודם, ‫n1 חלקי n2,
[02:05:52 - 02:05:56] ‫אלא n1 ועוד 1 חלקי n ועוד 2.
[02:05:57 - 02:06:00] ‫כן, זה קשור.
[02:06:11 - 02:06:18] ‫לא, אז זה תלוי בזה, אוקיי? ‫יש פה איזושהי פונקציה מורכבת ‫שגורמת הדבר הזה להיות 1.
[02:06:20 - 02:06:21] ‫והתוחלת תמיד תצא זה.
[02:06:26 - 02:06:30] ‫מה זה אומר כאילו, גרפית? ‫בעצם היה לנו את כל הטטות האפשריות,
[02:06:30 - 02:06:32] ‫איך הלייקלי עוד נראה?
[02:06:34 - 02:06:36] ‫פונקציית הלייקלי, נגיד שיצא לנו,
[02:06:37 - 02:06:37] ‫נגיד שיצא לנו, נגיד, שלוש,
[02:06:39 - 02:06:40] ‫n1 שווה שלוש, אוקיי?
[02:06:40 - 02:06:47] ‫נגיד שאין לך את שווי שלוש, ‫אז יצא לנו כמו שהיה לנו קודם בדוגמה, ‫שלושה האדס ואפשר שבעה טיילס.
[02:06:48 - 02:06:52] ‫אז מה ההסתברות שטטא שווה 0?
[02:06:58 - 02:07:02] ‫אפס. אפס, נכון? ‫לא יכול להיות שטטא שווה 0, ‫כי אז לא היינו יכולים לראות אף האדס.
[02:07:04 - 02:07:04] נכון?
[02:07:05 - 02:07:07] ‫אותו דבר לגבי 1, ‫גם 1 אין סיכוי,
[02:07:07 - 02:07:08] ‫כן לא היינו רואים את ה' לד,
[02:07:08 - 02:07:09] ‫שני.
[02:07:10 - 02:07:14] ‫ואיפה זה המקסימום של הדבר הזה?
[02:07:15 - 02:07:17] ‫שאני אומר גם המקסימום של הכול.
[02:07:18 - 02:07:19] ‫המקסימום לייקליות,
[02:07:21 - 02:07:24] ‫זה גם המקסימום של הכול, ‫כי רק הכפלנו ב-1, חילקנו בטבוע,
[02:07:25 - 02:07:26] ‫זה ב-3.
[02:07:27 - 02:07:29] ‫ב-3 יש לנו נקודות מקסימום,
[02:07:30 - 02:07:32] ‫אז זה עולה למקסימום, ולכן זה עולה,
[02:07:32 - 02:07:32] ‫אני מצפה, אז
[02:07:34 - 02:07:35] בואו נגיד
[02:07:37 - 02:07:38] ‫זה ב-3 נגיד.
[02:07:41 - 02:07:45] ‫אז המקסימום הוא ב-3,
[02:07:46 - 02:07:48] ‫אבל התוחלת של הדבר הזה ‫זה לא ב-3, אוקיי?
[02:07:49 - 02:07:52] ‫התוחלת יצא לנו 3 ועוד 1, ‫שזה 4,
[02:07:56 - 02:07:57] אוקיי?
[02:07:58 - 02:07:58] 12.
[02:08:00 - 02:08:03] לא, זה 3. רגע? כן, 4 חלקי 12.
[02:08:11 - 02:08:24] ‫כן, הבנתי 0.3. ‫זה שליש, זה שליש בעצם יוצא,
[02:08:25 - 02:08:32] ‫וזה 0.3. ‫-כן, אז המקסימום פה הוא באמת ‫כמו המקסימום לייקליות,
[02:08:33 - 02:08:34] ‫המקסימום של הפוסטריאור ‫הוא כמו המקסימום של הלייקליות,
[02:08:35 - 02:08:38] ‫אבל ראינו שהדבר האופטימלי, ‫אם אנחנו מתעניינים בשגיאה רגועית,
[02:08:39 - 02:08:41] ‫היא לא המקסימום, אלא התוחלת.
[02:08:43 - 02:08:45] ‫אז אם אנחנו רוצים למזער ‫את השגיאה הרגועית,
[02:08:46 - 02:08:50] ‫אנחנו לא יודעים כלום על פתא, ‫במובן הזה שאנחנו מניחים התפלגות
[02:08:50 - 02:08:52] ‫אכילה על כל הערכים האפשריים,
[02:08:52 - 02:08:55] ‫וזה הדבר האופטימלי של עצמו.
[02:08:57 - 02:08:57] ‫אוקיי?
[02:08:58 - 02:08:59] קצת מוזר, אבל זה מה שיוצא.
[02:09:09 - 02:09:10] ‫אומרת על זה?
[02:09:10 - 02:09:13] ‫הגובה זה כאילו פי של תתא מינתן די, או...
[02:09:14 - 02:09:20] ‫הגובה פה התכוונתי שזה, כן, פי, הפוסטריאור, ‫פי של תתא מינתן די,
[02:09:21 - 02:09:21] ‫שהוא נראה
[02:09:22 - 02:09:25] עד כדי קבוע כמו פי של די מינתן תתא.
[02:09:39 - 02:09:47] ‫אוקיי, הרבה פעמים אנחנו שוב מדברים ‫באופן כללי על סטטיסטיקה בייזיאנית, אוקיי? ‫ואנחנו
[02:09:49 - 02:09:54] בשבוע הבא רק נקשר את זה בחזרה ‫למשתנים חבויים.
[02:09:57 - 02:10:02] ‫אבל בסטטיסטיקה בייזיאנית, ‫או בסטטיסטיקה בכלל, ‫הרבה פעמים לא מעניין אותנו ‫בשלרכת תתא, נגיד בהטלת מטבע,
[02:10:03 - 02:10:06] ‫אנחנו רוצים לא לשלח את הפרמטר של המטבע, ‫אלא את ההטלה הבאה, אני חושב.
[02:10:07 - 02:10:09] ‫יש את פרדיקציה למטבע תהיה הטלה הבאה.
[02:10:10 - 02:10:12] ‫אז מה יהיה המשארך האופטימלי להטלה הבאה?
[02:10:15 - 02:10:16] ‫מישהו שומע היום?
[02:10:25 - 02:10:30] ‫בגלל זה זה יוצא זה, אבל באופן כללי,
[02:10:31 - 02:10:33] ‫ככה ניגשים לבעיה הזאת.
[02:10:34 - 02:10:36] ‫אז בעצם מה שנוח בגישה בייזיאנית,
[02:10:36 - 02:10:41] ‫זה שהכל זה משתנה, ‫אין הבדל בין פרמטר למשתנה מקרי,
[02:10:42 - 02:10:46] ‫לדאטה שראינו, לדאטה שאנחנו עדיין לא ראינו,
[02:10:46 - 02:10:47] ‫הכול זה משתנה מקרי,
[02:10:48 - 02:10:50] ‫אוקיי? מבחינת הגישה הבייאנית.
[02:10:51 - 02:10:53] ‫ולכן בעצם כל החישוב שעשינו פה,
[02:10:55 - 02:10:58] ‫אנחנו יכולים לעשות בדיוק אותו חישוב, ‫רק שהקטע מבחינתנו
[02:10:59 - 02:11:03] ‫לא יהיה ההטלה של המטבע, ההטייה של המטבע,
[02:11:04 - 02:11:05] ‫אלא הערך של המשתנה הבא.
[02:11:07 - 02:11:08] ‫אז בעצם
[02:11:16 - 02:11:18] ‫בי של N ועוד אחד תביא בעצם ההטלה הבאה,
[02:11:20 - 02:11:22] ‫הכי טוב מבחינת M עם C,
[02:11:25 - 02:11:26] ‫זה פשוט יהיה התוחלת
[02:11:27 - 02:11:29] ‫של ההטלה הבאה,
[02:11:33 - 02:11:35] ‫שיינתן כל מהדאטה שראינו עכשיו.
[02:11:37 - 02:11:45] ‫אז תמיד בעצם כשיש לנו משהו לא ידוע ‫שאנחנו רוצים לחזור,
[02:11:45 - 02:11:47] ‫משהו ידוע שכבר ראינו,
[02:11:48 - 02:11:51] ‫אז המשארך הכי טוב ‫מעט שקיעת MSE,
[02:11:52 - 02:11:53] ‫שיהיה למצוא את התוחלת
[02:11:53 - 02:11:54] ‫בדבר הזה,
[02:11:55 - 02:11:55] מה קרה?
[02:11:56 - 02:11:57] ‫זה דבר הרבה ידוע,
[02:11:57 - 02:11:59] ‫שיהיה אתנו כל הדברים האלויים.
[02:12:04 - 02:12:05] ‫במקרה הזה זה יוצר אותו דבר,
[02:12:07 - 02:12:18] ‫אם יש לנו איזשהו מודל כזה תטא, ‫אז בעצם ההסתברות של PN ועוד אחד בהינתן
[02:12:20 - 02:12:20] ‫לבדי
[02:12:22 - 02:12:23] ‫דעים אדומים,
[02:12:24 - 02:12:25] ‫P1 עד ל-N.
[02:12:29 - 02:12:30] ‫אפשר לכתוב את זה בתור
[02:12:33 - 02:12:34] ‫בדברז,
[02:12:35 - 02:12:40] ‫BN בוודל כזה בהינתן תטא,
[02:12:41 - 02:12:44] ‫זה תפקיד של תטא בהינתן
[02:12:45 - 02:12:50] ‫בוודל כזה ב-1, 1, N. אוקיי, אנחנו בעצם ‫רואים את המחורגים מהתטא,
[02:12:51 - 02:12:52] ‫אבל גם לא מעניין אותנו מה תטא.
[02:12:53 - 02:12:54] ‫אנחנו מבחינים על כל התטאות האפשרויות,
[02:12:56 - 02:12:59] ‫ועושים פשוט ממוצע של כל האפשרויות,
[02:13:00 - 02:13:01] ‫מוכלת בעצם
[02:13:02 - 02:13:03] לדבר הזה.
[02:13:03 - 02:13:05] ‫לפי ההתפלגות הוא יקשה פליל טל.
[02:13:10 - 02:13:14] ‫בסדר, הרבה פעמים כשיש לנו מודל, ‫זה מה שאנחנו צריכים לחשב. ‫יכול להיות שאנחנו יכולים לחשב את זה בצורה ישירה,
[02:13:15 - 02:13:16] ‫יכול להיות שאנחנו צריכים לעבור דרך תטא,
[02:13:18 - 02:13:18] ‫זה חישוב יותר מורחב.
[02:13:19 - 02:13:22] ‫אוקיי? אבל זה החישוב, שוב, האופטימלי,
[02:13:23 - 02:13:25] ‫בגבי ההתפלגות, ‫לפי ההתפלגות הזאת,
[02:13:26 - 02:13:28] ‫אבל יכול להיות שיהיה מסובך לחשב את זה.
[02:13:28 - 02:13:30] ‫במקרה של הדרת מטבע זה לא מסובך,
[02:13:30 - 02:13:37] ‫כי ההתפלגות הזאתי היא פשוט שווה לתטא.
[02:13:39 - 02:13:42] ‫ההתפלגות פשוט שווה לתטא, ‫הזדברות לראות
[02:13:43 - 02:13:45] ‫שהמחתלה הבאה תהיה אחד, ‫היא פשוט תטא.
[02:13:46 - 02:13:49] ‫כמו שכתוב כאן, ‫זו פשוט התוחלת בתטא.
[02:13:49 - 02:13:50] ‫ואז אני רוצה לראות
[02:13:54 - 02:13:56] ‫זה מסובך, אני לא רוצה
[02:13:58 - 02:13:59] ‫לעשות את החישוב הזה בצורה מפורשת.
[02:14:01 - 02:14:08] ‫-אוקיי, הנקודה החשובה כאן זה לזכור ‫קודם כול, מה זה תטא MMSC?
[02:14:10 - 02:14:19] ‫זה התוחלת של הקוסטיריו, של הפרמטר, ‫על הפרמטר הלא ידוע, בין הדאטה.
[02:14:20 - 02:14:21] ‫זו הנקודה הראשונה.
[02:14:22 - 02:14:30] ‫נקודה שנייה זה שלא כל כך משנה ‫למה אני קורא תטא ומה אני קורא הפרמטר,
[02:14:30 - 02:14:35] ‫או ההטלה הבאה, ‫או הפרדיקציה או הארטקוד של המודל,
[02:14:36 - 02:14:37] ‫מבחינת הגישה הביזיאנית.
[02:14:38 - 02:14:39] ‫כל דבר אני יכול לקרוא לו,
[02:14:40 - 02:14:43] ‫המשפנה הלא ידוע, זה מבחינת הגישה ‫הביזיאנית תטא,
[02:14:44 - 02:14:47] ‫וכל מה שאני כן רואה זה D. ‫אז אם אני יכול לחשב
[02:14:48 - 02:14:50] את התוחלת של מה שאני לא יודע ‫בהינתן מה שאני כן יודע,
[02:14:51 - 02:14:53] ‫זה יהיה הדבר הכי טוב ‫מבחינת הגישה הביזיאנית.
[02:14:54 - 02:15:00] ויש מקרים שזה קל לעשות,
[02:15:01 - 02:15:04] יש מקרים שאם חישבנו כבר את הדבר הזה לתטה אז יהיה לנו קל לעשות את זה לoutput,
[02:15:05 - 02:15:07] יש מקרים שזה יהיה יותר מסובך.
[02:15:14 - 02:15:16] תורם זמן לנקודה הבאה.
[02:15:23 - 02:15:36] ‫אוקיי, אז
[02:15:38 - 02:15:39] אני רק אני...
[02:15:43 - 02:15:51] ‫כן, אז לא מספיק היום לחזור ‫לעניין המשתנים החברויים, ‫זה נעשה שבוע הבא, ‫אבל עוד נקודה שהיא כללית להעסקה בייזיאנית,
[02:15:52 - 02:16:01] ‫זו הנקודה הבאה. ‫אוקיי, אנחנו דיברנו עכשיו על תטא MSE, ‫שזה התטא האופטימלי ‫עבור הקריטריון של בייז'ן MSE.
[02:16:02 - 02:16:05] ‫אבל הקריטריון הזה היה מאוד ספציפי, ‫היה עבור שגיאה ריבועית.
[02:16:05 - 02:16:08] ‫זו באמת השגיאה הכי נוחה,
[02:16:08 - 02:16:10] ‫אבל למה דווקא שנרצה שגיאה ריבועית?
[02:16:11 - 02:16:12] ‫זה יכול להיות שנרצה קריטריון אחר.
[02:16:13 - 02:16:16] ‫אבל בעצם עבור כל קריטריון ‫אנחנו יכולים לעשות את החישוב הזה.
[02:16:17 - 02:16:21] ‫עבור רוב הקריטריונים לא נצליח ‫למצוא פתרון אנליטי,
[02:16:22 - 02:16:29] ‫ויש פחות או יותר שתי דוגמאות ‫שעבורם אפשר למצוא פתרון אנליטי ‫ושמשתמשים בהן. ‫אחד, זה מה שעשינו, ‫זה עבור השגיאה הריבועית,
[02:16:30 - 02:16:35] ‫והשני, זה עבור שגיאה שאתר שמות,
[02:16:41 - 02:16:43] ‫אקסילון ניר
[02:16:45 - 02:16:48] או איט-מיס,
[02:16:49 - 02:17:01] ‫ובעולם הדיסקרטי שדיברנו עליו ‫בקורס של Machine Learning, ‫קראנו את זה 01. ‫העיקרון זה אם אנחנו צודקים
[02:17:02 - 02:17:04] ‫או לא צודקים, ‫או כל כך מעניין אותנו המרחק מה...
[02:17:06 - 02:17:08] ‫המרחק מהתוצאה הנכונה.
[02:17:15 - 02:17:18] ‫אז אם זה דיסקרטי, ‫אז אפשר פשוט לעשות 01 באמת.
[02:17:18 - 02:17:20] ‫אפשר להגדיר את זה, אבל אם זה רציף,
[02:17:21 - 02:17:24] ‫זו שאלה מה זה אומר שפגענו או שפספסנו,
[02:17:25 - 02:17:29] ‫זה אם אנחנו קרובים עד כדי Epsilon ‫לערך הנכון.
[02:17:30 - 02:17:33] ‫אבל התשמיעה היא מוגדרת ככה,
[02:17:34 - 02:17:35] ‫אם יהיה אפס
[02:17:36 - 02:17:38] עם ה-Teta כובע
[02:17:40 - 02:17:42] ‫כי קרוב לדטה,
[02:17:42 - 02:17:45] ‫קטן מאיזשהו Epsilon, מוטף שהוא קטן,
[02:17:45 - 02:17:47] ‫ואחרת זה יהיה פשוט אחת.
[02:17:47 - 02:17:48] ‫אין משהו בעומק למנחה עצמו.
[02:17:52 - 02:17:59] ‫אז מה יהיה הדבר האופטימלי? ‫אני שוב פעם אגדיר את הרגישה הביזיאנית, ‫זאת אומרת, כולנו עם ה-Teta אמיתי,
[02:18:00 - 02:18:02] ‫אנחנו עושים אינטגרציה ‫על כל ה-Teta האפשריות
[02:18:03 - 02:18:08] ‫של השגיאה הזאת, ‫אבל אני כבר אכתוב את זה הפוך, ‫כמו שכתבו קודם, ‫אז אני אעשה אינטגרציה ‫על
[02:18:09 - 02:18:12] קודם את D ועל אינטגרציה על D של
[02:18:13 - 02:18:16] ‫תתא ואינטגר D של השגיאה הזאת,
[02:18:17 - 02:18:24] ‫אז איך אני כבר אכתוב אותה, ‫הנה פונקציית אינדיקטור כזה, ‫שהיא שווה 1 כאשר התנאי הזה מתקיים.
[02:18:25 - 02:18:26] ‫זה צריך להיות בעצם התנאי הפוך,
[02:18:28 - 02:18:31] ‫תטא כובע שלי עוד תטא
[02:18:34 - 02:18:35] ‫גדול מאצו...
[02:18:47 - 02:18:51] ‫כן, זה שגיאה אחרת. ‫יש מקרים שזו השגיאה יותר רגיונית, ‫זו שגיאה רגועית.
[02:18:54 - 02:18:57] ‫ואז אני רוצה למזהר את זה, ‫אני רוצה למצוא מה האלגוריתם האופטימלי,
[02:18:59 - 02:19:00] ‫מה המשערך האופטימלי של תטא,
[02:19:01 - 02:19:02] ‫שתמיד יהיה נכון.
[02:19:03 - 02:19:05] ‫זאת אומרת, הדבר הזה אפשר לקרוא לו J,
[02:19:07 - 02:19:08] ‫הטופק שעשינו קודם, זה J,
[02:19:09 - 02:19:12] ‫זה בעצם סוג של תוכלת של J ‫על פני כל הדאטה האקשרי.
[02:19:13 - 02:19:16] ‫אני יכול פשוט לעשות אופטימיזציה ל-J עצמו,
[02:19:16 - 02:19:20] ‫עבור כל דאטה, וזה בטוח יהיה דבר אופטימלי, ‫גם בתוך התוכלת כזאת.
[02:19:21 - 02:19:23] ‫אם אני אגזור את ה-J הזה,
[02:19:23 - 02:19:27] ‫פה נגזור את זה, יותר קל לחשוב על זה ‫בצורה של ציור, נגיד
[02:19:34 - 02:19:35] ‫ה-J שווה לתוכלת
[02:19:36 - 02:19:38] ‫של P של
[02:19:40 - 02:19:41] F ל-D,
[02:19:42 - 02:19:43] ‫והדבר הזה
[02:19:46 - 02:19:57] ‫כובע ה-T גדול מ-A סוג של T. ‫אוקיי, מה זה הדבר הזה? ‫אם הכול היה פה מקבל אחד,
[02:19:59 - 02:20:01] ‫מה שהיה כתוב כאן זה פשוט ‫האינטגרל
[02:20:02 - 02:20:04] על ההתפלגות הזאת, ‫זה פשוט היה שווה אחד.
[02:20:05 - 02:20:07] ‫אם אפשר לדוגמה חד-מימדית,
[02:20:11 - 02:20:12] ‫יש לי איזה שלי פונקציית התפלגות,
[02:20:14 - 02:20:16] ‫נגיד שזה P של T ביניכם B,
[02:20:17 - 02:20:19] ‫השטח של כל הדבר הזה
[02:20:21 - 02:20:21] ‫הוא אחד, נכון?
[02:20:23 - 02:20:28] ‫עכשיו, מה אני שואל כאן? ‫אני שואל מה התטא שעבורו ‫האינטגרל הזה יהיה הכי קטן.
[02:20:30 - 02:20:36] ‫זאת אומרת, אני לא סוכם את כל האזורים, ‫אני סוכם את כל האזורים ‫חוץ מאיזשהו אזור שקרוב לתטא כובע.
[02:20:38 - 02:20:42] ‫אני סוכם על תטא, אז תטא זז כבר על הכול, ‫כל השטח חוץ מהשטח שהוא קרוב
[02:20:43 - 02:20:44] לתטא כובע.
[02:20:45 - 02:20:46] ‫אולי דרך יותר נוחה ‫לחשוב עוד ההפוך,
[02:20:47 - 02:20:49] ‫אחד פחות
[02:20:50 - 02:20:52] כל שאר השטחים של זה, ‫כל מה שקרוב לתטא.
[02:20:53 - 02:20:55] ‫זה אינטגרל על פי תטא מינוס NB,
[02:20:56 - 02:21:01] ‫של פונקציית אינטיקטור כזה, ‫של כל מה שקרוב,
[02:21:03 - 02:21:04] ‫שם שזה אפסולוג.
[02:21:04 - 02:21:15] ‫כן, אבל זה מזער את זה, ‫זה שקול למקסם את הדבר הזה.
[02:21:17 - 02:21:18] ‫אז אני מחפש בעצם
[02:21:19 - 02:21:21] ‫איזשהו אזור קטן,
[02:21:22 - 02:21:25] ‫בעצם האינטגרל הזה ‫הוא יהיה רק על האזור שהוא קרוב לתטא כובע.
[02:21:27 - 02:21:29] ‫אינטגרל זה אזור מאוד קטן, ‫אבל שייתן לי את הערך המקסימלי.
[02:21:30 - 02:21:38] ‫אוקיי, וההנחה שאפסילון הוא קטן מספיק, ‫ככה שהוא יותר קטן מהקצב ‫שהפונקציה הזאת היא זזה,
[02:21:39 - 02:21:40] ‫וב, מה יהיה המקום, האזור שבו
[02:21:41 - 02:21:44] ‫אינטגרציה על שטח קטן ‫ייתן לי את הערך הכי גדול?
[02:21:45 - 02:21:51] ‫לא מקסימום. ‫-הוא מקסימום, אז השטח הזה, נכון? ‫כנוס מינוס אפסילון של הנקודת מקסימום,
[02:21:52 - 02:21:52] ‫נכון? השטח הזה
[02:21:53 - 02:21:55] ‫הוא השטח ברוחב אפסילון ‫שהוא הכי גדול.
[02:21:56 - 02:22:00] ‫אז מה זה אומר? ‫תטא כובע
[02:22:04 - 02:22:06] ‫שמרוצה את הדבר הזה,
[02:22:06 - 02:22:09] ‫שממזער את השגיאה הזאת שהתחלנו ממנה,
[02:22:10 - 02:22:11] ‫זה פשוט
[02:22:12 - 02:22:13] ‫הארגמקס
[02:22:18 - 02:22:20] ‫של הפוסטיר הזה.
[02:22:23 - 02:22:24] ‫איך קוראים לזה?
[02:22:26 - 02:22:26] ‫איך זה שם?
[02:22:27 - 02:22:27] ‫-מאפ.
[02:22:28 - 02:22:30] ‫כן, מאפ. ‫מקסימום הפוסטריאורי.
[02:22:32 - 02:22:33] ‫כן, קודם,
[02:22:33 - 02:22:34] ‫-NMSC זה היה,
[02:22:35 - 02:22:39] ‫אם הוצאינו למקסם את השגיאה הרגועית, ‫זה ייתן לקחת את הפרץ של הפוסטריאור.
[02:22:40 - 02:22:45] ‫אם מה שמעניין אותנו זה שגיאה כזאת ‫של 0 או 1, ‫אם אנחנו קרובים מספיק לתוצאה או לא,
[02:22:46 - 02:22:48] ‫אז מה שהכי טוב זה לקחת את המקסימום של הפוסטריאורי.
[02:22:52 - 02:22:52] בסדר?
[02:22:52 - 02:22:56] ‫אוקיי, שתי הערות שקשורות לזה,
[02:22:57 - 02:22:59] ‫שאני מריח לבזוט מסיים.
[02:23:04 - 02:23:05] ‫אז הערה ראשונה,
[02:23:06 - 02:23:07] זה פה.
[02:23:15 - 02:23:16] כשיש לנו...
[02:23:17 - 02:23:19] מה קורה כשיש לנו פריור אחיד?
[02:23:22 - 02:23:35] ‫התערכו בחוק בייס,
[02:23:36 - 02:23:39] ‫אז בעצם איפה נמצא ‫המקסימום של הפוסטריאור?
[02:23:44 - 02:23:52] ‫הוא ייתו המקסימום של הלייטיות, נכון? ‫כי הפוסטריאור הוא אחיד, ‫הוא רק מוסיף ככה לגבוה, ‫יש מורמליזציה שהיא תמיד פגועה.
[02:23:53 - 02:23:57] ‫אז כל מה שגורם לשינוי בגובה של הפוסטריאור, ‫זה יהיה השינוי בגובה של הלייטיות.
[02:23:58 - 02:23:59] ‫ובעצם במקרה הזה,
[02:24:00 - 02:24:00] ‫טטא המט
[02:24:02 - 02:24:05] ‫שווה לדקולון רציום לייטיות.
[02:24:06 - 02:24:12] ‫אז מישהו שאל פה קודם, ‫אם טטא... אם הפריור אחיד, ‫אם זה לא... אם הגישה הבייזיאנית ‫היא לא אותו דבר.
[02:24:13 - 02:24:21] ‫במקרה שהטט האחיד ‫ומה שמעניין אותנו זה הלוס הזה, ‫אז בעצם הדבר הכי טוב לעשות זה ‫למצוא את המט.
[02:24:21 - 02:24:26] ‫זה עדיין לא אותו דבר, ‫כי עדיין אולי אנחנו רוצים ‫לשאול שאלות אחרות על הקוסטריאור.
[02:24:27 - 02:24:30] ‫יש תנאים מסוימים שבהם הדברים ‫יוצאים אותו דבר, ‫שזה יותר טוב משם.
[02:24:33 - 02:24:34] ‫אז זה למשל תנאי יותר כזה.
[02:24:34 - 02:24:36] ‫עוד נקודה זה אם הקוסטריאור,
[02:24:37 - 02:24:42] ‫לא הפריור, כאשר הקוסטריאור בסופו של דבר, ‫היא פי של פט בעבינת ים בי,
[02:24:43 - 02:24:44] ‫הוא באוסיאן,
[02:24:45 - 02:24:51] ‫מה אנחנו יודעים על גאוסיאן ‫בהקשר של ה...?
[02:24:52 - 02:24:52] ‫מה?
[02:24:53 - 02:24:54] ‫שווה לתוחלת מקסימום?
[02:25:01 - 02:25:05] ‫שווה לתוחלת. ‫אז זאת אומרת, אם המקסימום שווה לתוחלת, ‫זה אומר שהמשערך מה?
[02:25:08 - 02:25:10] ‫שווה למשערך MMC,
[02:25:12 - 02:25:14] ‫שהוא היה התוחלת של הפוסטריאור.
[02:25:15 - 02:25:17] ‫נכון, אז למשל ראינו בדוגמה ‫של ההטלת מטבע,
[02:25:18 - 02:25:19] ‫שראינו את הגרף הזה,
[02:25:20 - 02:25:22] ‫שהממוצע היה כאן, ‫אבל התוחלת הייתה כאן.
[02:25:22 - 02:25:23] ‫כלומר, ב-NMSC,
[02:25:24 - 02:25:25] ‫שבמקרה הזה לא יהיה טובה.
[02:25:26 - 02:25:28] ‫אבל אם מקרה הקוסטריאור יוצא לנו באוסיאן,
[02:25:29 - 02:25:30] ‫אז זה כן יפה.
[02:25:44 - 02:25:49] ‫טוב, טוב, זה היה.
[02:25:51 - 02:25:53] ‫אני רק אציג את השקף הזה, שיהיה לכם.
[02:25:55 - 02:25:56] ‫זה ככה.
[02:26:00 - 02:26:03] ‫אני אומר שאנחנו נדבר בעצם בשבוע הבא, ‫אנחנו נחזור
[02:26:08 - 02:26:10] לסיבה שהתחלנו לדבר על כוח דבר אוזל,
[02:26:10 - 02:26:16] ‫זה מה שנקרא ליטל פרמטרים במודל. ‫המודלים שאנחנו נבנה,
[02:26:17 - 02:26:21] ‫לא כולם, אבל חלקם יראו ככה, ‫בעצם יהיה לנו משתנה חבוי ‫שהרבה פעמים קוראים לו עוד ז,
[02:26:22 - 02:26:23] ‫יש לנו את הדעת העצמו,
[02:26:23 - 02:26:24] ‫לרוב אנחנו נקרא לו איקס,
[02:26:26 - 02:26:29] ‫ובעצם יש לנו איזשהו פרייר עוד ז,
[02:26:30 - 02:26:33] ‫אנחנו לא יודעים אותו. ‫עכשיו אנחנו, שאנחנו יודעים את הפרייר ואת הלייטיל.
[02:26:34 - 02:26:37] ‫במודלים שלנו יהיה לנו פרייר,
[02:26:37 - 02:26:39] ‫עם פרמטרים לא ידועים,
[02:26:39 - 02:26:45] ‫על זה, ויהיה לנו את ה-likelihood ‫שאמרנו איך זה ‫מתרגם ל-X ולתמונה,
[02:26:46 - 02:26:48] ‫שגם שם יהיה לנו פרמטרים לא ידועים.
[02:26:51 - 02:26:53] ‫והשאלה היא איך אנחנו יכולים ‫ללמוד מודלים כאלה,
[02:26:54 - 02:26:56] ‫איזה סוג של מודלים קל ללמוד את זה בצורה...
[02:26:59 - 02:27:01] ‫יש מודל שאפשר לעשות את זה ‫ממש אנליטית,
[02:27:01 - 02:27:03] ‫שוב, אם זה גאוסיאן
[02:27:04 - 02:27:05] וזה גאוסיאן,
[02:27:06 - 02:27:08] ‫שקשר בין X ו-Z הוא ליניארי,
[02:27:09 - 02:27:10] ‫אז הכול כאן גאוסיאן,
[02:27:11 - 02:27:16] ‫לא צריך בכלל לעשות את זה, ‫ההפרדה הזאת היא נגדרת, ‫ואפשר פשוט ללמוד גאוסיאן ‫באופן בשביל לרמוד ה-X.
[02:27:20 - 02:27:27] ‫זה צריך ללמוד ממודים, ‫לא תמיד ההפרדה היא לגמרי ביותר, ‫כי היא נגדרת, אבל בסופו של דבר, ‫המודל יהיה גאוסיאן ה-X, ‫אולי עם פחות פרמטרים,
[02:27:28 - 02:27:30] ‫אולי הוא לא יהיה עם קוביינס מלא,
[02:27:31 - 02:27:33] ‫אבל לא הרווחנו כלום ‫מבחינת הסיבוכיות של המודלים.
[02:27:34 - 02:27:39] ‫בזה אנחנו נתחיל בשבוע הבא, ‫אבל אז אנחנו נראה דוגמאות ‫שבהן זה לא בדיוק המצב.
[02:27:40 - 02:27:46] ‫למשל, הדוגמה הראשונה שנבדוקת נראה ‫זה ש-Z הוא משתנה בכלל לא רציף, ‫אבל הוא משתנה בדיל,
[02:27:47 - 02:27:51] ‫נגיד איזשהו מספר בין 0 ל-10,
[02:27:52 - 02:27:55] ‫ואז עבור כל אחד מה-Z'ים ‫יש לי פה גאוסיאן אחר.
[02:27:56 - 02:27:57] ‫דבר זה נקרא תערובת גאוסיאנית.
[02:27:58 - 02:28:02] ‫ובזה יש שיטה לא אנליטית ‫לכתוב את המקסימום היקליות,
[02:28:02 - 02:28:04] ‫אבל יש שיטה שהיא יחצית פעילה, ‫היא כיתה אינטרטיבית
[02:28:05 - 02:28:06] ‫שיכולה לפתור.
[02:28:08 - 02:28:12] ‫ובהמשך, הפרמטרים האלה ‫יהיו פרשתות גאונולוגיות.
[02:28:15 - 02:28:17] ‫אז זהו, יש תרגיל שפרסמתי, ‫ראיתם את זה?
[02:28:18 - 02:28:22] ‫אז הוא בעיקרון, כנראה שיהיה לכם זמן ‫לעוד שבועיים,
[02:28:23 - 02:28:30] ‫אבל אם למישהו יש זמן, השבוע, ‫אז עדיף לפתור אותו השבוע, ‫לעזור לכם קצת להבין ‫את החומר של שבוע הבא, של השיעור הבא.
[02:28:31 - 02:28:37] ‫שימו לב שיש שם כמה דברים ‫שהם בעצם מבחינת החישוב ‫הם אותו דבר ‫כמו שעשיתם בתרגיל הקודם,
[02:28:38 - 02:28:44] ‫אבל עכשיו זה הכול מוצג ‫בתור סטטיסטיקה בייזיאנית, ‫שאלות של מה הפריור, מה הקוסטיריור,
[02:28:45 - 02:28:45] ‫דברים כאלה.
[02:28:48 - 02:28:48] תודה רבה.
[02:29:00 - 02:29:00] ‫-תודה רבה.