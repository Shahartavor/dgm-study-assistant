[00:00:00 - 00:00:24] אוקיי אז בואו נתחיל ברוכים הבאים לכולם קורס מתקדם על מודלים גנרטיביים עמוקים
[00:00:25 - 00:00:28] למי שלא מכיר אותי אני קוראים דן
[00:00:30 - 00:00:38] אני אשמח להתחיל את הסמסטר ואת הקורס לצערי זה לא נפתח כמו שחשבנו שזה ייפתח
[00:00:39 - 00:00:45] אנחנו נאלצים לעשות את זה ככה בזום שזה קצת פחות נוח כי יש פה חלקים שיהיו
[00:00:45 - 00:00:49] שיומרים את הלוח נצליח להסתדר איתם
[00:00:50 - 00:00:56] אז כמו שאמרתי מי שלא נפגש איתי בעבר קוראים לי דן אני חבר סגל בחוג
[00:00:56 - 00:01:04] בחוג למדעי המחשב והתחום המחקר שלי הוא בעצם התחום הזה של הקורס הזה מודלים גנרטיביים
[00:01:04 - 00:01:06] אז אני מאוד שמח
[00:01:09 - 00:01:12] להעביר את הקורס הזה, זה קורס חדש, קורס בבנייה
[00:01:13 - 00:01:17] ואני מקווה שהוא יהיה טוב אבל הוא עדיין
[00:01:17 - 00:01:20] לא עבר אף פעם אז יש סיכוי שיהיו פה כמה תקלות
[00:01:21 - 00:01:23] מקווה שלא יהיה משהו משמעותי מדי
[00:01:26 - 00:01:26] אוקיי, אז
[00:01:27 - 00:01:38] בואו נתחיל מכמה דברים יותר מנהלתיים. המבנה של הקורס, אז קודם כל יש בשם של הקורס את המונח הזה קורס מתקדם, Advanced course, אז מה הכוונה?
[00:01:39 - 00:01:40] הכוונה היא שזה,
[00:01:41 - 00:01:45] החומר פה הוא חומר יחסית מתקדם, זאת אומרת הוא מניח
[00:01:46 - 00:01:46] ידע מוקדם
[00:01:49 - 00:01:51] די הרבה קורסים, תכף נעבור על זה
[00:01:52 - 00:01:59] וחלק מהדברים פה יהיו יחסית קשים, אני חושב, מורכבים.
[00:02:00 - 00:02:01] יותר מזה,
[00:02:02 - 00:02:11] המשמעות של Advanced course זה שבניגוד אליי לקורסי מבוא וקורסים ראשוניים בתואר,
[00:02:11 - 00:02:14] דברים פה לא לגמרי יעברו,
[00:02:14 - 00:02:19] לא תקבלו את הכל ממני בהרצאה, זאת אומרת, חלק מהדברים תצטרכו להתמודד איתם לבד.
[00:02:22 - 00:02:26] אנחנו נדבר על זה עוד קצת בהקשר של התרגילים.
[00:02:28 - 00:02:43] אז אוקיי, זו המשמעות של Advanced course והמבנה של הקורס איך שהוא יהיה זה שהחלק הראשון יהיה יותר תיאורטי ועל שיטות קלאסיות אפשר לקרוא לזה של מודלים גנרטיביים,
[00:02:44 - 00:02:48] קלאסיות הכוונה היא בלי רשתות ניאור-לרנינג,
[00:02:48 - 00:02:51] בלי Deep Learning בלי המילה Deep שבקורס.
[00:02:52 - 00:02:53] זה יהיה החלק הראשון,
[00:02:54 - 00:03:02] והוא באמת יהיה יותר תיאורטי, זאת אומרת, לא תצטרכו בתרגילים שיהיו בחלק הזה, לא תצטרכו לתכנת אלא לענות על כל מיני שאלות יותר תיאורטיות,
[00:03:02 - 00:03:05] תכף נראה בדיוק איזה סוג.
[00:03:06 - 00:03:14] זה יהיה החלק הראשון, והחלק השני יהיה לקחת את העקרונות שאנחנו נלמד בחלק הראשון ובעצם לראות איך
[00:03:15 - 00:03:21] פיתחו מהם מודלים עמוקים ושונים שמגיעים ליכולות
[00:03:22 - 00:03:23] מאוד טובות,
[00:03:24 - 00:03:26] וגם התרגילים יהיו פשוט לממש את
[00:03:28 - 00:03:29] המודלים האלה.
[00:03:32 - 00:03:33] שאלות על המבנה של החוץ?
[00:03:40 - 00:03:44] אוקיי, אז מה אני מניח שאתם יודעים?
[00:03:44 - 00:03:46] יש פה רשימה של כמה דברים.
[00:03:47 - 00:03:52] לא כל הדברים האלה מופיעים באופן רשמי בדרישות קדם של קורס,
[00:03:54 - 00:03:58] כי חלק מהדברים אני חושב שאפשר להסתדר איתם, אפשר ללמוד אותם גם לבד בצורה די טובה,
[00:03:59 - 00:04:00] בצורה מספיקה בשביל הקורס.
[00:04:01 - 00:04:02] לא רציתי שזה יהיה באופן רשמי,
[00:04:03 - 00:04:04] אבל שיהיה ברור, אני אומר את זה עכשיו,
[00:04:05 - 00:04:11] שאנחנו מניחים שידע מוקדם בחומר הזה. מי שמרגיש פתאום שאין לו את זה,
[00:04:12 - 00:04:15] והמטרה של התרגיל הראשון שכבר פרסמתי ונדבר עליו,
[00:04:16 - 00:04:19] זה גם קצת להכניס אתכם לעניינים,
[00:04:19 - 00:04:26] ליישר קו ולראות שאתם תראו, שאתם מבינים את כל מה שנדרש בתור רקע לקורס הזה.
[00:04:27 - 00:04:32] מי שמרגיש שהוא חסר לו משהו, אז זה לא אבוד, הוא יכול לנסות להשלים את זה.
[00:04:34 - 00:04:36] רוב הדברים האלה, במצב שאתם נמצאים,
[00:04:36 - 00:04:41] או שאתם כבר יודעים את זה או שאתם במצב שאתם יכולים להשלים את הפער לבד.
[00:04:42 - 00:04:45] אוקיי, אז התחומים עצמם הם הסתברות.
[00:04:47 - 00:04:47] אנחנו בעצם,
[00:04:48 - 00:04:50] כל המודלים שאתם עבוד איתם יהיו מודלים הסתברותיים,
[00:04:50 - 00:04:56] וכל הרעיון פה זה גישה הסתברותית למידול של דאטה ולעשות inference.
[00:04:58 - 00:04:58] אלגברה לינארית,
[00:04:59 - 00:05:02] אז אנחנו נעבוד בדברים בממדים גבוהים,
[00:05:02 - 00:05:12] ואנחנו כמעט תמיד נכתוב הכל בכתיב וקטורי ומטונציוני, ואנחנו נצטרך לכתוב ולמצוא ערכים עצמיים ולהבין את המשמעויות של וקטורים עצמיים.
[00:05:16 - 00:05:21] צריך רדווה, אנחנו בעצם הרבה פעמים נצטרך לגזור דברים ולהבין את המשמעויות של זה.
[00:05:23 - 00:05:31] אז זה יותר דברים ממש בסיסיים, ובתחום שלנו, אז Machine Learning זה ממש דרישת דם של הקורס.
[00:05:32 - 00:05:37] מה שבעצם הופך, אני חושב, את כל מה שמעליו גם לדרישת דם, פשוט אני לא יודע אם אתם זוכרים את הדברים האלה.
[00:05:39 - 00:05:46] אז ברור שאנחנו ניגשים לבעיה הזאת בגישה של למידה חישובית של Machine Learning.
[00:05:48 - 00:05:51] אז העקרונות הבסיסיים של Machine Learning תקפים גם בקורס הזה.
[00:05:52 - 00:05:53] ו-Deep Learning,
[00:05:54 - 00:06:00] אז גם יש כמה עקרונות וכמה סוגים של מודלים שאנחנו נניח שאתם מכירים,
[00:06:00 - 00:06:03] ומי שלא מכיר, שיכול, שיוכל להשתמש בקומו.
[00:06:05 - 00:06:10] זה בהקשר של העקרונות עצמן, ובהקשר של התרגילים, אתם תצטרכו ממש לממש
[00:06:13 - 00:06:15] כמה מהמודלים שנעבוד עליהם,
[00:06:16 - 00:06:19] שזה אומר להשתמש בחבילות של Deep Learning.
[00:06:20 - 00:06:23] לא יודע אם קצת קשה לשאול שאלות כאלה בזום,
[00:06:23 - 00:06:26] אבל מי, יש פה מישהו שלא עשה את הקורס ב-Deep Learning?
[00:06:28 - 00:06:29] או לא עשה איזשהו קורס?
[00:06:31 - 00:06:34] קשה להבין.
[00:06:36 - 00:06:38] משאלה כזאת פה,
[00:06:38 - 00:06:41] אבל אני מניח שאולי יש כמה שלא עשו,
[00:06:41 - 00:06:46] אבל ש... אז אני אומר את זה שוב בצורה מפורשת, שההנחה פה היא שאתם כן יכולים להסתדר עם זה.
[00:06:47 - 00:06:51] זאת אומרת שאתם תוכלו להגיע למצב שאתם מבינים על מה אנחנו מדברים, ולממש
[00:06:51 - 00:06:52] את המודלים האלה.
[00:06:55 - 00:06:59] אוקיי, שאלות על הידע המוקדם?
[00:07:00 - 00:07:00] זה נדרש?
[00:07:06 - 00:07:22] יהיה חזרות על קטע מהדברים שהן דרושות, או ש... היום אנחנו קצת נעשה חזרה על דברים, לא נספיק הרבה, והתרגיל הראשון הוא אמור לעזור לכם עם שלושת הנקודות הראשונות.
[00:07:25 - 00:07:28] הנקודה הרביעית היא קצת כזה, היא תהיה בתוך כל הקורס,
[00:07:29 - 00:07:32] אז אני... כן, וזה גם דרישת קדם, אני מניח שרוב האנשים עשו את זה לא מזמן,
[00:07:33 - 00:07:34] אז אני מניח שזה יהיה בסדר.
[00:07:36 - 00:07:39] ב-deep learning, כמו שאמרתי, זה בעיקר, אתם צריכים, יש כל מיני,
[00:07:40 - 00:07:42] לא יודע, אנחנו נדבר פתאום על רשת קונבולוציה,
[00:07:43 - 00:07:50] על מה הלוס של הרשת, מה... אתם צריכים להבין, כאילו, איך אנחנו מחברים את הקומפוננדות האלה, צריכים,
[00:07:51 - 00:07:53] צריך שיהיה לכם איזשהו ניסיון עם זה.
[00:07:54 - 00:07:58] זה יהיה בחלק השני, כן? אז יש לכם קצת זמן להשלים למי שזה חסר.
[00:07:58 - 00:08:07] ובעיקר אתם תצטרכו לממש את זה, אז בעיקר מה שדרוש זה שיהיה לכם איזשהו ידע לכתוב קוד ב... למשל פייטרוץ'.
[00:08:08 - 00:08:10] אני חושב שרוב התרגילים אנחנו...
[00:08:10 - 00:08:13] לא יודע אם אני אכריח אתכם לעבוד בפייטרוץ',
[00:08:13 - 00:08:15] אבל זה כנראה ה...
[00:08:16 - 00:08:17] בכל מקרה,
[00:08:17 - 00:08:20] החבילה המועדפת על כולם לעבוד איתה.
[00:08:21 - 00:08:26] אז כן, מניח שאתם כבר יודעים לכתוב בפייטרוץ',
[00:08:27 - 00:08:28] ותוכלו,
[00:08:28 - 00:08:50] תוכלו לפרוט את הרגילים ככה. אבל שוב, אני לא שמתי את Deep Learning בתור דרישת קדם, כי אני חושב שלהרבה אנשים בשלב הזה כבר הם יכולים להשלים את זה בפני עצמם, וגם קבוצים של מי עשה Machine Learning בסמסטר א' ומי ב', ולא יכול לעשות Deep Learning, לא רציתי להכניס את זה לכל מיני מכשולים כאלה.
[00:08:52 - 00:08:53] ואני כן מניח,
[00:08:54 - 00:08:57] אפשר לחשוב ש Deep Learning הוא דרישת קדם, אבל לא רשמית.
[00:08:58 - 00:09:07] אוקיי, אז הציון של הקורס, זה אולי השקף הכי חשוב של היום,
[00:09:08 - 00:09:12] אז הציון של הקורס יהיה בנוי מהתרגילי בית,
[00:09:12 - 00:09:14] כמו שאתם יודעים אנחנו לא עושים מבחנים
[00:09:16 - 00:09:19] בקורסים שהם לא חובה בסמסטר הזה,
[00:09:20 - 00:09:21] או בסמסטר שעבר,
[00:09:22 - 00:09:24] אז הציון יהיה מבוסס על תרגילי בית,
[00:09:25 - 00:09:27] יהיו שבעה תרגילים סך הכל,
[00:09:27 - 00:09:31] ששלושה ראשונים זה יהיה בחלק הראשון של הקורס, זה יהיה שאלות תיאורטיות
[00:09:32 - 00:09:36] שאתם צריכים להגיש איזשהו PDF עם הפתרון שלכם,
[00:09:36 - 00:09:42] וארבעה התרגילים של החלק השני יהיו תרגילי קוד.
[00:09:43 - 00:09:44] צריכים לכתוב איזשהו מודל
[00:09:45 - 00:09:48] שמממש את אחד מהמודלים שדיברנו עליהם בכיתה,
[00:09:50 - 00:09:54] ולהראות את האימון שלו על איזשהו דאטה סט ואת התוצאות.
[00:09:54 - 00:09:57] וההגשה תהיה בזוגות.
[00:10:01 - 00:10:01] בסדר?
[00:10:04 - 00:10:07] אז זהו לגבי החלק האדמיניסטרטיבי.
[00:10:08 - 00:10:13] יש לכם שאלות לפני שאנחנו עוברים לתוכן עצמו.
[00:10:24 - 00:10:33] אוקיי, אז נתחיל, מה אנחנו רוצים לעשות היום?
[00:10:43 - 00:10:54] לפני שאני אדבר על מה אנחנו עושים היום, אז זה השלב שבדרך כלל אני אומר שכדאי לאנשים להגיע פיזית ולא להקשיב בזום ולראות הקלטות.
[00:10:54 - 00:10:58] אנחנו מקליטים את ההרצאות רק,
[00:10:59 - 00:11:01] הכוונה היא שלאנשים שבמקרה לא יכולים,
[00:11:02 - 00:11:05] כל מיני שיקולים שכל נכללים במצב שאנחנו נמצאים בו,
[00:11:06 - 00:11:10] אז שבמצב כזה יהיה את ההקלטה ואנשים יוכלו להשלים בקלות,
[00:11:11 - 00:11:18] אבל הכוונה היא שאנשים יבואו לקורס. אני מקווה שעד השיעור הבא אנחנו נגיע למצב שבאמת זה אפשרי.
[00:11:19 - 00:11:22] אני שוב, אני אומר את זה כמעט בכל קורס,
[00:11:22 - 00:11:27] ותמיד אחוז גדול לא שומע על ההצעה הזאתי,
[00:11:28 - 00:11:34] כשלא מגיעים ומסתמכים על זה שרק נראה את ההרצאה לפני התרגיל ונפתור,
[00:11:34 - 00:11:38] זה פתאום מכניס לכל מיני עומסים ודברים שחושבים שברורים,
[00:11:39 - 00:11:41] הם לא מבינים ואין זמן לשאול בזמן
[00:11:41 - 00:11:47] לפני כל התרגילים ונוצר איזשהו צוואר בקבוק כזה, ובדרך כלל זה, לא בדרך כלל,
[00:11:47 - 00:11:50] לא יודע אם קשה לי לכמת את זה, אבל בהרבה מקרים זה
[00:11:52 - 00:11:55] אני כן רואה שזה יוצר הרבה בעיות. אני באמת ממליץ לבוא.
[00:11:56 - 00:12:03] אם לא נוכל להמשיך לעשות את השיעורים פיזית ונצטרך להמשיך אותם בזום,
[00:12:04 - 00:12:05] אז גם אני ממליץ
[00:12:06 - 00:12:11] לבוא לשיעור זום חי ולשאול שאלות תוך כדי קיים,
[00:12:11 - 00:12:15] מטרה כזאת שיהיה לנו זמן לדיון ולדבר אדומים.
[00:12:18 - 00:12:19] אוקיי, אז מה אנחנו נעשה היום?
[00:12:20 - 00:12:27] היום אנחנו נתחיל מהקדמה כללית על התחום, שתדעו על מה הקורס הזה בגדול ולמה כדאי
[00:12:28 - 00:12:29] ללמוד אותו.
[00:12:31 - 00:12:33] זה יהיה השלב הראשון. אחר כך אנחנו נדבר על,
[00:12:34 - 00:12:41] בעצם נתחיל כבר ללמוד, שהחלק הראשון, בעצם כל היום הוא מין יישור קו כזה בתחום של הסתברות.
[00:12:42 - 00:12:49] החלק הראשון, מה שפתור כאן, ה-basic concept learning generative model, זה יהיה קצת על הסתברות,
[00:12:49 - 00:12:52] על מה הבעיה שלנו, ולמה זה קשה,
[00:12:53 - 00:12:53] למה צריך
[00:12:54 - 00:12:59] כל מיני מודלים שונים, ולמה צריך בכלל רשתות עמוקות, וכל הדברים האלה.
[00:13:00 - 00:13:12] אז זה יהיה החלק השני. והחלק השלישי יהיה קצת יותר טכני ממש, אנחנו נעשה איזה פיתוח כזה של כל מיני תכונות של גרסיאנים רב-ממדיים,
[00:13:13 - 00:13:16] דברים גם שתצטרכו בשביל התרגיל הראשון שפורסמתי.
[00:13:16 - 00:13:17] אנגליתם שפרסמתי תרגיל, נכון?
[00:13:18 - 00:13:21] זהו, התרגיל יהיה,
[00:13:21 - 00:13:23] כמו שאמרתי, איזשהו יישור קו,
[00:13:23 - 00:13:29] דרך שלכם לראות שאנחנו מצליחים להסתדר עם כל החומו שמצטרך בקורס הזה,
[00:13:30 - 00:13:32] והכוונה היא, יש לנו גם ככה עכשיו את כל,
[00:13:33 - 00:13:37] בגלל פסח יש לנו שבועיים וחצי, שלושה אפילו,
[00:13:37 - 00:13:38] שלושה, אני חושב עד השיעור הבא,
[00:13:39 - 00:13:44] אז הכוונה היא שתפתרו את הרגיל הזה עד השבוע הבא, עד השיעור הבא,
[00:13:45 - 00:13:46] ובשיעור הבא אנחנו
[00:13:48 - 00:13:51] תקבלו עוד תרגיל שיהיה על החומה של השיעור הבא.
[00:13:53 - 00:13:55] אוקיי, אז בואו נתחיל עם הקדמה,
[00:13:55 - 00:13:58] אז הכוס הזה באמת מדבר על Deep Generative Models,
[00:13:58 - 00:14:00] אז מה זה Deep Generative Models?
[00:14:01 - 00:14:03] אז קודם כל, מה זה Generative Models?
[00:14:03 - 00:14:06] אופס, זה לא היה אמור להופיע כאן, זה לא הופיע רק אחרי.
[00:14:07 - 00:14:10] מה זה מודלים גנרטיביים?
[00:14:11 - 00:14:13] מישהו רוצה להגיד? יש מישהו רעיון?
[00:14:13 - 00:14:14] מה זה אומר?
[00:14:15 - 00:14:17] אני מניח ששמעתם את זה, יש קצת הייפ.
[00:14:18 - 00:14:19] בחנים האחרונות,
[00:14:20 - 00:14:22] סביב ה... לפחות סביב המילה Generative.
[00:14:26 - 00:14:31] מודל שמייצר משהו? שאנחנו מבקשים ממנו משהו ואז הוא מייצר משהו?
[00:14:32 - 00:14:44] אז זה בהחלט אפשר להגיד, ההגדרה אני חושב הפופולרית של זה, החדשה. אז המילה הזאת, Generative, בהקשר של משין לעניין, היא קצת שינתה משמעות אולי.
[00:14:45 - 00:14:46] נדבר על זה תכף.
[00:14:48 - 00:14:53] מה זה אומר שהוא מייצר משהו? זאת אומרת, מה ההבדל בין נגיד
[00:14:54 - 00:14:56] Classifier זה מודל גנרטיבי?
[00:14:59 - 00:15:01] מייצר פרדיקציה לאיזשהו קלאס?
[00:15:06 - 00:15:08] ביתרון רב זה יותר כמו שהוא מקבל
[00:15:09 - 00:15:10] דעת יותר מצומצם או
[00:15:12 - 00:15:14] סוג של ביט-סטרים שנותן לך בסופו של דבר את התוצאה המגודדית,
[00:15:15 - 00:15:16] משהו טיפה יותר גדול מזה.
[00:15:17 - 00:15:21] כמו דוגמאגנים שאתה יכול לתת לו סתם רעש ויוצר לך בסופו של דבר תמונה.
[00:15:22 - 00:15:28] כן, אז זה משהו בכיוון הזה, אני בהחלט הייתי מכניס את זה להגדרה,
[00:15:28 - 00:15:29] הממד הגדול של ה-output.
[00:15:30 - 00:15:33] זה בהחלט משהו שאפשר להכניס אותו להגדרה.
[00:15:34 - 00:15:36] נגיד אולי לומד מרחב הסתברות.
[00:15:37 - 00:15:40] מרחב הסתברות, כן, אז עוד נקודה.
[00:15:41 - 00:15:45] כן, אז זה שתי הנקודות בעצם שכתבתי כאן.
[00:15:45 - 00:15:47] אז כמו שאמרתי, אז יש
[00:15:53 - 00:15:56] כמה משמעויות למילה הזאת
[00:15:58 - 00:15:59] ג'נרטיבי.
[00:15:59 - 00:16:10] נדבר על זה אולי קצת היום, אבל אני חושב שאפשר להסכים על הגדרה כזאת שבעצם שני העקרונות העיקריים זה שהoutput הוא ממד גבוה.
[00:16:11 - 00:16:13] זאת אומרת, אם אני רק עושה output לאיזה קלאס,
[00:16:13 - 00:16:18] אז אנשים לא יחשיבו את זה בתור ג'נרטיב מודל. אם זה רק יהיה מספר ברגרסיה לינארית,
[00:16:19 - 00:16:25] זה גם כנראה שאם לא יקראו לזה מודל גנרטיבי, אבל אם זה יהיה תמונה שלמה,
[00:16:25 - 00:16:26] אז כן.
[00:16:27 - 00:16:30] אז זה דבר ראשון. ודבר שני זה שהמודל הוא הסתברותי,
[00:16:31 - 00:16:34] והוא בעצם,
[00:16:35 - 00:16:38] תכף נראה בדיוק את ההגדרה מה הכוונה שהוא הסתברותי,
[00:16:38 - 00:16:43] אבל בהרבה מובנים הנקודה הראשונה בעצם גוררת את הנקודה השנייה.
[00:16:44 - 00:16:47] צריכים לעבוד עם מודלים הסתברותיים כדי שנוכל
[00:16:49 - 00:16:52] לבנות מודלים טובים של דאטה במימד גבוה.
[00:16:54 - 00:16:58] אז אוקיי, זה שני עקרונות, תכף נראה הגדרה קצת יותר אולי מדויקת.
[00:17:00 - 00:17:03] דוגמאות של מודלים גנרטיביים, יש לכם רעיונות?
[00:17:06 - 00:17:07] לא הייתם מודלים גנרטיביים.
[00:17:07 - 00:17:09] ChatGPT. ChatGPT, כן.
[00:17:11 - 00:17:11] עוד דוגמאות?
[00:17:14 - 00:17:14] דלי
[00:17:17 - 00:17:18] דלי.
[00:17:18 - 00:17:18] נרף.
[00:17:22 - 00:17:25] נרף, אפשר להתווכח על זה, אפשר לדבר על זה.
[00:17:25 - 00:17:28] לנרף אין לו, למי שמכיר נרף אין לו,
[00:17:29 - 00:17:34] לפחות נרף סתנדרטי, אין לו תפיסה הסתברותית שלנו.
[00:17:35 - 00:17:37] דאטה, זה מאחורי צד מימדי שהוא מימדי.
[00:17:39 - 00:17:45] אז כן, אנחנו כל הזמן רואים עכשיו בשנים האחרונות הרבה מודלים גנרטיביים של דברים שונים.
[00:17:45 - 00:17:49] אז באמת ChatGPT זה אחת מהדוגמאות הכי פורסמות בימינו.
[00:17:51 - 00:17:55] זה מודל שהדאטה הרב-מימדי שהוא מייצר זה טקסט,
[00:17:55 - 00:17:56] טקסט ארוך.
[00:17:57 - 00:18:01] אם הוא היה מייצר אות נגיד, אז לא היינו מודל גנרטיבי, אבל בגלל שהוא מייצר טקסט,
[00:18:02 - 00:18:03] אז הוא מודל גנרטיבי.
[00:18:04 - 00:18:09] הוא תופס את ה... מודל הסתברותי, הוא תופס את ההסתברות של טקסטים
[00:18:11 - 00:18:17] באינטרנט ויודע לייצר טקסט שמתוך ההתפלגות הזאתי שהוא תפס, אוקיי? אז למשל פה שאלתי אותו
[00:18:17 - 00:18:29] מה זה מודלים גנרטיביים, אותם הגדרה שהיא לא ראה, היו פה, דרך אגב, כל פעם שההגדרה הראשונה שעשיתי נמחקה לי, ביקשתי שוב, קיבלתי משהו אחר, אוקיי? זה מודל הסתברותי, אז כל פעם יצא משהו קצת אחר.
[00:18:30 - 00:18:32] אז פה למשל מה שהוא כתב,
[00:18:32 - 00:18:36] המודלים הגנרטיביים הם מודלים קשה של משימות לתוכניות דאטה,
[00:18:37 - 00:18:39] שהם מסוימים לתוכניות דאטה סטות נזקות.
[00:18:39 - 00:18:47] מודלים אלה מכוונות ההסתברות של הדאטה המקומית והסתברות לחזרות מודלים לפעמים מבחינות הקרקטוריות של אורידנל דאטה.
[00:18:47 - 00:18:50] אז פה למשל בהגדרה אין את העניין הזה של הייד-אימנד שלנו,
[00:18:51 - 00:18:56] אבל עדיין אני חושב שלכולם ברור שקוראים את הטקסט הזה שזאת הכוונה,
[00:18:58 - 00:19:01] ויש פה את ה... איפה זה היה?
[00:19:03 - 00:19:07] underline input distribution, בהגדרה השנייה שראיתי היה distribution
[00:19:08 - 00:19:14] אז גם לא כתוב בפורש הזה הסתברותי, אבל כתוב learn the underline structure of the data
[00:19:15 - 00:19:18] וגם capable of producing new examples
[00:19:18 - 00:19:22] אוקיי, אז רמזים לזה שמודל הסתברותי זה משהו שיכול לעזור לנו פה
[00:19:23 - 00:19:27] אוקיי, אז זה מודלים של טקסט, אנחנו לא כל כך נעבוד דווקא בקורס הזה על מודלים של טקסט
[00:19:27 - 00:19:32] רוב הקורס, הדוגמאות שאנחנו נדבר זה יהיה על דאטה שהוא רצה
[00:19:32 - 00:19:33] ציף
[00:19:33 - 00:19:37] ולרוב הדוגמאות שאנחנו ניתן הן בדאטה של תמונות,
[00:19:37 - 00:19:39] אוקיי, אז זה תמונות שיוצרו
[00:19:41 - 00:19:45] אני לא זוכר כבר, לאחד מהמודלים שפתוחים
[00:19:47 - 00:19:50] והם מבוססים על מודל שנקרא diffusion,
[00:19:50 - 00:19:54] שזה אחד מהמודלים שאנחנו נדבר עליהם בחצי השני של הקורס
[00:19:55 - 00:19:59] גם המודל שמייצר את תעש ה-GPT זה מודל שהוא נקרא אוטו-רגרסיב
[00:19:59 - 00:20:01] גם אנחנו נדבר עליו,
[00:20:01 - 00:20:03] כנראה שהדוגמא שניתן שם זה,
[00:20:03 - 00:20:06] או את כל פעם שתצטרכו ממש בתרגיל זה לא יהיה עם טקסט אלא זה גם יהיה עם תמונות
[00:20:07 - 00:20:12] אוקיי, זה סוג המודל הזה שמשתמשים בוועד של ה-GPT גם אנחנו נדבר עליו ונראה אותו
[00:20:14 - 00:20:18] עוד דוגמא, יש עוד דברים, למשל מוזיקה
[00:20:18 - 00:20:18] אז
[00:20:24 - 00:20:24] אתם שומעים?
[00:20:31 - 00:20:37] זה מישהו שמזמזם וכותב באיזה קליאה הוא רוצה שזה יישמע
[00:20:37 - 00:20:45] אנחנו מקבלים יצירה כזאת
[00:20:56 - 00:21:01] אז באמת יש עוד ועוד תחומים שבהם אנחנו
[00:21:02 - 00:21:07] בשנים האחרונות מודלים יכולים לייצר
[00:21:08 - 00:21:12] דגימות של דאטה מתוך איזשהו סוג מסוים של דאטה
[00:21:12 - 00:21:22] נדבר עוד קצת בהמשך היום על בעצם איזה סוגים שונים של דברים אנחנו יכולים לעשות עם מודלים גנרטיביים ולקרוא להם מודלים גנרטיביים
[00:21:23 - 00:21:25] אוקיי זה רק היה איזושהי טעימה
[00:21:26 - 00:21:35] עוד תחום שנכנס במדע באופן כללי, למשל בכימיה
[00:21:37 - 00:21:41] מודלים גנרטיביים שמייצרים מולקולות חדשות,
[00:21:42 - 00:21:43] כמו שאתם יודעים לייצר מולקולות
[00:21:45 - 00:21:48] אתם אולי יודעים, מולקולות זה אחד מהבניין של
[00:21:50 - 00:21:52] תחומים שונים, גם בכימיה, גם בביולוגיה
[00:21:53 - 00:21:54] ויש איזשהו
[00:21:55 - 00:22:02] ערך גדול להצליח להנדס מולקולות שהן תכונות מסוימות, הרבה פעמים זה קשור למבנה של המולקולות
[00:22:03 - 00:22:09] אז יש איזושהי התפלגות של מולקולות בעולם ומנסים לייצר מולקולות חדשות
[00:22:10 - 00:22:15] שבאות מאיזושהי התפלגות ושיש להן תכונות מסוימות כדי שיוכלו להשתמש בהן לעשות כל מיני דברים
[00:22:16 - 00:22:19] אבל גם בתחום הזה מודלים גנרטיביים נכנסים
[00:22:19 - 00:22:22] ועוד כל מיני תחומים מדעיים ופחות
[00:22:23 - 00:22:29] פחות יומיומיים אולי כמו הדברים שכולנו נחשפים אליהם
[00:22:32 - 00:22:37] אוקיי אז בואו נדבר בעצם בסט-אפ שכבר ראינו של כולנו
[00:22:37 - 00:22:40] מניח שהרבה מכם עשו את הקורס של משינרנג איתי
[00:22:41 - 00:22:43] אני מזהה פה כמה שמות ופרצופים
[00:22:45 - 00:22:51] אבל גם לא אני מניח שנתקלתם במונח הזה מודל
[00:22:51 - 00:22:55] גנרטיבי כבר כשלמדתם במשין לרנינג בקורס הבסיסי
[00:22:56 - 00:22:59] ושם בעצם דיברנו על מודל גנרטיבי בניגוד למודל
[00:23:00 - 00:23:01] שהוא דיסקרימינטיבי
[00:23:04 - 00:23:07] אפשר לדבר על ההבדל הזה כשאנחנו מדברים על סופרוויזי לרנינג
[00:23:07 - 00:23:17] שיש לנו איזשהו x ויש לנו איזשהו y יש לנו זוג יש לנו input וoutput אנחנו נחשפים לדאטה כזה
[00:23:17 - 00:23:19] שהוא כבר מחולק לזוגות ה-x ו-y
[00:23:20 - 00:23:22] ואז בהינתן x חדש אנחנו רוצים לדעת לייצר את ה-y החדש
[00:23:24 - 00:23:26] אז בגישה דיסטרימינטיבית מה זה אומר?
[00:23:27 - 00:23:35] זה אומר שאנחנו נחשפים לזוגות האלה וישר לומדים איזשהו מודל שבהינתן x חוזה לנו את y זאת אומרת אנחנו לומדים מודל של y בהינתן x
[00:23:36 - 00:23:38] הרבה פעמים המודל הזה הוא הסתברותי, לא תמיד,
[00:23:38 - 00:23:42] במודל דיסטרימינטיבי קל דווקא לשכוח מהמודלים ההסתברותיים
[00:23:43 - 00:23:46] ובאופן ישיר לנסות לעשות משהו שחוזה את y
[00:23:46 - 00:23:49] אבל הרבה פעמים המודל הוא איזשהו מודל הסתברותי זאת אומרת
[00:23:49 - 00:23:51] שהוא נותן לנו התפלגות על y בהינתנות.
[00:23:52 - 00:23:55] אם y זה יכול להיות למשל שני קלאסים שונים,
[00:23:56 - 00:24:00] הוא נותן לנו ממש ההסתברות שx הוא כלב לממד חתול.
[00:24:02 - 00:24:04] וy באמת בהקשר הזה הוא יהיה משהו בממד נמוך,
[00:24:05 - 00:24:11] למשל הקטגוריה של המחלקה של האינפוט שלנו,
[00:24:12 - 00:24:18] כלב או חתול, או איזשהו סקלר, מה שאומר את המחיר של משהו בשוק.
[00:24:19 - 00:24:21] אבל משהו בממד יחסית נמוך.
[00:24:22 - 00:24:24] במודלים גנרטיביים,
[00:24:25 - 00:24:28] גם בסטאפ הזה של סופרווייד דרנינג אפשר לדבר על מודלים גנרטיביים,
[00:24:29 - 00:24:35] ששם בעצם אמרנו שבמקום לאמן מודל של y בהינתן x אנחנו מאמנים הפוך, את המודל של x בהינתן y.
[00:24:36 - 00:24:41] למשל אם y זה כלב או חתול אז x זה התמונה של הכלב או החתול,
[00:24:41 - 00:24:47] אז אנחנו לומדים מודל שבהינתן שאנחנו יודעים שזה כלב או חתול מה ההסתברות לראות תמונה מסוימת.
[00:24:49 - 00:24:52] אתם זוכרים שדיברנו על זה בקורס של משין לרנינג?
[00:24:54 - 00:24:56] זה ככה בנינו קלאסיפייר גנרטיבי,
[00:24:57 - 00:25:00] ובעצם איך משתמשים בו כשרוצים לעשות קלאסיפיקציה למשל.
[00:25:00 - 00:25:06] אז קודם כל, נקודה אחת זה שx הוא כנראה יהיה משהו בממד גבוה, x זה יהיה כל הפיצ'רים שאנחנו רוצים
[00:25:10 - 00:25:15] להסתכל עליהם בתור האינפוט שלנו, כשאנחנו עושים קלאסיפיקציה, נכון? אז למשל זה יכול להיות כל הפיקסלים בתמונה,
[00:25:15 - 00:25:20] או איזושהי רשימה של פיצ'רים שלפיהם אנחנו רוצים לעשות סיבוב,
[00:25:21 - 00:25:22] אבל x הוא משהו בממד גבוה,
[00:25:23 - 00:25:26] אז זו נקודה ראשונה שהיא הבדל מהמודל הדיסקרימינטיבי,
[00:25:26 - 00:25:30] ונקודה שנייה זה שבעצם עכשיו לא פתרנו ממש משימה בצורה מוגדרת,
[00:25:32 - 00:25:35] בניגוד למודל הדיסקרימינטיבי, כשהגדרנו את המשימה צריכים לחזות את y בנתן x,
[00:25:36 - 00:25:38] פה למדנו איזשהו מודל של הדאטה,
[00:25:38 - 00:25:41] ועכשיו אנחנו משתמשים במודל הזה כדי אחר כך לפתור את המשימה
[00:25:42 - 00:25:43] בטסט טיים, במבחן,
[00:25:44 - 00:25:45] אז אנחנו יכולים להשתמש בחוק בייס,
[00:25:46 - 00:25:49] ובעזרת חוק בייס, לעשות פרדיקציה ל-y בנתן x,
[00:25:49 - 00:25:50] כשיש לנו x חדש.
[00:25:53 - 00:25:57] אז בשיעור הבא בעצם אנחנו נדבר הרבה על העניין הזה של שימוש בחוק בייס,
[00:25:58 - 00:26:04] אבל זה רק תזכורת למה שכבר ראיתם בחוגים, בקורס של מה שלנו.
[00:26:05 - 00:26:06] שאלות על זה?
[00:26:08 - 00:26:09] קצת קשה לשאול שאלות בזום,
[00:26:09 - 00:26:15] אבל אני כן רוצה לעודד אתכם כן לנסות לפתח דיון ולשאול שאלות.
[00:26:15 - 00:26:16] אני כל הזמן שואל אם יש שאלות.
[00:26:21 - 00:26:26] עוד דרך להסתכל על ההבדל בין גישה דיסטרימנטיבית לגישה גנרטיבית לסופרוויזרים.
[00:26:27 - 00:26:29] כשיש לנו את x ואת ה-y במורש,
[00:26:30 - 00:26:32] אז למשל אם אנחנו רוצים לסווג פירות,
[00:26:34 - 00:26:36] אז יכול להיות לנו שה-input שלנו זה יהיה תמונה,
[00:26:37 - 00:26:40] יהיה לנו איזשהו מודל,
[00:26:40 - 00:26:44] למשל רשת נוירונים כזאתי שעושה פרוססים לתמונה וה-alput שלה הוא
[00:26:44 - 00:26:45] בגודל אחד,
[00:26:46 - 00:26:50] או אם יש כמה קלאסים אולי זה יהיה בגודל של מספר הקלאסים שיכולים להיות,
[00:26:51 - 00:26:54] וזה יגיד לנו מה הקלאס שלו, אוקיי? שזה יהיה אפל.
[00:26:55 - 00:26:56] זה מודל דיסטרימנטיבי.
[00:26:56 - 00:26:58] איך יהיה מודל גנרטיבי?
[00:26:59 - 00:27:06] מודל גנרטיבי, אנחנו נכניס את התפוח הזה או את הקטגוריה בתור
[00:27:06 - 00:27:08] ה-input של הבעיה,
[00:27:09 - 00:27:11] ואנחנו נרצה לייצר
[00:27:13 - 00:27:16] התפלגות של תפוחים,
[00:27:17 - 00:27:18] של הקטגוריה הזאת.
[00:27:19 - 00:27:20] בעצם אנחנו נצטרך,
[00:27:20 - 00:27:33] ה-input שאנחנו נצטרך שיהיה לנו זה לא רק התפוח, אלא גם איזושהי הטלת מטבע, איזושהי הטלת קוביות, איזשהו מספר רנדומלי, איזשהו וקטור שנוצר בצורה רנדומלית,
[00:27:34 - 00:27:35] כל פעם זה מקבל ערך אחר,
[00:27:36 - 00:27:40] ובעצם זה מה שיגרום למודל שלנו, כל פעם לייצר תפוח,
[00:27:40 - 00:27:41] תמונה אחרת של תפוח,
[00:27:42 - 00:27:42] יותר נכון.
[00:27:44 - 00:27:46] בסדר, זו הסיבה שאנחנו,
[00:27:47 - 00:27:55] על הכיוון הזה, הרבה פעמים אנחנו נצטרך יותר להסתמך על הסתברות של מודלים הסתברותיים מאשר בכיוון הדיסטרימנטיבי.
[00:27:57 - 00:28:03] בכיוון הדיסטרימנטיבי גם לפעמים אנחנו נרצה התפלגות, אבל בגלל שמראש הבעיה היא בממד יחסית קטן,
[00:28:04 - 00:28:12] אז ההתפלגות הזאת, ההסתברות שאנחנו צריכים לתפוס פה היא לא משהו שהוא מאוד יהיה חלק מהמודל,
[00:28:12 - 00:28:15] למשל אם יש לנו ארבעה קלאסים שונים,
[00:28:16 - 00:28:24] אז בהינתן תמונה אולי אנחנו נרצה לתת איזשהו סקור אחר לכל אחד מהקלאסים האלה, אפשר לחשוב על זה בתור איזשהו מודל הסתברותי שאנחנו נותנים על הקטגוריות,
[00:28:25 - 00:28:29] מה שאמרנו בשקף הקודם, ההסתברות של y בהינתן x,
[00:28:30 - 00:28:32] אז גם אפשר להגיד שזה מודל הסתברותי,
[00:28:32 - 00:28:41] אבל לרוב אנחנו לא צריכים ממש להשקיע בזה מחשבה ולבנות את הבנייה של המודל ההסתברותי הזאת ממש בתור חלק מהמודל.
[00:28:42 - 00:28:44] זאת אומרת, אתם אולי תבינו קצת יותר למה אני מתכוונת
[00:28:47 - 00:28:47] בהמשך השיעור היום.
[00:28:49 - 00:28:49] אוקיי.
[00:28:51 - 00:28:56] אז לסיכום ההשוואה הזאתי בין מודלים דיסקנטיביים וגנרטיביים,
[00:28:57 - 00:29:01] מודלים גנרטיביים אנחנו יכולים, אם למדנו מודל גנרטיבי מהדאטה שלנו,
[00:29:01 - 00:29:07] אנחנו יכולים להשתמש בו בדיוק כמו שהשתמשנו במודל הדיסקנטיבי בסופו של דבר, על ידי זה שנשתמש בחוק בייס.
[00:29:09 - 00:29:13] אבל עם מודל גנרטיבי אנחנו יכולים לעשות גם עוד הרבה דברים שאנחנו לא יכולים לעשות
[00:29:14 - 00:29:15] עם מודל דיסקנטיבי,
[00:29:15 - 00:29:21] זאת הסיבה שאנחנו מתעניינים בגישה הזאת, כי אחרת
[00:29:21 - 00:29:23] היה כבר עדיף לנו אולי,
[00:29:23 - 00:29:25] במקום שהיינו רוצים זה פשוט לחשב את זה,
[00:29:25 - 00:29:29] בהרבה מקרים כבר עדיף להשתמש בגישה דיסקנטיבית, לא תמיד,
[00:29:29 - 00:29:30] אבל בהרבה מקרים כן.
[00:29:31 - 00:29:36] אבל עוד יתרון של מודלים אינטרנטיביים זה באמת שאפשר לעשות איתם עוד הרבה דברים, למשל אנחנו יכולים פשוט לייצר
[00:29:38 - 00:29:41] תמונות חדשות של תפוחים, אוקיי? אחרי שלמדנו
[00:29:41 - 00:29:46] את המיפוי הזה בין תפוח לתמונה של תפוח
[00:29:49 - 00:29:54] אוקיי, פה זה בעצם, מה שאני אמרתי עכשיו זה יותר נכון לנקודה השנייה, לייצר דאטה שהוא מותנה
[00:29:55 - 00:30:00] x בהינתן y, x בהינתן תפוח, תמונה בהינתן תפוח, זה ייתן לי תמונה של תפוחים
[00:30:01 - 00:30:07] אבל אם למדתי את זה על ככה, על דאטה סט של הרבה מאוד פירות, אני גם יכול לייצר פשוט דוגמאות חדשות של x בלי התניה
[00:30:08 - 00:30:10] לפעמים ההבדל בין שני הדברים האלה הוא חשוב
[00:30:12 - 00:30:19] עוד נקודה שאפשר להשתמש בה, לפעמים אנחנו פתאום יהיה חסר לנו חלק מ-x, לא נראה את כל x
[00:30:20 - 00:30:24] בגלל שיש לנו עכשיו מודל הסתברותי על x, נוכל להשתמש במודל עצמו כדי להבין איך
[00:30:25 - 00:30:27] להשלים את ה-x הזה שראינו, ואז
[00:30:28 - 00:30:30] לגבי ההשלמה הזאתי לעשות למשל את הסיבוב
[00:30:31 - 00:30:38] מה שהיה מאוד קשה אם היה לנו רק מודל דיסקרימנטיבי, תחשבו על התמונה של התפוח אם היו חסרים שם כמה פיקסלים
[00:30:38 - 00:30:43] המודל הדיסקרימנטיבי היה לו מאוד קשה, הוא כנראה היה נכשל לגמרי
[00:30:44 - 00:30:49] זאת אומרת מודל גנרטיבי שבעיקרון הוא יודע לתפוס איך נראה תפוח
[00:30:50 - 00:30:57] אז הוא יכול להתמודד אולי עם כמה פיקסלים שחסרים כי מבחינת המודל יש לו את המידה של איך להשלים, הוא יודע כבר שהתפוח כנראה נראה אדום
[00:30:57 - 00:30:59] אז הוא יכול להשלים אותו ואז
[00:30:59 - 00:31:00] לסווג אותו.
[00:31:02 - 00:31:05] הוא לא יודע שזה תפוח בלילה, עכשיו הוא יודע שיש כאן איזשהו פרי שנראה אדום,
[00:31:05 - 00:31:13] יש חם אחורים בכמה מקומות אז הוא יודע שכנראה התמונות הן בדרך כלל חלקות, אז הוא יכול להשלים את זה בצורה יחסית טובה,
[00:31:13 - 00:31:15] אם יש איזשהו אג' אז כנראה שהאג' הזה ממשיך,
[00:31:16 - 00:31:19] הוא יודע להשלים את התמונה ואז לעשות על התמונה הזאת סיבוב.
[00:31:20 - 00:31:27] אז בעצם משהו שמודל אסטמטיבי לא יכול לעשות זה להתמודד עם מידע חסר ומודל גנטיבי כן יכול לעשות.
[00:31:28 - 00:31:34] ואנחנו נראה עוד כל מיני דוגמאות גם בהמשך היום וגם בהמשך הקורס על עוד דברים שאפשר לעשות עם מודלים גנרטיביים.
[00:31:37 - 00:31:41] אוקיי, אז עד עכשיו דיברנו על סופרוויז דאטה, זאת אומרת שיש לנו את x ו-y מראש,
[00:31:41 - 00:31:43] שאנחנו יודעים בעצם מה ה...
[00:31:44 - 00:31:53] איזושהי הגדרה של משימה או קטגוריות מסוימות שאנחנו רוצים לעשות להן פרדיקציה,
[00:31:54 - 00:31:55] הרבה פעמים זה לא המצב,
[00:31:55 - 00:32:00] וחלק מהיתרון של מודל גנרטיבי, שאנחנו לא צריכים בכלל שיהיה לנו y מוגדר מראש.
[00:32:01 - 00:32:02] אנחנו יכולים לקרוא לכל הדאטה שיש לנו x,
[00:32:03 - 00:32:09] ופשוט ללמוד מודל של ה-x הזה, שאחר כך ישמש אותנו לכל מיני דברים, לא רק לעשות פרדיקציה של y,
[00:32:09 - 00:32:11] כמו שאמרתי, גם לייצר xים חדשים,
[00:32:11 - 00:32:14] גם אולי להשלים דאטה שחסר לנו ב-x.
[00:32:16 - 00:32:23] אם כל הדאטה שלנו אנחנו קוראים לו x, אז יכול להיות שהוא מכיל גם את מה שהיינו רוצים לקרוא לו y, למשל זה כל הפיקסלים של התמונה,
[00:32:23 - 00:32:25] פלוס איזה קטגוריה זאתי,
[00:32:26 - 00:32:30] ואז אנחנו יכולים לחשוב על כל בעיה בתור השלמה של ה-x בתור מידע חסר.
[00:32:32 - 00:32:35] זה גם מאפשר לנו גישה להרבה יותר דאטה.
[00:32:36 - 00:32:39] אז כמו שאתם יודעים, המודלים האלה של ChatGPT ומודלים על התמונות,
[00:32:40 - 00:32:44] סוד ההצלחה שלהם זה שהם אומנו על כל האינטרנט,
[00:32:45 - 00:32:45] קצת או יותר,
[00:32:46 - 00:32:51] ובשביל להיות מסוגלים להתאמן על כל האינטרנט, צריך שיהיה דאטה שהוא לא מטוייג, אוקיי?
[00:32:51 - 00:32:55] שלא מישהו עבר ואמר מה ה-y הנכון של כל אחת מהנקודות האלה.
[00:32:56 - 00:32:59] זה פשוט הכל נמצא איכשהו ביחד, לכל הדבר הזה אנחנו קוראים x,
[00:33:00 - 00:33:01] ואנחנו פשוט תופסים את המודל
[00:33:04 - 00:33:05] ההסתברותי, תכף נדבר מה זה אומר,
[00:33:06 - 00:33:08] איזשהו מודל שאומר איך נראה x
[00:33:11 - 00:33:11] טיפוסי בדאטה שלי.
[00:33:13 - 00:33:16] וגם לא מאוד טיפוסי, אלא גם איך נראה x מוזר,
[00:33:17 - 00:33:19] ואיך נראה x שהוא בבירור לא בתוך הדאטה,
[00:33:20 - 00:33:21] לא אמור להיות בתוך הדאטה.
[00:33:22 - 00:33:24] אוקיי? אז זה יתרון שאנחנו לא צריכים דאטה מטוייג.
[00:33:29 - 00:33:29] זה לא זה זה?
[00:33:31 - 00:33:31] נקודה חשובה.
[00:33:34 - 00:33:34] אוקיי.
[00:33:36 - 00:33:49] עכשיו נקודה שלקראת העניין הזה, למה אנחנו צריכים מודלים הסתברותיים, אז בעצם היינו יכולים אולי, גם עם מודלים הסתברותיים וגם אם לא, היינו יכולים בכלל לא ללמוד את המודל שלנו, ופשוט לייצר מודל,
[00:33:50 - 00:33:58] זאת אומרת להנדס את המודל מתוך איזושהי רשימה של חוקים או כללים שאנחנו יודעים איך נראות תמונות.
[00:33:59 - 00:34:03] ועושים את זה. למשל, הדבר הזה זה תמונה מתוך
[00:34:07 - 00:34:12] מנוע גרפי שנקרא Unity, יש גם מנועי משחקים שהם די דומים,
[00:34:12 - 00:34:16] שבעצם המודל הזה הוא לא מודל שנלמד,
[00:34:17 - 00:34:27] הוא פשוט מודל שיודע לייצר תמונות של, במקרה הזה זה ערים, אפשר לכתוב לו רשימה של מה אנחנו רוצים, כמה בתים אנחנו רוצים, אולי אפשר להכניס גם רנדומליות מסוימת,
[00:34:28 - 00:34:32] שזה מבנה של ה... לא יודע, כמה קומות יש בכל בית, להכניס את זה בצורה רנדומלית,
[00:34:33 - 00:34:35] או הצבע של הלבנים,
[00:34:36 - 00:34:44] או בדיוק המבנה של הכבישים, ויש כל מיני פרמטרים שם שאנחנו יכולים ממש להנדס איזושהי רשימה של חוקים שכוללת גם כל מיני דברים רנדומליים,
[00:34:45 - 00:34:48] ואז לייצר ככה הרבה תמונות.
[00:34:49 - 00:34:52] זו גישה שמשתמשים בה הרבה פעמים,
[00:34:52 - 00:34:54] של אם רוצים ממש לייצר דאטה,
[00:34:55 - 00:34:57] אפשר לעשות את זה בצורה ידנית.
[00:34:59 - 00:35:04] אנחנו בקורס הזה נראה את ה... לא נדבר על הגישה הזאת, נדבר על הגישה שבה אנחנו רוצים ללמוד
[00:35:05 - 00:35:06] את המודל הזה מתוך הדאטה.
[00:35:07 - 00:35:10] אנחנו לא רוצים להנדס אותו בעצמנו,
[00:35:11 - 00:35:14] למרות שעד רמה מסוימת זה אפשרי,
[00:35:14 - 00:35:19] מגיעים להישגים מאוד טובים עם גרפיץ אנג'ינס בימינו,
[00:35:19 - 00:35:32] אבל הרבה פעמים אנחנו גם לא יכולים לעשות את זה, למשל בכל מיני מצבים שמגיעים ממש לגבול של היכולת האנושית,
[00:35:32 - 00:35:33] אפילו של צוותים גדולים,
[00:35:33 - 00:35:38] לייצר מערכת קוקים שתייצר דאטה, זה נכון גם בשביל תמונות, אבל גם בשביל למשל
[00:35:39 - 00:35:42] מבנים של חלבונים, או כל מיני דברים בתחום של מדע.
[00:35:43 - 00:35:46] אז בעצם בהרבה מקומות כבר מרגישים שהגענו לגבול, ואנחנו צריכים
[00:35:47 - 00:35:50] שיטות חדשות שיכולות לעזור לנו לפתח מודלים יותר טובים.
[00:35:51 - 00:35:57] אז זאת אחת מהסיבות שאנחנו רוצים לעבור מודלים שנלמדים ישירות מהדאטה.
[00:36:00 - 00:36:00] אוקיי,
[00:36:01 - 00:36:06] אז למה המודל הסתברותי? אז בעצם ההסתברות זה בעצם השפה שנותנת לנו,
[00:36:08 - 00:36:10] קודם כל זה לא חייב,
[00:36:10 - 00:36:13] אין איזושהי חובה שהמודל יהיה הסתברותי, אבל
[00:36:14 - 00:36:18] זה הפתרון, אני חושב, הכי טוב שמצאו עד עכשיו.
[00:36:18 - 00:36:23] אפשר עכשיו על הסתברות בתור השפה הכי נוחה,
[00:36:23 - 00:36:36] לציין איזה נקודות אנחנו רוצים שהן יהיו חלק מהדאטה-סט שלנו, זאת אומרת, חלק מהעולם הזה, שהן נקודות ולידיות, משהו שנראה כמו תמונה אמיתית,
[00:36:36 - 00:36:38] לעומת נקודות שמבחינתנו הן לא חוקיות,
[00:36:38 - 00:36:41] אנחנו לא רוצים לייצר סוג כזה של תמונות.
[00:36:42 - 00:36:45] ובעצם אנחנו משתמשים בהסתברות בתור שפה למדל את הדבר הזה,
[00:36:47 - 00:36:49] וזה בעצם אומר שהמידול הזה הוא רך,
[00:36:50 - 00:36:51] זאת אומרת זה לא שיהיה נקודות שאנחנו נגיד, אוקיי,
[00:36:52 - 00:36:54] זו נקודה שמבחינתנו היא חוקית, היא תמונה,
[00:36:55 - 00:36:58] ויש נקודות אחרות שנגיד מבחינתנו הנקודה הזאת היא לא חוקית,
[00:36:58 - 00:37:01] אנחנו פשוט ניתן איזושהי הסתברות, אוקיי? כל נקודה,
[00:37:01 - 00:37:03] נגיד אנחנו נדבר על תמונות,
[00:37:03 - 00:37:06] אז כל תמונה אנחנו נותנים לה איזושהי הסתברות,
[00:37:07 - 00:37:09] אוקיי? כל תמונה שאתם יכולים לחשוב עליה בעולם,
[00:37:09 - 00:37:11] תחת המודל שלנו,
[00:37:12 - 00:37:19] מה אומר, אם המודל שלנו הוא מודל הסתברותי, מה זה אומר? שהמודל של זה יכול לתת לנקודה הזאת ציון מ-0 ל-1,
[00:37:20 - 00:37:24] שאומר עד כמה התמונה הזאת היא תמונה סבירה
[00:37:25 - 00:37:28] מבחינתו, מבחינת המודל, אוקיי?
[00:37:29 - 00:37:33] וזו המשמעות שאנחנו עושים, אנחנו לומדים מודל הסתברותי של הנקודות.
[00:37:35 - 00:37:36] תחשבו על זה בתור איזושהי השמה רק.
[00:37:37 - 00:37:42] לפיצול הזה בין נקודות שהן לא חלק מהדאטה,
[00:37:42 - 00:37:47] או לא רוצים שהמודל יכליל אותן, לנקודות שאנחנו כן רוצים שהמודל יכליל.
[00:37:49 - 00:37:53] בזמן אימון כמובן אנחנו נותנים לו נקודות שאנחנו כן רוצים שהוא יכליל,
[00:37:54 - 00:37:54] אבל לא את כולם.
[00:37:55 - 00:38:01] זה לא יכול להיות מודל שבעצם זה סוג של אוברפיטינג,
[00:38:01 - 00:38:02] אנחנו גם נדבר על זה בהמשך,
[00:38:03 - 00:38:15] אוברפיטינג שעושים במודלים גנרטיביים, מה זה אומר? זה אומר שבעצם המודל ההסתברותי הוא כזה שכל התמונות שהוא ראה הוא חושב שהן תמונות חוקיות, וכל שאר התמונות אפילו הזזה מאוד קטנה הם לא חוקיים.
[00:38:16 - 00:38:23] אנחנו לא רוצים שזה יהיה המצב, אנחנו רוצים לעשות איזושהי הכללה שכל התמונות שהמודל ראה יחשבו בתור תמונות טובות,
[00:38:24 - 00:38:25] חוקיות,
[00:38:25 - 00:38:28] אבל גם כל מיני נקודות באמצע, כל מיני תמונות באמצע,
[00:38:29 - 00:38:31] שמה זה בדיוק באמצע זה לא כל כך מוגדר.
[00:38:32 - 00:38:35] וזה חלק ממה שהמודל אמור לתפוס בצורה אוטומטית.
[00:38:39 - 00:38:48] אוקיי, אז אפשר לחשוב על הדבר הזה בתור התרשים הזה. המטרה שלנו באימון זה בהינתן הרבה תמונות לבנות את המודל ההסתברותי הזה,
[00:38:49 - 00:38:52] למצוא התפלגות על פני כל הדאטה האפשרית.
[00:38:53 - 00:38:57] אז כאן ההתפלגות הזאת מצויירת אותו התפלגות על דאטה דו-ממדי,
[00:38:58 - 00:39:01] כי יש לכל נקודה בדו-מימד יש כאן איזשהו גובה,
[00:39:01 - 00:39:02] וגם צבע,
[00:39:02 - 00:39:04] זאת אומרת מה ההסתברות של הנקודה הזאת.
[00:39:06 - 00:39:07] או
[00:39:07 - 00:39:09] יכול להיות פה צפיפות ההסתברות.
[00:39:11 - 00:39:11] לא,
[00:39:11 - 00:39:13] צפיפות ההסתברות זה דאטה רציף.
[00:39:19 - 00:39:22] אותו דבר אנחנו נרצה גם נגיד אם הדאטה שלנו הוא תמונה,
[00:39:22 - 00:39:26] תמונה משהו בגודל מיליון פיצלים,
[00:39:26 - 00:39:30] אבל בעצם אנחנו רוצים לבנות איזשהו מודל הסתברותי שנראה בערך ככה,
[00:39:30 - 00:39:33] רק שהוא מוגדר על מיליון ממדים ולא על שני ממדים.
[00:39:35 - 00:39:37] שוב, מה המשמעות של הדברים האלה?
[00:39:37 - 00:39:39] אזור גבוה לעומת אזור נמוך,
[00:39:40 - 00:39:45] זה בדיוק הסופט אסיימנט הזה של דברים שאנחנו כן רוצים שיהיו, לעומת דברים שאנחנו לא רוצים שיהיו.
[00:39:46 - 00:39:51] אז אם אנחנו נסתכל איזה נקודה נמצאת בנקודה כאן גבוהה,
[00:39:51 - 00:39:55] אז יהיה איזושהי תמונה שנראית טוב, שהיא יחסית דומה אולי לדברים שראינו
[00:39:56 - 00:39:56] בפרנינסט שלנו,
[00:39:57 - 00:40:02] איזה שהם מובנים, לא מאוד שונה מדברים שראינו באותו אינסט.
[00:40:03 - 00:40:06] לעומת זאת, איך נראה נקודה שהיא נמצאת פה במקום שהיא בהסתברות מאוד נמוכה,
[00:40:07 - 00:40:11] אז נגיד משהו כזה, איזשהו רעש, משהו שמבחינתנו הוא לא תמונה חוקית.
[00:40:14 - 00:40:17] אבל שימו לב שאנחנו לא מדירים מראש מה זה תמונה חוקית או מה לא,
[00:40:18 - 00:40:21] בעצם אנחנו רוצים מתוך איזשהו דאטה סט של תמונות
[00:40:22 - 00:40:23] חוקיות ללמוד את ההתפלגות הזאת,
[00:40:24 - 00:40:29] ותהיה בין הנקודות שהוא ראה תהיה איזושהי הכללה שהוא יחליט,
[00:40:30 - 00:40:34] המודל הזה יחליט בעצמו לפי איך שהוא מתאמן שיש נקודות
[00:40:35 - 00:40:39] שהוא לא ראה אף פעם אבל מבחינתו הם עדיין בהסתברות גבוהה.
[00:40:40 - 00:40:43] לעומת דברים אחרים שהוא לא ראה אף פעם מבחינתו הם בהסתברות נמוכה.
[00:40:45 - 00:40:50] ובעצם האומנות כאן זה לגרום למודל הזה לתפוס בדיוק את מה שאנחנו היינו רוצים שהוא יתפוס.
[00:40:53 - 00:41:03] נקודת המקסימום היא נקודה שהייתה בטרנינג סט בדיוק.
[00:41:04 - 00:41:23] נקודת המקסימום לא תמיד זה תלוי איך אימנת את המודל יכול להיות שיש שיטות אימון שזה יכול להיות ככה למשל שיטת אימון שאני לא זוכר אם דיברנו עליה ואנשים ימים או לא יכולה להיות כזאת כל דוגמת אימון אני פשוט עושה גאוסיאן שזה המרכז
[00:41:23 - 00:41:25] ואז יהיה לי הרבה סכום של הרבה גאוסיאנים
[00:41:28 - 00:41:30] ובעצם כמו שאתה אומר כל
[00:41:31 - 00:41:31] פיק
[00:41:31 - 00:41:34] שיהיה לני בדאטה זה בדיוק יהיה הנקודה שהייתה לי בטרנינג סט
[00:41:35 - 00:41:42] כל מה שליד זה ההסתברות התחיל לרדת אבל אם יהיו שני דברים מאוד קרובים אז היא לא תרד כל כך יהיה ישר איזשהו עמק כזה גבוה לא עמק
[00:41:44 - 00:41:45] כף גבוה ביניהם
[00:41:46 - 00:41:50] שיהיו שם עוד כל מיני תמונות סבירות שהן דומות לשתי התמונות האלה.
[00:41:50 - 00:41:53] זה אזור שהיו בו מאה תמונות אז הוא עוד יותר גבוה
[00:41:57 - 00:41:59] זה תלוי במודל
[00:42:02 - 00:42:02] אוקיי
[00:42:07 - 00:42:10] בקורס יש את המילה Deep גם Deep Generative Models
[00:42:11 - 00:42:19] Deep Generative Models אז באמת לתפוס התפלגות כזאת בממד מיליון זה משהו שהוא די קשה ודי קשה לחשוב על מודל טוב שעושה את זה
[00:42:19 - 00:42:23] ואיך שעושים את זה בשנים האחרונות וכמו שאתם רואים בהצלחה די
[00:42:24 - 00:42:25] די גבוהה
[00:42:26 - 00:42:27] עם רשתות ניורונים.
[00:42:28 - 00:42:31] בעצם הרשתות ניורונים הן חלק מהמודל ההסתברותי שלנו
[00:42:32 - 00:42:37] ויש כל מיני דרכים לגרום לרשתות ניורונים להיות חלק מהמודל ההסתברותי, אנחנו נדבר עליהם
[00:42:39 - 00:42:46] אבל כבר הראיתי לכם את זה בתרשים הקודם עם התפוח משהו שמייצר תפוח אז תחשבו על איזושהי רשת שמקבלת בתור input
[00:42:47 - 00:42:49] את ההטרת קוביות הזאת
[00:42:49 - 00:42:55] איזשהו וקטור ונדומלי נגיד ואולי עוד איזשהו משהו שאומר מאיזה קלאס זה
[00:42:56 - 00:43:01] והיא בסופו של דבר מייצרת משהו בגודל של הדאטה שאנחנו רוצים למשל תמונה
[00:43:02 - 00:43:04] זה יהיה מודל הסתברותי
[00:43:04 - 00:43:10] שבהינתן איזושהי הטלה ונדומלית של האינפוט
[00:43:11 - 00:43:13] ייתן לנו כל פעם output אחר
[00:43:16 - 00:43:19] אז כמו שאמרתי אבל אנחנו נדבר על העניין הזה של
[00:43:19 - 00:43:23] לשלב הרשתות בתוך המודלים שלנו רק בחלק השני של חוץ
[00:43:27 - 00:43:37] אז מה אנחנו צריכים בעצם בשביל לאמן מודל גנרטיבי מה המרכיבים העיקריים אנחנו צריכים דאטה קודם כל אנחנו צריכים את הדאטה שמייצג
[00:43:38 - 00:43:39] את המרחב שאנחנו רוצים ללמוד
[00:43:43 - 00:43:49] ובכלל במשין לרנינג בשביל להגיע לביצועים טובים בעצם המרכיב העיקרי זה הדאטה
[00:43:50 - 00:43:55] בהרבה מקרים כבר בדיוק איזה סוג מודל משתמשים לא כזה משנה העיקר שיש מספיק דאטה
[00:43:56 - 00:43:58] זה הייתי אומר המרכיב הכי חשוב בכל מקרה
[00:44:00 - 00:44:06] אבל אנחנו לא כל כך נדבר עליו כי מבחינתנו אין כל כך מה לדבר עליו בקורס
[00:44:08 - 00:44:13] נקודה פה היא פשוט להשיג דאטה כמה שיותר טוב של מה שאנחנו רוצים ללמוד
[00:44:16 - 00:44:19] אז זו נקודה ראשונה נקודה שנייה זה המודל עצמו באמת
[00:44:20 - 00:44:26] ואיך נראה המודל ההסתברותי הזה שאנחנו רוצים שבסופו של דבר ילמד מהי הפרמטריזציה שלו
[00:44:27 - 00:44:30] אז מודל פשוט זה גאוסיאן
[00:44:31 - 00:44:35] יש לו צורה יחסית פשוטה הוא עדיין יכול לתפוס כל מיני דברים ויכול לעשות את כל מה שאמרנו
[00:44:36 - 00:44:39] דוגמה קצת יותר מורכבת זה תערובת של גאוסיאנים
[00:44:40 - 00:44:42] זה כמו שזכרתי עכשיו
[00:44:46 - 00:44:49] ועוד דוגמה שאנחנו נדבר עליה זה מה שנקרא
[00:44:49 - 00:44:53] latent variable model זאת אומרת מודל שיש בו משתנה חבוי
[00:44:54 - 00:45:00] גם מיקצ'ר אוף גארשנס תלוי איך אנחנו מאמנים אותם אפשר לחשוב עליהם בתור latent variable models
[00:45:01 - 00:45:06] וגם חלק מהמודלים העמוקים שאנחנו נאמן עליהם אפשר לחשוב עליהם בתור latent variable model
[00:45:07 - 00:45:13] אנחנו נדבר על זה וגם נדבר על עוד כל מיני סוגים של מודלים שאפשר להשתמש בהם
[00:45:14 - 00:45:15] אז זה המרכיב השני
[00:45:16 - 00:45:17] המרכיב השלישי זה
[00:45:19 - 00:45:21] בעצם הobjective function שלנו בזמן האימון
[00:45:22 - 00:45:24] היה צריך להיות פוקטור Training Objective
[00:45:25 - 00:45:29] איך אנחנו בהינתן שהחלטנו נגיד שאנחנו רוצים שזה יהיה גאוסיאן
[00:45:30 - 00:45:31] איך אנחנו
[00:45:33 - 00:45:38] יש לנו דאטה אנחנו יודעים שאנחנו רוצים ללמוד גאוסיאן מהדאטה, איך אנחנו עושים את זה, מה הobjective function
[00:45:39 - 00:45:41] מה אנחנו מנסים למזער או למחסם
[00:45:42 - 00:45:44] אז אחת מהשיטות הכי
[00:45:45 - 00:45:48] סטנדרטיות ושימושיות זה maximum likelihood כבר דיברנו על זה די הרבה
[00:45:49 - 00:45:54] בקורסים קודמים וגם בקורס הזה נדבר על זה די הרבה אבל יש גם עוד שיטות
[00:45:54 - 00:45:57] אז אחת מהשיטות שאני מדבר עליהן נקראת Score Matching
[00:45:58 - 00:45:59] שיטות שונה ממקסימום לייטליות
[00:46:02 - 00:46:10] והחלק הרביעי זה איך אנחנו עושים את האופטימיזציה בדיוק אז גם יש כל מיני דרכים לפעמים אנחנו רוצים לעשות מקסימום לייטליות אבל אי אפשר לעשות את זה בצורה מדויקת
[00:46:10 - 00:46:12] צריך לעשות כל מיני קירובים
[00:46:13 - 00:46:17] אז למשל שני קירובים שמשתמשים בהם אחד זה נקרא Variational inference שמדבר על זה הרבה
[00:46:18 - 00:46:19] ומונטה קרלו
[00:46:22 - 00:46:23] Mark of Changes
[00:46:23 - 00:46:25] MCMC זה שיטות
[00:46:26 - 00:46:28] הסתברותיות לעשות את האופטימיזציה
[00:46:28 - 00:46:31] אנחנו לומדים מודל הסתברותי בשיטה הסתברותית
[00:46:32 - 00:46:32] גם על זה אנחנו מדברים
[00:46:34 - 00:46:38] בעצם כל הדברים האלה אנחנו נדבר בהתחלה בלי קשר למודלים עמוקים
[00:46:40 - 00:46:45] על שיטות נוירונים וזה פחות או יותר הנושאים של החלק הראשון
[00:46:46 - 00:46:51] ואז בחלק השני אנחנו נדבר על כמה מודלים אני שכחתי לכתוב פה את ה... כתבתי את זה בסילבוס
[00:46:51 - 00:46:54] אבל שכחתי לכתוב את זה כאן בנצגת אז יש לכם את זה נכון אנחנו נדבר על
[00:46:58 - 00:47:01] מודלים אוטו-רגרסיב מודלס
[00:47:02 - 00:47:05] נדבר על VAES זה Variational Auto-Encorders
[00:47:06 - 00:47:10] זה מודל למשל שמשתמש ב-Varational inference כדי ללמוד מודל שהוא
[00:47:11 - 00:47:21] Late-Nferiable מודל גם עם maximum likelihood אנחנו נלמד מודלים מה שנקרא Normalizing Flows
[00:47:21 - 00:47:25] ו-Energy Based Models ו-Fusion Models
[00:47:25 - 00:47:27] מקווה שנספיק את כל הדברים האלה
[00:47:28 - 00:47:38] אז זה קופת יותר אני חושב הרשימה של כל המודלים שנדבר עליהם בחלק השני שכולם משתמשים בכל מיני קומבינציות שונות של הדברים שנלמד כאן
[00:47:39 - 00:47:43] עם רשתות ניורלנית
[00:47:57 - 00:47:58] אוקיי אז בואו נמשיך
[00:48:01 - 00:48:07] אוקיי אז יש לי כאן כמה דוגמאות של גם ברמת טיזר כזה לתת לכם קצת מוטיבציה
[00:48:07 - 00:48:10] איזה דברים בעצם אפשר לעשות עם מודלים גנרטיביים
[00:48:11 - 00:48:15] לא על כולם אנחנו ממש נדבר אבל כן כדאי שיהיה לכם את זה בראש
[00:48:16 - 00:48:18] יש פה רשימה ותכף יש כמה דוגמאות
[00:48:18 - 00:48:26] אז אנחנו יכולים לפתור ממש איזושהי משימה למשל אם הגדרנו מראש יש לנו איזשהו y שאנחנו רוצים לעשות לו פרדיקציה למשל קלסיפיקציה
[00:48:27 - 00:48:30] אז אנחנו יכולים להשתמש במודל גנרטיבי עם חוק בייס כדי לפתור את המשימה הזאת
[00:48:32 - 00:48:33] אנחנו יכולים לייצר דאטה
[00:48:34 - 00:48:38] אז זה כמו שאנחנו דיברנו כבר וראיתם דוגמאות.
[00:48:39 - 00:48:41] אנחנו יכולים לעשות מה שנקרא representation learning,
[00:48:41 - 00:48:54] שתכף אני אראה לכם דוגמה למה זה אומר. representation learning, המטרה של ללמוד את המודל הזה זה לא לייצר דאטה או לפתור איזושהי משימה, אלא זה לקבל בסופו של דבר ייצוג חדש
[00:48:55 - 00:48:58] של הדאטה שאנחנו עובדים איתו.
[00:48:59 - 00:49:01] למשל, אם אנחנו עובדים עם תמונות, אנחנו לא רוצים אחר כך,
[00:49:02 - 00:49:04] אנחנו לא רוצים להשתמש בתמונה בתור תמונה, זה לא כל כך נוח,
[00:49:05 - 00:49:11] אלא אנחנו רוצים שהמודל ההסתברותי ימצא בעצמו איזשהו ייצוג שמאפיין יותר טוב את התמונות.
[00:49:12 - 00:49:13] תכף אני אתן איזושהי דוגמה.
[00:49:13 - 00:49:16] ואז אנחנו רוצים לעשות עם הייצוג הזה משהו אחר.
[00:49:18 - 00:49:19] אולי אנחנו רוצים,
[00:49:20 - 00:49:25] וגם אפשר לחשוב על זה גם אולי קצת קשור, אנחנו רוצים להשתמש במודל ההסתברותי כדי,
[00:49:25 - 00:49:26] לא כדי לייצר דאטה,
[00:49:26 - 00:49:29] כדי בסופו של דבר לעשות איזה שהן החלטות
[00:49:30 - 00:49:35] שמבוססות על הדאטה שהיה יכול לייצר, או על המודל ההסתברותי הזה של הדאטה.
[00:49:36 - 00:49:37] נכתב דוגמא לזה.
[00:49:38 - 00:49:41] ואולי זה דרך כללית להציג הרבה דברים,
[00:49:41 - 00:49:44] ואנחנו נדבר על זה ברוב הקורס.
[00:49:47 - 00:49:49] פוביליסטיק אינפרנס, בעצם הנותרה היא,
[00:49:50 - 00:49:51] ברגע שיש לנו מודל הסתברותי,
[00:49:52 - 00:49:53] יכולים לעשות כל מיני דברים,
[00:49:53 - 00:49:54] יכולים לשאול כל מיני שאלות,
[00:49:54 - 00:49:59] תגיד מה אם הייתי יודע את הקטע הזה בדאטה ולא יודע את הקטע הזה,
[00:50:00 - 00:50:04] מה אם הייתי יודע שהקטע הזה הוא בטוח בין 0 ל-1,
[00:50:05 - 00:50:05] אבל לא יותר מזה,
[00:50:06 - 00:50:07] אפשר לחודק כל מיני
[00:50:09 - 00:50:14] סוגים שונים של אינפרנס ולצרף כמה מודלים שונים הסתברותיים אחד לשני.
[00:50:16 - 00:50:18] אוקיי, אז בוא נסתכל קצת על דוגמאות.
[00:50:18 - 00:50:20] ייצור דאטה, למה שנרצה לייצר דאטה?
[00:50:21 - 00:50:24] זה דוגמאות של, אני חושב שזה מיד-ג'ורני.
[00:50:24 - 00:50:28] זה גם מודל של מוסר חלקי דיפיוז'ן מודל שנלמד עליו,
[00:50:29 - 00:50:31] שאפשר להשתמש בו פשוט בשביל אומנות.
[00:50:33 - 00:50:37] אנשים כבר מתחילים להשתמש בדבר הזה כדי לייצר דוגמאות חדשות,
[00:50:38 - 00:50:40] אומנות או עיצוב,
[00:50:41 - 00:50:53] יש רמה מסוימת שאפשר לשלוט בדברים האלה ולגרום להם לייצר דברים שאתה רוצה או להיות מופתע מדברים שקורים ולהשתמש בזה בתור חלק מהאומנות.
[00:50:53 - 00:50:55] זה למשל דוגמה של מה אפשר לעשות עם מודלים בינתיים.
[00:50:58 - 00:51:01] אפשר לייצר דאטה אבל במטרה,
[00:51:01 - 00:51:05] כלומר לתקן את הדאטה שיש לנו או
[00:51:05 - 00:51:07] לעבד את הדאטה שיש לנו באיזשהו אופן.
[00:51:08 - 00:51:11] למשל אם יש לנו תמונה שיש בה חורים,
[00:51:11 - 00:51:13] אנחנו רוצים להשלים את החורים האלה.
[00:51:14 - 00:51:20] אם יש לנו איזשהו סיגנל אודיו עם הרבה רעש נוסף, אנחנו היינו רוצים להסיר את הרעש הזה.
[00:51:21 - 00:51:40] אנחנו רוצים גם פה לייצר בעצם דאטה אבל שהוא איכשהו מותנה ממה שהיה לנו קודם או אפשר לתאר את הבעיה הזאת בתור בעיית פרובליסטיק אינפרנס ואנחנו רוצים בהינתן איזה שהם מודל הסתברותי שייצר את הדאטה ולמצוא את המודל הנקי לפני שהדאטה הורעש
[00:51:41 - 00:51:44] להפוך את הבעיה, אז זה גם דרך לחשוב על זה
[00:51:44 - 00:51:48] סופר רזולוציה זה גם דוגמה לסוג של סיגנל פרוססינג,
[00:51:48 - 00:51:55] יש לנו תמונה ברזולוציה נמוכה ואנחנו רוצים לשפר את הרזולוציה שם.
[00:51:56 - 00:51:58] אנחנו רוצים להשלים פיקסלים חסרים,
[00:51:59 - 00:52:04] לא בדיוק להשלים פיקסלים כי כל פיקסל כאן הוא בעצם ממוצע של כמה פיקסלים, אנחנו רוצים לחדד את התמונת,
[00:52:05 - 00:52:08] להפוך כל פיקסל כאן ליותר פיקסל.
[00:52:10 - 00:52:11] זה עוד דוגמה לשימוש.
[00:52:13 - 00:52:16] representation learning מה שדיברתי עליו קודם, אז יש פה למשל דוגמה,
[00:52:16 - 00:52:24] אם אנחנו לומדים מודל של תמונות שנראות ככה, של כל מיני אובייקטים בכל מיני צבעים, בכל מיני חדרים עם קירות בצבעים שונים,
[00:52:24 - 00:52:26] ואז אנחנו רוצים להשתמש במודל הזה,
[00:52:27 - 00:52:35] אז מה שהיינו רוצים זה למשל ללמוד איזשהו קלאסיפייר או לא יודע, שיהיה איזשהו רובוט שיודע לקחת את
[00:52:36 - 00:52:40] הקובייה הזאת הנכונה מתוך החדר בהינתן התמונה הזאת.
[00:52:42 - 00:52:46] אז ללמוד איזשהו קלאסיפייר או כל משהו שפותר בעיה,
[00:52:46 - 00:52:51] בהינתן התמונה הזאת יכול להיות קשה כי בעצם המודל צריך ללמוד
[00:52:51 - 00:52:57] לסווג משהו לדעת איפה הוא נמצא באיזה צורה הוא הכל מעורבב ביחד
[00:52:57 - 00:53:00] אבל אם אנחנו לומדים מודל הסתברותי של הדאטה הזה
[00:53:01 - 00:53:05] שבעצם מודל שיודע לייצר תמונות כאלה הוא מבין מה המבנה
[00:53:05 - 00:53:09] של הדאטה סט הזה ויודע לייצר תמונות חדשות מתוך המבנה הזה
[00:53:10 - 00:53:15] אז יכול להיות שאיפשהו בתוך המודל יש את מה שנקרא כאן Feature Space שהוא מחלק
[00:53:16 - 00:53:20] את העולם הזה לחלקים שונים
[00:53:21 - 00:53:25] בצורה כזאת שיותר קל לגשת לכל אחד מהם בצורה עצמאית
[00:53:25 - 00:53:28] ואולי יש לו איזה חלק שיודע שחייב להיות כאן חדר
[00:53:29 - 00:53:35] בכל הטרניסט שהיה היה חדר אז חייב להיות כאן חדר שיש בו קירות ורצפה
[00:53:35 - 00:53:41] והקיר תמיד הוא באותו צבע והרצפה תמיד בצבע אחד אבל זה לא חייב להיות אותו צבע
[00:53:41 - 00:53:45] אז יש כאן חלק שאומר אוקיי מה הצבע של הרצפה ומה הצבע של הקיר
[00:53:45 - 00:53:54] יש בעצם, בגלל שהמודל הזה מקבל איזושהי הטלת קובייה, נכון? את הוקטור הרנדומלי הזה, של ה-random bits שלו,
[00:53:55 - 00:54:00] והוא צריך להגיד, הביט הראשון ברנדום איכשהו ממפה אותו לצבע של הקיר,
[00:54:00 - 00:54:07] והביט השני לצבע של הרצפה, אוקיי? אז יש איפשהו במודל משהו שתופס צבעים של קיר ומשהו שתופס צבעים של רצפה,
[00:54:07 - 00:54:10] ויש עוד משהו שתופס מה הצורה של האובייקט,
[00:54:10 - 00:54:10] איפה הוא נמצא,
[00:54:11 - 00:54:12] מה הגודל שלו,
[00:54:12 - 00:54:14] אז יכול להיות שיותר קל להסתכל
[00:54:15 - 00:54:20] על הפיצ'רים האלה, בסופו של דבר, אם נרצה לפתור איזושהי משימה, מאשר על התמונה המקורית.
[00:54:21 - 00:54:26] זה נקרא representation learning. אנחנו קודם לומדים איזשהו מודל הסתברותי של התמונות,
[00:54:27 - 00:54:35] ומקווים שהמודל הזה יתפוס את המבנה שמעניין אותנו בסופו של דבר, ויאפשר לנו לעשות משימות אחרות בצורה יותר קלה.
[00:54:38 - 00:54:39] אוקיי, זה representation learning.
[00:54:40 - 00:54:43] decision making, מה הכוונה שלי ב-decision making?
[00:54:44 - 00:54:50] למשל, תחשבו על רכב אוטונומי, אוקיי? שהוא מסתכל, הוא מזהה ממש טוב, יש לו איזושהי מערכת vision ממש טובה,
[00:54:51 - 00:54:55] שמזהה שיש פה רכבי רגל, יש רכבי אופניים, יש רכבים אחרים, הכביש הולך ככה,
[00:54:56 - 00:54:57] המדרכה נמצאת פה.
[00:54:59 - 00:55:08] עדיין צריך להחליט מה לעשות, אוקיי? איך הוא מחליט מה לעשות? זה קשור למה הולך לקרות בעתיד, אוקיי? צריך להחליט האם הבן אדם הזה הולך לחצות,
[00:55:09 - 00:55:11] האם האופניים האלה,
[00:55:11 - 00:55:15] מתי הם יגיעו למעבר החציה הזה, האם אני עכשיו צריך לעצור?
[00:55:16 - 00:55:22] אני לא יודע, בדוגמה הזאתי די ברור שצריך לעצור, אבל אפשר לחשוב על מצב שאם האנשים האלה לא היו פה והיה את האוכל האופניים האלה,
[00:55:22 - 00:55:24] האם הוא צריך כבר עכשיו לעצור או לא.
[00:55:25 - 00:55:31] אז כל מיני החלטות כאלה שבעצם דורשות איזושהי תחזית של מה הולך לקרות בעתיד,
[00:55:32 - 00:55:36] וגם על זה אפשר לחשוב בתור איזושהי דגימה מתוך מודל גנרטיבי. אם למדנו
[00:55:36 - 00:55:38] איך העולם מתנהג,
[00:55:39 - 00:55:45] יש לנו הרבה דאטה של הולכי רגל שהולכים ליד כבישים וחוצים כבישים ושל רוכבי אופניים,
[00:55:45 - 00:55:52] אנחנו יכולים להשתמש במודל הזה כדי להגיד מה ההסתברות שהרוכב אופניים שנמצא כאן יתקדם לתוך הכביש,
[00:55:52 - 00:55:53] ולפי זה לקבל את ההחלטות שלנו.
[00:55:55 - 00:56:02] שימו לב שמרכיב מאוד חשוב כאן זה העניין הזה של הסתברות, שאנחנו עוד נדבר על זה,
[00:56:02 - 00:56:08] אני עכשיו שם לב שלא הדגשתי את זה בהצעה היום,
[00:56:09 - 00:56:13] אין לי שקף לזה, אבל זה כן אחד מהדברים החשובים במודלים גנרטיביים, היכולת
[00:56:14 - 00:56:17] בעצם לתפוס חוסר ודאות,
[00:56:18 - 00:56:21] ואחר כך אפשר לחשוב על זה בתור חלק מ-propobelistic inference,
[00:56:22 - 00:56:22] אני לא רק יודע,
[00:56:23 - 00:56:26] לא רק נושא פרדיקציה האם הרוכב אופניים הזה יחצה את
[00:56:27 - 00:56:33] הכביש או לא, אלא מה ההסתברות שהוא יחצה את הכביש, ולפי זה אני יכול לקבל החלטות אחר כך בצורה יותר מושכלת.
[00:56:33 - 00:56:34] זאת אומרת,
[00:56:34 - 00:56:40] ברכב אוטונומי נגיד אני לא יכול לעצור תמיד אם יש הסתברות מאוד קטנה
[00:56:43 - 00:56:44] שיהיה לי איזשהו
[00:56:45 - 00:56:46] רוכב אופניים במועלך הציין,
[00:56:46 - 00:56:52] תמיד יהיה איזושהי הסתברות קטנה, אני צריך למצוא איזשהו סף שהוא בסדר מבחינת המערכת.
[00:56:53 - 00:56:55] כן, יכול להיות שיש פה כמה אפשרויות, להאט,
[00:56:55 - 00:56:58] להאיץ, לעצור,
[00:56:59 - 00:57:01] כל זה אם יש לנו מודל הסתברותי שיודע להגיד לנו
[00:57:01 - 00:57:05] גם מה ההסתברות לכל אחת מהאפשרויות,
[00:57:06 - 00:57:07] העתידים האפשריים,
[00:57:07 - 00:57:09] אז זה מאוד עוזר לקבלת החלטות.
[00:57:12 - 00:57:13] אוקיי, עוד דוגמה,
[00:57:13 - 00:57:17] שוב, ליצור דאטה, רק רציתי עוד פעם לתת דוגמה של טקסט,
[00:57:17 - 00:57:19] למרות שאנחנו לא נדבר על זה כל כך בטקסט,
[00:57:19 - 00:57:26] אבל אחד מהשימושים מאוד נפוצים של מודלים כמו ChatGPT זה בהשלמת קוד.
[00:57:27 - 00:57:28] גם אפשר לחשוב על זה בתור
[00:57:29 - 00:57:30] קצת כמו הדוגמה שראינו קודם,
[00:57:30 - 00:57:35] אנחנו בעצם בהינתן שהמודל ראה את המתחלה של הקוד שלנו,
[00:57:36 - 00:57:37] הוא יודע להשלים את הקוד,
[00:57:37 - 00:57:43] אז הוא בעצם חוזה את העתיד של כתיבת הקוד שלי,
[00:57:43 - 00:57:46] בדיוק כמו שהוא חזה אם הבן אדם יחצה את הכביש או לא,
[00:57:46 - 00:57:51] אז פה הוא יחזה מה הייתי הולך לכתוב בפונקציה הבאה.
[00:57:55 - 00:58:00] אוקיי, תרגום דאטה זה גם סוג של ייצור טקסט מותנה,
[00:58:01 - 00:58:01] אוקיי,
[00:58:01 - 00:58:08] גם חשוב פה העניין ההסתברותי, כי הרבה פעמים בטקסט בשפה מסוימת יש הרבה אפשרויות
[00:58:08 - 00:58:09] לתרגם אותו לשפה אחרת,
[00:58:10 - 00:58:12] אנחנו רוצים לתפוס את כל האפשרויות האלה באיזושהי דרך.
[00:58:15 - 00:58:18] זה עוד דוגמה שרציתי לתת מתחום המדע,
[00:58:19 - 00:58:21] זה קשור לפרויקט שאני איתי גם קשור בו,
[00:58:22 - 00:58:28] זה פרויקט של הבנה של המבנה תלת-ממדי של חלבונים,
[00:58:29 - 00:58:32] מתוך מיקרוסקופ אלקטרוני מאוד רועש,
[00:58:32 - 00:58:35] מקבלים הרבה תמונות של החלבונים,
[00:58:35 - 00:58:37] אנחנו צריכים להבין מה ההתפלגות,
[00:58:38 - 00:58:41] החלבונים האלה זה אותו חלבון, אבל יש לה התפלגות של צורות שונות,
[00:58:41 - 00:58:43] יכולת לפעמים מקופל, לפעמים הוא נפתח.
[00:58:43 - 00:58:47] רוצים לתפוס את ההתפלגות הזאת לא בשביל שנוכל לייצר לנו סתם דאטה כמו תמונות,
[00:58:47 - 00:58:49] אנחנו רוצים לייצר משהו יפה,
[00:58:50 - 00:58:56] אלא כדי להבין משהו על התהליך שקורה בגוף, אוקיי? אז היכולת הזאת לייצר דאטה ולהבין את ההתפלגות של הדאטה היא
[00:58:56 - 00:59:03] לא תמיד בשביל לייצר את הדעתה, אלא הרבה פעמים בשביל להבין את ההתנהגות של הסוג הזה של הדאטה.
[00:59:04 - 00:59:08] פה זה גם מודל מסוג Variational Auto-Encoder, V.A.E,
[00:59:08 - 00:59:10] שאנחנו גם נדבר עליו בחלק השני.
[00:59:13 - 00:59:17] אוקיי, ולסיום של האינטרודקשן הזה, של ההקדמה לקורס,
[00:59:18 - 00:59:20] שמתי כאן ציטוט של פיינמן,
[00:59:21 - 00:59:24] שהוא עכשיו אחד מהפיזיקאים או המורים לפיזיקה הגדולים,
[00:59:25 - 00:59:28] שאמר What I cannot create, I do not understand.
[00:59:29 - 00:59:35] אז בעצם הדרך היחידה להבין משהו באמת, זה להבין את התהליך שמייצר אותו.
[00:59:35 - 00:59:39] ועל ידי מודלים גנרטיביים זה אולי דרך
[00:59:40 - 00:59:43] להבין יותר טוב איך העולם עובד,
[00:59:44 - 00:59:47] באופן שלי הקדמתי כאן Path to AI,
[00:59:48 - 00:59:49] בעצם כדי,
[00:59:51 - 00:59:53] כנראה שגם ככה המוח שלנו עובד,
[00:59:53 - 00:59:59] ולהבין דברים בצורה יותר טובה על העולם שלנו, אנחנו צריכים להבין את התהליך
[01:00:00 - 01:00:00] שמייצר אותו.
[01:00:03 - 01:00:04] אוקיי,
[01:00:04 - 01:00:06] אז החלק השני,
[01:00:07 - 01:00:21] אני מצטער אם לעשות עכשיו הפסקה או להתחיל. אני חושב שנתחיל קצת, ואז נעשה הפסקה של עשר דקות, ואנחנו נחזור. באופן כללי, גם אם ניפגש פיזית, נעשה את זה, אני חושב, ככה, נעשה הפסקה אחת באמצע,
[01:00:22 - 01:00:23] אני חושב שזה מקובל על כולם.
[01:00:24 - 01:00:28] אוקיי, אז איך אנחנו ניגשים לבעיה הזאת? אנחנו רוצים ללמוד מודל גנרטיבי.
[01:00:30 - 01:00:33] אוקיי, אז מה הסטאפ שלנו מבחינת Machine Learning
[01:00:34 - 01:00:36] של הבעיה שאנחנו מתעניינים בה?
[01:00:36 - 01:00:41] אז אוקיי, אנחנו מניחים שיש לנו דאטה, אוקיי? שהדאטה נוצר על ידי איזושהי התפלגות
[01:00:41 - 01:00:42] לא ידועה, אוקיי?
[01:00:43 - 01:00:45] התפלגות של העולם שמייצרת לנו את הדאטה.
[01:00:46 - 01:00:48] אנחנו נקרא להתפלגות הזאת P-Data.
[01:00:49 - 01:00:51] אוקיי? לפעמים קוראים לזה D גדול,
[01:00:52 - 01:00:58] פה כדי לא לבלבל אנחנו עומדים הרבה עם מודלים הסתברותיים, אז אנחנו נקרא לזה פשוט P-Underscore Data.
[01:01:03 - 01:01:08] ועכשיו אנחנו רוצים ללמוד התפלגות חדשה.
[01:01:09 - 01:01:12] אז יש לנו התפלגות שיש לה פרמטרים לא ידועים,
[01:01:13 - 01:01:15] אנחנו באופן כללי נקרא לפרמטרים האלה תטא,
[01:01:16 - 01:01:18] וההתפלגות שלנו נקרא לה P-Teta.
[01:01:19 - 01:01:26] ואנחנו נרצה למצוא את הפרמטרים תטא ככה ש-P-Teta יהיה כמה שיותר קרוב לפי דאטה.
[01:01:28 - 01:01:29] אם נצליח ממש להיות
[01:01:30 - 01:01:30] שווים לפי דאטה,
[01:01:31 - 01:01:34] אז ניצחנו, תפסנו את ההתפלגות שמייצרת את הדאטה,
[01:01:35 - 01:01:41] ואנחנו יכולים לייצר בעצמנו דאטה חדש ולהבין איזושהי נקודה, להבין מה ההסתברות שלה
[01:01:42 - 01:01:44] ולעשות כל מה שאנחנו רוצים. כנראה לא נגיע למצב הזה,
[01:01:45 - 01:01:49] אבל עדיין אנחנו רוצים למצוא תטא ככה ש-P-Teta יהיה כמה שיותר קרוב לפי דאטה.
[01:01:51 - 01:01:54] יש פה איזה תרשים שאפשר לראות, נגיד שיש לנו דאטה סט של כלבים,
[01:01:55 - 01:01:58] יש לנו XI, X זה התמונות,
[01:01:58 - 01:02:01] שנוצרו מתוך ההתפלגות פי דאטה.
[01:02:03 - 01:02:04] פי דאטה נמצא כאן,
[01:02:04 - 01:02:08] זה המרחב עכשיו של כל ההתפלגויות האפשריות.
[01:02:09 - 01:02:13] מה שאתם רואים כאן, כאילו כל האזור הזה זה כל התטות האפשריות.
[01:02:13 - 01:02:14] זה מצויר בדו-ממד,
[01:02:14 - 01:02:15] כי זה שקף,
[01:02:16 - 01:02:21] זה כאילו שיש לי כאן רק שני פרמטרים של תטא, אבל תחשבו שתטא הוא איזשהו מודל עם מיליוני פרמטרים.
[01:02:23 - 01:02:25] אז P-Data נמצא איפשהו במרחב הזה של
[01:02:26 - 01:02:27] בממד מיליונים,
[01:02:28 - 01:02:32] ויש לי את המודל שאני בניתי, שאני אקרוא לו P-Teta,
[01:02:32 - 01:02:35] שהוא כל המשפחה הזאתי,
[01:02:35 - 01:02:38] משפחה של מודלים אפשריים,
[01:02:38 - 01:02:40] כל נקודה כאן זה תטא אחר,
[01:02:41 - 01:02:46] זאת אומרת שכל נקודה כאן זה יהיה מודל אחר עם פרמטרים אחרים,
[01:02:47 - 01:02:49] ואני רוצה למצוא את התטא שהוא הכי קרוב, איכשהו, לפי דאטה שלו.
[01:02:52 - 01:02:58] אז מה המדד מרחק הזה, זה בעצם מה שאמרנו קודם, מה ה-objective function שלנו, איך בדיוק אנחנו אומרים
[01:02:59 - 01:03:01] מה אנחנו רוצים למזער,
[01:03:02 - 01:03:04] ואיך אנחנו עושים בדיוק את החיפוש הזה,
[01:03:04 - 01:03:06] אז זה בעיית האופטימיזציה,
[01:03:07 - 01:03:08] שגם את זה.
[01:03:11 - 01:03:21] אוקיי, אז קודם כל אנחנו נדבר על הבעיה הראשונה והמרכיבים שלנו, שזה איזה מידולים אנחנו יכולים לדבר עליהם, איך אנחנו, איזה סוגים של מודלים אנחנו יכולים לדבר עליהם, אז למשל,
[01:03:21 - 01:03:22] המודל הכי פשוט,
[01:03:23 - 01:03:28] שדיברנו עליו פה בקורס להסתברות, זה מודל ברנולי, התפלגות ברנולי,
[01:03:29 - 01:03:32] שזה פשוט הטלת מטבע עם איזושהי הטייה,
[01:03:32 - 01:03:32] אוקיי?
[01:03:33 - 01:03:37] באופן כללי זה משהו שיש לו שני אלפוטים אפשריים,
[01:03:38 - 01:03:38] 0 או 1,
[01:03:39 - 01:03:41] ויש לו איזושהי הסתברות לאפס והסתברות לאחד.
[01:03:43 - 01:03:54] אוקיי? אז נגיד שהדאטה שלנו מגיעה מהתפלגות כזאת, ואנחנו רוצים למצוא את התטה שממקסימים או שהכי קרובים להתפלגות האמיתית
[01:03:55 - 01:03:55] שייצרה את הדאטה.
[01:03:57 - 01:03:59] ואם זה בית מטבע, אז נגיד x הוא Heads-Tales,
[01:04:00 - 01:04:05] והמודל שלנו זה איזשהו מודל שנותן לנו הסתברות מסוימת ל-Heads,
[01:04:06 - 01:04:07] נקרא להם p,
[01:04:09 - 01:04:11] אני יכול לקרוא לזה תטא, כי זה בעצם כל תטא,
[01:04:11 - 01:04:15] זה כל הפרמטרים שלי, כי ההסתברות ל-Tales זה יהיה פשוט 1 מינוס p.
[01:04:17 - 01:04:19] אוקיי? אז יש לי כאן פרמטר, יש לי מודל שיש לו פרמטר אחד.
[01:04:21 - 01:04:27] אוקיי? זה במקרה הפשוט, אם הדאטה שהייתי רוצה ללמוד מודל גנרטיבי שלו זה הטלות מטבע.
[01:04:28 - 01:04:34] כמו שאמרנו כאן, בדרך כלל אנחנו לא נקרא לכזה דבר מודל גנרטיבי, כי הרעיון המודל גנרטיבי זה שהדאטה הוא בממד גבוה.
[01:04:35 - 01:04:40] אבל לצורך השיעור כאן, אז בואו נתחיל מזה.
[01:04:41 - 01:04:43] זה הפרמטר שלי.
[01:04:44 - 01:04:49] אוקיי, עכשיו בואו נתחיל לסבך את העניינים. נגיד שיש לנו יותר אפשרויות משתיים.
[01:04:50 - 01:04:52] נגיד שיש לנו עשר אפשרויות.
[01:04:55 - 01:04:59] אז איך אנחנו יכולים לבנות, מה הפרמטריזציה שאנחנו יכולים לבנות שלנו זה?
[01:05:03 - 01:05:04] אני רוצה להגיד,
[01:05:05 - 01:05:06] נו,
[01:05:11 - 01:05:14] אם יש לנו מודל שזה לא הטלת מטבע, אלא בואו נעשה הטלת קובייה, אוקיי?
[01:05:15 - 01:05:16] יש שש אפשרויות.
[01:05:16 - 01:05:18] אז מה הפרמטרים שאנחנו יכולים,
[01:05:18 - 01:05:20] איך אנחנו יכולים למדל את הבעיה הזאתי?
[01:05:22 - 01:05:26] לווקטור בגודל 6 אולי?
[01:05:28 - 01:05:31] כן, אז אפשר וקטור בגודל 6, זה לא לגמרי מדויק.
[01:05:32 - 01:05:33] ואם יש לנו שש אפשרויות,
[01:05:33 - 01:05:35] אז מה המספר הפרמטרים שאנחנו צריכים?
[01:05:36 - 01:05:39] 5. 5, נכון? כי השישי חייב להיות השלמה ל-1 פשוט,
[01:05:40 - 01:05:47] כמו שפה יש לנו רק פרמטר 1. אז באמת אם זה באופן כללי אנחנו נצטרך n-1 פרמטרים.
[01:05:49 - 01:05:50] נכון? אם יש לנו התפלגות קטגורית.
[01:05:53 - 01:05:55] אוקיי, זה השלב הבא.
[01:05:56 - 01:05:57] עכשיו אנחנו רוצים להתקדם,
[01:05:58 - 01:06:01] אנחנו נגיד שאנחנו עדיין מדברים על התפלגות קטגורית,
[01:06:02 - 01:06:07] זאת אומרת שיש כמה אפשרויות שונות דיסקרטיות,
[01:06:07 - 01:06:09] אבל מולטיוורייט,
[01:06:10 - 01:06:12] אוקיי? זאת אומרת שיש לנו כמה
[01:06:16 - 01:06:16] ממדים.
[01:06:17 - 01:06:20] ופה אנחנו כבר נכנסים לעולם של תמונות.
[01:06:21 - 01:06:25] זה דאטה סט מפורסם, אני יודע איך קוראים לו?
[01:06:29 - 01:06:29] אתה יודע?
[01:06:30 - 01:06:31] לא יודעת ככה ככה מפורסם?
[01:06:32 - 01:06:32] אמניסט.
[01:06:33 - 01:06:33] אמניסט.
[01:06:34 - 01:06:35] כן, נכון. אמניסט,
[01:06:35 - 01:06:37] אחד מהדאטה סטים שהתחילו את
[01:06:41 - 01:06:42] או גרמו לפיתוח של Machine Learning,
[01:06:44 - 01:06:46] הדאטה סט של ספרות, אוקיי?
[01:06:47 - 01:06:52] זו בעיקרון, יש פה כמה דרגות אפור, אבל אפילו אם נחשוב על זה שזה בינארי, אוקיי? שיש פה או לבן או שחור,
[01:06:54 - 01:06:55] אוקיי? אז יש כאן,
[01:06:56 - 01:06:58] אז זה קטגורי אבל זה בעצם ברנולי, אוקיי?
[01:06:58 - 01:07:00] אז אפשר לשאול על זה בתור מולטי וריאט ברנולי,
[01:07:01 - 01:07:04] זה רק שני ערכים בכל מימד, אוקיי? יש כאן,
[01:07:05 - 01:07:08] הדאטה סט הזה הוא בגודל 28 על 28,
[01:07:09 - 01:07:10] אז כמה פיקסלים זה יוצא?
[01:07:11 - 01:07:13] 700 ומשהו?
[01:07:14 - 01:07:15] כן, זה 700 ומשהו פיקסלים,
[01:07:17 - 01:07:20] יש לי 700 ומשהו ממדים, שכל אחד מהם יכול להיות
[01:07:21 - 01:07:21] 0 או 1,
[01:07:24 - 01:07:24] אוקיי?
[01:07:25 - 01:07:27] אז כמה פרמטרים צריך בשביל
[01:07:28 - 01:07:31] לתפוס את ההתפלגות הזאת.
[01:07:46 - 01:07:47] אתה רוצה לנסות?
[01:07:48 - 01:07:49] גודל התמנה.
[01:07:51 - 01:07:52] אז אם יש לי כאן 700,
[01:07:53 - 01:07:55] כמה זה 780 ו...
[01:07:55 - 01:07:55] וארבע.
[01:07:56 - 01:07:57] 984,
[01:07:57 - 01:08:01] אם יש לי כאן 784 פיקסלים אז אני צריך 784 פרמטרים?
[01:08:04 - 01:08:04] זה לא נכון,
[01:08:05 - 01:08:06] זו התשובה לכם.
[01:08:11 - 01:08:14] כמה אפשרויות שונות לתמונות שונות יש פה?
[01:08:15 - 01:08:17] שתיים בחזקת 780 ו...
[01:08:18 - 01:08:19] בדיוק. אז כמה פרמטרים צריך?
[01:08:22 - 01:08:23] שתיים בחזקת
[01:08:24 - 01:08:24] מספר זה.
[01:08:25 - 01:08:26] פחות 1. כן.
[01:08:27 - 01:08:32] נכון? אז אם אנחנו רוצים ללכת בדיוק בגישה שהיינו פה, בעצם כאן יש לנו שתי אפשרויות,
[01:08:32 - 01:08:33] כאן היה לנו 6 אפשרויות,
[01:08:34 - 01:08:36] וכאן יש לנו 700,
[01:08:36 - 01:08:39] 2 בחזקת 784 אפשרויות.
[01:08:40 - 01:08:42] אז זה פשוט יהיה באופן כללי n
[01:08:43 - 01:08:46] פחות 1, ש-n זה מספר האיברים האפשריים במרחב הזה.
[01:08:48 - 01:08:48] אוקיי?
[01:08:49 - 01:08:53] אז זה קצת יותר מדי פרמטרים כבר בתמונות בינאריות של n. תחשבו על תמונות
[01:08:56 - 01:08:58] מגה-פיקסל צבעוניות.
[01:08:59 - 01:08:59] אוקיי?
[01:09:00 - 01:09:02] תמונות צבעוניות, דרך אגב, איך אנחנו ממדלים פיקסלים?
[01:09:03 - 01:09:04] אנחנו נדבר על זה קצת בהמשך.
[01:09:05 - 01:09:09] כל פיקסל הוא פשוט
[01:09:10 - 01:09:12] שלושה ערכים, הערך של ה-red,
[01:09:13 - 01:09:15] הערך של ה-green והערך של ה-blue.
[01:09:16 - 01:09:17] אוקיי? זה ה-rgb.
[01:09:18 - 01:09:19] אז אנחנו יכולים לחשוב על,
[01:09:19 - 01:09:20] אם זה היה צבעוני,
[01:09:20 - 01:09:24] אז זה היה לנו כאן פי שלוש בחזקה.
[01:09:26 - 01:09:30] אנחנו לא יודעים שיש לנו בעצם כל תמונה היא שלוש תמונות
[01:09:30 - 01:09:32] אחת לכל צבע,
[01:09:33 - 01:09:38] וגם כל האפשרויות אפשריות, וגם בתמונה צבעונית, בדרך כלל אנחנו לא נדבר על
[01:09:39 - 01:09:40] שני ערכים אפשריים,
[01:09:40 - 01:09:45] אלא 256 זה הסטנדרט ערכים שונים בכל פיקסל.
[01:09:46 - 01:09:46] אוקיי? אז זה יהיה 256,
[01:09:49 - 01:09:51] גם בדרך כלל אנחנו מדברים על תמונות יותר גדולות, נגיד של מיליון פיקסלים.
[01:09:52 - 01:09:57] אוקיי? אז זה יהיה 256 בחזקת מיליון כפול שלוש. זה יהיה בחזקת שלוש מיליון.
[01:09:58 - 01:10:00] אז זה יהיה אם אנחנו רוצים לחשוב על כל האפשרויות,
[01:10:01 - 01:10:04] כל התמונות האפשריות הצבעוניות במיליון פיקסלים,
[01:10:05 - 01:10:07] אז זה בבירור יותר מדי פרמטרים,
[01:10:08 - 01:10:09] וגם כשאנחנו,
[01:10:09 - 01:10:17] דוגמאות מאוד קטנות כאלה, זה די מהר נהיה יותר מדי פרמטרים, אוקיי? אז זה נקרא באופן כללי, דיברנו על זה כבר בקורס של Machine Learning, נקראת המימד,
[01:10:18 - 01:10:20] וגם פה זה המצב.
[01:10:21 - 01:10:26] ובעצם אפשר לחשוב על כל הקורס הזה, על כל התחום הזה, איך אנחנו פותרים את הכללת המימד הזאת.
[01:10:28 - 01:10:31] אוקיי? האם היינו יכולים פשוט לתפוס את המודל הזה ככה,
[01:10:32 - 01:10:34] בצורה ישירה, אז אולי זה היה הדבר הכי טוב לעשות.
[01:10:35 - 01:10:39] אבל זה יותר מדי פרמטרים, זה אומר שגם פיזית אנחנו לא יכולים לממש את זה,
[01:10:40 - 01:10:41] גם מבחינת ההכללה.
[01:10:42 - 01:10:46] ראינו ב-Machine Learning שזה אומר שאנחנו כנראה,
[01:10:46 - 01:10:51] לא היו לנו מספיק דוגמאות קרובות אחת לשנייה באיזשהו מדד שהוא הגיוני,
[01:10:52 - 01:10:53] ולכן זה בלתי אפשרי.
[01:10:54 - 01:10:59] אז בעצם אפשר לחשוב על כל הקורס וכל התחום הזה בתור דרך לפתור את
[01:11:00 - 01:11:00] קללת המימד.
[01:11:03 - 01:11:05] טוב, אז נעשה עכשיו הפסקה של איזה רבע שעה,
[01:11:07 - 01:11:11] ואז בוא נעשה עשר דקות בעצם, כי יש עוד מה להספיק.
[01:11:12 - 01:11:15] אז ניפגש ב-440.
[01:11:16 - 01:11:18] תודה רבה.
[01:11:46 - 01:11:47] תודה רבה.
[01:21:16 - 01:21:35] טוב בואו נמשיך, חזרתם?
[01:21:46 - 01:21:53] אוקיי, שני אמיצים עם
[01:21:55 - 01:21:56] הצדמות פתוחות,
[01:21:57 - 01:21:58] כמו שכולם פרשים כבר לגמרי,
[01:21:59 - 01:22:04] ובואו נמשיך. אז כמו שאמרנו, אנחנו רוצים לפתור את בעיית קללת המימד
[01:22:05 - 01:22:07] ואיכשהו להצליח למצוא מודלים
[01:22:08 - 01:22:12] של תמונות, שזה דאטה בממד מאוד גבוה.
[01:22:13 - 01:22:20] אוקיי, ואחד מהכלים שנשתמש בהם זה חוסר תלות, אי תלות,
[01:22:20 - 01:22:21] אינדפנדנס.
[01:22:22 - 01:22:25] בואו נזכר רגע מה זה. אם יש לנו משתנים מקריים, x1 עד xn,
[01:22:26 - 01:22:28] למשל זה כל הפיקסלים והתמונה,
[01:22:29 - 01:22:32] אז אם אנחנו מניחים שהם בלתי תלויים,
[01:22:33 - 01:22:39] עכשיו כמה אפשרויות שונות יש לנו וכמה פרמטרים שונים אנחנו צריכים כדי לתפוס אותם.
[01:22:41 - 01:22:42] אז כמה אפשרויות שונות יש לנו?
[01:22:43 - 01:22:46] נגיד שזה, שוב פעם, תמונות באמניסט,
[01:22:47 - 01:22:47] יש לנו 700,
[01:22:49 - 01:22:51] אני אף פעם שוכח כמה זה, 784 אמרנו?
[01:22:53 - 01:23:01] 784 פיקסלים בינאריים, אז כמה אפשרויות יש לנו אם אנחנו מניחים שזה בלתי תלוי?
[01:23:01 - 01:23:03] כמה פרמטרים אנחנו צריכים?
[01:23:05 - 01:23:05] אתה רוצה להגיד?
[01:23:06 - 01:23:14] זה לא מה שהיה קודם, 2 בחזקת
[01:23:15 - 01:23:16] 784,
[01:23:16 - 01:23:17] אני סליחה.
[01:23:18 - 01:23:21] כן, אז מספר ה-states זה אותו דבר,
[01:23:23 - 01:23:25] זה 2 בחזקת 784,
[01:23:26 - 01:23:28] זה לא משתנה, ועדיין הכל אפשרי,
[01:23:29 - 01:23:31] אבל מה מספר הפרמטרים שאנחנו צריכים?
[01:23:32 - 01:23:40] עכשיו, בעצם, מה זה אומר שיש לנו חוסר קלוץ? זה בעצם שאנחנו יכולים למדל כל אחד מהם בנפרד ולהכפיל את ההסתברות.
[01:23:41 - 01:23:43] נכון? אז מה זה אומר למדל כל אחד מהם בנפרד?
[01:23:45 - 01:23:45] סליחה,
[01:23:46 - 01:23:47] 984 רק,
[01:23:47 - 01:23:48] כל אחד מהם.
[01:23:49 - 01:23:51] בדיוק, אז כל אחד מהם אנחנו רק צריכים פרמטר אחד,
[01:23:52 - 01:23:54] נגיד מה ההסתברות שהוא יהיה דולק או כבוי,
[01:23:56 - 01:24:01] ואז אנחנו צריכים, בהינתן איזושהי תמונה, אנחנו יכולים להכפיל פשוט את כל ההסתברויות הכבויות,
[01:24:01 - 01:24:03] את כל ההסתברויות הדולקות,
[01:24:05 - 01:24:08] ולקבל את ההסתברות, סך הכול, של לקבל את התמונה הזאת.
[01:24:09 - 01:24:10] אוקיי? זה הרבה פחות.
[01:24:10 - 01:24:11] צריכים n פחות אחד.
[01:24:12 - 01:24:12] סליחה, n.
[01:24:13 - 01:24:18] n פרמטרים, אבל ההנחה הזאת היא כנראה חזקה מדי,
[01:24:18 - 01:24:22] נכון? בעצם איך ייראה, אם אנחנו נדגום מתוך ההתפלגות הזאת, אז התמונות
[01:24:23 - 01:24:25] של למדנו על Mnist ייראו ככה,
[01:24:26 - 01:24:30] אוקיי? כי בעצם אין שום קשר בין פיקסל אחד לשני,
[01:24:31 - 01:24:34] אז הוא לא יכול להתראות מראש שהוא הולך וצייר פה איזה אחד,
[01:24:35 - 01:24:39] ואז לדאוג שכל הפיקסלים יהיו קוהרנטיים לאחד הזה, כי כל פיקסל
[01:24:39 - 01:24:41] יידגם בצורה בלתי תלויה, זאת ההנחה,
[01:24:42 - 01:24:43] שאין קשר בין הפיקסלים.
[01:24:44 - 01:24:45] בברור זאת אמרה שהיא לא טובה.
[01:24:47 - 01:24:48] אוקיי, ועדיין אנחנו רוצים איכשהו להשתמש
[01:24:51 - 01:24:56] בשיטה הזאת כדי לצמצם את מספר הפרמטרים שלנו, והשיטה שנשתמש בה זה נקרא conditional dependents.
[01:24:56 - 01:25:00] ואז יש כאן פירוט של
[01:25:01 - 01:25:05] מה זה אומר, החל מאירועים, מאורעות, מה שהגדרנו בהסתברות,
[01:25:06 - 01:25:09] עד לממש איך אנחנו מסיימנים את זה
[01:25:11 - 01:25:14] במשתנים מקרים, אוקיי? אז אם יש לנו שני מאורעות, A ו-B,
[01:25:15 - 01:25:20] אנחנו רוצים להגיד שהם בלתי תלויים, אבל בהינתן C. מה זה אומר? זה אומר הדבר הזה. אז קודם כל מסיימנים את זה ככה,
[01:25:22 - 01:25:25] וזה אומר שההתפלגות של A בהינתן C,
[01:25:25 - 01:25:28] ההתפלגות של שניהם בהינתן C,
[01:25:28 - 01:25:36] שווה להתפלגות של A בהינתן C כפול ההתפלגות של B בהינתן C. אוקיי? אז הדבר הזה לא חייב להיות נכון אם אני לא מתנה כאן ב-C,
[01:25:37 - 01:25:38] אבל אם אני כן מתנה ב-C,
[01:25:39 - 01:25:39] אז זה כן נכון.
[01:25:41 - 01:25:43] אוקיי? אז למשל, איך אנחנו יכולים להשתמש בזה בדוגמה שהיה לנו קודם?
[01:25:46 - 01:25:52] בדוגמה של Mnist, אז אנחנו יכולים להגיד, אוקיי, בהינתן שאני יודע כבר שזה 1, נגיד ש-C זה אומר איזה ספרה זה,
[01:25:53 - 01:25:54] אם אני יודע כבר שזה 1,
[01:25:54 - 01:25:58] אז אולי זה יותר סביר שאני אוכל להגיד שהפיקסלים הם בלתי תלויים,
[01:25:59 - 01:26:03] כי אז אני יודע שיש את הקו הזה באמצע,
[01:26:03 - 01:26:06] שהוא יש לו הסתברות מאוד מאוד גבוהה להיות דולק,
[01:26:08 - 01:26:11] כל השאר יש לנו הסתברות מאוד מאוד נמוכה להיות כבוי,
[01:26:12 - 01:26:17] ואולי יש כמה כאלה על הקצה של האחד שיש להם הסתברות יותר קרובה לחצי.
[01:26:18 - 01:26:21] אז מה שיקרה, אני אקבל כל מיני אחדים,
[01:26:21 - 01:26:22] קצת שונים מכולם,
[01:26:22 - 01:26:26] אולי מדי פעם יהיה איזשהו פיקסל חסר כזה באמצע של האחד, אבל זה לא כזה נורא,
[01:26:28 - 01:26:33] ואולי מדי פעם ידלק איזה משהו קצת מוזר בצד של האחד,
[01:26:34 - 01:26:38] אבל סך הכל זה אולי יתפוס את הדאטה, יראה הרבה יותר סביר ממה שהיה לנו קודם.
[01:26:39 - 01:26:45] אוקיי, אז פה כל מה שכתוב כאן זה אותו דבר לגבי משתנים מקריים, איך מקדימים את זה למשתנים מקריים, לא חשוב כרגע.
[01:26:46 - 01:26:49] חשוב הסימונים, אוקיי, אז לפעמים מסמנים את זה ככה,
[01:26:50 - 01:26:55] פי של x y בהינתן z שווה ל-px בהינתן z כפול p y בהינתן z,
[01:26:56 - 01:27:00] אוקיי, זה אומר שעבור כל הערכים של x ו-y מגדירים מהוראות ככה שזה קורה,
[01:27:01 - 01:27:03] מי שלא זוכר את זה שהסתכל על השופט הזה יותר,
[01:27:03 - 01:27:05] והם מסמנים את זה גם ככה,
[01:27:06 - 01:27:09] x ניצב כאילו ל-y בהינתן z.
[01:27:11 - 01:27:18] וגם עוד דרך להגדיר את זה, זה ש-x בהינתן y ו-z שווה ל-x רק בהינתן z.
[01:27:20 - 01:27:23] אוקיי, כי אתם זוכרים את זה מכלל השרשרת,
[01:27:23 - 01:27:26] אוקיי, מי שלא זוכר שייזכר למה הדברים האלה הם אותו דבר.
[01:27:28 - 01:27:34] אוקיי, שני חוקים חשובים שאנחנו נשתמש בהם הרבה בקורס, אחד, זה כבר אמרתי אותו בדיוק הרגע, זה כלל השרשרת,
[01:27:35 - 01:27:38] השני זה חוק בייס, אוקיי? כלל השרשרת אומר שאם יש לנו הרבה משתנים מקריים,
[01:27:39 - 01:27:42] או שוב, פה זה כתוב בשפה של מאורעות,
[01:27:44 - 01:27:47] אז אנחנו יכולים לכתוב את ההסתברות של כולם,
[01:27:48 - 01:27:50] אז תחשבו על זה בתור ההסתברות של כל הפיקסלים,
[01:27:51 - 01:27:52] אוקיי, של התמונה.
[01:27:53 - 01:27:56] אנחנו תמיד יכולים לפרק אותה באיזושהי דרך כזאת,
[01:27:56 - 01:28:01] ההסתברות של פיקסל אחד כפול ההסתברות של פיקסל השני, בהינתן פיקסל ראשון,
[01:28:02 - 01:28:06] ככה הלאה, עד שהפיקסל האחרון הוא בהינתן כל הפיקסלים שהיו לפניו.
[01:28:07 - 01:28:09] אוקיי, זה דרך לפרק את ההתפלגות.
[01:28:11 - 01:28:16] איך שאנחנו כותבים את ההסתברות הזאת, אנחנו יכולים לכתוב אותה בתור מכפלה של הסתברויות.
[01:28:17 - 01:28:19] חוק בייס, אתם זוכרים,
[01:28:19 - 01:28:21] אנחנו נדבר עליו הרבה בשבוע הבא,
[01:28:21 - 01:28:22] זאת אומרת,
[01:28:23 - 01:28:26] לא אכנס לעולם עכשיו, זה בעצם הדרך שלנו להפוך את ההסתברויות,
[01:28:27 - 01:28:29] הסתברות של s1 בהינתן s2,
[01:28:30 - 01:28:32] שווה להסתברות של s2 בהינתן s1,
[01:28:32 - 01:28:35] קפוא להסתברות של s1 וכי ההסתברות של s2,
[01:28:36 - 01:28:39] ואם נתון לנו הדברים בכיוון הזה,
[01:28:39 - 01:28:42] אנחנו יכולים להשתמש בהם כדי לקבל את הכיוון ההפוך.
[01:28:44 - 01:28:46] אבל היום אני רוצה לדבר קצת על ה-Chain rule,
[01:28:46 - 01:28:47] על המשמעות שלו,
[01:28:48 - 01:28:53] בהקשר של מה שאמרנו עכשיו, שאנחנו רוצים להוריד את מספר הפרמטרים שלנו.
[01:28:55 - 01:29:01] אז איך אנחנו יכולים להשתמש בזה כדי להוריד את מספר הפרמטרים? נגיד שבאמת ככה אנחנו ממדלים את הפיקסלים שלנו,
[01:29:01 - 01:29:01] אוקיי?
[01:29:02 - 01:29:03] בתור המכפלה הזאתי.
[01:29:05 - 01:29:08] עולם ההסתברות של הפיקסל הראשון, נגיד שזה פיקסלים בינאריים,
[01:29:09 - 01:29:09] כן?
[01:29:09 - 01:29:11] אותו דבר, אותה תמונה ב-M-List.
[01:29:12 - 01:29:14] אז כמה פרמטרים אנחנו צריכים עכשיו
[01:29:15 - 01:29:17] כדי לתפוס את ההסתברות הזאת ככה?
[01:29:18 - 01:29:21] בשביל הפיקסל הראשון הזה נגיד, כמה פרמטרים אנחנו צריכים?
[01:29:28 - 01:29:28] אחד,
[01:29:29 - 01:29:30] נכון? זו הסתברות שהוא דולק או לא?
[01:29:31 - 01:29:33] כמה פרמטרים אנחנו צריכים כדי לתפוס את הדבר הזה?
[01:29:40 - 01:29:43] אם x1 נתון כבר, אז 1, אתה צודק,
[01:29:43 - 01:29:46] אבל באופן כללי x1 יכול לקבל שני ערכים.
[01:29:48 - 01:29:50] אנחנו צריכים בעצם את x2 בהינתן שx1 היה 0,
[01:29:51 - 01:29:55] והתפלגות אחרת שזה x2 בהינתן שx1 היה 1.
[01:29:56 - 01:30:00] זה קצת מטעה שזה כתוב גם ככה, אז בעצם יש פה שתי התפלגויות שונות.
[01:30:01 - 01:30:02] אז בעצם צריך פה שני פרמטרים.
[01:30:05 - 01:30:10] אתם רואים לאן זה הולך פה, ההסתברות של x3 בהינתן 1 ו-2 צריך 4 פרמטרים,
[01:30:10 - 01:30:14] כי עבור כל קומבינציה של 1 ו-2 זה יגדיר לנו התפלגות חדשה על x3,
[01:30:15 - 01:30:17] שזה התפלגות ברנולית בפרמטר 1,
[01:30:18 - 01:30:18] אבל יש 4 כאלה.
[01:30:19 - 01:30:23] וככה הלאה. אז בעצם אנחנו שוב פעם מגיעים בדיוק לאותו מצב שהיינו קודם.
[01:30:25 - 01:30:27] וזה הגיוני, כי לא שינינו את ההסתברות.
[01:30:27 - 01:30:29] את ההתפלגות שלנו, תמיד
[01:30:30 - 01:30:34] חוק השרשרת אומר שתמיד אנחנו יכולים לכתוב את ההתפלגות הזאת בתור מכפלה כזאתי,
[01:30:35 - 01:30:38] אז אכן לא הגיוני שאם אנחנו עושים פרמטריזציה של כל המצבים כאן,
[01:30:38 - 01:30:40] זה יהיה שונה מפרמטריזציה של כל המצבים כאן,
[01:30:41 - 01:30:42] אז לא הרווחנו כלום.
[01:30:44 - 01:30:45] מה כן אפשר לעשות?
[01:30:46 - 01:30:50] איך כן אפשר להשתמש בכלל השרשרת כדי להוריד את ההתפלגות?
[01:30:51 - 01:30:52] זה מה שנקרא structure,
[01:30:52 - 01:30:54] אנחנו מכניסים בעצם איזשהו מבנה.
[01:30:55 - 01:30:57] אנחנו כותבים את כלל השרשרת,
[01:30:58 - 01:31:02] אבל בחלק מהמקומות אנחנו אומרים שאנחנו יכולים להתעלם מחלק מהפיקסל.
[01:31:04 - 01:31:07] ומה המשמעות שאנחנו מוחקים כאן, נגיד,
[01:31:08 - 01:31:08] את x1.
[01:31:13 - 01:31:14] שאין זיכרון למערכת?
[01:31:15 - 01:31:17] אפשר לחשוב על זה בתור אין זיכרון, נכון?
[01:31:18 - 01:31:20] תכונה מרכוביות,
[01:31:21 - 01:31:23] אם אנחנו חושבים על זה בתור איזשהו זמן, כן?
[01:31:23 - 01:31:24] אבל יש שני משפחות.
[01:31:25 - 01:31:26] מה שכתוב בכותרת.
[01:31:32 - 01:31:37] זה בדיוק התכונה הזאת, אנחנו מניחים שx3 בלתי תלוי מx1,
[01:31:37 - 01:31:38] ויינתן x2.
[01:31:40 - 01:31:44] שאם אני רואה את הפיקסל x2 אז אני מניח שx1 וx3 הם בלתי תלויים.
[01:31:48 - 01:31:57] אז בעצם אנחנו יכולים להכניס כאן את האי-תלות המותנית המותנית לכל מיני דרכים שונות.
[01:31:58 - 01:31:59] איך אפשר לבנות דרכים שונות?
[01:31:59 - 01:32:03] כלל השרשרת הזאת אפשר לכתוב לפי איזה סדר שאנחנו רוצים,
[01:32:04 - 01:32:06] ולהחליט כל פעם מה אנחנו מוחקים כאן ומה לא.
[01:32:07 - 01:32:08] זה בעצם אומר איזה הנחות
[01:32:09 - 01:32:12] של התלות אנחנו עושים על המערכת שלנו, על האדאטה שלנו.
[01:32:15 - 01:32:16] כמה פרמטרים יהיו לנו כאן?
[01:32:19 - 01:32:23] אז בדוגמה הזאת הספציפית הצלחנו להוריד זה לשני n פחות אחד,
[01:32:24 - 01:32:30] כי בעצם כאן כל פיקסל הוא רק תלוי בפיקסל הקודם שהיה לו.
[01:32:31 - 01:32:32] הוא לא תלוי בכל השאר,
[01:32:33 - 01:32:34] אז הפיקסל הראשון יש בו פרמטר אחד,
[01:32:35 - 01:32:37] השני יהיה לו באמת שניים, כמו שראינו,
[01:32:38 - 01:32:40] השלישי, אבל זה לא יגדל ל-4, אז יישאר שניים.
[01:32:40 - 01:32:46] ככה והלאה כל המכפלות, כל הגורמים האחרים במכפלה,
[01:32:46 - 01:32:49] יהיו להם רק שני פרמטרים תמיד, אז אנחנו נקבל שני n,
[01:32:49 - 01:32:53] פחות אחד בגלל הראשון שיש לו רק ככה.
[01:32:55 - 01:33:00] אז זו דרך שהיא בעצם קצת יותר חלשה מלהגדיר שהכול בלתי תלוי באופן כללי,
[01:33:00 - 01:33:03] אבל היא עדיין יכולה להוריד.
[01:33:03 - 01:33:05] ויש לנו כאן איזשהו trade-off שאנחנו יכולים לשחק איתו.
[01:33:07 - 01:33:15] אוקיי, יש תחום שלם שמתעסק בדיוק בעניין הזה, של לבנות את המודלים עם החוסר התלויות האלה, עם האי תלויות במקום הנכון,
[01:33:18 - 01:33:20] והמודלים האלה נקראים בייזן נטוורטס,
[01:33:21 - 01:33:23] ובאופן כללי אפשר,
[01:33:23 - 01:33:30] אוקיי, מה שאתה אומר באופן כללי זה שאנחנו יכולים לכתוב כל מכפלה רב-ממדית כזאת בתור מכפלה של הסתברויות,
[01:33:31 - 01:33:32] סליחה, אמרתי את זה לא נכון,
[01:33:32 - 01:33:35] כל הסתברות רב-ממדית אפשר לכתוב בתור מכפלה של הסתברויות,
[01:33:36 - 01:33:41] ככה שלכל איבר בהסתברות אנחנו אומרים מי ה...
[01:33:41 - 01:33:43] על מי אנחנו כן רוצים להתנות,
[01:33:43 - 01:33:48] מי הם הסט של הפיקסלים או הערכים בווקטור הרב-ממדי שאנחנו רוצים שיהיו
[01:33:53 - 01:33:54] הערכים שעליהם אנחנו מתנים.
[01:33:56 - 01:33:59] אוקיי, אבל סך הכל הדבר הזה צריך להיות מתאים לאיזשהו פירוק
[01:34:00 - 01:34:01] לפי chain rule,
[01:34:01 - 01:34:07] והדרך להבטיח שזה יקרה זה אם כשאנחנו כותבים את זה בתור גרף, שכל אחד יש לנו חץ
[01:34:08 - 01:34:11] שנכנס למודל מכל האלה שתלויים בו,
[01:34:13 - 01:34:13] אוקיי?
[01:34:14 - 01:34:21] הפוך, זאת אומרת אם ה-node הזה הוא תלוי בשני המודל הזה, הוא מותנה על שני המודלים האלה, על שני ה-nodes האלה,
[01:34:22 - 01:34:23] אז יש לנו עוד שני החצים האלה,
[01:34:23 - 01:34:26] אבל ה-node הזה הוא רק מותנה בנוד הזה,
[01:34:27 - 01:34:30] אוקיי? אז כל עוד הגרף הזה שאנחנו בונים, אם הוא דג,
[01:34:31 - 01:34:37] אינטרקטד א-סיקלי גרף, גרף בלי מעגלים ומכוון,
[01:34:38 - 01:34:41] אז אנחנו, זה מבטיח לנו שזה פירוק חוקי,
[01:34:42 - 01:34:43] וזה בעצם מגדיר התפלגות.
[01:34:44 - 01:34:48] כל התחום הזה שמקשר התפלגויות וההנחות שאנחנו עושים על חוסר תלות
[01:34:49 - 01:34:51] עם גרפים נקרא פרוביליסטיק גרפיקל מודלס,
[01:34:52 - 01:34:55] אפשר היה לעשות קורס בתחום הזה, זה לא בדיוק הקורס שלנו,
[01:34:55 - 01:35:00] אנחנו אבל כן קצת נתעסק בעקרונות שנובעים מזה.
[01:35:00 - 01:35:04] אוקיי? אנחנו בדרך כלל לא כל כך נתעכב על לבנות מבנה כזה מאוד מסובך
[01:35:05 - 01:35:08] ולהבין את המשמעות שלו מבחינת הגרף והזמן חישוב,
[01:35:09 - 01:35:15] שזה התחום הזה של פרוביליסטיק גרפיקל מודלס, אבל העקרונות האלה שאנחנו עושים,
[01:35:15 - 01:35:23] מניחים שיש לנו איזשהו משתנה שמשפיע על משתנה אחר, ושיש אי תלויות בתוך המערכת,
[01:35:24 - 01:35:25] אנחנו כן נשתמש בזה הרבה.
[01:35:26 - 01:35:31] אוקיי, יש פה דוגמא קטן.
[01:35:31 - 01:35:35] כן, כן. לא יכול להיות במצב הקודם שנרצה ששני משתנים יהיו תלויים אחד בשני?
[01:35:38 - 01:35:39] במעגל?
[01:35:39 - 01:35:42] לא, אם הם תלויים אחד בשני במעגל אז יש איזה גרף עם מעגל.
[01:35:43 - 01:35:45] אה, אז יש גם מקרה לזה כאילו?
[01:35:46 - 01:35:52] יש סוגים אחרים של גרפים שגם אפשר להשתמש בהם כדי להגדיר התפלגויות,
[01:35:52 - 01:35:59] שזה נקרא Mark of Networks, שזה גרפים לא מכוונים ושיכולים להיות בהם מעגלים,
[01:36:00 - 01:36:02] אבל זה קצת יותר מורכב, לא ניכנס לזה.
[01:36:03 - 01:36:09] אבל אם אנחנו רוצים להגדיר את המודלים בצורה הזאת, מה שנקרא Bayesian Networks, שכל X הוא פשוט תלוי, זה פשוט כלל שרשרת
[01:36:10 - 01:36:12] שאתה מחליט את מי אתה מוחק כאן, אוקיי?
[01:36:12 - 01:36:15] אז אתה יכול לחשוב על זה ככה, יש לך גרף מלא,
[01:36:15 - 01:36:21] וכל משהו שאתה מוחק כאן זה קשת שאתה מוחק בין X1 ל-X3,
[01:36:21 - 01:36:21] את הקשת הזאת מחקתי.
[01:36:22 - 01:36:23] כאן מחקתי את הקשת בין X,
[01:36:24 - 01:36:27] כל ה-Xים חוץ מ-XN-1 ל-XN.
[01:36:28 - 01:36:31] סך הכל זה תמיד יוצא גרף בלי מעגלים.
[01:36:34 - 01:36:35] אוקיי, תודה.
[01:36:35 - 01:36:36] ואפשר לעשות את זה הפוך,
[01:36:36 - 01:36:43] אפשר לבנות גרף ללא מעגלים ולהראות שהוא שקול להתפלגות חוקית.
[01:36:46 - 01:36:50] אוקיי, יש כאן איזה דוגמה, אין לנו כל כך זמן להיכנס אליה.
[01:36:51 - 01:36:55] אולי זה דוגמה פשוט שמראה, יש לנו כאן 1, 2, 3, 4, 5 איברים.
[01:36:56 - 01:37:00] זה דאטה ממימד 5. לכל אחד מה...
[01:37:01 - 01:37:04] יש כאן חלק מהערכים מקבלים שני ערכים אפשריים,
[01:37:04 - 01:37:06] וחלק מהמשתנים מקבלים שלושה.
[01:37:07 - 01:37:11] וסך הכל במקום שיהיה לנו משהו בסגנון 2 בחזקת 5 ערכים,
[01:37:11 - 01:37:13] יש לנו הרבה פחות פרמטרים,
[01:37:13 - 01:37:15] מה שאתם רואים כאן זה הפרמטרים.
[01:37:16 - 01:37:21] זה למשל עבור כל קומבינציה של דיפיקולטי
[01:37:21 - 01:37:24] ואינטליג'נס מה ההסתברות לגרייד 1, גרייד 2 או גרייד 3.
[01:37:26 - 01:37:27] כל שורה כאן זה
[01:37:29 - 01:37:34] האפשרויות של ההורים של גרייד.
[01:37:35 - 01:37:36] דרך אגב, משהו שחשוב להבין,
[01:37:37 - 01:37:38] יש לי לפעמים מאוד מבלבל,
[01:37:38 - 01:37:42] למשל האם letter הוא בלתי תלוי בדיפיקולטי?
[01:37:45 - 01:37:50] זה לא נכון, אם אני אגיד לכם את הדיפיקולטי כנראה זה ישפיע על מה שאתם יודעים על letter, זה לא בלתי תלוי.
[01:37:51 - 01:37:52] אבל זה כן בלתי תלוי בהינתן גרייד.
[01:37:53 - 01:37:54] זאת אומרת אם אמרתי לכם את הגרייד,
[01:37:55 - 01:38:01] אז יש לכם את כל המידע שאתם צריכים בשביל לדעת את ההתפלגות על letter ואתם לא צריכים,
[01:38:02 - 01:38:05] אם אני אוסיף לכם את המידע של דיפיקולטי זה לא יעזור לכם.
[01:38:06 - 01:38:07] אתם יודעים כבר את גרייד.
[01:38:08 - 01:38:12] אז העיקרון הזה של חוסר תלות מותנית הוא חשוב,
[01:38:13 - 01:38:16] ההבדל בינו לבין אינטליג'נס באופן כללי.
[01:38:19 - 01:38:19] חשוב להבין את זה.
[01:38:21 - 01:38:24] אבל שוב, אנחנו לא ניכנס לכל הדקויות שיש בתחום הזה,
[01:38:24 - 01:38:32] כי כמו שאמרתי זה יכול להיות קורס שלם בתחום הזה, אבל כן ניגע מדי פעם בדברים שקשורים.
[01:38:34 - 01:38:40] אוקיי, עכשיו אנחנו נעבור לצד קצת יותר טכני של קורס של השיעור,
[01:38:40 - 01:38:45] סליחה, עם כמה דברים שאתם תצטרכו גם בשביל התרגיל שיש לכם.
[01:38:47 - 01:38:48] אוקיי, אז דבר ראשון,
[01:38:48 - 01:38:50] לנוסחה שהיא מאוד שימושית שדיברנו עליה,
[01:38:51 - 01:38:54] יש הצטברות, אז דיברנו עליה כבר שם.
[01:38:56 - 01:39:00] נקראת הנוסחת Change of Variable Formula,
[01:39:02 - 01:39:05] ובעצם היא מדברת על המקרה הבאה.
[01:39:05 - 01:39:08] עכשיו, רוב הקורס אנחנו נדבר על משתנים מקריים רציפים.
[01:39:10 - 01:39:12] הדוגמאות עכשיו שעשיתי בשביל להבין,
[01:39:12 - 01:39:16] לתת לכם קצת את העקרונות היו על דאטה שהוא היה בדיל,
[01:39:16 - 01:39:18] אבל רוב הקורס, כמו שאמרתי,
[01:39:18 - 01:39:19] נגיע לתמונות,
[01:39:20 - 01:39:22] אנחנו נניח שהדאטה שם הוא רציף,
[01:39:23 - 01:39:24] למרות שהרבה פעמים,
[01:39:25 - 01:39:28] אוקיי, אנחנו צריכים לדבר על זה קצת יותר לעומק, אבל על תמונות אפשר,
[01:39:30 - 01:39:34] לפעמים מתייחסים לזה בתור דאטה בדיד ולפעמים בתור דאטה רציף,
[01:39:35 - 01:39:35] כי זה נכון
[01:39:38 - 01:39:40] שאם יש לנו, הדוגמאות שלנו עכשיו בהמניסט זה היה רק בינארי,
[01:39:41 - 01:39:41] זה לא מעניין,
[01:39:42 - 01:39:45] אבל אם אנחנו חושבים על דאטה שהוא תמונה צבעונית,
[01:39:46 - 01:39:52] אז כל פיקסול יכול לקבל בעצם סט דיסקרטי של ערכים, ככה זה ממש במחשב, בין 0 ל-255,
[01:39:54 - 01:39:56] אפשר לחשוב על זה בתור דאטה דיסקרטי,
[01:39:57 - 01:40:00] אבל הרבה פעמים יותר טוב, יותר מאפשר לחשוב על זה בתור דאטה רציף,
[01:40:00 - 01:40:02] שנקבל איזשהו ערך רציף,
[01:40:02 - 01:40:04] ואחר כך אנחנו שומרים אותו פשוט,
[01:40:04 - 01:40:07] עושים לו איזושהי קוונטיזציה כדי לשמור אותו במחשב.
[01:40:08 - 01:40:13] אז הרבה פעמים אנחנו נעדיף לחשוב על תמונות שלנו בתור דאטה רציף,
[01:40:14 - 01:40:16] אבל לא בכל המודלים, אנחנו נראה גם וגם.
[01:40:18 - 01:40:19] אוקיי, אז אם x הוא רציף,
[01:40:19 - 01:40:21] אז זו נוסחה שהיא שימושית,
[01:40:22 - 01:40:26] x הוא וקטור רציף,
[01:40:27 - 01:40:28] אנחנו יודעים את ה-PDF שלו,
[01:40:29 - 01:40:33] ה-PDF שלו, אתם זוכרים על זה, זה צפיפות של ההסתברות,
[01:40:37 - 01:40:39] ה-PDF שלו נתון זה PX,
[01:40:40 - 01:40:41] כי הסאבסטריט הזה זה X,
[01:40:42 - 01:40:45] ועכשיו אנחנו מגדירים משתנה מקרי חדש, y,
[01:40:46 - 01:40:48] שהוא פשוט איזושהי פונקציה של x.
[01:40:49 - 01:40:50] נגיד לקחנו את x,
[01:40:51 - 01:40:52] נפלנו בריבוע,
[01:40:52 - 01:40:53] כלומר לקחנו את x,
[01:40:53 - 01:40:54] הוספנו לו 5,
[01:40:54 - 01:40:58] והכפלנו בין 2. עשינו איזושהי פונקציה ל-x,
[01:40:58 - 01:40:59] זה מגדיר לנו את y,
[01:41:00 - 01:41:01] עכשיו השאלה, מה ה-PDF של y?
[01:41:03 - 01:41:04] אוקיי, אז
[01:41:04 - 01:41:10] אם המשפט הזה אומר שאם ה-f הוא פונקציה מונוטונית והפיכה,
[01:41:11 - 01:41:15] אז ככה אנחנו יכולים להגדיר את ה-PDF של y מתוך ה-PDF של x.
[01:41:16 - 01:41:19] ומה כתוב כאן? אז החלק השני זה החלק הקל,
[01:41:20 - 01:41:22] זה ההסתברות ש-y מקבל ערך,
[01:41:23 - 01:41:25] או שהצפיפות שלו מקבל ערך,
[01:41:26 - 01:41:28] סליחה,
[01:41:28 - 01:41:29] הצפיפות של ההסתברות
[01:41:30 - 01:41:32] ש-y יהיה בערך מסוים,
[01:41:32 - 01:41:36] שווה לצפיפות של ההסתברות ש-x יהיה בערך אחר,
[01:41:37 - 01:41:39] ה-x שגרם ל-f למכות אותו ל-y,
[01:41:40 - 01:41:40] אוקיי? אם f היא הפיכה,
[01:41:41 - 01:41:42] אז מה המשמעות של f מינוס 1?
[01:41:42 - 01:41:46] זאת אומרת, אני הולך בחזרה מה-y שקיבלתי ל-x שגרם לו.
[01:41:47 - 01:41:49] אוקיי, אז מה ההסתברות שקיבלתי?
[01:41:52 - 01:41:55] בוא נגיד שאני מטיל קובייה,
[01:41:56 - 01:41:58] אני מעלה בריבוע את התוצאה.
[01:41:59 - 01:42:00] אז מה ההסתברות שקיבלתי?
[01:42:01 - 01:42:04] 16?
[01:42:06 - 01:42:08] זה אותה הסתברות שה-x,
[01:42:08 - 01:42:11] ההטלת קובייה, נתנה לי 4.
[01:42:12 - 01:42:13] זאת ההסתברות.
[01:42:13 - 01:42:14] זה מה שכתוב כאן.
[01:42:14 - 01:42:16] זו המשמעות של f מינוס 1y.
[01:42:17 - 01:42:19] אבל במשתנים רציפים יש לנו את האיבר הזה,
[01:42:20 - 01:42:26] שהוא מה שכתוב כאן במגזרת של המיפוי החזרה הזה, f מינוס 1,
[01:42:27 - 01:42:27] לפי y.
[01:42:29 - 01:42:29] בערך מוקלט.
[01:42:31 - 01:42:32] למה?
[01:42:33 - 01:42:36] בדרך כלל תחשבו על זה, יש לנו איזושהי התפלגות.
[01:42:38 - 01:42:46] יש לנו התפלגות, נגיד שהפונקציה שלנו היא להכפיל ב-2, אז אנחנו נותחים את ההתפלגות הזאת, נכון?
[01:42:47 - 01:42:51] במקום שהערכים האפשריים יהיו בין 0 ל-1, הם עכשיו יהיו בין 0 ל-2.
[01:42:52 - 01:42:55] אז מתחנו את הצפיפות של ההתפלגות,
[01:42:56 - 01:42:59] אבל אנחנו צריכים שהשטח מתחת לזה עדיין יהיה 1,
[01:43:00 - 01:43:01] אוקיי? אנחנו צריכים,
[01:43:02 - 01:43:03] הכול צריך קצת לקטום,
[01:43:04 - 01:43:07] אי אפשר רק למתוח את הכול,
[01:43:07 - 01:43:09] זה כבר לא יהיה הסתברות פונקציה,
[01:43:09 - 01:43:10] זה לא יהיה הסתברות,
[01:43:11 - 01:43:13] זה היה פונקציית התפלגות תקינה.
[01:43:15 - 01:43:20] אז זה האיבר שהוא מתקן את זה, תחשבו על זה ככה, אוקיי?
[01:43:20 - 01:43:26] ובכל מקום הוא אומר עד כמה מהר ה-y השתנה קודם לעומת כמה x משתנה.
[01:43:28 - 01:43:33] הפוך צריך להגיד, עד כמה מהר ה-x השתנה קודם לעומת כמה ה-y משתנה עכשיו.
[01:43:35 - 01:43:35] בסדר?
[01:43:35 - 01:43:37] אז בואו נוכיח שזה המצב,
[01:43:38 - 01:43:42] נוכיח את זה במקרה ש-f הוא פונקציה מומטונית עולה.
[01:43:44 - 01:43:49] אוקיי, אז תזכורת ל-CDF, אתם זוכרים מה זה CDF ו-PDF?
[01:43:53 - 01:43:53] אתם זוכרים מה זה תזכורת?
[01:43:55 - 01:44:00] הסכום שלהם עד כה? כן, CDF זה פונקציית ה-cימולטיב.
[01:44:00 - 01:44:01] קימולטיב.
[01:44:02 - 01:44:07] זה לא יעבוד פה, היה יכול לעשות כמה דברים חודשניים,
[01:44:08 - 01:44:08] זה יעבוד.
[01:44:09 - 01:44:12] אז אם ה-PDF
[01:44:13 - 01:44:14] והצפיפות נגיד
[01:44:14 - 01:44:16] נראית משהו כזה,
[01:44:18 - 01:44:20] משהו אחיד כזה, אז זה ה-PDF.
[01:44:22 - 01:44:23] זה איך יראה ה-CDF של זה?
[01:44:25 - 01:44:26] CDF זה פונקציה עולה.
[01:44:27 - 01:44:27] כן, בדיוק.
[01:44:28 - 01:44:30] CDF זה פשוט פונקציה עולה יש שם, נכון?
[01:44:30 - 01:44:31] נגיעה ל...
[01:44:35 - 01:44:36] מקסימום שלה זה אחד.
[01:44:38 - 01:44:38] אוקיי?
[01:44:39 - 01:44:41] זה אומר כמה שטח
[01:44:42 - 01:44:46] יש לי עד לנקודה מסוימת כל פעם.
[01:44:47 - 01:44:47] מה ההסתברות?
[01:44:48 - 01:44:51] ש-Y יקבל פחות,
[01:44:51 - 01:44:54] יהיה פחות מאיזשהו, זה כל השטח שיש פה.
[01:44:55 - 01:44:55] אוקיי? אז
[01:44:56 - 01:45:01] ה-CDF מוגדר בתור האינטגרל
[01:45:02 - 01:45:04] של ה-PDF עד לערך מסוים.
[01:45:07 - 01:45:13] כן? אז הגרף הזה זה השטח שיש, אוקיי. אז זה CDF.
[01:45:14 - 01:45:20] עכשיו, ב-CDF אפשר לעשות דברים כי CDF הם באמת מדברים על הסתברויות ועל מאורעות של דברים שאנחנו יכולים להבין. ב-PDF זה קצת קשה כי זה
[01:45:21 - 01:45:21] צפיפות של הסתברות.
[01:45:23 - 01:45:25] אנחנו נעשה את החישוב ב-PDF,
[01:45:25 - 01:45:27] ואז נגזור את זה חזרה ונקבל את ה...
[01:45:27 - 01:45:28] סליחה, נעשה את החישוב ב-CDF,
[01:45:29 - 01:45:31] נגזור את זה חזרה ונקבל את ה-PDF.
[01:45:32 - 01:45:34] אוקיי, אז מה אנחנו יודעים על ה-CDF?
[01:45:35 - 01:45:38] ההסתברות ש-Y קטן מאיזשהו ערך
[01:45:38 - 01:45:41] שווה להסתברות ש-F של X קטן מאיזשהו ערך,
[01:45:42 - 01:45:42] אותו ערך, נכון?
[01:45:43 - 01:45:46] זה F של X פשוט שווה Y, ככה אנחנו הגדרנו את המשתנה במקרי Y.
[01:45:48 - 01:45:53] זה מה אומר? זה פשוט כל... זה ההסתברות ש-X קיבל את כל הערכים שבהם
[01:45:53 - 01:45:55] F של X קטן מ-Y.
[01:45:57 - 01:45:59] ההסתברות ש-X מקבל את כל הערכים האפשריים של X,
[01:46:00 - 01:46:02] ככה שאם נפעיל את F על X,
[01:46:02 - 01:46:03] נקבל משהו שהוא קטן מ-Y.
[01:46:05 - 01:46:08] אם F היא פונקציה שהיא מונוטונית עולה, אז מה זה כל הערכים האלה?
[01:46:10 - 01:46:12] זה כל הערכים האלה שהם בדיוק
[01:46:12 - 01:46:16] קטנים מ-F מינוס אחד של Y שמופיע כאן.
[01:46:18 - 01:46:23] אז זו בעצם ההסתברות ש-X קטן מלקחת את ה-Y הזה ולמפות אותו חזרה
[01:46:23 - 01:46:24] ל-X.
[01:46:26 - 01:46:27] זה בגלל שזה מונטוניו.
[01:46:29 - 01:46:32] והדבר הזה, זה פשוט ההגדרה של ה-CDF של X.
[01:46:34 - 01:46:36] זו ההסתברות ש-X קטן שווה מאיזשהו ערך,
[01:46:37 - 01:46:39] זה ה-CDF של ה-X הזה.
[01:46:39 - 01:46:40] והערך הזה זה ה-L.
[01:46:41 - 01:46:42] אז זה החלק הראשון
[01:46:44 - 01:46:45] של ההוכחה.
[01:46:46 - 01:46:48] בעצם עכשיו אנחנו נגזור את זה
[01:46:49 - 01:46:51] כדי לחזור בחזרה ל-PDM.
[01:46:51 - 01:46:56] יש כאן את הגזירה, אם אנחנו לוקחים pdf של Y שווה לנגזרת
[01:46:57 - 01:47:00] של ה-CDF של Y.
[01:47:02 - 01:47:08] אנחנו עכשיו רוצים לגזור את ה-CDF של Y, אנחנו מציבים ב-Y את הערך שקיבלנו, שזה f מינוס אחד.
[01:47:11 - 01:47:11] סליחה,
[01:47:12 - 01:47:16] קיבלנו כאן שה-CDF של Y שווה למה שיש כאן, אז אנחנו מציבים את זה פה.
[01:47:17 - 01:47:21] אז הנגזרת לפי Y של ה-CDF של X,
[01:47:22 - 01:47:23] לפי ה-f מינוס אחד ב-Y.
[01:47:23 - 01:47:25] בוא נקרא כאן X, כתוב כאן f מינוס אחד Y.
[01:47:26 - 01:47:28] אז יש לי כאן פונקציה בתוך פונקציה,
[01:47:29 - 01:47:34] אז אני צריך לגזור את זה לפי נגזרת חיצונית כפי נגזרת פנימית.
[01:47:36 - 01:47:39] אז הנגזרת החיצונית היא נופיעה פעם, זה העבר השני.
[01:47:41 - 01:47:41] זה פשוט,
[01:47:44 - 01:47:46] כן, זה כאן מוגדר הנגזרת השנייה.
[01:47:46 - 01:47:52] אם אנחנו חושבים על זה, זה פשוט, אנחנו מקבלים את ה-PDF, כן? הנגזרת החיצונית של ה-CDF, זה פשוט ה-PDF.
[01:47:54 - 01:47:56] הדבר הזה שנוסף לנו זה הנגזרת הפנימית,
[01:47:57 - 01:48:02] אוקיי? זה הנגזרת הפנימית של הפונקציה הפנימית הזאת, שזה f מינוס אחד של y.
[01:48:03 - 01:48:05] זה בדיוק הדבר שנוסף כאן, וזה כבר תלוי ב-f.
[01:48:07 - 01:48:09] אם הפונקציה שלנו הייתה אחידה,
[01:48:10 - 01:48:12] אז מה יהיה המקדם הזה?
[01:48:16 - 01:48:19] זאת אומרת שזה היה כאן משהו קו ליניארי, ה-CDF היה כאן קו ליניארי.
[01:48:22 - 01:48:32] סליחה, התכוונתי לשאול, אם ה-PDF היה נגיד, לא משנה מה היה ה-PDF, אבל ה-F הזאתי שמרחיבה אותו הייתה פשוט מכפילה בשתיים, דוגמה שנתתי קודם.
[01:48:38 - 01:48:40] ה-CDF הזה זה היה f מינוס אחד, זה היה חצי,
[01:48:41 - 01:48:42] והנגזרת של זה,
[01:48:42 - 01:48:43] של חצי x,
[01:48:43 - 01:48:44] זה פשוט חצי,
[01:48:44 - 01:48:46] זה פשוט תמיד יו-פשוט יוריד את הכול בחצי.
[01:48:49 - 01:48:52] אוקיי? אני עובר את זה טיפה מהר, זה דברים שאתם אמורים אה... להקים.
[01:48:54 - 01:48:57] אה... אוקיי, מי שמסתבך עם זה, שיסתכל אחר כך על ה...
[01:48:59 - 01:49:00] על השקף הזה, ו...
[01:49:01 - 01:49:06] אה... גם אפשר, כל הדברים האלה, די קל ללכת למצוא מקורות לזה, אם אתם צריכים.
[01:49:07 - 01:49:13] אה... כמו שאמרתי, אנחנו נניח שאתם יודעים את זה, מי שרואה שהוא קצת שכח כל מיני דברים,
[01:49:13 - 01:49:14] אז אה...
[01:49:14 - 01:49:17] התרגיל שנתתי, וה...
[01:49:18 - 01:49:20] שלושה שבועות שיש לכם עכשיו זה זמן טוב ל...
[01:49:21 - 01:49:21] לחזור על הדברים האלה.
[01:49:24 - 01:49:26] אה... אוקיי, בואו נסתכל אה...
[01:49:27 - 01:49:29] דוגמה, יש לנו זמן דוגמה, אני רוצה
[01:49:29 - 01:49:31] לעבור לחלק הבא.
[01:49:33 - 01:49:37] אוקיי, אז בואו נעבור לדוגמה, אבל יש פה דוגמה של הדבר הזה, אוקיי? אז יש פה
[01:49:37 - 01:49:39] בדיוק פחות או יותר מה שאני אמרתי, יש כאן pdf שהוא
[01:49:40 - 01:49:44] אה... התרגלות אחידה, התרגלות אחידה,
[01:49:45 - 01:49:46] בין 0 ל-Alpha,
[01:49:47 - 01:49:49] ויש כאן איזושהי f,
[01:49:49 - 01:49:51] שהיא אה...
[01:49:51 - 01:49:56] איפה הולכים אותה? הנה, f זה z שווה x בריבוע, זאת אומרת ש-f הוא פשוט לעלות בריבוע.
[01:49:57 - 01:49:59] אוקיי? אז ה-f מינוס 1 זה השורש,
[01:50:00 - 01:50:01] תסתכלו כאן מה יוצא החישוב.
[01:50:06 - 01:50:08] אוקיי, שאלות על זה? לפני שאני עובר ל...
[01:50:09 - 01:50:11] התפלגויות אה...
[01:50:13 - 01:50:14] מולטי-וריאטיות.
[01:50:15 - 01:50:15] אוקיי.
[01:50:25 - 01:50:29] אז איך אנחנו בכלל מסתכלים על התפלגויות שהן על רב-ממדיות?
[01:50:30 - 01:50:33] אוקיי? על הרבה מלבדים, כמו על תמונות, זה מה שאנחנו רוצים.
[01:50:34 - 01:50:37] אז בעצם אנחנו יכולים פשוט להגדיר את ה...
[01:50:38 - 01:50:40] את המשתנה המקרי הזה שלנו,
[01:50:42 - 01:50:43] אה... בתור וקטור פשוט.
[01:50:47 - 01:50:50] אנחנו נקרא ל... עדיין יכולים לקרוא לזה x,
[01:50:50 - 01:50:52] אבל זה יהיה, ה-x הזה פשוט יהיה וקטור מבחינתנו,
[01:50:52 - 01:50:53] שיהיה לו הרבה ממדים,
[01:50:54 - 01:50:56] וה-p של x יהיה פשוט ההסתברות
[01:50:57 - 01:50:59] של הוקטור, הצפיפות של ההסתברות,
[01:51:00 - 01:51:00] ש-x
[01:51:01 - 01:51:01] אה...
[01:51:02 - 01:51:04] שווה לאיזשהו ערך x.
[01:51:05 - 01:51:06] אוקיי? שמשתנה המקרי x,
[01:51:06 - 01:51:07] שווה לאיזשהו ערך x.
[01:51:08 - 01:51:09] שהכל פשוט בוקטורים.
[01:51:12 - 01:51:14] אוקיי? אז x יהיה פשוט וקטור של הרבה
[01:51:16 - 01:51:18] משתנים מקרים סקלריים רגילים.
[01:51:21 - 01:51:24] מה זה אומר? זאת אומרת, התוחלת של x זה גם יהיה וקטור.
[01:51:26 - 01:51:31] אוקיי? שכל איבר בתוחלת הזו נהיה התוחלת של המימד הרלוונטי.
[01:51:33 - 01:51:35] אוקיי? אז גם התוחלת של x זה יהיה וקטור.
[01:51:36 - 01:51:38] הקוואריאנס של x
[01:51:38 - 01:51:39] זה יהיה מטריצה.
[01:51:43 - 01:51:45] שאנחנו נשאיר אותה בתור סיגמא בדרך כלל.
[01:51:46 - 01:51:49] והסיגמא זה יהיה אה...
[01:51:49 - 01:51:51] האיבר ה-i-j במטריצה הזאת,
[01:51:52 - 01:51:57] זה יהיה בעצם הקוואריאנס בין המשתנה המקרי xi למשתנה המקרי xj,
[01:51:58 - 01:51:59] שהם משתנים מקרים סקלריים.
[01:52:02 - 01:52:04] מה יהיה באלכסון של המטריצה הזאת?
[01:52:07 - 01:52:08] רוצה להגיד?
[01:52:12 - 01:52:13] האחדות?
[01:52:15 - 01:52:15] מה?
[01:52:17 - 01:52:18] האחדות?
[01:52:18 - 01:52:22] האחדות זה יהיה קוואריאנס של xi עם xi,
[01:52:23 - 01:52:29] שזה הגדרה של וריאנס, אותו דבר כמו וריאנס, זה פשוט יהיה וריאנס של כל אחד מהמשתנים במקרים האלה הסקלריים.
[01:52:33 - 01:52:35] בסדר? אז זה יהיה ה... תכף אנחנו נדון.
[01:52:36 - 01:52:40] ניתן לזה קצת את הדעת יותר על המטריצה הזאת.
[01:52:42 - 01:52:43] המטריצה הזאת היא סימטרית,
[01:52:45 - 01:52:46] קוואריאנס זה בכל איזשהו סימטרי,
[01:52:46 - 01:52:50] לא משנה אם אני כותב כאן קוויריאנס xxj או קוויריאנס xxi,
[01:52:51 - 01:52:52] היא גם positive definite
[01:52:55 - 01:53:00] והפיכה, זה נובע מזה שהיא positive definite. מה זה אומר positive definite? שזה כמה משמעויות,
[01:53:00 - 01:53:04] אחת זה שזה כל הערכים העצמיים
[01:53:06 - 01:53:07] ‫חיוביים.
[01:53:13 - 01:53:17] ושתיים, זה שתבנית ריגועית ‫עם וקטור, וגם פנים חיוביים.
[01:53:22 - 01:53:24] יש לכם בתרגיל קצת תזכורת ‫להסתכל על הדברים האלה.
[01:53:25 - 01:53:30] סיגמה היא מסימטרית, ‫זאת אומרת שאפשר לפרק אותה לערכים עצמאיים,
[01:53:31 - 01:53:33] ובצורה של...
[01:53:34 - 01:53:36] אוקיי, אני רוצה לנסות לכתוב כאן ‫למשהו סוג של לוח.
[01:53:38 - 01:53:40] טוב, נעשה את זה רגע, ‫בשקף הבא.
[01:53:45 - 01:53:45] אוקיי.
[01:53:47 - 01:53:49] תכף נדבר עוד קצת על סיגמה,
[01:53:50 - 01:53:50] אוקיי?
[01:53:53 - 01:53:54] על מטריצת הקובריץ.
[01:53:56 - 01:54:02] עכשיו, סתם דיברנו קודם על המסחר הזאת של Change of Variable, איך זה נראה כשהדאטה הוא וקטורי?
[01:54:04 - 01:54:06] אז יש לנו פי וואי,
[01:54:08 - 01:54:09] שהוא עכשיו וקטור,
[01:54:11 - 01:54:13] אז אותו דבר כמו קודם,
[01:54:14 - 01:54:16] זה פשוט פי של איקס על F מינוס אחד של Y,
[01:54:17 - 01:54:19] ונקדם נרמול כאן,
[01:54:20 - 01:54:23] יהיה לנו j של f מינוס אחד של y.
[01:54:23 - 01:54:26] אפשר לנחש מה זה j? אה, כתוב פה.
[01:54:26 - 01:54:28] j זה היעקוביאן,
[01:54:29 - 01:54:31] שזה בעצם הכללה של נגזרת,
[01:54:32 - 01:54:34] כאשר הפונקציה היא רב-ממדית.
[01:54:38 - 01:54:40] יעקוביאן זה מטריצה,
[01:54:41 - 01:54:42] שכל עמודה במטריצה
[01:54:43 - 01:54:44] היא הגרדיאנט
[01:54:46 - 01:54:50] של x לפי אחד מהממדים של f.
[01:54:51 - 01:54:56] אז אם f היא פונקציה מ-n-ממדים ל...
[01:54:57 - 01:54:58] אנחנו הולכים כאן שזה הפיך,
[01:54:58 - 01:55:01] אז נגיד שזה חייב להיות מ-n-ממדים ל-n-ממדים,
[01:55:02 - 01:55:10] אז יהיה לנו n שורות, ובכל שורה זה יהיה הגרדיאנט של x, אבל גרדיאנט הוא מוגדר לפי איזושהי פונקציה אחת,
[01:55:12 - 01:55:12] פונקציה סקלרית אחת.
[01:55:13 - 01:55:23] אז יהיו פה n גרדיאנטים שונים, וקוברו כל אחת מהממדים של ה-output של f.
[01:55:27 - 01:55:28] אוקיי, מי שצריך קצת לזכור את זה,
[01:55:29 - 01:55:31] אז אני משוב שנסתכל על זה קצת
[01:55:33 - 01:55:33] עד לשיעור הבא.
[01:55:34 - 01:55:36] גם בתרגיל קצת אתם תסתכלו על זה.
[01:55:39 - 01:55:42] אוקיי, ההתפלגות שלנו עבוד איתה הכי הרבה היא גאוסיאן.
[01:55:43 - 01:55:45] למה זו התפלגות שעובדים איתה הכי הרבה?
[01:55:50 - 01:55:53] מישהו יש רעיון?
[01:55:59 - 01:56:01] היא הכי פופולרית.
[01:56:02 - 01:56:05] כן, למה? נחקרת הרבה, כן.
[01:56:05 - 01:56:05] טבעית?
[01:56:07 - 01:56:09] אפשר לתאר אותה עם שני פרנסים.
[01:56:11 - 01:56:13] אז יש כל מיני הסברים,
[01:56:13 - 01:56:19] ניסיונות להסביר, אני חושב שדי מוסכם על כולם, למרות שאולי לא כולם מודיעים בזה,
[01:56:20 - 01:56:22] שזה פשוט ההתפלגות שהכי נוח לעבוד איתה.
[01:56:24 - 01:56:27] הרבה דברים אפשר לפתור אותם בצורה אנליטית.
[01:56:28 - 01:56:40] ופשוט הרבה יותר נוח לעבוד עם התפלגות כזאת, אפשר להכליל אותה להרבה ממדים בצורה נוחה.
[01:56:44 - 01:56:48] ומתקיימים למשל שלושת התכונות האלה שאנחנו נדבר עליהן עכשיו.
[01:56:49 - 01:56:53] אז בוא נסתכל קודם מה זה אומר, גאוסיאן רב-ממדי.
[01:56:54 - 01:56:56] זו ההגדרה של זה, אוקיי? אז
[01:56:57 - 01:56:57] ה-PDF
[01:56:58 - 01:57:01] של הוקטור X, אוקיי? אז X מקבל כאן כמה ערכים.
[01:57:03 - 01:57:08] זה לקחתי מוויקיפדיה, אני חושב. אז ה-K זה מספר, זה הגודל של X,
[01:57:09 - 01:57:11] ו-K זה למשל מספר הפיקסלים באמניסט.
[01:57:14 - 01:57:17] אז ההתפלגות הזאת מוגדרת ככה, זה אקספוננט
[01:57:18 - 01:57:20] של הדבר הזה שכתוב כאן,
[01:57:21 - 01:57:22] שמה זה?
[01:57:22 - 01:57:23] קומינוס חצי,
[01:57:24 - 01:57:27] ומה שכתוב כאן זה נקרא תבנית ריבועית.
[01:57:27 - 01:57:28] יש לנו כאן וקטור,
[01:57:30 - 01:57:33] הוקטור מופיע פה גם, זה אותו וקטור,
[01:57:34 - 01:57:36] ובאמצע יש איזושהי מטריצה, סיגמא ומינוס אחד.
[01:57:39 - 01:57:44] אוקיי? דיברנו על זה כבר במקשר לקורסים, גם בהסתברות וגם במשין לרנינג, אז תזכורת.
[01:57:44 - 01:57:46] הדבר הזה נקרא כבנית ריבועית.
[01:57:51 - 01:57:52] מה שפה למטה
[01:57:52 - 01:57:54] זה פשוט סקלר.
[01:57:55 - 01:57:56] זה מספר אחד,
[01:57:56 - 01:57:58] לא משנה מה המימד של איקס,
[01:57:59 - 01:58:04] זה מספר אחד שהוא פשוט מנרמל את הפונקציה הזאת ככה שכשננסה לאינטגרל
[01:58:05 - 01:58:06] היא תהיה שווה אחד.
[01:58:07 - 01:58:15] אז זו הייתה נורמליזציה קוראים לזה גם, או הפונקציות נרמול של הגאוסיאן.
[01:58:16 - 01:58:20] מיו לא משפיע כאן, מיו וסיגמא זה הפרמטרים בעצם של הגאוסיאן,
[01:58:21 - 01:58:24] אבל אפשר לחשוב גאוסיאנים שונים, ומה שמפריד ביניהם זה מיו וסיגמא.
[01:58:25 - 01:58:27] פה למטה מופיע רק סיגמא,
[01:58:27 - 01:58:30] זה הדטרמיננטה של סיגמא, סיגמא זה מטריצה,
[01:58:31 - 01:58:33] מה שכתוב כאן זה הדטרמיננטה של המטריצה הזאת.
[01:58:35 - 01:58:39] מה שכתוב כאן למעלה קוראים לזה לפעמים הלנוביס דיסטנס,
[01:58:41 - 01:58:43] זה המרחק בעצם בין איקס למיו,
[01:58:45 - 01:58:48] אבל בצורה כזאתי שהיא ממושקלת לפי סיגמא.
[01:58:49 - 01:58:50] אז הדבר הזה זה גם סקלר, כן?
[01:58:51 - 01:58:52] זה נקרא תבנית ריבועית,
[01:58:53 - 01:58:54] זה גם סקלר.
[01:58:55 - 01:59:04] ומה שהראינו כבר בהסתברות ואולי אפילו גם במשין לרנינג, שהתוחלת של ההסתברות הזאת היא שווה למה?
[01:59:09 - 01:59:10] זה מיו.
[01:59:10 - 01:59:13] זה מיו, נכון? אז מה שכתוב כאן זה בעצם התוחלת.
[01:59:14 - 01:59:17] אם נחשב מה התוחלת של ההסתברות הזאת, זה יצא לנו מה שכתוב כאן.
[01:59:18 - 01:59:19] אז מי זה בעצם התוחלת?
[01:59:20 - 01:59:22] אם נחשב את הווריאנס או את ה-co-וריאנס,
[01:59:22 - 01:59:24] המטריצת קו-וריאנס של ההתפלגות הזאת,
[01:59:25 - 01:59:26] אנחנו נקבל את סיגמא.
[01:59:26 - 01:59:28] סיגמא זה מטריצת הקו-וריאנס
[01:59:29 - 01:59:30] של ההתפלגות הזאת.
[01:59:30 - 01:59:34] אתם יכולים לבדוק בבית שבמקרה שאנחנו מסתכלים על מימד אחד,
[01:59:34 - 01:59:37] אז הכל מסתדר פה גם עם ההגדרה של גאוסיאנס בממימד אחד.
[01:59:43 - 01:59:44] אוקיי, איך
[01:59:50 - 01:59:53] שאמרו פה קודם זה שמה שבמיוחד גם בגאוסיאנס,
[01:59:53 - 01:59:56] שהוא בעצם מוגדר על ידי רק שני הפרמטרים האלה, התוחלת
[01:59:57 - 01:59:58] והקובריאנס.
[01:59:59 - 02:00:00] כמובן שאם זה רב-ממדי אז יש לי כאן
[02:00:01 - 02:00:04] לא באמת שני פרמטרים, זה n פרמטרים,
[02:00:04 - 02:00:06] וכאן יש לי n כפול n פרמטרים.
[02:00:07 - 02:00:10] זה סימטרי, אז לא בדיוק n כפול n, אבל בערך חצי מ-n כפול
[02:00:11 - 02:00:12] פרמטרים.
[02:00:14 - 02:00:16] אבל הפרמטרים האלה הם יחסית,
[02:00:17 - 02:00:20] יש להם משמעות והם ברורים, אז זה מאוד נוח.
[02:00:20 - 02:00:25] עוד שלוש תכונות שיש לגאוסיאנס, שאני מדבר עליהן.
[02:00:26 - 02:00:30] אחת זה שטרנספורמציה אפינית של גאוסיאנס היא גם גאוסיאנס.
[02:00:32 - 02:00:35] אם אני לוקח איזשהו x, נכפיל אותו במשהו ונוסיף לו משהו,
[02:00:35 - 02:00:36] זה עדיין נשאר גאוסיאנס,
[02:00:37 - 02:00:39] ואת זה אתם תוכיחו בתרגיל.
[02:00:40 - 02:00:42] את שני הדברים האלה אנחנו נוכיח עכשיו
[02:00:45 - 02:00:47] שהמרגינל
[02:00:48 - 02:00:48] של x
[02:00:49 - 02:00:50] הוא גם גאוסיאנס.
[02:00:51 - 02:00:51] מה זה מרג'ינל?
[02:00:52 - 02:00:54] זה ההתפלגות השולית. זאת אומרת, אם אני לוקח
[02:00:55 - 02:00:57] את ההתפלגות על וקטור x,
[02:00:58 - 02:01:02] אבל עכשיו אני רוצה לשכוח שהוא בממד k, ורק להסתכל על ממד 1
[02:01:04 - 02:01:04] מה ההתפלגות
[02:01:05 - 02:01:07] של הממד האחד הזה, זה התפלגות שולית.
[02:01:08 - 02:01:12] זה לא חייב להיות 1, כן? זה יכול להיות איזשהו מספר קטן יותר מ-k.
[02:01:13 - 02:01:13] אז זה נקרא
[02:01:14 - 02:01:17] התפלגות שולית, אז גם היא גאוסיאנס, פשוט בממד יותר נמוך.
[02:01:18 - 02:01:22] ועוד תכונה זה ש-conditional distribution
[02:01:23 - 02:01:25] היא גם גאוסיאנס. מה זה conditional distribution?
[02:01:25 - 02:01:28] של אם אני יודע, נגיד, שחלק מהאיברים כאן
[02:01:29 - 02:01:30] מקבלים ערך מסוים,
[02:01:31 - 02:01:38] אני רוצה להגיד, אוקיי, מה ההתפלגות של הערכים האחרים, בהינתן שהערכים האחרים קיבלו משהו אחר,
[02:01:38 - 02:01:39] קיבלו איזשהו ערך,
[02:01:39 - 02:01:41] אז ההתפלגות המותנת הזאת היא גם גאוסיאנס.
[02:01:44 - 02:01:46] אוקיי, אז אני רוצה להוכיח את שני הדברים האלה עכשיו.
[02:01:47 - 02:01:49] אני מקווה שאני אצליח, זה יהיה טיפה מהיר,
[02:01:50 - 02:01:53] זה יהיה לא על לוח, זה יהיה קצת לא נוח,
[02:01:53 - 02:01:55] אבל אני מקווה שזה יסתדר.
[02:01:56 - 02:01:58] הבטחתי לכם גם להסביר משהו על
[02:02:02 - 02:02:04] הפונקציות covariance, אולי נתחיל מזה.
[02:02:16 - 02:02:25] אוקיי, בואו נתחיל מזה שאיך נראית פונקציית גאוסיאן,
[02:02:25 - 02:02:27] אוקיי, אז אנחנו כולם מכירים בממד אחד שזה
[02:02:28 - 02:02:30] פונקציה שהיא נראית כמו פעמון,
[02:02:31 - 02:02:34] כאן מצוייר הפונקציות בשני ממדים,
[02:02:34 - 02:02:38] אוקיי, בשני ממדים זה גם נראה כמו איזשהו פעמון תלת-ממדי כזה,
[02:02:39 - 02:02:42] אוקיי, שתופס את ההתפלגות בשני ממדים,
[02:02:42 - 02:02:46] וברב-ממד זה גם יהיה סוג של אליפסואיד כזה.
[02:03:12 - 02:03:12] תודה.
[02:03:42 - 02:04:02] אוקיי, אז נראה שהצלחתי.
[02:04:12 - 02:04:17] אתם רואים את הלוח הזה?
[02:04:24 - 02:04:24] יש את כאן?
[02:04:25 - 02:04:34] כן. אז בעצם אם אנחנו נדגום הרבה דגימות מההתפלגות גאוסיאנית רב-ממדית, אני אצייר את זה עכשיו בדו-ממד,
[02:04:34 - 02:04:35] אנחנו נקבל משהו
[02:04:40 - 02:04:41] שפשוט יש לנו הרבה נקודות.
[02:04:42 - 02:04:47] באיזשהו אזור מרכזי.
[02:05:12 - 02:05:13] תודה רבה.
[02:05:42 - 02:05:42] תודה רבה.
[02:06:42 - 02:07:04] טוב, אז זהו. אז בעצם הדאטה שאנחנו נייצר.
[02:07:05 - 02:07:12] אוקיי, נראה משהו כזה.
[02:07:13 - 02:07:17] באזור המרכז אנחנו נקבל הרבה נקודות,
[02:07:18 - 02:07:21] וככל שנסתכל יותר רחוק נקבל פחות נקודות,
[02:07:22 - 02:07:27] ובעצם הדאטה יהיה מוגדר על ידי איזשהו סט של
[02:07:33 - 02:07:33] אליפסות.
[02:07:42 - 02:07:51] אליפסות האלה בעצם מתארות קווי גובה
[02:07:56 - 02:07:57] בהתפלגות שלנו.
[02:08:04 - 02:08:06] אוקיי, עכשיו המטריצת קובריאנס
[02:08:07 - 02:08:17] היא מגדירה לנו את האליפסה הזאת בעצם, את הכיוון של האליפסה הזאת ועד כמה היא מתוחה בכל אחד מהכיוונים.
[02:08:18 - 02:08:20] ואם אנחנו נכתוב פשוט את ה...
[02:08:21 - 02:08:24] כמו שאמרתי, אנחנו יכולים לכתוב את המטריצות קובריאנס בתור
[02:08:25 - 02:08:28] איזושהי מטריצה אורטונורמלית
[02:08:29 - 02:08:31] כפול איזושהי מטריצה אלכסונית
[02:08:32 - 02:08:34] כפול אותה מטריצה אורטונורמלית טרנספוז.
[02:08:35 - 02:08:38] אוקיי, אז זה נכון לכל מטריצה שהיא סימטרית אפשר לכתוב אותה ככה.
[02:08:39 - 02:08:46] ולמטריצה שהיא pd אז ה-d הזאת שהיא מטריצה אלכסונית,
[02:08:47 - 02:08:50] כל העברים על האלכסון יהיו חיוביים.
[02:08:51 - 02:08:52] אוקיי,
[02:08:53 - 02:08:59] ויורים בעצם מגדירים לנו את הוקטורים העצמיים של המטריצה הזאת ו-d את הערכים העצמיים של המטריצה.
[02:09:00 - 02:09:03] אז אותם וקטורים עצמיים משמאל ומימין במטריצה סימטרית.
[02:09:05 - 02:09:15] אוקיי עכשיו מה המשמעות של הדברים האלה? הערכים העצמיים וזה בעצם יהיו הכיוונים של האליפסה, הכיוונים הראשיים של האליפסה, כאן הציור הוא בדו-מימד
[02:09:16 - 02:09:19] אוקיי אז יש לנו בעצם שני כיוונים בדו-מימד
[02:09:20 - 02:09:24] אבל ברב-מימד יהיו לנו הרבה יותר וקטורים כאלה
[02:09:26 - 02:09:33] וה-D אומר לנו בעצם כמה חזק כל אחד מהכיוונים האלה עד כמה אליפסה היא מוערכת
[02:09:34 - 02:09:38] ושוב זה מגדיר לנו בעצם את ההסתברות, את ההתפלגות כולה
[02:09:39 - 02:09:44] כי חוץ ממה שנשאר לנו זה ה-mue, ה-mue בעצם אומר לנו איפה נקודת המרכז של האליפסה הזאת
[02:09:45 - 02:09:52] וברגע שאנחנו יודעים את ה-mue ואת ה-sigma אנחנו יודעים מה האזורים שבהם ההסתברות יותר גבוהה ומה האזורים שההסתברות יותר נמוכה
[02:09:52 - 02:09:59] אם אנחנו מתקדמים על החץ בכיוון הזה ההסתברות יורדת יותר מהר מאשר אם אנחנו מתקדמים על החץ בכיוון הזה
[02:10:03 - 02:10:04] אוקיי,
[02:10:05 - 02:10:06] קיוויתי קצת
[02:10:09 - 02:10:10] לדבר על זה קצת יותר,
[02:10:10 - 02:10:16] אבל אני מקווה שהם מכירים את כל זה בעצם כבר לפני זה,
[02:10:16 - 02:10:19] ונוכל לעבור עכשיו להוכחה בעצם של מה שרצינו.
[02:10:21 - 02:10:24] יש שאלות על זה? יש מישהו שבכלל לא מכיר את הדבר הזה?
[02:10:25 - 02:10:26] כנראה אף אחד לא יענה כשאני שואל את זה ככה,
[02:10:27 - 02:10:30] אבל אם זה המצב אז אתם צריכים קצת
[02:10:30 - 02:10:32] תסתכלו ותנסו לעשות איזשהו
[02:10:34 - 02:10:34] רענון
[02:10:35 - 02:10:36] של העניין הזה.
[02:10:37 - 02:10:41] קצת תהיה לכם אינטואיציה של הגיאומטריה של משתנים גאוסיאנים רב-למדים.
[02:10:44 - 02:10:49] אוקיי, אז אנחנו רוצים להוכיח את שלושת הדברים האלה. את הראשון אתם תוכיחו בתרגיל,
[02:10:51 - 02:10:54] בשיטה שהיא די דומה למה שאנחנו נראה עכשיו,
[02:10:55 - 02:10:56] ואנחנו נעשה את שתיים ושלוש.
[02:10:58 - 02:10:59] משהו שאנחנו נשתמש בו,
[02:11:01 - 02:11:05] זה מה שכתוב בשקף הזה,
[02:11:06 - 02:11:07] בואו נעבור על זה רגע.
[02:11:07 - 02:11:14] אז בעצם אנחנו יכולים ל-X מבחינתנו זה איזשהו וקטור, אנחנו עכשיו שואלים כל מיני שאלות שהן קשורות לפירוק של הוקטור הזה לשני חלקים.
[02:11:15 - 02:11:16] גם כשאנחנו מדברים על מרג'ינל,
[02:11:17 - 02:11:21] אנחנו נסתכל רק על ה-margin' של אחד מהחלקים האלה,
[02:11:21 - 02:11:26] וכשנדבר על התפלגות מותנית אז זה גם, זה יהיה ההתפלגות המותנית של חלק אחד בחלק השני.
[02:11:28 - 02:11:29] אז באופן כללי אנחנו יכולים לקרוא ל-X,
[02:11:29 - 02:11:31] אנחנו נקרא ל-XA החלק הראשון,
[02:11:32 - 02:11:33] XB החלק השני,
[02:11:34 - 02:11:36] אז בהתאם גם התוחלת
[02:11:37 - 02:11:38] יהיה לחלק ראשון
[02:11:39 - 02:11:40] וחלק שני, New A ו-New B,
[02:11:41 - 02:11:44] וה-Covarience אנחנו יכולים לחלק אותו לארבעה חלקים כאלה,
[02:11:44 - 02:11:46] Sigma AA,
[02:11:46 - 02:11:48] שזה פשוט החלק שקשור רק ב-A,
[02:11:49 - 02:11:54] אז אם נגיד A הוא ארבעה ממדים, אז Sigma AA יהיה 4 על 4,
[02:11:55 - 02:11:59] Sigma BB זה יהיה החלק של מה שנשאר,
[02:12:00 - 02:12:03] החלק של M�ריקס זה החלק השני של B,
[02:12:03 - 02:12:08] ו-AB ו-BA שהם אחד טרנספורט של השני, כי זה מטרנסה סימטרית,
[02:12:09 - 02:12:11] זה יהיה מה שיש מחוץ לאלכסונים.
[02:12:13 - 02:12:14] עכשיו עוד משהו שאנחנו נגדיר,
[02:12:15 - 02:12:20] שהרבה פעמים קל יותר לעבוד איתם בגאוסיאנים, זה מה שנקרא Precision M�ריקס,
[02:12:20 - 02:12:22] זה מוגדר כאן עם
[02:12:23 - 02:12:24] הדלתא הגדולה הזאת,
[02:12:25 - 02:12:27] שזה ההופכי של Sigma,
[02:12:27 - 02:12:28] נכון? ראינו פה
[02:12:29 - 02:12:31] בעצם Sigma באזור הזה,
[02:12:31 - 02:12:33] היא מופיעה בתור Sigma מינוס אחד,
[02:12:34 - 02:12:37] אז יותר קל הרבה פעמים כבר לעבוד ישירות עם ה- Sigma מינוס אחד הזה,
[02:12:37 - 02:12:39] וקוראים לזה Precision M�ריקס,
[02:12:40 - 02:12:43] וגם אותו אנחנו נסתכל על החלקים השונים,
[02:12:44 - 02:12:48] איך היא מתחלקת למה שרלוונטי ל-A ומה שרלוונטי ל-B,
[02:12:49 - 02:12:59] וכדי לעבור מאחד לשני בתוך החלוקה הזאת, נשתמש בזהות הזאת של מטריצות, שאם אנחנו לוקחים מטריצה שהיא מפורקת ככה לארבע חלקים שונים,
[02:12:59 - 02:13:06] ההופכי שלה אפשר לחשב כל אחד מהארבעה חלקים האלה לפי
[02:13:09 - 02:13:13] הנוסחאות שיש פה, אוקיי? כש-M שמופיע פה בכל המקומות,
[02:13:14 - 02:13:18] מוגדר ככה A מינוס BD מינוס אחד C, כל זה במינוס אחד,
[02:13:18 - 02:13:21] זה גם נקרא Shure's Compliment.
[02:13:22 - 02:13:23] זה משהו שהוא מאוד שימושי בכל מיני
[02:13:24 - 02:13:29] חישובים שעושים עם מטריצה, אוקיי? בהקשר שלנו של לעבור מ-Covariance
[02:13:29 - 02:13:32] ל-Precision אנחנו מקבלים את הזהות הזאת,
[02:13:32 - 02:13:33] שתי הזהויות כאלה,
[02:13:34 - 02:13:37] שה-Precision של החלק של A שווה
[02:13:38 - 02:13:41] לנוסחה הזאת, שכמו שאתם רואים ה-Precision של החלק של A הוא לא רק
[02:13:42 - 02:13:47] תלוי ב-Covariance של החלק של A, אלא גם ב-Covariance של החלק של B.
[02:13:50 - 02:13:55] אז זה זהות אחת, והזהות השנייה זה שה-Precision בחלק הזה של ה-Precision
[02:13:56 - 02:13:59] Delta A-B שווה לנוסחה הזאת.
[02:14:00 - 02:14:03] אוקיי, אנחנו נשתמש בזה עוד מעט בחישובים שנעשה.
[02:14:04 - 02:14:08] בעצם הטריק העיקרי שאנחנו נשתמש בו,
[02:14:09 - 02:14:11] גם אתם תצטרכו להשתמש בו בתרגיל,
[02:14:12 - 02:14:13] נקרא Completing
[02:14:14 - 02:14:14] ו...
[02:14:18 - 02:14:18] ...נורמלי.
[02:14:26 - 02:14:28] אז הוא נקרא Completing the Square.
[02:14:30 - 02:14:30] השתמה לריבוע.
[02:14:43 - 02:14:43] אוקיי,
[02:14:43 - 02:14:48] אנחנו קודם כל נוכיח אם זה משהו שאנחנו נשתמש בו בשני החלקים האחרים.
[02:14:49 - 02:14:50] אני כבר רואה שיהיה לנו קצת בעיית זמן.
[02:14:53 - 02:14:56] אז הדבר הראשון שאנחנו נוכיח זה
[02:14:59 - 02:15:00] את הטענה הבאה.
[02:15:05 - 02:15:07] אם יש לנו איזשהו PX,
[02:15:10 - 02:15:10] נתפלג
[02:15:11 - 02:15:14] בצורה שהיא פרופורציונית
[02:15:16 - 02:15:17] לאקספוננט
[02:15:19 - 02:15:20] מינוס חצי
[02:15:29 - 02:15:35] X ועוד שני X פרספוס B ועוד C.
[02:15:38 - 02:15:39] מה כתוב כאן?
[02:15:40 - 02:15:47] כתוב, מה זה אומר שזה מתפלג פרופורציונית? זאת אומרת שאני לא מתעלם כרגע מהאיבר של הנורמליזציה,
[02:15:47 - 02:15:48] מה שהיה כאן במכנה.
[02:15:50 - 02:15:53] ובעצם מה שאני אומר כאן זה יש כאן פונקציה כללית,
[02:15:53 - 02:15:55] ריבועית כללית, אוקיי? X
[02:15:55 - 02:15:56] מופיע פה בריבוע,
[02:15:57 - 02:16:00] פה הוא מופיע לא בריבוע, פשוט מכפלה פנימית עם איזשהו וקטור,
[02:16:01 - 02:16:03] וכאן יש פה איזשהו קבוע.
[02:16:03 - 02:16:05] אוקיי? אז זו צורה יחסית כללית
[02:16:06 - 02:16:09] של משוואה ריבועית וקטורית של X.
[02:16:10 - 02:16:18] יש פה את המינוס חצי כאן ואת השתיים כאן כדי שהחישובים יראו קצת יותר יפה, אבל זה לא באמת צריך אותם, כי אפשר להכניס אותם לתוך הקבועים שיש פה.
[02:16:20 - 02:16:23] אז אם כל בעצם התפלגות
[02:16:24 - 02:16:24] שהיא
[02:16:25 - 02:16:26] מהצורה הזאתי,
[02:16:27 - 02:16:30] שהיא פרופורציונית למשהו בצורה הזאתי,
[02:16:31 - 02:16:34] שהאקספוננט שלה הוא איזושהי פונקציה ריבועית כללית של X,
[02:16:35 - 02:16:38] אז הטענה היא שכל פונקציה כזאת
[02:16:39 - 02:16:40] היא בעצם גאוסיאנית.
[02:16:48 - 02:16:49] אוקיי, אז בואו נוכיח את זה.
[02:16:53 - 02:16:55] אנחנו נוכיח את זה רק למקרה
[02:16:59 - 02:17:02] של איזשהו הפיך.
[02:17:08 - 02:17:12] אוקיי,
[02:17:21 - 02:17:26] אז אוקיי, אז נתחיל מזה ש-P של X
[02:17:28 - 02:17:29] הוא בעצם
[02:17:34 - 02:17:35] קודם.
[02:17:36 - 02:17:37] אה, אוקיי, סליחה.
[02:17:37 - 02:17:37] אנחנו,
[02:17:39 - 02:17:45] כן, אז אני לא אכתוב את זה שוב, אני אגיד מה אנחנו רוצים להראות. בעצם אנחנו רוצים להביא את מה שכתבנו קודם לצורה אחרת.
[02:17:46 - 02:17:47] אנחנו רוצים להביא את זה,
[02:17:49 - 02:17:51] טוב, כן, we want to show.
[02:17:54 - 02:17:57] אנחנו רוצים להביא את זה לצורה שבה אנחנו מכירים את הגאוסיאנים.
[02:17:59 - 02:18:02] שזה הצורה הזאתי, שחצי
[02:18:03 - 02:18:05] של X פחות מיור
[02:18:05 - 02:18:21] סיגמא במינוס אחד אינספוס פול אינספוננט של זה, אוקיי? שההתפלגות שלנו היא צריכה להיות פרופורציונלית למה שאנחנו רואים כאן.
[02:18:24 - 02:18:26] אוקיי, אז אנחנו רוצים להביא את הצורה הזאת
[02:18:28 - 02:18:28] לצורה הזאת.
[02:18:31 - 02:18:33] אוקיי, זה ברור? אתם צריכים לקרוא מה כתוב כאן?
[02:18:33 - 02:18:34] זה צריך ל...
[02:18:35 - 02:18:36] אני עדיין לא כל כך ב...
[02:18:39 - 02:18:44] תכננתי שיהיה בזמן לעשות את זה על הלוח כמו שצריך.
[02:18:55 - 02:19:03] אוקיי, אז איך אנחנו עושים את זה? אז בואו נסתכל רק פשוט על ה... נתעלם מה-X מינוס חצי, נסתכל פשוט על הדבר הזה ונראה איך אנחנו מביאים אותו לצורה הזאת.
[02:19:03 - 02:19:06] ומה-מה משתנה זה מה שיש לנו.
[02:19:07 - 02:19:08] אוקיי? אתם איתי?
[02:19:08 - 02:19:09] חלק לפחות?
[02:19:11 - 02:19:13] אוקיי, אז יש לנו,
[02:19:14 - 02:19:16] אנחנו מתחילים מ-X
[02:19:17 - 02:19:18] קונספוס
[02:19:19 - 02:19:20] A-X
[02:19:22 - 02:19:22] ועוד
[02:19:24 - 02:19:26] X קונספוס B ועוד C.
[02:19:27 - 02:19:30] אנחנו רוצים להביא את זה לצורה הזאת.
[02:19:31 - 02:19:33] אז מה-מה יש לנו פה?
[02:19:34 - 02:19:43] אנחנו נעשה כמה טריקים, אוקיי? זה נקרא השלמה לריבוע. בטח ראיתם את זה בכל מיני צטאפים אחרים, אולי לא בצורה וקטורית, אלא בצורה סקלרית.
[02:19:45 - 02:19:50] אז אנחנו בעצם נוסיף באיבר הזה קודם כל,
[02:19:51 - 02:19:52] אנחנו נוסיף פה A ו-A-1.
[02:19:53 - 02:19:54] נקבל
[02:19:54 - 02:20:18] X טרנספוס A-X ועוד X טרנספוס A כפול A מינוס 1 D ועוד C.
[02:20:19 - 02:20:21] A הפיכה, אז אם אני שם פה A
[02:20:22 - 02:20:24] זה מכפיל A מינוס 1, לא עשיתי כלום.
[02:20:26 - 02:20:28] אוקיי, זה לא טוב, לא שומעתי שום דבר.
[02:20:29 - 02:20:33] עכשיו אני אוסיף משהו קצת יותר מוזר.
[02:20:37 - 02:20:39] טוב, כאן X טרנספוס A X,
[02:20:39 - 02:20:45] אותו דבר, ועוד שני X טרנספוס A A מינוס 1 בינוס C.
[02:20:46 - 02:20:49] ואני אוסיף איבר ואני אוריד אותו ישר, אוקיי?
[02:20:50 - 02:20:51] אני אוסיף
[02:20:52 - 02:20:54] אוקיי, בואי אני עושה את זה לפני ה-C בעצם.
[02:20:56 - 02:20:56] אז אני אוסיף
[02:20:59 - 02:21:00] אני אוסיף
[02:21:00 - 02:21:02] B טרנספוס
[02:21:03 - 02:21:06] A מינוס 1 A
[02:21:07 - 02:21:14] A מינוס 1 B. אני אוריד את הדבר הזה.
[02:21:15 - 02:21:21] B טרנספוס A מינוס 1 A A מינוס 1 B.
[02:21:22 - 02:21:24] ויש לי גם את ה-C שהיה לי קודם.
[02:21:25 - 02:21:29] אוקיי, אז בעצם הוספתי איזה משהו מוזר כזה והורדתי, אבל למה הוספתי את הדבר המוזר הזה?
[02:21:30 - 02:21:33] כי אם אני רוצה לחשוב על שלושת האיברים כאן בתור
[02:21:34 - 02:21:39] פתיחה של סוגריים של ריבוע, אז יש לי כאן את האיבר הראשון בריבוע.
[02:21:39 - 02:21:41] לפעמים יש לי גם A באמצע, כן?
[02:21:41 - 02:21:42] אבל יש לי את ה-X הזה בריבוע.
[02:21:43 - 02:21:44] כאן יש לי את
[02:21:44 - 02:21:46] X פעמיים X כפול AB,
[02:21:47 - 02:21:49] שזה איזשהו וקטור קבוע,
[02:21:49 - 02:21:51] וכאן אני רוצה שיהיה לי את ה-AB הזה פעמיים.
[02:21:52 - 02:21:54] יש לי כאן את ה-AB כפול ה-AB.
[02:21:54 - 02:21:57] זה שלושת האיברים שיש כי פותחים את הסוגריים.
[02:21:58 - 02:22:01] בגלל שהוספתי את זה אני גם צריך להוריד את זה כדי שזה יהיה שווה.
[02:22:01 - 02:22:02] סך הכל
[02:22:03 - 02:22:08] אני יכול להגיד שמה שיש לי כאן זה שווה ל-X פחות A
[02:22:09 - 02:22:10] מינוס 1B,
[02:22:12 - 02:22:13] כל הזמן אספוס,
[02:22:14 - 02:22:15] כפול A,
[02:22:17 - 02:22:21] תבנית ריבועית כזאת עם טרווקטור שיש לי כאן, X מינוס
[02:22:22 - 02:22:24] A ו-C1B.
[02:22:27 - 02:22:29] אוקיי, אז זה שלושת האיברים האלה,
[02:22:31 - 02:22:32] הם שווים לשני האיברים האלה,
[02:22:33 - 02:22:34] לתבנית הריבועית הזאת.
[02:22:35 - 02:22:38] ויש לי כאן עוד איברים, אבל האיברים האלה כבר לא תלויים ב-X.
[02:22:40 - 02:22:42] נכון? ואני רק רוצה לראות משהו שהוא פרופורציוני
[02:22:44 - 02:22:47] לדבר הזה. בגלל שהדבר הזה הוא אקספוננט,
[02:22:48 - 02:22:49] אם משהו פרופורציוני לדבר הזה,
[02:22:50 - 02:22:53] אז זה כמו שיש לו איזושהי תוספת של קבוע.
[02:22:56 - 02:22:59] אוקיי, אז אני פשוט יכול לקרוא לכל הדבר הזה קבוע חדש,
[02:23:00 - 02:23:00] אוקיי?
[02:23:01 - 02:23:01] סינגן.
[02:23:05 - 02:23:12] אז הראינו בעצם שהאכלנו מהביטוי הזה והגענו לביטוי כזה ועוד קבוע,
[02:23:13 - 02:23:14] מה שאומר שאם יש לי התפלגות
[02:23:15 - 02:23:16] שהיא
[02:23:16 - 02:23:18] פרופורציונית לדבר הזה,
[02:23:20 - 02:23:21] אוקיי?
[02:23:21 - 02:23:25] אז ההתפלגות הזאת היא גם פרופורציונית לצורה שבה הגדרנו את הגאוסיאנט.
[02:23:28 - 02:23:33] למה היא פרופורציונית? כי פה יש לי פה עוד קבוע, אבל עוד קבוע באקספרנט זה כאילו שאני מחלק במשהו,
[02:23:34 - 02:23:35] אופן במשהו.
[02:23:38 - 02:23:42] אוקיי, אז מה ה-co-variance ומה התוחלת של ההתפלגות הזאת שקיבלתי?
[02:23:48 - 02:23:49] A מינוס אחד.
[02:23:50 - 02:23:50] כן?
[02:23:51 - 02:23:52] A מינוס אחד ל-co-variance.
[02:23:52 - 02:23:54] נכון, אז ה-co-variance בדיוק זה
[02:23:57 - 02:23:57] decision matrix
[02:23:59 - 02:24:00] ל-a
[02:24:02 - 02:24:03] שווה a,
[02:24:03 - 02:24:05] מה שאומר שה-co-variance
[02:24:06 - 02:24:10] שווה ל-a מינוס אחד, נכון? זה הדבר הזה שמופיע כאן.
[02:24:12 - 02:24:15] וה-mu שלי, בעצם התוחלת, זה הדבר הזה.
[02:24:16 - 02:24:18] אז מu שווה ל...
[02:24:20 - 02:24:21] מינוס אחד,
[02:24:22 - 02:24:22] b.
[02:24:38 - 02:24:42] אז זה נקרא השלמה לריבוע, וזה בעצם מה שאנחנו נשתמש בו כדי להוכיח
[02:24:44 - 02:24:45] מה שרצינו להוכיח,
[02:24:45 - 02:24:47] ונגמר לנו הזמן,
[02:24:48 - 02:24:49] שזה
[02:24:50 - 02:24:51] שני הדברים האלה.
[02:24:52 - 02:24:54] אני קצת תוהה מה לעשות עכשיו.
[02:24:56 - 02:25:01] אופציה אחת זה שנמשיך לדבר כאן על זה,
[02:25:02 - 02:25:07] אבל זה נראה לי יפה בעייתי. אז מה שאני יכול לעשות אולי זה להקליט את החלק הזה,
[02:25:08 - 02:25:10] ונשלוח לכם.
[02:25:11 - 02:25:13] בסדר? אז אני אעשה עוד הקלטה,
[02:25:14 - 02:25:16] ונשלח לכם
[02:25:16 - 02:25:24] הקלטה של ההוכחה של 2 ו-3 שמשתמשת בהשלמה לריבוע.
[02:25:28 - 02:25:30] אבל בגדול, רק כדי להבין את השיטה,
[02:25:30 - 02:25:31] מי שרוצה כבר להתחיל,
[02:25:34 - 02:25:36] אנחנו רוצים להראות, להוכיח
[02:25:39 - 02:25:43] שה-3 אולי זה יותר קל להגיד את זה, אנחנו רוצים להוכיח שמשהו הוא גאוסיאן,
[02:25:44 - 02:25:46] ואנחנו נרצה לראות שהצורה שלו
[02:25:47 - 02:25:56] היא פשוט ריבועית. אם מספיק שנראה שהצורה שלו היא או מהצורה הזאתי, זה הכי טוב, כי אז אנחנו גם נדע מה ה-co-verיאנס במה התוכלת,
[02:25:57 - 02:26:00] או אפילו מהצורה היותר-כללית הזאת,
[02:26:01 - 02:26:05] כל צורה שנגיע שהיא ריבועית, אנחנו נדע שזה אומר בעצם שהמשתמעי הזה
[02:26:06 - 02:26:07] הוא גאוסיאן,
[02:26:08 - 02:26:13] ואז מה שישאר לנו זה למצוא מה ה-co-verיאנס ומה
[02:26:13 - 02:26:15] ה-new של הקרסיון הזה.
[02:26:17 - 02:26:20] טוב, אז תגמרנו הזמן,
[02:26:21 - 02:26:23] לא הספקנו את כל מה שרציתי,
[02:26:24 - 02:26:28] אבל כמו שאמרתי, אני אשלח לכם סרטון של רקע ההוכחה הזאתי,
[02:26:29 - 02:26:35] קצת הסתבכתי פה עם הלוח, זה דרך חיזום, אני מקווה שאני מסתדר כשאני אעשה את ההקלטה.
[02:26:37 - 02:26:39] מי שיש לו איזה שהן שאלות,
[02:26:39 - 02:26:42] אז גם עכשיו פשוט כמה דקות יכול לשאול,
[02:26:42 - 02:26:45] גם יכול לשלוח לי אימייל.
[02:26:47 - 02:26:48] אני מציע, כמו שאמרתי,
[02:26:49 - 02:26:54] התרגיל הראשון הזה זה קצת כדי להכניס אתכם לעניינים, אם יש לכם שלושה שבועות עד השיעור הבא,
[02:26:54 - 02:27:00] אז המטרה היא שתרגישו שאתם בעניינים ומוכנים לקורס.
[02:27:00 - 02:27:11] הדבר היחיד שהתרגיל הזה לא מכניס אתכם לעניינים זה העניין של הקידוד בספריות של Deep Learning, אז כמו שאמרתי, זה כן נדרש בקורס שתוכלו לקדד את זה.
[02:27:13 - 02:27:17] לממש מודלים בפייתון שהוא משהו דומה.
[02:27:18 - 02:27:22] אז רק רציתי שהכל יהיה ברור לכולם לפני תחילת הקורס,
[02:27:23 - 02:27:25] וזהו להיום. אם מישהו יש שאלות,
[02:27:26 - 02:27:28] מוזמן להישאר לשאול עכשיו או לשלוח לי אימייל.
[02:27:29 - 02:27:30] תודה רבה.
[02:27:31 - 02:27:32] תודה רבה,
[02:27:33 - 02:27:33] דן.
[02:27:33 - 02:27:34] תודה.
[02:27:42 - 02:28:12] תודה רבה.
[02:28:12 - 02:28:27] שלום, אז אמשיך את מה שלא הספקנו בשיעור.
[02:28:27 - 02:28:37] זה בעצם את שתי הוכחות של שתי התכונות של גאוסיאן רד-למדי שמופיעות פה מספר 2 ו-3.
[02:28:37 - 02:28:46] גאוסיאן רב-למדי הוא מוגדר ככה זה שקף מתוך ההרצאה שהייתה.
[02:28:47 - 02:28:51] ככה אנחנו מגדירים גאוסיאן רב-למדי יש פה את החלק שהוא בתוך האקספוננט
[02:28:52 - 02:28:58] שהוא מכיל את האיברים את הפרמטרים מיו וסיגמא שזה הפרמטרים של הגאוסיאן.
[02:28:59 - 02:29:06] מיו זה התוכלת של הגאוסיאן וסיגמא זה הקווריאנס של הגאוסיאן מטריצה שמגדירה את הקווריאנס של הגאוסיאן.
[02:29:07 - 02:29:13] את החלק הזה באקספוננט אנחנו קוראים לו לפעמים מרחק מהלנוביס
[02:29:14 - 02:29:16] והוא מסומן באות דלתא.
[02:29:19 - 02:29:24] שלוש תכונות שדיברנו עליהן, התכונה הראשונה זה שטרנספורמציה אפינית של גרסיאן היא גם כן גרסיאן,
[02:29:24 - 02:29:26] את זה אתם תוכיחו בתרגיל,
[02:29:27 - 02:29:31] ומה שאנחנו נראה היום זה את שתי התכונות האחרות שההתפלגות שולית
[02:29:32 - 02:29:33] של חלק
[02:29:34 - 02:29:36] מהאיברים בוקטור x
[02:29:37 - 02:29:40] זה גם התפלגות שהיא גרסיאנית רב-ממדית,
[02:29:41 - 02:29:42] וההתפלגות מותנית
[02:29:43 - 02:29:47] של חלק מהאיברים ב-x בחלק אחר מהאיברים ב-x
[02:29:48 - 02:29:49] גם כן התפלגות גרסיאנית.
[02:29:52 - 02:29:55] דיברנו על התכונות האלה
[02:29:56 - 02:29:57] שאנחנו נשתמש בהן,
[02:29:58 - 02:30:02] אז כשאנחנו מדברים על וקטור או על התפלגות על וקטור
[02:30:02 - 02:30:06] אנחנו בעצם נרצה עכשיו לחלק את הווקטור הזה לחלקים שונים,
[02:30:06 - 02:30:11] לא כל כך משנה עכשיו מה הגודל של החלקים האלה והתוצאה כאן תהיה כללית,
[02:30:12 - 02:30:17] אנחנו בעצם נקרא לשני החלקים של הוקטור x, נקרא לחלק הראשון xa ולחלק השני xb,
[02:30:19 - 02:30:22] ו-x יכול להיות אחד מהאיברים, אחד מהממדים,
[02:30:22 - 02:30:24] או שהוא יכול להיות איזה שהם k-ממדים מתוך
[02:30:25 - 02:30:26] ה-n-ממדים הכלליים של x,
[02:30:27 - 02:30:31] ובהתאם גם התוחלת new, שהיא גם וקטור באותו גודל כמו x,
[02:30:32 - 02:30:36] אנחנו נחלק אותו לשני החלקים מu a ומu b,
[02:30:36 - 02:30:40] וסיגמא, מטריצת ה-covarianz, נחלק אותה לארבעה חלקים,
[02:30:41 - 02:30:44] החלק הראשון שמתאים לאיברים של a,
[02:30:45 - 02:30:46] נקרא סיגמא aa,
[02:30:47 - 02:30:50] החלק השני שמתאים לאיברים של b הוא סיגמא bb,
[02:30:50 - 02:30:57] והחלקים שהם מחוץ לאלכסון סיגמא a b והטרנספורס שלו סיגמא ba.
[02:30:58 - 02:31:04] גם אמרנו שוב שאנחנו, בגלל שהאיבר סיגמא מופיע במינוס 1
[02:31:06 - 02:31:09] באקספוננט, אנחנו הרבה פעמים נעדיף לעבוד
[02:31:09 - 02:31:12] ישירות עם ההופכי של ה-covariance,
[02:31:13 - 02:31:15] אנחנו נקרא לזה ה-precision matrix,
[02:31:15 - 02:31:18] אני אסמן את זה באות הדלתא הגדולה הזאת,
[02:31:19 - 02:31:21] וגם אותה אפשר לחלק את המטריצה הזאת לחלק,
[02:31:22 - 02:31:23] לדלתא aa,
[02:31:23 - 02:31:25] דלתא bb ולחלקים שמחוץ לאלכסון.
[02:31:25 - 02:31:30] כדי לעבור מהייצוג הזה שבסיגמא לה ייצוג
[02:31:31 - 02:31:34] של סיגמא לה ייצוג של דלתא, של ה-pecision matrix,
[02:31:35 - 02:31:39] יהיה לנו נוח להשתמש בזהות הזאתי,
[02:31:40 - 02:31:46] בזהות המטריציונית הזאתי, שאומרת ההופכי של מטריצה, אם אנחנו מחלקים אותה לארבעת החלקים האלה,
[02:31:46 - 02:31:55] אנחנו יכולים לחשב אותם לפי הנוסחאות שמופיעות פה, שיש כאן את האיבר m שמופיע הרבה פעמים, וה-m הזה זה שם שיש לו,
[02:31:56 - 02:31:56] שורס קומפלמנט,
[02:31:57 - 02:31:58] והוא מוגדר כאן.
[02:31:59 - 02:32:01] בעצם מה שזה אומר לענייננו,
[02:32:02 - 02:32:06] המעבר בין דלתא לסיגמא מופיע כאן, משלושת המוספאות כאן,
[02:32:07 - 02:32:08] שאנחנו נשתמש בהן תכף.
[02:32:09 - 02:32:13] זהו דבר שהראינו כבר בשיעור, זה בעצם את ההוכחה הראשונה,
[02:32:15 - 02:32:20] בעצם זה לא בדיוק הוכחה, זה הוכחה של איזושהי טענה שלא מופיעה קודם,
[02:32:20 - 02:32:22] אבל בעצם המטרה כאן זה להראות את
[02:32:26 - 02:32:32] טריק כזה אפשר לקרוא לו של השלמה לריבוע, שאנחנו נשתמש בו גם בהוכחה שנראית עכשיו,
[02:32:33 - 02:32:40] ובעצם הטענה הזאת שיוכחנו פה זה שאם יש לנו איזושהי התפלגות של x שהיא פרופורציונית
[02:32:41 - 02:32:45] להתפלגות שהאקספוננט שלה הוא איזושהי נוסחה ריבועית כללית,
[02:32:46 - 02:32:51] גם שיש כאן, יש כאן תבנית ריבועית ועוד איבר בינארי ב-x,
[02:32:52 - 02:32:54] יש כאן מכפלה פנימית בין x לאיזשהו וקטור b,
[02:32:55 - 02:32:55] ועוד קבוע,
[02:32:55 - 02:33:01] אז זו נוסחה כללית של תשוואה ריבועית וקטורית ב-x,
[02:33:02 - 02:33:08] אז אם יש לנו התפלגות שהיא פרופורציונית לאקספוננט של נוסחה ריבועית,
[02:33:09 - 02:33:10] אז p של x הוא גרסיאן,
[02:33:11 - 02:33:21] ואיך שוכחנו את זה? זה פשוט הראינו שאנחנו יכולים לעשות את הטריק הזה של השלמה לריבוע, כדי להביא את הצורה הזאת לצורה של הגרסיאן שאנחנו מכירים,
[02:33:21 - 02:33:25] שיש פה איזשהו וקטור שאנחנו רואים,
[02:33:25 - 02:33:27] הוא יהיה התוחלת שלנו, מיור,
[02:33:27 - 02:33:29] וכאן יש איזושהי מטריצה שהיא תהיה ה-precision-metrics,
[02:33:30 - 02:33:31] ההופכי של ה-covaryants.
[02:33:33 - 02:33:37] אוקיי, ראינו שבמקרה הכללי זה יוצא ה-covaryants או ה-precision-metrics
[02:33:38 - 02:33:40] והתוחלת מיורית.
[02:33:41 - 02:33:45] אוקיי, אז עכשיו אנחנו נחזור לשתי הטענות שאנחנו רוצים להוכיח, 2 ו-3.
[02:33:47 - 02:33:49] נוכיח, ואני מקווה שעכשיו יהיה יותר קל
[02:33:50 - 02:33:51] לכתוב
[02:33:52 - 02:33:55] על הלוח הזה.
[02:33:56 - 02:33:59] אז אוקיי, אז בעצם אנחנו רוצים להוכיח
[02:34:01 - 02:34:01] את
[02:34:07 - 02:34:09] בהינתן
[02:34:10 - 02:34:18] יש לנו את הוקטור הזה xa, שמחלק לשני חלקים, כמו שאמרנו xa זה ytb,
[02:34:18 - 02:34:21] והוא בעצמו ורסיאן רב-לומדי,
[02:34:22 - 02:34:24] שמים את זה ככה, ולוורסיאן שהתוחלת
[02:34:26 - 02:34:29] גם מתחלקת לשני חלקים, ua ו-bb,
[02:34:30 - 02:34:35] ומטריצת ה-covaryants מתחלקת לארבעה חלקים, אנחנו אמרנו,
[02:34:35 - 02:34:36] סיגמא a,
[02:34:37 - 02:34:44] סיגמא bb, ומחוץ לאלכסונים סיגמא a b וסיגמא b a.
[02:34:44 - 02:34:50] אוקיי, בהינתן שזה הגלוסיאן שאנחנו מתחילים ממנו,
[02:34:51 - 02:34:53] אנחנו רוצים להראות
[02:34:56 - 02:34:57] מיני דברים,
[02:35:00 - 02:35:03] אחד זה מה שקראנו לו טענה,
[02:35:03 - 02:35:06] טענה שלוש,
[02:35:06 - 02:35:14] זה ש-p של xb בהינתן xa
[02:35:15 - 02:35:16] הוא גם גרסיאן,
[02:35:22 - 02:35:30] והטענה השנייה שנוכיח שקראנו לה 2 זה שההתפלגות השולית של xa
[02:35:31 - 02:35:32] גם גרסיאן.
[02:35:36 - 02:35:40] אוקיי,
[02:35:41 - 02:35:42] אז זה מה שאנחנו רוצים להראות,
[02:35:43 - 02:35:45] ועכשיו, אז מה התוכנית שלנו?
[02:35:45 - 02:35:46] טוב, כאן ב-plan.
[02:35:50 - 02:35:51] אנחנו בעצם נעשה
[02:35:52 - 02:35:53] שני צעדים.
[02:35:59 - 02:36:01] קודם כל אנחנו נחשב
[02:36:02 - 02:36:08] xp של xb בהינתן xa
[02:36:11 - 02:36:12] על ידי זה
[02:36:15 - 02:36:19] שמתייחס לכל מקום שאנחנו נשאר xa מופיע בתור איזשהו קבוע.
[02:36:20 - 02:36:25] זו המשמעות שאנחנו מתנים על xa, אנחנו מתייחסים ל-xa בתור קבוע, הוא כבר מפסיק להיות משתנה מקרה.
[02:36:31 - 02:36:37] אוקיי, אז זה הדבר הראשון שנעשה, והדבר השני שנעשה
[02:36:38 - 02:36:38] זה
[02:36:41 - 02:36:42] נחשב
[02:36:43 - 02:36:47] p של xa בתור
[02:36:49 - 02:36:57] דבר זה איזושהי פונקציה של xa כפול האינטגרל של p של xb
[02:36:59 - 02:37:00] בהינתן xa
[02:37:01 - 02:37:06] אינטגרל על כל הערכים האפשריים,
[02:37:07 - 02:37:09] בגלל שהדבר הזה זה התפלגות,
[02:37:10 - 02:37:12] אז אם אנחנו עושים אינטגרל על כל הערכים האפשריים של xb,
[02:37:13 - 02:37:17] הדבר הזה בעצם שווה אחד, וכל מה שאנחנו נשארים איתו זה ההתפלגות של xa.
[02:37:18 - 02:37:20] בעצם כשאנחנו מחשבים את ההתפלגות הזאת אנחנו צריכים לראות,
[02:37:21 - 02:37:25] כשנחשב אותה אנחנו נתייחס לדברים של xa בתור קבועים,
[02:37:26 - 02:37:30] אנחנו נרצה לזכור מה הקבועים האלה כי הקבועים האלה ייכנסו לתוך
[02:37:31 - 02:37:34] ההתפלגות הזאתי אחר כך, כשנעשה את האינטגרציה הזאתי
[02:37:35 - 02:37:39] כל ההתפלגות הזאת בעצם תיעלם וכל מה שנשאר, כל האיברים שרצויים ב-XA
[02:37:40 - 02:37:43] הם מה שייכנסו לנו לתוך ההתפלגות הזאתי,
[02:37:43 - 02:37:53] השולבית של XA. אוקיי, אז זאת התוכנית שלנו ואנחנו נעשה עכשיו קודם כל איזשהו פישוט
[02:37:55 - 02:37:56] קצת בדאטה
[02:37:56 - 02:38:02] אז אנחנו רוצים להיפטר מהתוכלות
[02:38:05 - 02:38:09] הם סתם יסבכו את החישובים
[02:38:10 - 02:38:16] בעצם אפשר לחשוב על התוכלות זה בעצם נקודת ההתחלה מי הוא
[02:38:19 - 02:38:22] חלום X ו-Y התכוונתי אותו כאן
[02:38:26 - 02:38:34] מי הוא A ו-B אז מי הוא התוכלת שלנו של ההתפלגות
[02:38:35 - 02:38:45] היא בעצם נקודת המרכז של הגרסיאן הזה וזה לא כל כך משנה אם נקודת המרכז היא באמת באיזשהו מקום מסוים או באפס אנחנו יכולים
[02:38:46 - 02:38:47] להתייחס לנקודת המרכז בתור אפס
[02:38:48 - 02:38:53] וכל מה שיצא לנו בסופו של דבר נזיז את זה לאן שזה יצא בסופו של דבר
[02:38:54 - 02:38:55] אוקיי אז אולי דרך יותר
[02:38:56 - 02:38:58] יותר מדויקת
[02:38:59 - 02:39:05] לכתוב את הדבר הזה זה שאנחנו נגדיר משתנה מקרי חדש Y שהוא שווה X פחות מי הוא
[02:39:05 - 02:39:08] אז אם יש לנו איזושהי תוכלת שהיא לא אפס
[02:39:09 - 02:39:12] אנחנו נזיז את זה מההתפלגות
[02:39:14 - 02:39:16] ובסופו של דבר אנחנו נחזיר את זה אחר כך
[02:39:17 - 02:39:22] אוקיי אז זה יעזור לנו קצת בחישובים כי לא נצטרך לגרור את המי הוא איתנו כל הזמן
[02:39:23 - 02:39:26] אוקיי אז עכשיו בעצם מה התוכנית אנחנו רוצים
[02:39:26 - 02:39:36] לחשב את P של XB בלתיים XA והדרך שנעשה את זה נסתכל על מה יש לנו בתוך האקספוננט, נסתכל על המרחק מעל הנוביס
[02:39:37 - 02:39:42] ונחשב אותו נראה איזה איברים יש שם וננסה להפריד את כל האיברים שתלויים ב-XB
[02:39:44 - 02:39:50] אוקיי אז המרחק מעל הנוביס שווה ל-Y
[02:39:51 - 02:39:55] נקרא לזה עכשיו Y פונספוז A Y אז תזכרו למה יש לי פה Y
[02:39:55 - 02:39:57] ולמה אין לי מיואם
[02:40:06 - 02:40:19] סליחה זה לא YA זה Y Delta Y זה בעצם ה-Precision Metrics זה ההופכי של הקובריאנס
[02:40:19 - 02:40:24] ואין לי כאן מיואם כי הגדרתי את Y בתור X מינוס מיואם אז זה מפשוט למצב את העניין הזה
[02:40:25 - 02:40:30] אז זה מה שיש לנו כאן במרחק מעל הנוביס
[02:40:34 - 02:40:40] ובוא נכתוב את זה בצורה מפורשת עכשיו עם הפירוק שעשינו לשני החלקים אז יש לנו את Y A ו-Y B
[02:40:42 - 02:40:45] כי Y מורכב משני החלקים שאנחנו רוצים להפריד אותם
[02:40:47 - 02:40:53] במטריצה ההופכית הזאת, ה-Precision Metrics אנחנו נקרא לחלקים של ה-Delta A
[02:40:55 - 02:40:55] Delta B
[02:40:56 - 02:40:59] והעבר הזה אני אפשט אותו קצת נקרא לו B פשוט
[02:41:00 - 02:41:01] B ו-Bet-Morz-Pose
[02:41:02 - 02:41:07] ופה יש לנו שוב פעם YA ו-YB
[02:41:09 - 02:41:11] אוקיי ועכשיו אנחנו צריכים לעשות
[02:41:14 - 02:41:17] יש לנו כאן וקטור, מטריצה ווקטור
[02:41:18 - 02:41:19] בואו נחשב
[02:41:19 - 02:41:22] את המכפלה הוקטורית הזאת קודם לשני אלה
[02:41:25 - 02:41:34] זה נשאר דבר Y B וכאן יש לנו את השורה הזאתי כפול העמודה הזאת
[02:41:35 - 02:41:39] אנחנו מקבלים Delta A Y A
[02:41:40 - 02:41:50] ועוד B Y B ולמטה אנחנו מקבלים B Transpose Y A
[02:41:51 - 02:41:55] ועוד Delta B Y B
[02:41:57 - 02:41:59] אז עשינו את המכפלה הזאת
[02:42:00 - 02:42:02] ועכשיו נעשה את המכפלה הזאת
[02:42:06 - 02:42:09] עכשיו יש לנו ארכה אנשים בטרנספורט שצריך להיות
[02:42:10 - 02:42:12] אז יש לנו הדבר הזה בעצם שורה,
[02:42:13 - 02:42:15] אז השורה הזאת כפול העמודה הזאת
[02:42:16 - 02:42:19] קודם האיבר הראשון הוא כפול שני איברים פה
[02:42:19 - 02:42:22] והאיבר השני Y B יש לי כפול שני איברים כאן
[02:42:23 - 02:42:30] אז אנחנו מקבלים Y A Transpose Delta A Y A
[02:42:32 - 02:42:36] עכשיו האיבר השני Y A כפול B Y B מופיע פעמיים
[02:42:37 - 02:42:40] פעם פה ופעם ה-Y B הזה כפול B Transpose Y A
[02:42:41 - 02:42:44] זה Transpose אחד של השני אבל זה סקלארי זה לא משנה אותו דבר
[02:42:45 - 02:42:46] אנחנו מקבלים ועוד שני
[02:42:50 - 02:43:06] Y B B Transpose B Transpose Y A ועוד האיבר שנשארנו זה ה-Y B הזה כפול מה שיש כאן שזה
[02:43:07 - 02:43:13] Y B Delta B Y B Transpose B
[02:43:14 - 02:43:15] אוקיי
[02:43:15 - 02:43:21] אז עכשיו כבר אנחנו רואים שאם אנחנו קובעים את Y A להיות קבוע
[02:43:22 - 02:43:27] אנחנו מסתכלים על המשוואה הזאת רק כפונקציה של Y B אז זו משוואה ריבועית כללית
[02:43:28 - 02:43:32] ובעצם אנחנו כבר עכשיו יודעים שמה שזה אומר לפי מה שראינו קודם
[02:43:33 - 02:43:36] ש-B בהינתן A
[02:43:37 - 02:43:43] Y B בהינתן Y A זאת התפלגות גאוסיאנית
[02:43:43 - 02:43:51] אוקיי אבל אנחנו בואו נסתכל רק באמת ממש לחשב את ה-Covarיאנס ואת התוחלת של ההתפלגות הזאת
[02:43:51 - 02:43:59] וגם אנחנו נצטרך לראות איזה איברים של Y A נשארים לנו בשביל ההתפלגות השולית של Y A ו-X A
[02:44:00 - 02:44:04] אוקיי אז בעצם איזה איברים יש לנו כאן שתלויים ב-B
[02:44:05 - 02:44:06] אז זה שני האיברים האלה
[02:44:07 - 02:44:08] נקרא להם רגע כוכבית
[02:44:08 - 02:44:14] ועכשיו אנחנו יכולים לחשב איזה שכוכבית
[02:44:19 - 02:44:24] ולעשות את ההשלמה לריבוע כמו שאמרתי קודם, אז הכוכבית שווה
[02:44:24 - 02:44:28] קודם כל את האיבר הריבועי ב-YB
[02:44:29 - 02:44:31] ב-YB, Delta B
[02:44:32 - 02:44:35] ועוד
[02:44:36 - 02:44:40] אז עכשיו יש לי כאן את האיבר הזה אבל אני אוסיף לו כבר את
[02:44:44 - 02:44:46] Delta ו-Delta מינוס 1
[02:44:47 - 02:44:49] כמו שעשינו בהשלמה לריבוע אז יש לנו כאן
[02:44:50 - 02:44:55] שני Y B Transpose Delta B
[02:44:57 - 02:44:59] Delta B במינוס 1
[02:45:00 - 02:45:03] B Transpose Y A
[02:45:04 - 02:45:07] יש לנו כאן קצת יותר איברים ממה שהיה לנו קודם אבל הדבר הזה
[02:45:08 - 02:45:10] הוא בעצם וקטור קבוע מבחינתנו
[02:45:11 - 02:45:13] ואז זה בדיוק כמו שראינו קודם
[02:45:14 - 02:45:16] ועכשיו אנחנו נעשה את
[02:45:18 - 02:45:25] הקטע הקצת יותר מורכב של ההשלמה לריבוע שזה להוסיף את האיבר ולהוריד את האיבר יש לנו כאן בעצם YB
[02:45:27 - 02:45:29] שמופיע כאן בריבוע עם התדמית הריבועית הזאת
[02:45:29 - 02:45:30] ופה יש לנו את המכפלה
[02:45:31 - 02:45:31] בין
[02:45:32 - 02:45:36] האיברים של YB לאיבר השני בתוך הסוגריים
[02:45:37 - 02:45:43] זה הדבר הזה ועכשיו אנחנו צריכים את האיבר שמופיע, האיבר הריבועי
[02:45:44 - 02:45:46] של מה שמופיע כאן
[02:45:47 - 02:45:51] אז אנחנו צריכים לכתוב את כל הדבר הזה פעמיים פלוס Delta באמצע אז זה יהיה קצת ארוך
[02:45:52 - 02:45:54] בואו נכתוב את זה
[02:45:55 - 02:45:57] אז זה יוצא לנו
[02:45:58 - 02:46:01] Y A Transpose B
[02:46:02 - 02:46:08] A, סליחה Delta B במינוס 1, Delta B,
[02:46:09 - 02:46:12] Delta B במינוס 1, שוב פעם,
[02:46:14 - 02:46:17] B Transpose Y A,
[02:46:18 - 02:46:20] אז אספנו את האיבר הזה, אנחנו צריכים גם אז להוריד אותו,
[02:46:22 - 02:46:23] פחות
[02:46:24 - 02:46:31] בדיוק אותו דבר Y A Transpose
[02:46:32 - 02:46:54] B B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. A. אוקיי, אז קיבלנו את הביטוי
[02:46:54 - 02:46:56] הארוך הזה,
[02:46:56 - 02:47:02] ובעצם מה שנכנס לתוך ההתפלגות
[02:47:03 - 02:47:06] של yb זה שלושת האיברים הראשונים כאן,
[02:47:06 - 02:47:09] זה בעצם מה שיוצא לנו שאנחנו נפתח את הסוגריים
[02:47:09 - 02:47:10] למה שנראה כמו גאוסיאן.
[02:47:11 - 02:47:12] בואו נכתוב את זה בעצם,
[02:47:13 - 02:47:16] ששלושת האיברים האלה,
[02:47:17 - 02:47:25] בסך הכל אנחנו יכולים לכתוב אותם בצורה שאנחנו רגילים להיות גאוסיאן, שיש לנו yb פחות
[02:47:26 - 02:47:29] איזשהו קבוע שזה יהיה התוחלת של yb בעצם,
[02:47:30 - 02:47:36] אז זה יהיה delta b מינוס 1 b טרנספוס y a,
[02:47:38 - 02:47:48] אז כל זה טרנספוס כפול המטריצת פרסיזן שלנו שאנחנו יכולים לנזור אותה לפי מה שמופיע כאן בתדמית הריבועית, זה פשוט
[02:47:49 - 02:47:50] delta b
[02:47:56 - 02:47:58] כפול אותו סוגריים של הצד השני,
[02:47:59 - 02:48:01] זה yb פחות
[02:48:02 - 02:48:08] delta b במינוס 1 b טרנספוס y a.
[02:48:10 - 02:48:16] אז החלק הזה בתוך האקספוננט זה החלק שנראה כמו דרסיאן שאנחנו מכירים, התבנית הריבועית הזאת,
[02:48:18 - 02:48:24] וזה בעצם מה שמגדיר לנו את ההתפלגות של
[02:48:24 - 02:48:28] yb בהינתן ya.
[02:48:29 - 02:48:31] בואו רק נוודא שאנחנו מבינים
[02:48:32 - 02:48:35] מה התוחלת והקוברנט שיוצאים לנו כאן,
[02:48:38 - 02:48:38] אז
[02:48:41 - 02:48:41] תוחלת
[02:48:43 - 02:48:50] תוחלת של yb בהינתן ya,
[02:48:51 - 02:48:54] זה בעצם מה שאנחנו רואים כאן,
[02:48:55 - 02:49:06] זה delta b במינוס 1 b טרנספוס y a.
[02:49:10 - 02:49:19] תזכרו שהיה לנו את התוחלות של האיקסים שהעלמנו אותן, עכשיו אמרנו שבסוף נחזיר אותן, אז בואו נחזיר אותן כבר שנראה מה יוצא לנו,
[02:49:19 - 02:49:26] אז תוחלת של מה שרצינו במקור של xb בהינתן xa.
[02:49:27 - 02:49:29] זה אותו דבר, כשאנחנו צריכים להוסיף את התוחלות
[02:49:30 - 02:49:36] של xa ו-xb, אז יש לנו את מיובי,
[02:49:37 - 02:49:38] ועוד מה שיצא לנו קודם,
[02:49:39 - 02:49:43] שזה delta b במינוס 1 b טרנספוס,
[02:49:44 - 02:49:48] כשאני כל כך לכתוב ya אני אכתוב כאן xa פחות
[02:49:48 - 02:49:49] מיוב.
[02:49:51 - 02:49:52] אוקיי, אז זהו,
[02:49:52 - 02:49:54] רצינו את התוחלת של ההתפלגות המותנית
[02:49:55 - 02:49:57] של xb בהינתן xa.
[02:49:58 - 02:49:59] ומה לגבי ה covariance?
[02:50:00 - 02:50:02] אז ה covariance
[02:50:06 - 02:50:06] של
[02:50:09 - 02:50:10] yb בהינתן
[02:50:11 - 02:50:12] ya
[02:50:13 - 02:50:15] לא משנה, הזזה של התוחלות היא לא כל כך משנה,
[02:50:15 - 02:50:20] אז זה בעצם אותו דבר כמו ה covariance של xb בהינתן xa.
[02:50:23 - 02:50:28] ואנחנו יודעים שזה ההפך מההפכי של הפרסיזן מטריקס, זאת אומרת זה
[02:50:28 - 02:50:35] delta b במינוס 1. אוקיי, אז עכשיו אם אנחנו רוצים לחשב את delta b
[02:50:36 - 02:50:38] במינוס 1,
[02:50:39 - 02:50:44] אנחנו רוצים לבטא את זה בצורה של המטריצת covariance,
[02:50:45 - 02:50:45] ולא של הפרסיזן,
[02:50:46 - 02:50:49] את התקובה האנס המקורית שלנו של x. אז נסתכל רגע,
[02:50:49 - 02:50:50] פה היה לנו בדיוק את
[02:50:54 - 02:50:55] הנוסחה שיצאה לנו,
[02:50:56 - 02:50:59] אז שימו לב שאנחנו מקבלים בדיוק את מה שאנחנו רואים כאן,
[02:50:59 - 02:51:02] רק הפוך בעצם, אנחנו עוברים לכיוון ההפוך
[02:51:03 - 02:51:04] מהסיגמות
[02:51:06 - 02:51:08] לדלתא שלנו.
[02:51:10 - 02:51:12] סליחה, פה זה מסיגמות לדלתא, אנחנו רוצים לעבור
[02:51:13 - 02:51:14] מדלתא
[02:51:14 - 02:51:17] מדלתא ל...
[02:51:19 - 02:51:19] לא, סליחה, זה אותו כיוון.
[02:51:20 - 02:51:23] כן, הדלתא מינוס 1 זה בדיוק כמו שכתוב כאן בסוגריים,
[02:51:24 - 02:51:29] אוקיי? אז Sigma bb פחות Sigma ba, Sigma aa מינוס 1, Sigma bb,
[02:51:29 - 02:51:31] נכון, טוב על זה שיהיה כתוב.
[02:51:35 - 02:51:42] בעצם כל מה שכתוב כאן אני בעצם אוסיף ל-PDF של המצגת שיהיה לכם.
[02:51:43 - 02:52:02] אז מה קיבלנו? קיבלנו בדיוק מה שכתוב שם בנוסחה, זה Sigma bb פחות Sigma ba Sigma aa במינוס 1 Sigma bb.
[02:52:05 - 02:52:09] אוקיי, אז יש לנו ממש את ה-covariance של yb בינתיים ya לפי,
[02:52:10 - 02:52:12] כפונקציה של ה-covariance המקורית שהייתה לנו.
[02:52:16 - 02:52:18] אוקיי, אפשר גם להיפטר דרך אגב פה המבט אוכלת
[02:52:20 - 02:52:21] מהס
[02:52:23 - 02:52:26] דלתא הזאת, אם אנחנו גם את זה רוצים לבטא
[02:52:27 - 02:52:31] רק בעזרת סיגמאות במקום בעזרת ה-pecision-matrix.
[02:52:34 - 02:52:36] חישוב קצת מעיק, אני לא אראה את כל החישוב,
[02:52:37 - 02:52:41] אני רק אכתוב את זה פה, זה יוצא לתוכלת של xb
[02:52:41 - 02:52:42] כאן xa.
[02:52:44 - 02:52:46] אני אכתוב את זה ככה, זה יוצא
[02:52:51 - 02:52:53] mb, כמו שאנחנו קיבלנו קודם,
[02:52:54 - 02:52:57] Sigma Sigma
[02:53:00 - 02:53:01] mb
[02:53:03 - 02:53:03] ועוד
[02:53:04 - 02:53:06] Sigma
[02:53:10 - 02:53:20] mb כפול Sigma mn-1 כפול xa מינוס mn.
[02:53:22 - 02:53:23] אוקיי,
[02:53:24 - 02:53:31] בזה בעצם סיימנו להוכיח את הטענה הראשונה שלנו שקראנו את הטענה הראשונה שלנו שקראנו את הטענה הראשונה שלנו כאן,
[02:53:31 - 02:53:32] סיימנו להוכיח
[02:53:33 - 02:53:34] את הטענה הראשונה.
[02:53:37 - 02:53:41] אוקיי, אז תזכרו מה התוכנית, אנחנו הוכחנו את זה, ואנחנו עדיין רוצים גם לראות
[02:53:42 - 02:53:46] שההתפלמות השולית של xa היא גם גאוסיאן.
[02:53:47 - 02:53:49] אז בואו נזכור רגע מה הייתה התוכנית שלנו,
[02:53:50 - 02:53:50] חישבנו את זה,
[02:53:51 - 02:53:53] עכשיו אנחנו נעשה אינטגרל של הדבר הזה.
[02:53:54 - 02:53:55] מה זה אומר לעשות אינטגרל של התפלמות?
[02:53:56 - 02:53:58] זאת אומרת שהאינטגרל הזה פשוט ייעלם,
[02:53:59 - 02:53:59] אוקיי?
[02:54:00 - 02:54:01] יישאר כאן איזשהו,
[02:54:01 - 02:54:03] אנחנו צריכים לדעת בדיוק מה ההתפלגות הזאת,
[02:54:03 - 02:54:06] כדי שהאינטגרל הזה יעבור,
[02:54:07 - 02:54:07] יהפוך לאחד.
[02:54:08 - 02:54:15] כל האיברים שנשארו מחוץ להתפלגות הזאת שתלויים ב-xa בעצם ייכנסו לתוך
[02:54:16 - 02:54:17] ההתפלגות של xa.
[02:54:17 - 02:54:20] ככה נדע איך נראית ההתפלגות השולית של xa.
[02:54:21 - 02:54:23] בואו נראה איזה איברים היו לנו סך הכל.
[02:54:25 - 02:54:29] אז אנחנו נעבור את כאן שזה מסודר.
[02:54:30 - 02:54:41] אז אהההההההההההההה להוכיח את שתיים.
[02:54:41 - 02:54:42] אנחנו רוצים לקחת
[02:54:51 - 02:54:56] כל האיברים שלנו באקספוננט המקורי שתלויים ב...
[02:54:59 - 02:55:15] אז כל מה שתלוי ב-YA
[02:55:19 - 02:55:19] ולא נמצא
[02:55:24 - 02:55:27] בהתפרגות שחישבנו של XB בהינתן
[02:55:29 - 02:55:31] נכנסנו ב-Y, אז נמצא את זה ב-YB
[02:55:38 - 02:55:43] אוקיי, אז בואו נסתכל איזה איברים היו לנו שם
[02:55:47 - 02:55:51] אז האיבר הראשון שתלם ממנו כי התייחסנו לזה בתור קבוע זה האיבר הזה
[02:55:52 - 02:55:53] התבנית הריבועית הזאת
[02:55:54 - 02:55:58] כל זה, כל מה שיש לנו כאן נכנס לנו לתוך ההתפלגות של B
[02:55:58 - 02:55:59] אנחנו לא רוצים להתייחס אליו
[02:56:00 - 02:56:02] אז האיבר השני שנוסף לנו זה מה שמופיע כאן
[02:56:03 - 02:56:04] זה בעצם שני האיברים האלה
[02:56:05 - 02:56:07] האיבר הזה והאיבר הזה
[02:56:09 - 02:56:09] מתיוספים לנו
[02:56:10 - 02:56:11] אנחנו מקבלים פה
[02:56:19 - 02:56:21] FyA
[02:56:23 - 02:56:26] שווה ל-YA פרנספוז
[02:56:28 - 02:56:29] Delta A
[02:56:30 - 02:56:33] Delta A פחות
[02:56:34 - 02:56:42] Delta B Delta B במינוס 1 B טונספוז
[02:56:43 - 02:56:45] YA
[02:56:50 - 02:56:51] שוב,
[02:56:51 - 02:56:53] מאיפה פתחתי את הדברים האלה? האיבר הראשון
[02:56:54 - 02:56:55] זה האיבר
[02:56:56 - 02:56:56] שהיה פה,
[02:56:57 - 02:56:58] בהתחלה התעלמנו ממנו כבר,
[02:56:59 - 02:57:10] והאיבר השני זה מה שנוסף לנו כשעשינו את ההשלמה לריבוע בשביל לחשב את Y במינתיים את B.A זה מה שמופיע כאן, קצת פישטתי את זה, אוקיי? כי יש לנו פה,
[02:57:11 - 02:57:14] פה אנחנו לא צריכים בדיוק את התבנית הזאתי, אז אפשר להתעלם,
[02:57:15 - 02:57:19] אפשר לאחד את שתי הדלקאות האלה שהן הופכי, אחת של השנייה,
[02:57:20 - 02:57:21] וזה פשוט להתעלמם.
[02:57:23 - 02:57:25] אוקיי, אז זה מה שמופיע לנו כאן,
[02:57:26 - 02:57:31] וכבר פה אנחנו רואים שמה שמופיע לנו זה שוב פעם נוסחה ריבועית ב-YA,
[02:57:32 - 02:57:39] כלומר בעצם הוכחנו עכשיו שההתפלגות השולית של YA היא גם לא סיימתי,
[02:57:40 - 02:57:47] אוקיי? אבל בוא נחשב שוב מה יצא בדיוק התוחלת שלנו וה-Covariance,
[02:57:47 - 02:57:47] אז
[02:57:49 - 02:57:53] אפשר לכתוב את, לאחד את שני ה... שני הדברים האלה זה תבנית ריבועית,
[02:57:53 - 02:57:54] יש לנו פה
[02:57:55 - 02:57:57] מטריצה אחת, וכאן יש לנו מטריצה אחרת.
[02:57:58 - 02:58:00] אנחנו יכולים פשוט לאחד את שתי המטריצות האלה,
[02:58:01 - 02:58:03] מה שכתוב לנו כאן זה YA טרנספוז
[02:58:04 - 02:58:06] כפול המטריצה שהיא
[02:58:07 - 02:58:08] Delta A פחות
[02:58:09 - 02:58:12] B Delta B במינוס 1
[02:58:13 - 02:58:14] B טרנספוז
[02:58:17 - 02:58:18] כפול YA.
[02:58:18 - 02:58:23] אז יש לנו תבנית ריבועית שהמטריצה באמצע נראית ככה.
[02:58:25 - 02:58:28] זה יהיה בעצם ה-Precision מטריקס של ההתפלגות השולית
[02:58:29 - 02:58:32] של YA. ומה התוחלת
[02:58:33 - 02:58:34] של YA?
[02:58:35 - 02:58:37] התוחלת היא 0. אוקיי? לא נוסף לנו פה שום איבר התוחלת.
[02:58:40 - 02:58:41] אני יכול לדבר יותר מפורשת.
[02:58:43 - 02:58:43] התוחלת
[02:58:44 - 02:58:44] של
[02:58:46 - 02:58:47] YA,
[02:58:48 - 02:58:51] תוחלת של XA, בעצם בואו נעבור כבר ישר לתוחלת של XA,
[02:58:52 - 02:58:53] שזה מה שרצינו במקור,
[02:58:54 - 02:58:58] לתוחלת של YA היא 0. ואנחנו צריכים רק להוסיף את התוחלת המקורית שהייתה לנו של XA,
[02:58:59 - 02:59:07] שזה New A. אוקיי? זו תוחלת של XA ל-New A. וה-Covarience, שזה גם אותו דבר,
[02:59:07 - 02:59:10] קוויריאנס של YA או של XA, אז ה-Covarience של
[02:59:11 - 02:59:12] YA
[02:59:16 - 02:59:18] ולהופכי של מה שכתוב כאן.
[02:59:19 - 02:59:22] זאת אומרת זה Delta A פחות
[02:59:23 - 02:59:28] B Delta A A P Transpose
[02:59:29 - 02:59:30] כל זה בנוסחה.
[02:59:34 - 02:59:35] אם שוב פעם נסתכל בנוסחה
[02:59:37 - 02:59:38] קטענו כאן
[02:59:41 - 02:59:45] ושוב אנחנו מקבלים נוסחה שהיא מאוד דומה, לאיבר הזה אנחנו קראנו B
[02:59:46 - 02:59:48] ועכשיו זה הכיוון ההפוך,
[02:59:48 - 02:59:52] אז בעצם פה מופיע הדלקאות, אנחנו רוצים לחזור לסיגמא,
[02:59:52 - 02:59:56] אז בדיוק כתוב אותנו כאן מסחר רק עם בינקאות, אז ההופכי של זה זה פשוט יהיה Sigma A,
[02:59:58 - 03:00:01] סליחה אנחנו ב-A, ההופכי של זה פשוט יהיה Sigma של A.
[03:00:05 - 03:00:05] זה פשוט תהיה
[03:00:06 - 03:00:13] מה שקראנו לו Sigma A A. אז קיבלנו תוצאה מאוד פשוטה,
[03:00:14 - 03:00:17] שהתוחלת של XA זה פשוט,
[03:00:18 - 03:00:21] התוחלת השולית של XA זה פשוט החלק מהתוחלת הכללית
[03:00:22 - 03:00:25] שהוא מתאים לאיברים
[03:00:25 - 03:00:26] של A,
[03:00:28 - 03:00:34] וה-Covarience של YA זה פשוט החלק המפריצת covariance שמתאים ל-A,
[03:00:35 - 03:00:36] אוקיי? אז בעצם
[03:00:37 - 03:00:39] עשינו דרך מאוד ארוכה והגענו לתוצאה שהיא
[03:00:40 - 03:00:47] מאוד פשוטה והיא מאוד הגיונית, וזו אחת מהסיבות שקל לנו לעבוד עם התפלגות גאוסיאנית,
[03:00:48 - 03:00:49] בסופו של דבר אם אנחנו,
[03:00:49 - 03:00:51] יש לנו התפלגות על איזשהו וקטור אבל מעניין אותנו
[03:00:51 - 03:00:52] רק
[03:00:52 - 03:00:55] להסתכל על חלק מהווקטור אנחנו יכולים
[03:00:58 - 03:01:06] אם יש לנו התפלגות על וקטור ומעניין אותנו להסתכל רק על חלק מהווקטור, רק על XA אנחנו יכולים להתעלם מכל השאר לקחת את החלק הרלוונטי בתוחלת
[03:01:07 - 03:01:10] ואת החלק הרלוונטי במטריצת הקוברץ
[03:01:15 - 03:01:17] אז זהו לגבי השיעור הזה,
[03:01:17 - 03:01:22] ואתם בתרגיל תפריכו את המשפט הראשון שמופיע כאן
[03:01:23 - 03:01:27] בשיטה מאוד דומה, מה שראינו עוד עכשיו, אז אני מקווה שזה יעזור לכם לפתור את זה,
[03:01:28 - 03:01:30] אם השאלות מוזמנים לשלוח לי אימייל
[03:01:31 - 03:01:34] וניפגש עוד שלושה שבועות.