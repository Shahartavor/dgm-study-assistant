[00:00:00 - 00:00:04] ‫עכשיו, בואו נתחיל, בסדר, ‫לדבר אמר לכיתה הזאתי באופן קבוע.
[00:00:05 - 00:00:08] ‫אמרתי שיש לך הודעה, ‫אני חושב שזה לא קרה.
[00:00:09 - 00:00:11] ‫אמת שזו כיתה שהיא יותר טובה,
[00:00:12 - 00:00:12] ‫אני חושב,
[00:00:13 - 00:00:17] ‫לגבי ההקלטה אנחנו נראה.
[00:00:18 - 00:00:21] ‫זה יותר טוב או פחות, ‫שזה פשוט יותר טוב.
[00:00:22 - 00:00:28] ‫טוב, אנחנו עדיין עמוק ‫בתוך החלק התיאורטי של הקורס.
[00:00:30 - 00:00:31] ‫חזיקו מעמד עוד קצת,
[00:00:32 - 00:00:36] ‫ואז זה ייגמר. ‫הפחות זה שהיום, בשבוע הבא,
[00:00:37 - 00:00:52] ‫יהיה עדיין תיאורטי, ‫ואז אנחנו נתחיל יותר לדבר על מודלים ‫שאתם יכולים לנו להשוות, ‫בלי פחות דגש על תיאוריה ‫וביתוח של אלגוריתמים, ‫יותר על...
[00:00:53 - 00:00:56] ‫עדיין יהיה בשיעור עקרונות ‫והצגה של שיטות,
[00:00:56 - 00:01:02] ‫מתוך כוונה, אבל, ‫שיכולנו לממש את זה ‫בתרגילים אחת החובה האלה.
[00:01:02 - 00:01:05] ‫בעצם מה שדיברנו בשבוע שעבר,
[00:01:05 - 00:01:06] ‫ואני פשוט לדבר היום,
[00:01:08 - 00:01:13] ‫נתחיל להשאיר אותי פה בטוח ‫בשביל מישהו בכיתה, אבל יש רעש כאן.
[00:01:16 - 00:01:17] ‫בשבוע חמש דקות בעד ההסברות.
[00:01:18 - 00:01:23] ‫אז האמת שגם מה שדיברנו בשבוע שעבר ‫ונתחיל את היום,
[00:01:24 - 00:01:25] ‫זה אלגוריתם E.M.
[00:01:26 - 00:01:34] ‫היום נדבר גם על דגימת גיבס, ‫אני אדבר על זה, ‫גם אלגוריתמים מאוד מעשיים, כן? ‫שמשתמשים בהם הרבה במעשי.
[00:01:35 - 00:01:39] ‫שכבתי אפילו שבתרגיל הבא ‫אתם רשו את זה בצורה מעשית,
[00:01:39 - 00:01:42] ‫זו בעיה של ריכוי רעשים מתמונה,
[00:01:43 - 00:01:47] ‫אבל יכול להיות שזה יהיה יותר מדי לתרגיל, ‫אם ככה יש הרבה ‫תרגילים מעשיים.
[00:01:48 - 00:01:53] ‫אני לא בטוח, אני אעשה את זה. ‫אולי מה נעשה זה תרגיל בונוס שיהיה על זה.
[00:01:54 - 00:01:56] ‫כל מילה, אז אנחנו, איך שאנחנו,
[00:01:56 - 00:02:02] ‫איך שאני מלמד את זה בחלק הראשון, ‫זה יותר תיאורטי, ‫אבל זה גם אלגוריתמים ממש מעשיים, ‫שמשתמשים בהגינת.
[00:02:04 - 00:02:08] ‫אוקיי, אז אנחנו צריכים לסיים ‫את מה שדיברנו שבוע שעבר,
[00:02:08 - 00:02:14] ‫שזה היה אלגוריתם E.M. ‫וספציפית על מודל של תערובת גאוסיאנים,
[00:02:15 - 00:02:23] ‫קו של מיקס של מודל, שבאות ב-G.M.M. ‫-ואז אנחנו נדבר על שיטות דגימה.
[00:02:23 - 00:02:28] ‫טיפית, שנקרא TEMCMZ,
[00:02:28 - 00:02:29] ‫מרלימוד צ'יין מונדקאולו,
[00:02:31 - 00:02:32] ‫ואלגוריתם גיבס.
[00:02:33 - 00:02:35] ‫אני מקווה שנגיע לכל זה.
[00:02:35 - 00:02:36] ‫אוקיי,
[00:02:37 - 00:02:38] בואו נתחיל.
[00:02:39 - 00:02:42] ‫אז בעצם אני רוצה להיזכר מה התמונה הגדולה.
[00:02:42 - 00:02:49] ‫אני מקווה שהשתכנעתם שיש שם מודלים עם דברים חבועים.
[00:02:54 - 00:03:00] ‫אוקיי, אז אני מקווה שהשתכנעתם ‫שיש עניין במשתנים חבועים.
[00:03:03 - 00:03:04] ‫אנחנו רוצים בעצם למודל
[00:03:05 - 00:03:09] ‫לא רק התפלגות על כל ה-X שניתן לנו, אוקיי, X זה שם כללי ‫שאנחנו נותנים לדאטה שלנו,
[00:03:10 - 00:03:11] ‫אלא ל-X ו-Z,
[00:03:11 - 00:03:14] ‫באיזשהו משתנה חבועי שאנחנו לא רואים.
[00:03:15 - 00:03:16] ‫אני פשוט זוכר למה זה טוב,
[00:03:17 - 00:03:21] ‫לבנות את המודל ככה במקום לבנות ‫מודל על X, בצורה ישירה.
[00:03:22 - 00:03:23] מה זה נותן לנו?
[00:03:24 - 00:03:28] ‫-לעבוד עם גרסיאנים?
[00:03:30 - 00:03:30] ‫אז
[00:03:32 - 00:03:37] אפשר להגיד את זה ככה, ‫זה נותן לנו לעבוד עם גרסיאנים, ‫הם מודלים שאנחנו יודעים לעבוד איתם,
[00:03:38 - 00:03:39] ‫אבל שהמודל בסופו של דבר ‫הוא לא יהיה גרסיאן.
[00:03:40 - 00:03:43] ‫אנחנו רוצים שהמודל יהיה יחסית מורכב יותר מגרסיאן,
[00:03:44 - 00:03:46] ‫ואנחנו לא יודעים לעבוד עם מודלים מורכבים,
[00:03:47 - 00:03:51] ‫אז זה נותן לנו איזושהי אפשרות ‫לעבוד עם מודלים מורכבים,
[00:03:52 - 00:03:54] ‫בלי שאנחנו בעצם באופן ישיר ‫עובדים איתם,
[00:03:55 - 00:03:56] ‫אלא עוברים דרך
[00:03:57 - 00:04:04] משתנים חבויים, ‫שהכול במרחב הזה של המשתנים החבויים ‫והמשתנים שאנחנו רואים, ‫הוא כן מוגדר בתור גאוסיאן.
[00:04:07 - 00:04:20] ‫אוקיי, אז זה באופן כללי. ‫אנחנו נראה בעצם, יש עוד כמה מוטיבציות. ‫אחת, עוד מוטיבציות זה שאנחנו רוצים ‫שהמשתנה החבוי הזה ‫אולי יתפוס איזה משהו מעניין, סמנטי, על הדאטה שלנו, ‫שאחר כך נשתמש בו.
[00:04:20 - 00:04:22] ‫אז זה בעצם דרך שלנו להכניס ‫איזושהי מבניות
[00:04:23 - 00:04:24] ‫של תוך המודל,
[00:04:25 - 00:04:36] ‫ככה שאנחנו יכולים אחר כך ‫לאלץ את המודל לייצג את הדאטה ‫דרך המבנה הזה, ‫ואחר כך יכול להיות ‫שזה יכול להוביל לדברים שימושיים. ‫אם ראינו דוגמאות למשל,
[00:04:36 - 00:04:43] ‫שהמשתנה החבוי תופס את הספרה, ‫למשל, בתמונות של ספרות.
[00:04:44 - 00:04:44] ‫-כן,
[00:04:44 - 00:04:54] ‫כל מיני תכונות של שקלעות קטנים, ‫מישהו בגבר או אישה מחייך ‫או לא מחייך או לא מחייך, דברים כאלה.
[00:04:56 - 00:05:02] ‫אוקיי, אז אנחנו, הדוגמה שהתחלנו ממנה, ‫זו דוגמה שאנחנו יודעים ‫לעבוד עליה בצורה ישירה,
[00:05:03 - 00:05:04] ‫נקראת העובד הגאוסיאני,
[00:05:05 - 00:05:07] ‫ושם אנחנו, זה חצי דרך, ‫זה לא בדיוק.
[00:05:08 - 00:05:09] ‫נותן לנו את כל מה שאנחנו רוצים.
[00:05:11 - 00:05:14] ‫זאת אומרת, זה כן נותן לנו משפחה של,
[00:05:14 - 00:05:17] ‫התפלגויות שהיא מאוד עשירה ‫ויכולה בעצם לקרב כל התפלגות.
[00:05:18 - 00:05:20] ‫אז אנחנו יכולים באמת לסבך ‫את ההתפלגות שלנו
[00:05:21 - 00:05:24] כמה שאנחנו רוצים, ‫על-ידי תוספת של קומפוננטות,
[00:05:25 - 00:05:26] ‫דאטה-אורקט כזאת,
[00:05:26 - 00:05:35] ‫אבל זה לא נותן לנו כמה, ‫יש כמה דברים שזה לא נותן לנו. ‫למשל, זה עדיין אנחנו צריכים למדל ‫קובריאנס שלם על כל הדאטה שלנו,
[00:05:37 - 00:05:42] ‫שזה משהו שאנחנו, אם הדאטה מאוד גדול, ‫אנחנו בעצם זה ריבועי,
[00:05:42 - 00:05:45] ‫בספר המימד של הדאטה, ‫שזה בעייתי.
[00:05:46 - 00:05:47] ‫וזה גם,
[00:05:47 - 00:05:49] בעצם,
[00:05:50 - 00:05:57] ‫השיטה שאנחנו עובדים איתה, ‫בגלל שהיא שיטה ישירה, ‫היא עובדת רק כשאנחנו יכולים ‫לחשב בצורה יעילה,
[00:05:58 - 00:05:59] ‫תכף נראה את זה, את ההתפלגות
[00:06:01 - 00:06:04] ‫הפוסטריורית על ה-Data-Varial הזה, על המשתנה החבוי,
[00:06:04 - 00:06:08] ‫מה שאנחנו לא יכולים לעבוד ‫כשמספר הפונפוננטות הוא גדול מאוד.
[00:06:08 - 00:06:14] ‫אז יש עדיין איזשהו גבול ‫למתי אנחנו יכולים עדיין ‫לעבוד עם דבר כזה בצורה ישירה.
[00:06:15 - 00:06:19] ‫אוקיי, בואו נגיד את כל מה שאמרתי ‫בצורה קצת יותר פורמלית,
[00:06:20 - 00:06:24] ‫דרך האלגוריתם הזה שנקרא EM. ‫אז
[00:06:25 - 00:06:26] בעצם אמרנו, אנחנו יכולים לעשות,
[00:06:28 - 00:06:33] ‫לא נכתוב קודם, נזכר מה זה GMM, אוקיי? ‫אז GMM אמרנו שהוא
[00:06:38 - 00:06:47] X, איזה סכום, יש לנו כאן פרמטר כזה, ‫אז כאן פרמטר כזה, זה K שווה 1 ל-K גדול.
[00:06:48 - 00:06:53] ‫יש לנו איזושהי משקולת כזאתי ‫לכל אחת מהקומפוננטות, Pi K,
[00:06:53 - 00:06:58] ‫וכל אחת מהקומפוננטות היא בעצמה גאוסיאן, ‫אפשר להקריא בככה, גאוסיאן על X,
[00:06:59 - 00:07:00] ‫שיש לו את הפרמטרים.
[00:07:02 - 00:07:06] ‫לכל K, לכל קומפוננט יש פרמטרים אחרים, ‫יש מי הוא אחר, יש תוכלת אחרת,
[00:07:07 - 00:07:08] ‫ויש קובר אינס יותר.
[00:07:09 - 00:07:10] ‫זה מי הוא K וסיגמא הוא K.
[00:07:15 - 00:07:20] ‫אז עכשיו אנחנו רוצים, נגיד, בהינתן דאטה, ‫ללמוד את התערובת גאוסיאני הזאת.
[00:07:20 - 00:07:24] ‫אז הדרך שאנחנו מכירים עד עכשיו ‫זה לכתוב את הלוג לייטליות של זה,
[00:07:25 - 00:07:30] ‫ולעשות מקסימום של הלוג לייטליות ‫על ידי חישוב של הגרדיאנט.
[00:07:31 - 00:07:34] ‫אנחנו ראינו שהחישוב של הגרדיאנט ‫יוצא משהו מורכב, שאנחנו לא יכולים
[00:07:34 - 00:07:37] ‫לבודד שם את המשתנים, ‫את ה-mune וסיגמא וה-five,
[00:07:38 - 00:07:41] ‫ולכן אנחנו לא יכולים לחשב את זה בצורה אנליטית, ‫אבל עדיין אפשר לחשב עם gradient descent.
[00:07:43 - 00:07:45] ‫אז אפשר לחשב עם gradient descent.
[00:07:51 - 00:07:55] ‫אפשר לחשב MRE, ‫במקסימום לייטליות סימטר,
[00:07:56 - 00:07:57] ‫עם gradient descent,
[00:07:58 - 00:07:59] ‫אבל זה לא יעיל.
[00:07:59 - 00:08:01] ‫אבל פתאום כבר יש דרך יותר יעילה.
[00:08:02 - 00:08:04] ‫אבל יש דרך
[00:08:06 - 00:08:07] יותר יעילה.
[00:08:09 - 00:08:17] ‫זה E.M. אוקיי? ‫אז בואו נראה מה זה M. ‫שוב, זה אחר, זה אקספקטיישן.
[00:08:22 - 00:08:35] ‫אם לא תקציב לרמוד של M. ‫אם זה תמיד E.M. ‫זה הדבר הבא, אנחנו רוצים
[00:08:39 - 00:08:42] ‫למלך אינטרטיבי, אז בואו נכון לצדוק כאן. ‫-Gדול E.E.E.R.A.
[00:08:44 - 00:08:45] ‫זה נקרא E.S.T.E.
[00:08:48 - 00:08:56] ‫ולכתב פונקציה שקראנו לה Q. ‫-Teta, מה זה? זה השם שבכלל שלנו ‫בכל הפרמטרים שאנחנו מחפשים.
[00:08:56 - 00:09:00] ‫במקרה שבכלל של מיקשה מודל זה ה-Pi, ה-Mu והסיגמאות השונים.
[00:09:02 - 00:09:02] ‫ה-Q של תתא,
[00:09:03 - 00:09:04] ‫בהינתן
[00:09:05 - 00:09:06] התתא הקודם שהיה לנו,
[00:09:07 - 00:09:09] ‫זה שווה
[00:09:11 - 00:09:17] ‫לתוחלת של המשתנה שאנחנו לא רואים, ‫החבוי,
[00:09:19 - 00:09:20] ‫בהינתן
[00:09:23 - 00:09:24] ‫תינתן לדאטה,
[00:09:25 - 00:09:28] ‫V והתתא מהצהל הקודם.
[00:09:29 - 00:09:30] ‫תתא t-1,
[00:09:32 - 00:09:35] ‫אולי נחזרנו את t, t-1,
[00:09:36 - 00:09:37] ‫x נקרא לדאטה, t,
[00:09:40 - 00:09:43] ‫אז בצד הניחוש הנוכחי שלנו, ‫של תתא,
[00:09:43 - 00:09:46] ‫תא t, של לוג t,
[00:09:49 - 00:09:51] ‫זה פונקציה עם פרמטרים, תתא,
[00:09:53 - 00:09:56] ‫של x ו-z.
[00:09:59 - 00:10:05] ‫אז בעצם אפשר עכשיו על הצד הזה, ‫כי אנחנו בעצם משלימים את הדאטה שלנו ‫על ידי איזושהי תוחלת כזאת.
[00:10:06 - 00:10:07] ‫את ה-z שאנחנו לא רואים,
[00:10:08 - 00:10:11] ‫אנחנו משלימים על ידי תוחלת ‫שאנחנו מחשבים אותה לפי הפרמטר,
[00:10:12 - 00:10:14] ‫הניחוש הנוכחי שיש לנו על הפרמטרים,
[00:10:15 - 00:10:22] ‫ואז ה-q-step יוכחנו ה-q-step למקסימיזיישן,
[00:10:26 - 00:10:30] ‫ולמצוא את הארגומט על פני כל התטות
[00:10:31 - 00:10:31] ‫של הדבר הזה.
[00:10:32 - 00:10:32] ‫יש ה-q
[00:10:36 - 00:10:45] ‫בסדר, אז שוב, מה האינטואיציה של הדבר הזה? ‫אנחנו לא יודעים לחשב p של x,
[00:10:47 - 00:10:50] ‫זה משהו מורכב, אז הגדרנו אותו ‫על ידי כל מיני משתנה חבוי,
[00:10:51 - 00:10:51] ‫כרנו ל-z,
[00:10:52 - 00:10:53] ‫אנחנו כן יודעים לחשב,
[00:10:54 - 00:10:55] ‫pית התכשלית של z,
[00:10:56 - 00:10:57] ‫וזה יחסית
[00:11:00 - 00:11:02] ‫התפלגות שקל לנו לעבוד איתה,
[00:11:02 - 00:11:05] ‫שיש לה את כל הפרמטרים שאנחנו לא...
[00:11:07 - 00:11:09] סליחה, אנחנו רואים בעצם פה את כל המשתנים,
[00:11:10 - 00:11:13] ‫אם אנחנו רואים את כל המשתנים, ‫אז קל לנו לעבוד עם ההתפלגות הזאת,
[00:11:13 - 00:11:14] מה שרציתי להגיד.
[00:11:15 - 00:11:17] ‫אז עכשיו אנחנו בעצם לא רואים את ה-z,
[00:11:17 - 00:11:20] ‫אז אנחנו משלימים אותם ‫על ידי איזשהו ניחוש
[00:11:21 - 00:11:23] לתוכלת הזאת, ‫ואז אנחנו מומצאים את המקסימום.
[00:11:25 - 00:11:26] ‫אוקיי, אז מתי הדבר הזה יהיה יעיל?
[00:11:27 - 00:11:28] ‫אחרי כדאי לנו להשתמש ב-EM,
[00:11:34 - 00:11:35] ‫קודם כול, כשהבעיה היא קשה בראשה,
[00:11:35 - 00:11:37] ‫יש לנו כשל... לעשות...
[00:11:41 - 00:11:43] ‫כשקשה לעשות
[00:11:46 - 00:11:46] ‫אוטומיזציה
[00:11:51 - 00:11:51] ‫ללוג
[00:11:52 - 00:11:53] פטאטא X,
[00:11:55 - 00:11:56] ‫זה מה שאנחנו רוצים,
[00:11:59 - 00:11:59] ‫קל
[00:12:02 - 00:12:02] לעשות
[00:12:03 - 00:12:05] ‫אוטומיזציה
[00:12:11 - 00:12:12] ‫להתפלגות המלאה,
[00:12:13 - 00:12:20] ‫זה EZ. ‫טימיניזציה זה אקטימיניזציה של תטא, כן? ‫למצוא את התטא שממקסמת
[00:12:20 - 00:12:22] ‫את זה עבור איזשהו דאטה.
[00:12:23 - 00:12:24] ‫ואת משהו שלא אמרתי שבוע שעבר,
[00:12:24 - 00:12:28] ‫אבל אתם תראו מהאלגוריתם של...
[00:12:29 - 00:12:30] ‫אנחנו צריכים את זה גם.
[00:12:31 - 00:12:32] ‫כשאפשר
[00:12:36 - 00:12:36] ‫לחשב
[00:12:41 - 00:12:51] ‫את P של Z ניתן X. ‫אוקיי, שזה נקרא ה-Posterיאו על המשתנה החבורית.
[00:12:54 - 00:12:57] ‫בעצם זה הדבר שאנחנו עושים ‫לפיו את התופלת הזאת.
[00:12:58 - 00:13:04] ‫-D ו-X אותו דבר, D זה כל הדאטה שלנו, X. ‫פה שכתוב X זה הכוונה למשתנה חד,
[00:13:04 - 00:13:06] ‫לנקודה אחת בתוך הדאטה-סט שלנו,
[00:13:07 - 00:13:09] ‫ודי זה הכול, כל הדאטה-סט.
[00:13:12 - 00:13:12] ‫ברור?
[00:13:13 - 00:13:14] זה לא תובב?
[00:13:16 - 00:13:18] אתה מגדיר את ה-D, נכון?
[00:13:18 - 00:13:20] ‫אני מגדיר את ההתפלגות הזאתי.
[00:13:21 - 00:13:22] ‫אני מגדיר אותה ככה,
[00:13:23 - 00:13:26] ‫כי אני רוצה שההתפלגות הזאתי ‫תהיה משהו יותר טוב,
[00:13:26 - 00:13:30] ‫אבל אני מגדיר אותה ככה ‫שאני יודע לעבוד עם הדבר הזה,
[00:13:31 - 00:13:38] ‫אוקיי? ‫ואז אני יכול לעשות EM. ‫אם ההתפלגות הזאתי זה משהו ‫שאני יודע לעבוד איתו, ‫למשל לחשב את ה-Botterior ‫ולעשות איזו אופטימיזציה,
[00:13:39 - 00:13:41] ‫אז אני יכול לעשות לזה EM, ‫אפילו שאני לא רואה את ה-Z.
[00:13:48 - 00:13:52] ‫אוקיי, דרון הוכיח שה-OECD הזה עובד,
[00:13:52 - 00:13:56] ‫אז נבחר מה זה אומר ל-GNM של פעם.
[00:14:23 - 00:14:28] ‫דו שווה לוג P, תתא, תחי, מינוס פאדג'י.
[00:14:31 - 00:14:37] ‫זאת אומרת שאחרי שעברנו איתרציה ‫של האלגוריתם ועדכנו את התתא שלנו, ‫את הפרמטרים של המודל,
[00:14:37 - 00:14:40] ‫קיבלנו לייקליות שיכול להיות ‫רק יותר גבוה
[00:14:41 - 00:14:42] ‫ממה שהיה לנו קודם.
[00:14:46 - 00:14:51] אוקיי? אז זה עדיין, זה לא אומר שאנחנו ‫כל פעם משתפרים, זה עדיין לא אומר ‫שזה מתכנס למקסימום גלובלי,
[00:14:52 - 00:15:03] ‫וזה באמת לא מתכנס למקסימום גלובלי, אוקיי? ‫זה לא מתכנס למקסימום גלובלי.
[00:15:06 - 00:15:09] ‫אוקיי, אבל זה עדיין כמו gradient descent, בעצם,
[00:15:10 - 00:15:12] ‫מתכנס למקסימום לוקאלי,
[00:15:12 - 00:15:18] ‫אבל זה יותר יעיל מ-GNM, זאת אומרת, ‫הפעדים שאנחנו נעשה יהיו יותר גדולים, ‫השתפר יותר מהם.
[00:15:21 - 00:15:27] ‫בואו נוכיח את זה, אוקיי, ‫אז בואו נעשה את זה בפרק.
[00:15:29 - 00:15:33] ‫קודם נגדיר Q גל כזה.
[00:15:36 - 00:15:40] ‫נראה שזה כמו Q מוגדר שם,
[00:15:43 - 00:15:45] ‫רק שאנחנו מחזירים ממנו
[00:15:47 - 00:15:51] את האנטרופיה של
[00:15:51 - 00:15:55] ‫בהתפלגות הזאת. ‫בעצם אנחנו...
[00:15:59 - 00:16:04] ‫אני אעשה הכול עם סכומים, ‫למרות ש-Z יכול להיות משתנה רציף גם,
[00:16:05 - 00:16:06] ‫גם משתנה בדיד.
[00:16:07 - 00:16:09] ‫אני אכתוב הכול עם סכומים, ‫זה יותר קל לכתוב,
[00:16:09 - 00:16:13] ‫אבל זה יכול אפשר לעשות פה... ‫אם זה משתנה רציף, ‫זה אותו דבר עם אינטגרלים.
[00:16:13 - 00:16:23] ‫אז בעצם אני שתוכלת, ‫מה תוכלת ששם? ‫על ההתפלגות של Z בהינתן
[00:16:26 - 00:16:27] ‫לטנ D,
[00:16:30 - 00:16:31] ‫הוא כותב כאן תטא T,
[00:16:34 - 00:16:36] ‫התפלגות לפי הפרמטר תטא T,
[00:16:37 - 00:16:41] ‫של לוג של ההתפלגות הזאת. ‫לוג T
[00:16:43 - 00:16:44] ו-D
[00:16:45 - 00:16:46] ‫לפי תטא T.
[00:16:47 - 00:16:48] ‫אתם רואים את זה?
[00:16:50 - 00:16:56] ‫פשוט הגדרתי פה משהו חדש, ‫שהוא די דומה ל-Q שיש לנו שם, ‫אבל אם אנחנו מחסירים ממנו איזשהו פרמטר,
[00:16:56 - 00:17:00] ‫שימו לב שהדבר הזה שאנחנו מחסירים ממנו, ‫הוא אין בו, לא מופיע פה תטא.
[00:17:01 - 00:17:03] ‫תטא הפרמטר החופשי הזה,
[00:17:04 - 00:17:05] ‫שעליו אנחנו נעשה מקסימיזציה פה,
[00:17:07 - 00:17:07] ‫הוא לא מופיע כאן.
[00:17:09 - 00:17:10] ‫כאילו כשכתוב תטא T,
[00:17:11 - 00:17:12] זה קצת מתבלבל שאני אומר.
[00:17:13 - 00:17:15] ‫כשיש תטא T או תטא T מינוס 1,
[00:17:16 - 00:17:20] ‫בעצם הכוונה היא שאנחנו מקבעים ‫את הערך שיש לנו מהניחוש האחרון.
[00:17:21 - 00:17:25] ‫כשכתוב תטא, זה משתנה חופשי ‫שאנחנו נעשה עליו אחר כך מקסימיזציה.
[00:17:25 - 00:17:28] ‫הדבר הזה זה פונקציה של תטא, ‫תטא הוא משתנה כאן,
[00:17:28 - 00:17:31] ‫אבל השאר זה משהו שכבר נתון לנו ‫מהאיתרציה הקודמת.
[00:17:34 - 00:17:35] ‫זה עוד D, זה הדאטה שלנו שנתון,
[00:17:36 - 00:17:38] ‫ותטא T זה מה שנתון לנו ‫מהאיתרציה הקודמת.
[00:17:38 - 00:17:53] ‫בסדר, ופה אנחנו סוכמים על Z. ‫כל האפשרות של Z. ‫אז הכול פה נתון, חוץ מתטא, ‫חוץ מתטא, זה פונקציה של תטא, עדיין הדבר הזה. ‫זה פונקציה של תטא, עדיין הדבר הזה. ‫זה נכון גם לגמרי מה שכתוב כאן.
[00:17:54 - 00:17:58] ‫דבר זה זה פונקציה של תטא, ‫כל השאר זה נתון לנו ‫או מהדאטה או מהאיתרציה הקודמת.
[00:18:00 - 00:18:03] ‫אוקיי, די נגזרנו את הדבר הזה, ‫אז בואו נכתוב עכשיו,
[00:18:04 - 00:18:07] ‫בואו נפתח רגע את זה ונדען מה אנחנו מקבלים.
[00:18:09 - 00:18:10] ‫Q גדל ו-N.
[00:18:13 - 00:18:13] ‫זה תטא T.
[00:18:17 - 00:18:18] ‫אוקיי, זה שווה ל...
[00:18:22 - 00:18:32] ‫זה שווה... אז נכתב את הדבר הזה, ‫הדבר הזה זה גם נראה כמו סכום כזה. ‫הוא נסכום על פני כל הפרומט של Z. ‫דיוק אותה ההתפלגות כאן, ‫T של Z בהינתן B
[00:18:34 - 00:18:38] ‫של תטא T. ‫רק שכאן אני מכפיל את לוג
[00:18:39 - 00:18:50] ‫P של Z. ‫כן, XZ חבל גם לפי תטא, ‫לא לפי תטא T. ‫עם הפרמטר תטא.
[00:18:51 - 00:18:54] ‫אז פה יש לי את הפרמטר החופשי הזה ‫שאני אעשה לך את המקסימילציה.
[00:18:56 - 00:18:58] ‫וזה, אף פעם אני צריך להוריד ‫את הדבר הזה.
[00:18:59 - 00:19:04] ‫נקפוץ פה כבר השלב, ‫במקום, שימו לב שהסכום כאן ‫הוא אותו דבר, נכון? ‫אז אני יכול פשוט לכתוב את הלוג הזה,
[00:19:05 - 00:19:05] ‫פחות הלוג הזה.
[00:19:08 - 00:19:13] ‫גם כאן, לא בזה, פחות הלוג P חטא P,
[00:19:15 - 00:19:30] ‫זה ההתפלגות של X. ‫אוקיי, עכשיו את זה אני יכול לפרט ‫להההסתברות של X,
[00:19:32 - 00:19:34] ‫כפול ההסתברות של Z בינתן X.
[00:19:39 - 00:19:46] ‫זה שווה ל-B פחטא של X, ‫כפול P פחטא של Z בינתן X.
[00:19:49 - 00:19:51] ‫בגלל שיש לי לוג של מכפלה של דברים,
[00:19:53 - 00:19:57] ‫אז זה פשוט עוד אה... עוד דבר ‫שמתפסס ללוגית, ללוג.
[00:19:58 - 00:19:58] ‫אוקיי, אז יש לי כאן
[00:19:59 - 00:20:00] ‫החוזריים האלה,
[00:20:01 - 00:20:03] ‫כל סכום של Z,
[00:20:05 - 00:20:07] ‫B פחטא Z בינתן D.
[00:20:08 - 00:20:15] ‫יש לי כאן שלושה איברים בלוגית, ‫של לוג P קטע X,
[00:20:19 - 00:20:23] ‫לוג P ל-B נותן X,
[00:20:24 - 00:20:34] ‫לוג פחות לוג P קטע X ו-B נותן X.
[00:20:35 - 00:20:37] אוקיי, נראה שעברתי את זה לפעמים X ולפעמים B.
[00:20:38 - 00:20:39] ‫-B בכל מקום.
[00:20:52 - 00:20:54] ‫זה טוב, עכשיו אני אסדר את זה מחדש.
[00:20:56 - 00:20:57] ‫אני אקבל...
[00:21:07 - 00:21:16] ‫אקבל...
[00:21:37 - 00:21:37] ‫אקבל...
[00:22:07 - 00:22:11] ‫תודה רבה.
[00:22:31 - 00:22:33] ‫אבל ברור שאני רואה את כל הסדר של הזרים, ‫לפי לוג.
[00:22:35 - 00:22:36] ‫בין החוקים של הלוג.
[00:22:37 - 00:22:39] ‫אוקיי, עכשיו האיברים האלה, ‫אתם הולכים לעזור אותם.
[00:22:40 - 00:22:40] ‫פה
[00:22:42 - 00:22:47] מה יש לנו? ‫יש לנו התפלגות על פני משתנה מקרי Z, ‫שבכלל הוא מופיע בפונקציה הזאת.
[00:22:48 - 00:22:52] ‫אוקיי, אז הסכום הזה יהיה שווה לאחד פשוט, ‫נשאר לי רק האיבר הזה.
[00:22:59 - 00:23:00] ‫והדבר הזה, מה זה?
[00:23:07 - 00:23:17] ‫זה KL Divergence בין, יש פה שתי התפלגויות שונות, ‫ההתפלגות הזאת וההתפלגות זה אותו דבר, ‫זה לפי תטא T,
[00:23:18 - 00:23:18] ‫וכאן זה לפי תטא.
[00:23:20 - 00:23:26] ‫אז זה ב-KL ההתפלגות שכאן צריכה ‫להופיע גם למעלה, ‫במינוס ה-KL בעצם,
[00:23:27 - 00:23:31] ‫זה KL בין E
[00:23:32 - 00:23:33] תטא T,
[00:23:34 - 00:23:35] ‫זה בהינתן D,
[00:23:37 - 00:23:42] ‫לפי תטא.
[00:23:50 - 00:23:52] ‫מה קודם על KL Divergence?
[00:23:53 - 00:23:54] ‫תמיד חיובי,
[00:23:55 - 00:23:56] ‫לעומתי הוא יושב אפס?
[00:24:00 - 00:24:02] ‫כן, כשההתפלגות הזאת שבר ההתפלגות הזאת.
[00:24:03 - 00:24:05] ‫משתמשים בזה בתור סוג של פונקציית מרחק,
[00:24:05 - 00:24:07] ‫למרות שזה לא בדיוק מרחק, ‫כי זה לא סימטרי.
[00:24:09 - 00:24:12] ‫אבל חוץ מזה זה מתנהג כמו מרחק, ‫זאת אומרת, זה תמיד חיובי,
[00:24:13 - 00:24:16] ‫וזה אפס רק כששני האיברים שווים.
[00:24:17 - 00:24:18] ‫-זה בין תלמידים היותריים.
[00:24:19 - 00:24:20] ‫אוקיי, אז
[00:24:23 - 00:24:26] ‫כן, בואו נראה בדיוק את הצד הבא, אוקיי? ‫אז מה המשמעות של הדבר הזה?
[00:24:27 - 00:24:29] ‫אם התטא שאני בוחר זה תטא T,
[00:24:31 - 00:24:32] ‫אז הגעתי,
[00:24:33 - 00:24:35] ‫פה חסר לי
[00:24:38 - 00:24:38] זה תטא.
[00:24:41 - 00:24:44] ‫אז אם אני בוחר את תטא,
[00:24:44 - 00:24:46] ‫אני מציב בתטא תטא פי,
[00:24:46 - 00:24:49] ‫בעצם הדבר הזה מתאפס, ‫ומה שנשאר לי זה פשוט הלוג
[00:24:50 - 00:24:53] ‫של ההתפלגות של הדאטה שלי.
[00:24:53 - 00:24:56] ‫אני מצטער קצת שכתבתי די במקום x, ‫תבואו נראה לי עם x,
[00:24:57 - 00:24:57] ‫אבל
[00:24:59 - 00:24:59] שיפו לי.
[00:25:03 - 00:25:18] ‫אוקיי, זה ברור העניין הזה? ‫תכף נראה עכשיו מה המשמעות של זה.
[00:25:18 - 00:25:24] ‫אם אני בוחר בתטא, אם אני מציב בתטא ‫את תטא T, זאת אומרת, ‫את התטא של אחת מהאיתרציות,
[00:25:25 - 00:25:26] ‫זה האיתרציה T,
[00:25:26 - 00:25:29] ‫אז הדבר הזה מתאפס, ‫ואני מקבל פשוט את הלוג ‫של פי של תטא שני,
[00:25:30 - 00:25:31] ‫שזה הדבר שאני רוצה למקסם.
[00:25:33 - 00:25:39] ‫אוקיי, אז עכשיו, המשפט הזה אומר שגם פה, ‫המשפט הזה אני סביר, זה די ברור, זה טובה.
[00:25:44 - 00:25:51] ‫אני רוצה להגיד שבצעד התי, ‫אני שיפרתי את ה-Teta-Line-U של הדאטה שלי,
[00:25:51 - 00:25:57] ‫מכרתי תטא שמעלה את ה-Line-U, ‫את פני האיתרציה הקודמת.
[00:25:59 - 00:26:01] ‫אז בואו נראה איך זה נובע ‫ממה שאנחנו רואים כאן.
[00:26:02 - 00:26:04] ‫אז ככה, אנחנו יכולים להגיד שלוג...
[00:26:10 - 00:26:10] ‫לוג T,
[00:26:11 - 00:26:13] ‫תטא T,
[00:26:14 - 00:26:16] ‫אני לא יודעת בשלילי, הפנייה הקלטה,
[00:26:17 - 00:26:24] ‫אז אמרנו שזה שווה למה שקוראים ‫נציב בתטא תטא T, ‫אנחנו נקבל את זה בדיוק פה, אוקיי? ‫אז זה שווה ל-Q גל הזה שהגדרנו.
[00:26:28 - 00:26:29] ‫הצבנו תטא T,
[00:26:30 - 00:26:39] ‫הוא ה-Q שמוגדר על ידי תטא T, ‫יש לו משתנה חוק שתי תטא, ‫אם אני מציב בתטא הזה את התטא T הזה, ‫שלפי הגדרנו אותו,
[00:26:39 - 00:26:42] ‫אז האיבר הזה מתאפס, ‫ונשאר לי רק האיבר הזה,
[00:26:42 - 00:26:43] ‫ואז אלה שווים.
[00:26:45 - 00:26:48] ‫עכשיו הדבר הזה גדול שווה
[00:26:59 - 00:27:22] ‫אלה שווה ל-Q גל, אלא
[00:27:29 - 00:27:37] ‫מוגדר על ידי T מינוס אחד. ‫אוקיי? כי שם האיבר הזה נשאר אותו איבר,
[00:27:38 - 00:27:41] ‫ועכשיו יש לי פה k-divergence ‫שהוא חייב להיות חיובי,
[00:27:42 - 00:27:43] ‫אוקיי? ואני מחזיר אותו.
[00:27:43 - 00:27:45] ‫ואז אני רק הקטנתי עכשיו את הביטוי.
[00:27:47 - 00:27:50] ‫אוקיי? אם הצבתי פה, בטט ה-T הזה, ‫משהו אחר.
[00:27:51 - 00:27:54] ‫אוקיי? עכשיו יש לי k-divergence ‫שיש לו איזשהו ערך.
[00:27:55 - 00:27:58] עכשיו, מה זה, זה לא סתם T ל-T מינוס אחד מחרתי,
[00:27:58 - 00:28:03] ‫זה מה שקרה באיטרציה T, ‫אחרי שהציתי את ה-n-step. ‫-n-step זה אומר שאני מקסמתי.
[00:28:04 - 00:28:07] ‫אני אומר שזה מקסם את זה, ‫זה אותו דבר כמו למקסם את Q גל.
[00:28:08 - 00:28:19] ‫אז בחרתי את ה-T שימקסם את זה, אוקיי? ‫אז זה בטוח גדול מ-Q גל של תתא T מינוס אחד,
[00:28:20 - 00:28:22] ‫בהינתן תתא T מינוס אחד.
[00:28:24 - 00:28:26] נכון? כי בחרתי את ה-T הבא,
[00:28:26 - 00:28:28] ‫בחרתי בתור זה שאני ממקסם,
[00:28:29 - 00:28:30] ‫הוא בוחר את התתא המקסימלי כאן.
[00:28:32 - 00:28:33] ‫נתן לכל אופן בהערה.
[00:28:35 - 00:28:40] ‫תתא T מינוס אחד שווה r גמרקס.
[00:28:41 - 00:28:43] ‫לפני תתא שווי כאילו...
[00:28:43 - 00:28:56] ‫אז זאת הסיבה שהגדול שווה את זה כאן.
[00:28:57 - 00:28:59] ‫אוקיי? ולמה זה שווה?
[00:28:59 - 00:29:02] ‫שוב פעם יש לי כאן פרמטר שהם שווים כאן.
[00:29:04 - 00:29:07] ‫זה שווה ללוג E של תתא
[00:29:13 - 00:29:17] ‫אוקיי, זה בעצם מוכיח את המשפט הזה.
[00:29:20 - 00:29:22] ‫באיטרציה הבאה, כשהגעתי לאיטרציה T,
[00:29:23 - 00:29:27] ‫הלייקליות שלי על פני הדאטה ‫יותר גבוהה מבאיטרציה ה-T מינוס אחד.
[00:29:27 - 00:29:29] ‫נתחיל פרמטרם באיטרציה T מינוס אחת.
[00:29:30 - 00:29:31] ‫שאלות על זה?
[00:29:40 - 00:29:43] אוקיי, אז יפה. בוא נעשה קצת אינטואיציה של מה קורה.
[00:29:43 - 00:29:45] ‫קצת יותר קל להבין את זה גרפית אולי.
[00:29:52 - 00:29:55] ‫כן, למה זה ממקסם את Q גם ממקסם את Q גל?
[00:29:56 - 00:29:58] ‫כי בדיוק מוכרחתי את זה, אבל האיבר הזה שהוספנו,
[00:29:58 - 00:30:00] ‫כדי להפוך את Q ל-Q גל, ‫לא תלוי בטטא.
[00:30:01 - 00:30:06] ‫הוא היה רק תלוי בטטא T. ‫אם הוא לא היה פונקי של Q גל.
[00:30:11 - 00:30:13] ‫אוקיי, אז תגיד, יש לנו...
[00:30:14 - 00:30:16] ‫אם נגיד שהטטא שלנו חד-ממדי.
[00:30:18 - 00:30:19] ‫זה לא
[00:30:24 - 00:30:25] ‫היא טטא מהדאטה שלנו.
[00:30:27 - 00:30:29] ‫זה משהו מורכב, נכון? ‫יש לנו איזה...
[00:30:30 - 00:30:31] ‫התפלגות כזאת מורכבת.
[00:30:33 - 00:30:33] ‫זה היה רעיון.
[00:30:34 - 00:30:37] ‫אנחנו רוצים למצוא את המקסימום, ‫אנחנו מחפשים בעצם את הטטא ‫שממקסמת את זה.
[00:30:38 - 00:30:40] ‫אז אמרנו, אופציה אחת זה לעשות ‫גרדיאנט דיסן.
[00:30:41 - 00:30:43] ‫כל מקום שאנחנו נמצאים, ‫לחשב את הגרדיאנט,
[00:30:44 - 00:30:49] ‫להתקדם ככה בצעדים קטנים, ‫כי יש כל מיני צעדים ‫שהולכים ודורכים.
[00:30:49 - 00:30:52] ‫אפשר להגיע ככה ‫לאיזשהו מקסימום לוקאי.
[00:30:54 - 00:30:58] ‫אבל זה לא תמיד זה מאוד יעיל,
[00:30:58 - 00:31:10] ‫במקרה שאנחנו יכולים, ‫כשהתנאים האלה מתקיימים, ‫אנחנו יכולים להקביל EM שהוא יותר יעיל, ‫ובעצם ההוכחה הזאת אומרת ‫איך נראה האלגוריתם EM, ‫אנחנו נמצאים באיזושהי נקודה.
[00:31:13 - 00:31:15] ‫זה נקרא, אנחנו עושים את זה לרקע.
[00:31:16 - 00:31:17] ‫אנחנו נמצאים באיזושהי נקודה.
[00:31:21 - 00:31:22] ‫הנקודה הזאת תטע T,
[00:31:24 - 00:31:26] ‫ואנחנו מחשבים פונקציה,
[00:31:27 - 00:31:29] ‫אנחנו קוראים לה Q או ה-Q גל הזה, ‫זה לא משנה,
[00:31:30 - 00:31:33] ‫שאנחנו יודעים לעשות לה אופטימיזציה ‫בצורה יעילה.
[00:31:34 - 00:31:38] ‫במקרה למשל של גאוסיאנים, ‫הפונקציה הזאת, שהיא תהיה לוב של גאוסיאנים,
[00:31:39 - 00:31:40] ‫תהיה בעצם פונקציה ריבועית.
[00:31:41 - 00:31:43] ‫והפונקציה הזאת, כשאנחנו מציבים,
[00:31:44 - 00:31:51] ‫פונקציה זאת היא פונקציה של תטא, ‫וכשאנחנו מציבים בפונקציה הזאת תטא T, ‫היא שווה בדיוק ללוג של P,
[00:31:51 - 00:31:54] של P של D. ‫אוקיי, זאת אומרת שבנקודה הזאת היא שווה בדיוק,
[00:31:55 - 00:31:56] ‫נקודה שאנחנו מתחילים ממנה,
[00:31:57 - 00:32:02] ‫והיא פונקציה שקל לנו לעשות לה אופטימיזציה, ‫למשל, הדוגמה שלנו זה יהיה פשוט פרבולה.
[00:32:04 - 00:32:05] ‫זה יהיה משהו כזה.
[00:32:10 - 00:32:26] ‫זה יהיה פשוט איזושהי פונקציה כזאת, ‫פרבולה.
[00:32:28 - 00:32:31] ‫עכשיו אנחנו יודעים למצוא את המקסימום שלה, ‫המקסימום שלה
[00:32:32 - 00:32:32] יהיה פופט,
[00:32:36 - 00:32:40] ‫ומוצץ לנו גם שכל הפונקציה הזאת, ‫שהיא נוגעת לאיפה שהתחלנו,
[00:32:40 - 00:32:41] ‫ושכל הפונקציות נמצאות מתחת
[00:32:42 - 00:32:45] ‫להתקבלות הזאת עבור כל אחת מה...
[00:32:46 - 00:32:49] ‫כל תטא שונה, נכון? ‫זה בגלל שה-Kל הזה הוא חיובי,
[00:32:49 - 00:32:51] ‫הפונקציה הזאת היא תמיד מתחת לדבר הזה.
[00:32:52 - 00:32:54] ‫זאת אומרת שאם אנחנו מוצאים את המקסימום,
[00:32:54 - 00:33:00] ‫בטוח שהגענו לנקודה שבה גם הלוג של P של D,
[00:33:00 - 00:33:03] של תטא,
[00:33:04 - 00:33:04] ‫הוא יותר גדול.
[00:33:05 - 00:33:08] ‫אז הגענו לנקודה הזאת, ‫פה אנחנו נגדיר את תטא T פלוס 1.
[00:33:10 - 00:33:13] ‫ובנקודה הזאת מובטח לנו,
[00:33:14 - 00:33:19] ‫הגענו ל-P של תטא יותר גבוה, ‫ואנחנו מקבלים קפיצות יותר משמעותיות ‫מתרגיינטסנט.
[00:33:20 - 00:33:27] ‫בחד ממד זה נראה לנו משהו ‫שהוא אולי לא כל כך משמעותי, ‫אבל ברגע שתזכירו שהתטאות האלה ‫יכולים להיות מיליונים של פרמטרים,
[00:33:29 - 00:33:32] ‫וזה מאוד משמעותי, ‫יש לנו דרך לעשות קפיצות למקומות שאנחנו...
[00:33:33 - 00:33:34] ‫מובטח לנו שאנחנו משפרים.
[00:33:37 - 00:33:39] ‫רגיינטסנט, אפילו צעד קטן,
[00:33:39 - 00:33:40] ‫זה לא מובטח לנו שאנחנו משופרים.
[00:33:44 - 00:33:46] ‫זה רק אם הצעד הוא אינטיסימלי קטן.
[00:33:51 - 00:33:53] ‫שאלות על המיטוייקר הזאת שיהיה?
[00:33:54 - 00:33:56] ‫למה הענשת שזה פרמבר?
[00:33:57 - 00:33:59] ‫אני מבין שזה צריך להיות קטן,
[00:33:59 - 00:34:00] ‫אבל זה יכול להיות שהוא קבוע ב...
[00:34:01 - 00:34:03] ‫אז זה תלוי קצת במודל שלך,
[00:34:04 - 00:34:09] ‫אבל ההנחה היא שהמודל הזה זה משהו ‫שאתה יודע לעשות לו אופטימיזציה בצורה...
[00:34:09 - 00:34:12] ‫בצורה קלה.
[00:34:13 - 00:34:14] ‫במשל, אונאליטית,
[00:34:15 - 00:34:17] ‫שזה בעיקרון עומד לפרבולה.
[00:34:19 - 00:34:22] ‫לא רק של גאוסיאן, ‫זה בעצם פונקציה ריבוי, זה פרבולה.
[00:34:24 - 00:34:28] ‫אז אם ההתפלגויות המשותפות האלה ‫שמחה הן יוצאות גאוסיאנים,
[00:34:29 - 00:34:30] ‫אז זה בעצם מה ש...
[00:34:32 - 00:34:33] ‫אבל זה לא חייבים, צריכים להיות קודם במובן.
[00:34:34 - 00:34:43] ‫מה שמובטח לך זה שזה נוגע ‫בנקודה הזאת פה, ‫ושזה תמיד מבטח לפונקציות.
[00:34:45 - 00:34:48] ‫אם הפונקציה הזאת היא גם קשה ‫לאופטימיזציה, ‫אז אתה לא מביא הפרקט, ‫אז
[00:34:49 - 00:34:53] אתה לא יודע לעשות פה ישר לגפות וכאן, ‫כאן אתה צריך לעשות גודיין-דיסנט,
[00:34:53 - 00:34:57] ‫ואז לא בטוח שתרווחת ‫על פני סודיון-דיסנט ‫כבר מלאחר המקורי.
[00:34:57 - 00:35:06] ‫אתה יכול לדבר על שוב את המעבר ‫באינטיביון בין השוברות פה?
[00:35:07 - 00:35:07] ‫כן.
[00:35:08 - 00:35:09] ‫בין ה...
[00:35:11 - 00:35:12] ‫מכאן לכאן? כן.
[00:35:14 - 00:35:15] ‫זה היה ה-M step, אוקיי?
[00:35:16 - 00:35:19] ‫זה בעצם השלב שסיימנו את ‫האיתרציית t מינוס 1,
[00:35:20 - 00:35:21] ‫והגענו לאיתרציית t,
[00:35:22 - 00:35:25] אוקיי? אז פה בעצם ‫באיתרציית t מינוס 1 ‫הגדרנו את הדבר הזה,
[00:35:26 - 00:35:32] ‫ואז בחרנו את ה-teta ‫שממקסם לנו את הפונקציה הזאת,
[00:35:33 - 00:35:38] ‫זאת אומרת שה-teta פה מודר על ה- s, ‫ה-teta מודר על ה-q בלי העגל,
[00:35:38 - 00:35:42] ‫אבל זה אותו דבר, לעשות מקסימום לזה, ‫ואנחנו מקסימום ל-q במעגל, זה אותו דבר.
[00:35:43 - 00:35:47] ‫אז בעצם אנחנו בוחרים תeta אחרת ‫במקום התeta הזאתי,
[00:35:48 - 00:35:48] ‫שתמקסם את זה.
[00:35:49 - 00:35:50] זה חייב להיות יותר גדול.
[00:35:51 - 00:35:53] ‫יכול להיות שווה, אם בדיוק היינו בטeta שאני מוצמת את זה,
[00:35:54 - 00:35:56] ‫אבל אחרת זה יהיה רק יכול לגדול.
[00:36:02 - 00:36:05] ‫זה כתוב הפוך, אנחנו הולכים מהצעדתי ‫לצעדתי מינוס 1,
[00:36:06 - 00:36:06] ‫זה יקטן.
[00:36:07 - 00:36:10] ‫אבל בעצם באיתרציה ‫שאנחנו מתחילים מכאן, אנחנו הולכים מכאן.
[00:36:14 - 00:36:14] טוב.
[00:36:15 - 00:36:24] ‫בראש דבר, ראינו מה המשמעות של E.M. ו-J.M.
[00:36:29 - 00:36:37] ‫זה היה, כן, הדגמה של זה, ‫אבל ראינו את הכללי קונט בעצם, ‫איך נראה E-STEP וה-M-STEP.
[00:36:38 - 00:36:39] ‫עשינו את זה, אני חושב, בקצרה, לא?
[00:36:44 - 00:36:48] ‫אז זה,
[00:36:50 - 00:36:51] ‫יש עוד פעם פה על זה?
[00:37:14 - 00:37:20] ‫אז זה G.M.M. ‫זה נראה E-STEP.
[00:37:25 - 00:37:29] ‫אנחנו רוצים לחשב את התוחלת על פני Z של לוג
[00:37:30 - 00:37:30] ‫לחיבור של XZ.
[00:37:44 - 00:37:56] ‫זאת ה-T, המיחוש הנוכחי שלנו, ‫הוא שווה לתוחלת של Z ‫להינתן לדאטה שלנו ‫מהניחוש הנוכחי,
[00:37:58 - 00:37:59] ‫ואיך יהיה לוג P
[00:38:01 - 00:38:05] של איבר של מיקסר מודל, G.M.M.
[00:38:09 - 00:38:12] ‫יש לנו סכום על כל הנקודות שלנו, ‫על כל הדאטה.
[00:38:12 - 00:38:18] XI בתוך הלאטה מתוכנית.
[00:38:20 - 00:38:21] ‫כל אחד מהם זה לוג
[00:38:23 - 00:38:23] של
[00:38:26 - 00:38:26] ‫חום.
[00:38:38 - 00:38:40] בעצם אחרי כל כך נכתוב בככה, זה פשוט לוג של
[00:38:41 - 00:38:43] E של Z ו-XI.
[00:38:46 - 00:38:48] ‫מה זה Z במקרה שלנו, של G.M.M.M?
[00:38:49 - 00:38:51] ‫-מזיג ועוד סמדי G.M. כן.
[00:38:51 - 00:38:57] כן, בוחרים G.M.M.M. מבחינתנו, זה מודל עם משתנה חבוי, ‫שיש לנו Z,
[00:38:58 - 00:39:00] שיש לו איזשהו מספר, ‫נגיד אם יש לנו 100 קומפונטות,
[00:39:01 - 00:39:03] ‫זאת יכולה לקבל ערכים מ-1 ל-100,
[00:39:03 - 00:39:09] ‫והוא קובע את ה-X ככה ש-N שווה,
[00:39:10 - 00:39:13] ‫ש-X הוא מתפלג באוסיאניק ‫עם איזשהו מיוב
[00:39:15 - 00:39:16] תלוי ל-Z,
[00:39:17 - 00:39:18] ‫חצי גם שתלוי ל-Z, אוקיי?
[00:39:18 - 00:39:20] ‫אז אם נתחיל לזה K, נתחיל לזה Z.
[00:39:21 - 00:39:22] ‫משתנה חבובה באופן כללי, קוראים ל-Z,
[00:39:22 - 00:39:24] ‫וב-G.M.M.M.M. זה רק קוראים ל-K.
[00:39:25 - 00:39:28] ‫זה פה חלק מהדברים כלליים, ‫חלק מהדברים חצי חצי מ-G.M.M.M.M. זה קצת מתבלבלת.
[00:39:29 - 00:39:33] אוקיי, אז זה היה בעצם ה-Q שאנחנו רוצים לחשב.
[00:39:34 - 00:39:37] ‫שימו לב שזו תוחלת, זה סכום, ‫אנחנו יכולים להפוך פה את
[00:39:38 - 00:39:39] סדר הדברים.
[00:39:39 - 00:39:44] ‫אנחנו מקבלים סכום על כל ה-XI, ‫בתוך הדאטה שלנו.
[00:39:49 - 00:39:49] ‫תוחלת
[00:39:52 - 00:39:56] ‫של Z בהינתן, עכשיו זה רק בהינתן XI,
[00:39:57 - 00:40:02] ‫זה ה-Z שרלוונטי רק ל-XI הזה, ‫הפכתי פה את הסדר.
[00:40:03 - 00:40:06] ‫ו-Tטא-T של לוג
[00:40:10 - 00:40:12] E לטטא, כן? לוג T-Tטא
[00:40:14 - 00:40:14] ‫של
[00:40:17 - 00:40:19] XI.
[00:40:22 - 00:40:24] אוקיי, עכשיו מה התוחלת הזאת עבור X מסוים?
[00:40:26 - 00:40:27] ‫אנחנו נכתוב את זה בפירוש.
[00:40:31 - 00:40:32] תוחלת, אני יכול לעבור על כל האיברים,
[00:40:32 - 00:40:37] ‫עם כל הערכים האפשרים של Z, ‫ולכפול בהסתברות של זה, נכון?
[00:40:38 - 00:40:40] ‫לפי ההתפלגות הזאתי שלפיה אני יוצאת בתוכלת.
[00:40:41 - 00:40:42] ‫יש לי פה עוד סכום.
[00:40:48 - 00:40:54] ‫-Z שווה 1 עד K. ‫נקבע לזה, מעדיפים שאני אפשר ל-K?
[00:40:55 - 00:40:56] ‫אפשר לבדק?
[00:40:57 - 00:41:02] ‫-K של 1 עד K גדול, זה היה בעצם המשתנה החבוי שלי,
[00:41:02 - 00:41:03] ‫ה-Z,
[00:41:04 - 00:41:07] ‫הוא נקרא ספציפי של ג'רסיאן, של ג'ימיין,
[00:41:10 - 00:41:14] ‫והדבר הזה לוג של פי Z בהינתן XI,
[00:41:17 - 00:41:18] ‫או, סליחה, צריך לקרוא בהסתברות,
[00:41:19 - 00:41:23] ‫ה-P של Z שווה ל-K,
[00:41:23 - 00:41:29] ‫לעבר ה-K בהינתן XI,
[00:41:29 - 00:41:34] ‫אתם מבינים מאיפה זה הגיע?
[00:41:38 - 00:41:45] ‫אני פותח פה את התוחלת הזאת, אוקיי? ‫תוחלת זה סכום וכל האיברים שונים, ‫כל אחד פחולי ההתפלגות שלו, ‫שהפונקציה נכנסה על התוחלת,
[00:41:46 - 00:41:46] ‫זה לוג
[00:41:47 - 00:41:48] פי תטא,
[00:41:50 - 00:41:52] ‫ועכשיו Z של לוג K,
[00:41:54 - 00:41:54] ‫A.
[00:41:54 - 00:42:00] ‫אוקיי, אז מה זה הדבר הזה?
[00:42:03 - 00:42:04] ‫בואו נחשב אותו.
[00:42:06 - 00:42:12] ‫P בין XI, איך אפשר לחשב אותו?
[00:42:15 - 00:42:17] ‫אבל נשתמש בחוק, בחוק בייס,
[00:42:18 - 00:42:19] ‫זה שווה ל-P
[00:42:20 - 00:42:20] של
[00:42:21 - 00:42:26] ה-Prior, זה אפשר לחשוב ‫בתור הפוסטריור על Z,
[00:42:27 - 00:42:33] ‫אז אפשר לחשב את זה לפי חוק בייס, ‫זה ה-Prior על Z, איך הקדמנו את ה-Prior על Z, אתם זוכרים?
[00:42:34 - 00:42:36] ‫Prior על Z, אוקיי.
[00:42:38 - 00:42:42] תחשבו על המודל הגרפי הזה, ‫מה ה-Prior על Z?
[00:42:42 - 00:42:44] איך נוצר דאטה? קודם נוצר
[00:42:45 - 00:42:49] בוחר איזשהו Z לפי התפלגות, זה ה-Prior על Z, ‫ואחר כך אני מייצר את X לפי Z.
[00:42:51 - 00:42:54] ‫ב-GMM שלי, איך מוגדר ה-Prior על Z?
[00:42:55 - 00:42:55] ‫Py K?
[00:42:56 - 00:42:58] ‫כן, זו פשוט ההתפלגות הקטגורית הזאתי על זה,
[00:42:59 - 00:43:00] ‫זה Pi K,
[00:43:01 - 00:43:02] כפול ה-likelihood,
[00:43:02 - 00:43:03] ‫זה פשוט גאוסיאן,
[00:43:05 - 00:43:08] X עם New K וסיגמה K,
[00:43:10 - 00:43:13] ‫על K הנרמול של זה,
[00:43:14 - 00:43:17] ‫ואנחנו יכולים לעשות את זה על ידי סכום ‫על כל ה-K עם האקשרים,
[00:43:18 - 00:43:21] ‫אז הוא K-Tג שווה 1 עד K גדול,
[00:43:21 - 00:43:22] ‫זה מה שכתוב למעלה.
[00:43:23 - 00:43:26] בסוף אנחנו נרמל את זה, Pi K-Tג N, X
[00:43:27 - 00:43:31] ניו K-Tג פיזמק פי-Tג.
[00:43:37 - 00:43:38] אוקיי, אז הדבר הזה,
[00:43:39 - 00:43:41] מה הוא מגדיר לנו? הוא מגדיר עבור כל נקודה,
[00:43:43 - 00:43:44] זאת אומרת עבור נקודה I,
[00:43:44 - 00:43:49] ‫מה ההסתברות שהנקודה הזאת הגיעה ‫מכל אחד מהגאוסיאנים של התערובת.
[00:43:51 - 00:43:52] ‫כן, קוראים לזה
[00:43:56 - 00:43:57] נותנים לזה עוד פעם ב-R,
[00:43:58 - 00:43:59] ב-RiP,
[00:44:03 - 00:44:03] וזה,
[00:44:05 - 00:44:07] קוראים לזה R כי זה Responsilitude.
[00:44:14 - 00:44:19] ‫מה האחריות של כל אחד מהגאוסיאנים ‫לפני הנקודה הזאת?
[00:44:21 - 00:44:25] ‫זו התפלגות על פני המרחב התסקרטי הזה ‫של מספר הגאוסיאנים.
[00:44:26 - 00:44:32] ‫אם סוכמים את זה עבור כל ה-K, ‫זה צריך להסתכם לאחד, ‫זה הנרמון הזה שאתם רואים כאן.
[00:44:34 - 00:44:39] ‫זה בעצם אומר, אפשר לחשוב על זה ‫בתור איזושהי אמונה ‫שהנקודה הזאת הגיעה ‫מכל אחד מהגאוסיאנים.
[00:44:39 - 00:44:42] ‫אז אם היא תהיה מאוד קרובה ‫למרכז של אחד מהגאוסיאנים,
[00:44:42 - 00:44:44] ‫אז ה-R שם יהיה גדול, הגאוסיאן הזה,
[00:44:45 - 00:44:49] ‫ואם תהיה רחוקה, ה-R יהיה קטן. ‫והסכום של זה עבור כל הגאוסיאנים ‫צריך להיות אחד.
[00:44:51 - 00:44:57] ראיתם את זה? זה היה הצבע, ‫האנימציה שהייתה שבוע שעבר, ‫זה היה הצבע של הנקודות.
[00:44:59 - 00:45:03] אוקיי, אז זה R IK, ‫אפשר לקרוא לזה R IK, ‫ואנחנו יודעים לחשב את זה.
[00:45:03 - 00:45:07] שימו לב, זה היה התנאי שלנו כאן, ‫שאנחנו יודעים לחשב את זה בצורה יעילה.
[00:45:07 - 00:45:11] ‫בגלל שיש לנו כאן מספר סופי ‫יחסית קטן של קומפוננטות,
[00:45:11 - 00:45:16] ‫אז אנחנו יכולים לחשב את הנרמול הזה. ‫זה מוגדר לנו, זה לפי הפרמטרים שיש לנו,
[00:45:17 - 00:45:18] ‫באיטרציה הנוכחית,
[00:45:19 - 00:45:23] ‫והנרמול, אנחנו פשוט צריכים ‫לחשב את זה עבור כל הקומפוננטות ‫ולחלק לזה.
[00:45:23 - 00:45:26] ‫אם מספר הקומפוננטות כאן הוא סופי, ‫אז אפשר לחשב את זה בצורה איילה.
[00:45:29 - 00:45:30] אוקיי, אז זה...
[00:45:31 - 00:45:34] בואו נכתוב את זה עכשיו בפעול, ‫אז מה קיבלנו? קיבלנו XI,
[00:45:35 - 00:45:39] XI ו-XI, עבור כל נקודה בדאטה שלי,
[00:45:40 - 00:45:41] ‫אני עובר על כל הקומפוננטות,
[00:45:44 - 00:45:46] ‫מחשב את ה-responsibility הזה, R, I, K,
[00:45:48 - 00:45:51] ‫ומכפיל את זה בלוג של ההסתברות,
[00:45:52 - 00:45:53] ‫שאני נמצא בגרסיון ה-K,
[00:45:57 - 00:45:59] ‫והנקודה שלי שווה ל-X.
[00:46:00 - 00:46:03] אה, משהו שחשוב, הדבר הזה הוא מוגדר לטטא T, אוקיי?
[00:46:04 - 00:46:12] ‫אז זה עם הפרמטרים שהיה לי בזמן T. ‫ה-Pi הזה, זה לפי ה-Pi שהיה לי בזמן T, ‫ה-Mu שהיה לי בזמן T, ‫לסיגמאל בזמן T.
[00:46:16 - 00:46:19] כן, זה כתוב כאן כללי, אבל אה...
[00:46:21 - 00:46:21] ‫אני חושב שאני אשים את זה פה.
[00:46:26 - 00:46:31] ‫משתמשים את זה תמיד עבור פרמטרים מסוימים ‫שידועים לנו באיתראציה הקודמת.
[00:46:31 - 00:46:42] ‫אז זה T לפי תטא T. ‫אוקיי, אז אנחנו חושבים את זה ‫לפי התטא הקודמים,
[00:46:42 - 00:46:48] ‫ולוג הזה לפי תטא שאנחנו לא יודעים אותו, ‫שאנחנו נעשה עליו מקסימיזציה.
[00:46:49 - 00:46:52] ‫אפשר לפרק את זה לשני איברים, ‫יש לנו את לוג
[00:46:53 - 00:46:55] של ה-Prior-Z, שזה Pi,
[00:46:56 - 00:47:01] Pi, K ועוד לוג
[00:47:03 - 00:47:03] של הגלוסיאנל.
[00:47:05 - 00:47:06] ‫לוג של הגלוסיאנל XI,
[00:47:08 - 00:47:10] ‫ניתן מיוב וסיגמא.
[00:47:12 - 00:47:15] ‫אז זה ה-E-Step, חשב את הדבר הזה,
[00:47:16 - 00:47:18] ‫ועכשיו ה-N-Step זה יהיה לעשות מקסימיזציה לזה,
[00:47:19 - 00:47:22] לפי הפרמטרים, ‫יש לנו שלושה סוגים של פרמטרים, ‫שהיינו Pi,
[00:47:22 - 00:47:23] מיוב וסיגמא,
[00:47:24 - 00:47:25] אוקיי, זה התטא שלנו.
[00:47:26 - 00:47:32] ‫ואנחנו נרצה למצוא את המקסימום של פאי ‫למקסימום של מיוב, ‫את המקסימום של סיגמא, לפי
[00:47:33 - 00:47:34] מה שיש לנו כאן,
[00:47:35 - 00:47:35] ‫לפי המשוואה הזאת.
[00:47:37 - 00:47:41] ‫תשימו לב שזה מאוד דומה למצוא את המקסימום של ‫האיברים האלה, זה מאוד דומה
[00:47:45 - 00:47:47] ‫למקסימום רגיל שאנחנו עושים על גאוסיאן.
[00:47:48 - 00:47:49] ‫מה ההבדל שיש לנו?
[00:47:49 - 00:47:51] ‫במקום שאנחנו סתם עושים סכום על כל ה-X'ים,
[00:47:53 - 00:47:54] ‫אנחנו עושים סכום על כל ה-X'ים,
[00:47:54 - 00:47:56] ‫וכל אחד מהאיקסים הוא ממושקל בצורה אחרת,
[00:47:58 - 00:47:59] ‫כל נקודה ממושקלת בצורה אחרת.
[00:48:01 - 00:48:04] ‫במקום לעשות ממוצע על כל האיקסים, ‫אנחנו עושים ממוצע משוקלל על כל זה.
[00:48:04 - 00:48:07] ‫זה יהיה באמת כל ההבדל ‫במיומד והסיגמוד שנמחקנו.
[00:48:15 - 00:48:19] ‫בואו נראה איך זה ייראה את ה-m's take.
[00:48:24 - 00:48:25] ‫אז ארגומקס,
[00:48:54 - 00:48:56] ‫אנחנו נותנים מיום.
[00:49:01 - 00:49:06] ‫אלה קיוווי ופי.
[00:49:08 - 00:49:09] ‫מה זה יהיה?
[00:49:10 - 00:49:11] ‫תהיה לנו פוספורג.
[00:49:15 - 00:49:21] ‫נתחיל לך שאני יודע, ‫אז בואו נעשה פעם בזה. נגזור את קיווי ופי.
[00:49:24 - 00:49:24] ‫בטא
[00:49:28 - 00:49:31] ‫נגזור בלפי מיווי, איזשהו מיווי פי.
[00:49:33 - 00:49:34] בסדר?
[00:49:35 - 00:49:36] ‫אני רוצה לדוגמה רק על המיווים ‫ואחר כך
[00:49:39 - 00:49:40] נראה את התוצאה של הכול.
[00:49:41 - 00:49:44] ‫אני רוצה לגזור את הדבר הזה ‫לפי המיווי
[00:49:45 - 00:49:46] של אחד מהאיברים,
[00:49:47 - 00:49:47] ‫אחד מהקומפונטות,
[00:49:48 - 00:49:49] ‫באמצעות המקסימום.
[00:49:50 - 00:49:59] ‫איך היא תראה הנגזרת הזאת? ‫יש לי כאן סכומים של דברים קבועים, ‫זה לא תלוי במיווי, ‫זה לפי הפרמטר ‫מהאיתרציה הקודמת.
[00:50:00 - 00:50:04] ‫פה חישבתי את זה עם המינוס של ה-p. ‫אני חושבת ש-p זה לא פרמטר חופשי,
[00:50:05 - 00:50:06] ‫כל הדברים האלה הם קבועים.
[00:50:07 - 00:50:10] ‫כאן יש לי איבר שהוא לא תולי במיווי, זה לא יפה.
[00:50:11 - 00:50:12] ‫יש לי רק לגזור את הדבר הזה.
[00:50:13 - 00:50:16] ‫אז אני מכניס את הנגזרת לתוך הסכומים.
[00:50:20 - 00:50:25] ‫זה לא קיי, נכון? ‫והנגזרת של לוג של גאוסיאן.
[00:50:26 - 00:50:29] ‫אתם רואים, זה היה בתרגיל הראשון ‫שעשיתם לפי מיו,
[00:50:29 - 00:50:35] ‫זה x מינוס מיו,
[00:50:37 - 00:50:38] ‫זה מינוס מינוס מו.
[00:50:44 - 00:50:44] ‫אתם רואים את זה?
[00:50:45 - 00:50:47] ‫נכון, לוג של גאוסיאן,
[00:50:47 - 00:50:49] ‫גאוסיאן יש לנו את האיבר הנרמוד,
[00:50:49 - 00:50:50] ‫ששם מיו לא מופיע בכלל,
[00:50:51 - 00:50:52] ‫ויש לנו את האיבר שבאקספוננט,
[00:50:53 - 00:50:56] ‫נראה כמו תבנית ריבועית כזאת. ‫האנגזרת של תבנית ריבועית
[00:50:58 - 00:50:59] ‫זה הדבר הזה.
[00:51:00 - 00:51:02] ‫כן, עכשיו אנחנו רוצים להשוות, סליחה, ‫שכחתי את ה...
[00:51:03 - 00:51:04] ‫את ה-Asponservity.
[00:51:05 - 00:51:06] ‫כמו בזה,
[00:51:07 - 00:51:07] נכון,
[00:51:09 - 00:51:12] ‫עכשיו אני רוצה להשוות את הדבר הזה לאפס,
[00:51:13 - 00:51:14] ‫ואני מצא את ה-mue,
[00:51:15 - 00:51:16] ‫מחלץ מו.
[00:51:17 - 00:51:19] ‫אני מקווה ש-mue שווה ל...
[00:51:28 - 00:51:29] ‫מה הצעות שיש לי פה?
[00:51:30 - 00:51:34] ‫אני חוזר רק לפי מיו-k, אוקיי? ‫בתוך כל הסכומים האלה,
[00:51:34 - 00:51:37] ‫רק אחד מהאיברים תלוי ב-k,
[00:51:38 - 00:51:39] ‫נכון, האיבר ה-k?
[00:51:39 - 00:51:41] ‫כל השאר לא מסתכלים בכלל על מיו-k,
[00:51:42 - 00:51:43] ‫אז בעצם זה נופל לי,
[00:51:44 - 00:51:44] הסכומים האלה.
[00:51:46 - 00:52:04] ‫אבל שם היה רק האיבר ה-k, אז זה סכום, ‫שזה ה-covarions, כן? קovarions, אוקיי?
[00:52:07 - 00:52:09] ‫אז אם אני משווה את זה לאפס, ‫מה אני מקבל?
[00:52:09 - 00:52:11] ‫אני יכול להפריד בין שני האיברים האלה,
[00:52:11 - 00:52:13] ‫קovarions לא משפיע עליי,
[00:52:13 - 00:52:15] ‫אני מקבל את זה.
[00:52:17 - 00:52:19] ‫סכום של r, i, k,
[00:52:21 - 00:52:22] ‫באמפרציה t,
[00:52:22 - 00:52:25] ‫של x ל-x, אין שלי,
[00:52:26 - 00:52:27] ‫של הנתידות,
[00:52:28 - 00:52:30] ‫כי לסכום של r, i, k,
[00:52:32 - 00:52:34] ‫זה ממוצע משוכלל.
[00:52:36 - 00:52:38] ‫כשהיה לי רק גאוסיין אחד, ‫זה היה פשוט ממוצע של ה-x.
[00:52:40 - 00:52:42] ‫עכשיו, יש לי הרבה גאוסיונים, ‫אז זה ממוצע משוכלל,
[00:52:42 - 00:52:45] ‫לפי כמה אני חושב שהגאוסיין הזה אחראי עליך.
[00:52:46 - 00:52:48] ‫זה ה-inset, כל אחד מה-inset.
[00:52:51 - 00:52:53] ‫זה יהיה מיום פיל עוד אחד.
[00:53:02 - 00:53:05] ‫אוקיי, אז ככה אנחנו נותנים את מיום.
[00:53:06 - 00:53:12] ‫בתוך חישוב אפשר לעשות לסיגמא ולפאי, ‫שזה הפרמטרים האחרים שלנו, ‫אז סיגמא
[00:53:15 - 00:53:15] יוצא
[00:53:17 - 00:53:33] ‫אתם עוברים על כל הנקודות ועושים
[00:53:35 - 00:53:35] ‫מכפלה חיצונית.
[00:53:46 - 00:53:54] ‫זה כל איבר עושה לנו, בונה מדריצה כזאת, ‫נכון? אנחנו סוכמים את כל המדריצות האלה.
[00:53:56 - 00:54:03] ‫ובכפסיין רגיל זה היה פשוט סכום הדבר הזה, חלקי n. ‫פה זה יהיה פשוט ממוצע משוכלל של הדבר הזה.
[00:54:03 - 00:54:05] ‫אני אכפיל כל אחד מהם ב-RiK,
[00:54:07 - 00:54:09] ‫ואני חלק לסכום ה-RiK.
[00:54:17 - 00:54:18] ‫אוקיי, ומי שרוצה לדחש מה יהיה ה-Ri?
[00:54:35 - 00:54:37] ‫מה נראה לכם מגניב שיוצא ה-Ri מקסימום לייקליות?
[00:54:37 - 00:54:47] ‫-Ri היחס של ה-RiK. ‫-RiK יהיה פשוט ה-RiK.
[00:55:07 - 00:55:17] ‫בביתה יעשו את ה-RiK ותרגלו את ה-RiK.
[00:55:21 - 00:55:22] ‫ובעצם מה זה אומר? זה אומר,
[00:55:26 - 00:55:27] ‫אם יש לי גאוסיאן,
[00:55:29 - 00:55:33] ‫באיתרציה הקודמת היה נראה שהוא אחראי ‫על הרבה נקודות בצורה משמעותית,
[00:55:34 - 00:55:36] ‫אז זה אומר שיש לו ה-RiK,
[00:55:36 - 00:55:39] ‫שהנקודה הבאה תגיע ‫מהגאוסיאנים הזה יותר גבוה.
[00:55:40 - 00:55:41] ‫באופן יחסי,
[00:55:41 - 00:55:45] ‫ה-RiR של כל אחד מהגאוסיאנים האלה ‫הוא לפי כמה נקודות ‫הם היו אחראיים.
[00:55:46 - 00:55:48] ‫אבל כל ההשמות האלה של הנקודות ‫זה בצורה רכה.
[00:55:49 - 00:55:56] ‫אמרנו כבר בשבוע שעבר ‫שזה קצת הכללה של אלבוריתם קיימית,
[00:55:57 - 00:56:00] ‫שאמרו במקום בכל איטרציה ‫להחליט. ‫כל הנקודות האלה נמצאות ב...
[00:56:02 - 00:56:06] ‫אני ניצם אותן במרכז הכי קרוב אליהן,
[00:56:06 - 00:56:07] ‫שזה היה מה שעשינו בקיימינג.
[00:56:08 - 00:56:18] ‫פה, לכל נקודה, אני מחלק אותה בעצם ‫בכל המרכזים לפי ה... בצורה רכה, אוקיי? ‫אז זה יגיע קצת יותר ‫מאחד מהמרכזים ‫וקצת פחות ממרכז אחר,
[00:56:19 - 00:56:22] ‫ואת כל החישובים שעשינו בקיימינג ‫אפשר לעשות גם בצורה רכה.
[00:56:23 - 00:56:25] ‫זה יוצא מאוד גמרי.
[00:56:28 - 00:56:28] טוב,
[00:56:28 - 00:56:29] ‫מרמות דברים על המגבלים.
[00:56:30 - 00:56:37] ‫אני רק אכתוב באופן כללי ‫את האלגוריתם שיוצא.
[00:56:38 - 00:56:39] ‫תראי לנו את האלגוריתם כתוב.
[00:56:59 - 00:57:01] ‫אז נכנעים ל-GMMM.
[00:57:13 - 00:57:17] ‫אז מתחילים את כל הפרמטרים שלנו, ‫שזה Pi,
[00:57:20 - 00:57:21] 0 עבור כל ה-K,
[00:57:23 - 00:57:27] ‫new-er, סקייל, סטיגמארק,
[00:57:28 - 00:57:31] ‫ליפרציה 0, אוקיי? ‫אנחנו מתחילים אותם.
[00:57:32 - 00:57:36] ‫בו K שווה 1 ל-K גדול.
[00:57:39 - 00:57:41] ‫ואז אנחנו נכנסים ל-A.
[00:57:57 - 00:58:06] ‫אנחנו עושים את ה-R1. ‫קודם כול, עבור כל אחד מהמנייה I, ‫מכל הנקודות שלה, ‫יש לנו ודאטה סט,
[00:58:07 - 00:58:16] ‫כי כל הקומפוננטות, עבור כל I וכל K. ‫אנחנו מחשבים את ה-R, I, K שווה.
[00:58:27 - 00:58:35] ‫אם את ה-R, P שווה 1 ל-T גדול.
[00:58:36 - 00:58:39] ‫יש, זה לפי ה... כמה קראתי שיותר לעשות, ‫אבל עד שזה מתכנס.
[00:58:42 - 00:58:44] R, I, K, T שווה
[00:58:44 - 00:58:46] למה שהגדרנו פה.
[00:58:48 - 00:58:50] ‫על T מינוס 1.
[00:58:51 - 00:58:52] שוב,
[00:58:52 - 00:59:01] ‫אניברסיין ול-XI שמוגדר לפי הפרמטרים ‫הקודמים שלנו.
[00:59:22 - 00:59:31] ‫אוקיי, ועכשיו, בעצם אני לא צריך ‫לחשב ממש את ה-E ב-E,
[00:59:32 - 00:59:33] ‫את ה-R, מספיק ל-Rספונסיביליטיז,
[00:59:33 - 00:59:37] ‫אני יודע כבר מתוכם ‫לחשב את ה-N-State.
[00:59:38 - 00:59:40] אוקיי, זה בעצם היה E-E, ‫ועכשיו,
[00:59:40 - 00:59:45] עבור כל K אני מחשב את שלושת הדברים האלה.
[00:59:47 - 00:59:48] אוקיי, אז מי הוא K
[00:59:50 - 00:59:51] שווה ל-R.
[00:59:53 - 00:59:55] ‫אחד, סיגמא-קיי,
[01:00:13 - 01:00:15] ‫פורם נמצא משתוכלל,
[01:00:16 - 01:00:18] ‫אחד של X מיוס מיור.
[01:00:22 - 01:00:28] ‫עוד כ-I, כ-B.A.T.E.
[01:00:52 - 01:01:22] ‫עוד כ-B.A.A.A.A.A.A.A.A.A.A.A.A
[01:01:22 - 01:01:27] ‫האם מי שרוצה לממש את זה,
[01:01:29 - 01:01:30] ‫שצריך תמיד לממש את זה
[01:01:33 - 01:01:34] ‫במרחב הלוג של ההתנגדויות,
[01:01:35 - 01:01:36] ‫לא מרחב ההתנגדויות,
[01:01:36 - 01:01:41] ‫כי תמיד יש פגיעות נומריות, ‫אם אתם שואלים את זה במרחב ההתנגדויות.
[01:01:44 - 01:01:45] ‫יש טריק
[01:01:46 - 01:01:48] ‫אחדות את זה כאן, אין מי שיחמש, נראה.
[01:01:52 - 01:02:00] ‫יש פונקציה של הפונקציות האלה ‫בכל חבילות, אוקיי.
[01:02:00 - 01:02:02] ‫לוג.
[01:02:07 - 01:02:08] ‫נראה לי להשתמש.
[01:02:09 - 01:02:12] ‫יש פונקציה שנקראת לוג קראנג אקסט.
[01:02:17 - 01:02:22] ‫אחד להשתמש, יש מימושים של הפונקציות האלה ‫בכל חבילות קוד.
[01:02:23 - 01:02:24] ‫זו בעצם פונקציה שמחשבת
[01:02:25 - 01:02:29] ‫בכמה מקומות שאתה חושב לוג ‫של סכום של אקספוננטים.
[01:02:30 - 01:02:33] ‫אוקיי, למשל פה,
[01:02:35 - 01:02:39] ‫אם אנחנו שומרים את הכול פה בלוג, ‫אנחנו צריכים לעשות אקספוננט של זה, ‫והסקורק של זה,
[01:02:39 - 01:02:41] ‫ואז אנחנו רוצים לחזור בחזרה ללוג,
[01:02:42 - 01:02:55] ‫אז כדאי להשתמש בזה ‫במקום לעשות את זה בצורה ישירה. ‫אז אנחנו תמיד נמצאים במרחב הלוג, ‫אף אחד לא נמצאים במרחב של ההתפלגות עצמה, ‫של ההתפלגות של ההתפלגות עצמה, ‫שזה מספרים שהכולים...
[01:02:56 - 01:03:00] המנעד שם בין הסתברות גבוהה לנמוכה ‫מאוד גדול,
[01:03:00 - 01:03:03] ‫שזה יכול לעשות שגיאות נומריות בחישורת.
[01:03:04 - 01:03:07] ‫הטריק הזה של איך שאלות סמק סתם מחושב,
[01:03:07 - 01:03:09] ‫שתמיד אפשר להוריד איזשהו קבוע,
[01:03:10 - 01:03:13] ‫ואז להישאר באזורים שהם סבירים ‫בחישור נומרית,
[01:03:14 - 01:03:16] ‫ולהוסיף את הקבוע הזה ‫אחרי שעושים את היתרון.
[01:03:21 - 01:03:24] מי שרוצה להשתמש בזה, ‫נממש את זה, ‫שיסתכל על העניין הזה, ‫כן אחרת
[01:03:26 - 01:03:27] הסיכוי שזה יעבוד מאוד נמוך.
[01:03:28 - 01:03:28] טוב, בואו נצא
[01:03:30 - 01:03:31] ‫עוד הפסקה,
[01:03:32 - 01:03:33] ‫עכשיו נתחיל לדבר על דגימה.
[01:04:00 - 01:04:01] ‫-תודה רבה.
[01:04:30 - 01:04:32] ‫-תודה רבה.
[01:05:00 - 01:05:01] ‫-תודה רבה.
[01:05:30 - 01:05:31] ‫-תודה רבה.
[01:06:00 - 01:06:02] ‫-תודה רבה.
[01:06:30 - 01:06:32] ‫-תודה רבה.
[01:07:00 - 01:07:02] ‫-היא חברת הכנסת גלאון.
[01:07:30 - 01:07:30] ‫-תודה רבה.
[01:08:00 - 01:08:02] ‫-היא חברת הכנסת גלאון.
[01:08:30 - 01:08:32] ‫-היא חברת הכנסת גלאון.
[01:09:00 - 01:09:02] ‫-היא חברת הכנסת גלאון.
[01:09:30 - 01:09:32] ‫-היא חברת הכנסת גלאון.
[01:10:00 - 01:10:02] ‫-היא חברת הכנסת גלאון.
[01:10:30 - 01:10:32] ‫-היא חברת הכנסת גלאון.
[01:11:00 - 01:11:02] ‫-היא חברת הכנסת גלאון.
[01:11:30 - 01:11:32] ‫-היא חברת הכנסת גלאון.
[01:12:00 - 01:12:02] ‫-היא חברת הכנסת גלאון.
[01:12:30 - 01:12:32] ‫-היא חברת הכנסת גלאון.
[01:13:00 - 01:13:02] ‫-היא חברת הכנסת גלאון.
[01:13:30 - 01:13:32] ‫-היא חברת הכנסת גלאון.
[01:14:00 - 01:14:02] ‫-היא חברת הכנסת גלאון.
[01:14:30 - 01:14:32] ‫-היא חברת הכנסת גלאון.
[01:15:00 - 01:15:02] ‫-היא חברת הכנסת גלאון.
[01:15:30 - 01:15:32] ‫-היא חברת הכנסת גלאון.
[01:16:00 - 01:16:02] ‫-היא חברת הכנסת גלאון.
[01:16:30 - 01:16:58] ‫אוקיי, אז ראינו GMM, ‫שאנחנו יודעים ללמוד את המציאות ‫המקסימום היפיות לידי אלגוריתם PM. ‫אנחנו יכולים ללמוד את הפרמטרים שלו, ‫למציאות דאטה.
[01:16:59 - 01:17:06] ‫לארגוריתם הזה גם באופן כללי, ‫אם שמחקתי כבר, אפשר להשתמש בו ‫למודלים משתנים חברואיים אחרים.
[01:17:07 - 01:17:11] ‫הנוסחות האלה השתנו, ‫זאת אומרת, יהיה משהו אחר פה,
[01:17:12 - 01:17:17] ‫ולא לכל מודל אפשר לעשות EM.
[01:17:17 - 01:17:28] ‫רק למודלים שמקיימים את התנאים, ‫אמרנו בהתחלה, שאנחנו יודעים לעשות ‫מקסימיזציה בצורה יעילה ללוג של פי אקס לזה,
[01:17:29 - 01:17:35] ‫ושאנחנו יכולים לחשב את הפוסטריאות, ‫את ההתפגרות של Z בהינתנת.
[01:17:38 - 01:17:39] ‫אוקיי, אז
[01:17:40 - 01:17:42] ‫אנחנו נדבר בהמשך על מודלים אחרים.
[01:17:48 - 01:17:50] ‫קודם כול, האם המודל GMM, שוב,
[01:17:50 - 01:17:52] ‫דיברנו על זה כבר קצת, ‫האם הוא,
[01:17:53 - 01:17:55] אמרנו שגאוסיאן הוא לא מודל טוב לתמונה.
[01:17:55 - 01:17:58] ‫האם אתה חושב ש-GMM הוא כן מודל טוב לתמונה?
[01:17:59 - 01:18:02] ‫טוטו מיגרסיאן. ‫-טוטו מיגרסיאן,
[01:18:02 - 01:18:08] ‫אבל עדיין הוא לא מודל טוב לתמונה, ‫מכמה סיבות. אחד, שוב, אנחנו צריכים לתפוס ‫את ה-CO�ריאנס על הכול,
[01:18:09 - 01:18:17] ‫אפילו יותר מזה, עכשיו יש כבר K קובריאנסים ‫לכל הדאטה שלנו, אוקיי? ‫אז לא חשפתי פה שום דבר ‫מבחינת המספר הפרמטרים.
[01:18:18 - 01:18:23] ‫אמרנו שאנחנו יכולים עם GMM לקרב כל התפלגות,
[01:18:24 - 01:18:25] ‫לפרט התפלגות של תמונות,
[01:18:26 - 01:18:29] ‫אבל לא אמרנו שום דבר ‫למספר הקונפוננטות.
[01:18:30 - 01:18:33] ‫יכול להיות שצריך המון קונפוננטות ‫בשביל לבדל
[01:18:34 - 01:18:36] ‫ההתפלגות המורכבת של תמונות,
[01:18:36 - 01:18:38] ‫וכנראה באמת המצב.
[01:18:38 - 01:18:43] ‫זאת אומרת, ניסו, לא כל כך הצליחו להגיע ‫להתפלגויות כל כך טובות של תמונות,
[01:18:45 - 01:18:49] ‫עם מודל כזה, GMM. מה שכן אפשר לעשות,
[01:18:50 - 01:19:04] ‫והוא מודל די טוב למה שנקרא פאץ' של תמונות, ‫איזשהו אזור קטן בתמונה, נגיד, ‫אזור במודל עשר על עשר כזה, של פיקסלים,
[01:19:05 - 01:19:07] ‫בתמונות טבעיות מאוד מורכבות,
[01:19:08 - 01:19:12] ‫ואפילו בתמונות כאלה מאוד מורכבות, ‫אזורים קטנים כאלה של עשר על עשר,
[01:19:12 - 01:19:14] ‫שזה 100 משתנים, 100 פיקסלים,
[01:19:15 - 01:19:19] ‫אז אותם אפשר למדל יחסית ביעילות ב-GLM.
[01:19:20 - 01:19:24] ‫ואנגיד, אם משהו כמו כמה עשרות קומפוננטות,
[01:19:25 - 01:19:27] ‫או 100-200 קומפוננטות,
[01:19:28 - 01:19:31] ‫זה מודל כבר די טוב שאפשר להשתמש בו ‫כדי לעשות כל מיני דברים.
[01:19:32 - 01:19:35] ‫למשל, אחד מהאלגוריתמים הפופולריים ‫שמשתמשים ב-GMM,
[01:19:36 - 01:19:38] ‫שמשתמשים עם GMM, ‫זה לניקוי רעשים מתמונת תמונת פרעה.
[01:19:40 - 01:19:46] ‫כאשר בעצם כל איזור בתמונה, ‫אנחנו מניחים שהפרעה היא הוראה ‫היא מותגרת של מיקסרמן וג'-GMM,
[01:19:47 - 01:19:49] ‫ובהינתן תמונה רועשת, אנחנו יכולים,
[01:19:50 - 01:19:51] ‫למצוא את הפוסטריו ולתמונה לכאבי.
[01:19:54 - 01:20:04] ‫חשבתי שאולי נגיע לזה היום, ‫כנראה היום לא נגיע לזה, ‫אבל אולי ניתן את הדוגמה הזאת בשבוע הבא, ‫ואני אנסה איכשהו בתרגיל ‫לשלב משהו שכתוב ‫לאימג' דינו איתנו.
[01:20:05 - 01:20:09] אוקיי,
[01:20:10 - 01:20:15] ‫אני רוצה כאן להתעכב שנייה, ‫מה זה אומר שה-GMM הוא פרייר על פאצ'ינג,
[01:20:17 - 01:20:19] ‫זה מוביל לנושא הבא שלנו,
[01:20:19 - 01:20:20] ‫שזה בזיגימות.
[01:20:31 - 01:20:35] ‫בעצם אני יכול להניח שבתמונה יש לי הרבה פאצ'ינג,
[01:20:36 - 01:20:40] ‫אני יכול לחלק אותה לפאצ'ינג כאלה לא חופפים,
[01:20:42 - 01:20:46] ‫אבל יכולנו לחלק את זה לפאצ'ינג חופפים. ‫אני יכול להגיד שכל הפאצ'ינג בתמונה,
[01:20:47 - 01:20:49] ‫וזה כולל כל האזורים בגודל 10 על 10,
[01:20:50 - 01:20:54] ‫כל אחד מהם מבחינתי הוא פאצ' של תמונה,
[01:20:55 - 01:21:08] ‫והפרייר שאני מניח על הפאצ'ים האלה, ‫זה שהם גאושן מיקסימו, של GMM. ‫וזה פראייר די טוב לדבר כזה, ‫אבל השאלה היא אם אני יכול לבנות מכזה דבר
[01:21:08 - 01:21:10] ‫מודל טוב של תמונה.
[01:21:12 - 01:21:13] ‫אז יש שתי בעיות.
[01:21:14 - 01:21:14] ‫קודם כול,
[01:21:15 - 01:21:22] ‫ברגע שהפרייר שלי רק מוגדר ‫על איזשהו פאצ', ‫אפילו שהוא מוגדר על כל הפאצ'ים בתמונה,
[01:21:22 - 01:21:25] ‫הוא לא יכול לתפוס תכונות גלובליות ‫של התמונה.
[01:21:26 - 01:21:30] ‫דיברנו למשל על כל מיני סימטריות, ‫אם זו תמונה של פנים,
[01:21:31 - 01:21:35] ‫אז הצבע של העין פה צריך להיות ‫אותו דבר כמו העין השנייה,
[01:21:36 - 01:21:40] ‫ויש כל מיני דברים, תכונות גלובליות ‫שקורות בתמונות.
[01:21:41 - 01:21:43] ‫אם יש לי פרייר שרק מוגדר על פאצ', ‫הוא לא יכול לתפוס כל מיני גלובליות.
[01:21:44 - 01:21:46] ‫זו בעיה אחת. בעיה שנייה,
[01:21:47 - 01:21:51] ‫לחלק מהאפליקציה זה לא נורא. ‫למשל, לאפליקציה של ניקוי רעשים,
[01:21:51 - 01:21:54] ‫שהרעש הוא מאוד מקומי, ‫זה אולי לא כל כך נורא.
[01:21:56 - 01:22:00] יש לי מספיק אינפורמציה ‫באזור של כל פיקסל,
[01:22:00 - 01:22:01] ‫שככה ש...
[01:22:02 - 01:22:11] ‫ש-ROLT, ‫שיהיה מספיק טוב כדי לנקוב את הרעשים. ‫אבל אם אני רוצה לייצר תמונות חדשות ‫או להגיד משהו כללי על התמונה, ‫זה כנראה לא יהיה מספיק.
[01:22:13 - 01:22:13] זאת בעיה אחת.
[01:22:14 - 01:22:16] ‫בעיה שנייה זה שאני לא...
[01:22:17 - 01:22:18] ‫מתוך ההתפלגות הזאת על פאצ'ים,
[01:22:19 - 01:22:22] ‫באופן כללי אין לי דרך ‫לבנות התפלגות על תמונות.
[01:22:24 - 01:22:27] ‫והסיבה היא שיש לי כאן אוסף של,
[01:22:28 - 01:22:29] ‫אפשר לקבל את זה פקטורים,
[01:22:30 - 01:22:34] ‫התפלגות, אבל אני לא יודע ‫איך לנרמל סך הכול את ההתפלגות הזאת.
[01:22:36 - 01:22:39] ‫אחד הדרכים לכתוב את זה ‫זה שאני יודע שההתפלגות של X,
[01:22:41 - 01:22:42] ‫אני יכול להניח שההתפלגות של X,
[01:22:44 - 01:22:48] ‫היא שווה פרופורציונית
[01:22:49 - 01:22:51] ‫לאיזושהי מכפלה של הרבה פקטורים,
[01:22:52 - 01:22:57] ‫כל אחד על איזשהו פתח אחר של אינדקסים, אוקיי? ‫-I.
[01:22:58 - 01:23:02] ‫נגיד יש לי איזושהי התפלגות ‫על האזור הזה, התפלגות על אזור אחר,
[01:23:02 - 01:23:04] ‫וככה על הרבה אזורים.
[01:23:04 - 01:23:08] ‫אני יכול להגיד ש-P של X ‫הוא פרופורציוני למכפלה הזאת,
[01:23:09 - 01:23:11] ‫אבל זה לא נותן לי דרך ‫לחשב ממש את P של X,
[01:23:11 - 01:23:14] ‫כי אין דרך יעילה לחשב את ה...
[01:23:16 - 01:23:19] ‫בין ערמלת לבית הדלת, ‫זה נקרא ה-Partition function.
[01:23:23 - 01:23:25] ‫למה זה מסובך? כי...
[01:23:26 - 01:23:32] ‫אוקיי, מתי זה פשוט? ‫אם נגיד כל ה... אם הייתי מגדיר ‫שההתפלגות שלי היא בלי חפיפות,
[01:23:32 - 01:23:33] ‫וכל הפרטישן בלי חפיפות,
[01:23:34 - 01:23:40] ‫אז זה לא בעיה. ‫אז בעצם אני יכול לחשוב על זה ‫בתור זה שאני מחלק את הדאטה שלי ‫לאזורים שונים,
[01:23:40 - 01:23:41] אבל כל אחד מהם
[01:23:42 - 01:23:44] ‫יש לי פריור, ‫הם לא מדברים אחד עם השני בכלל.
[01:23:45 - 01:23:49] ‫כל אחד מהם, אם אני יודע לנרמל ‫כל אחד מהם, אז המכפלה של כולם,
[01:23:50 - 01:23:51] ‫היא עדיין תהיה מנורמלת.
[01:23:53 - 01:23:54] ‫אבל אם יש חפיפות,
[01:23:55 - 01:24:02] ‫אז המכפלה, בעצם יש לנו משהו ‫שאפשר עכשיו עליו בתור, ‫אנחנו סופרים פעמיים כל מיני דברים, ‫יש כל מיני משתנים ‫שאנחנו סופרים אותם גם ב...
[01:24:03 - 01:24:04] ‫גם בהתפלגות אחת, ‫גם בהתפלגות שנייה,
[01:24:05 - 01:24:08] ‫ולכן המכפלה שלהם פשוט היא לא...
[01:24:09 - 01:24:12] ‫היא לא מנורמלת.
[01:24:13 - 01:24:16] ‫זה כמו שאנחנו הגדרנו משתנים ‫במקרים שהם בלתי תלויים,
[01:24:17 - 01:24:21] ‫לכן אנחנו יכולים להגדיר ‫את פשוט המכפלה שלהם ‫בתור ההתפלגות המשותפת,
[01:24:21 - 01:24:24] ‫אבל ברגע שיש כל מיני תלויות ‫ומשתנים, ‫ואם יש חטיפות,
[01:24:24 - 01:24:26] ‫זה בעצם מגדיר תלויות,
[01:24:27 - 01:24:29] ‫אז אנחנו צריכים לעשות כל מיני, ‫עוד פעם כללית, משפטי
[01:24:30 - 01:24:35] ‫הכלה והדחה כאלה, ‫שאנחנו צריכים לבטל ‫את כל הספירות פעמיים שיש לנו,
[01:24:35 - 01:24:37] ‫וזה מהר מאוד נהיה מאוד מאוד מסובך.
[01:24:38 - 01:24:43] ‫אני זורק עכשיו קצת כמה מושגים, ‫אנחנו לא ניכנס אליהם כל כך, ‫אבל סתם שתכירו,
[01:24:44 - 01:24:52] ‫אז יש דרך לנסות לטפל ‫בכל הדברים האלה. ‫אז קודם כול יש דרך ל... ‫כמו שהיה לנו בייז נט, זה היה...
[01:24:55 - 01:24:59] ‫בייז נטרוורקס, אתם זוכרים מה זה היה? ‫זה היה דרך לכתוב התפלגויות
[01:25:00 - 01:25:02] ‫על ידי דן, נכון?
[01:25:05 - 01:25:06] ‫עם גרף כזה.
[01:25:08 - 01:25:15] ‫פה לא היה לנו בעיה, כי בעצם ידענו ‫לבנות את ההתפלגות הזאתי ‫לפי איזשהו סדר.
[01:25:16 - 01:25:19] ‫ברגע שאנחנו מגדירים משהו כזה, ‫כמו ההתפלגות על כל הפאצ'ים של התמונות,
[01:25:20 - 01:25:25] ‫אז יש לנו בעצם הרבה מעגלים ‫בתוך הסדר שמייצר לנו את הדאטה.
[01:25:26 - 01:25:29] ‫אחת הדרכים להגדיר את זה, ‫הם קוראים לזה מרקוב נטוורקס.
[01:25:30 - 01:25:40] ‫וזה פשוט מוגדר על ידי גרף לא מכוון.
[01:25:41 - 01:25:43] ‫שיש את זה שם יכולים להיות במעגלים.
[01:25:45 - 01:25:53] ‫אוקיי, ויש כל מיני דרכים להתמודד עם זה, ‫אבל באופן כללי המשפחה הזאתי, ‫שאפשר להגדיר על ידי מרקוב נטוורקס,
[01:25:53 - 01:25:57] ‫זה שקול למשפחה הזאתי, ‫שאפשר להגדיר על ידי מכפלה ‫של כל מיני פקטורים.
[01:25:57 - 01:25:57] ‫פקטורים?
[01:25:59 - 01:26:00] ‫שאנחנו לא יודעים לנרמל.
[01:26:01 - 01:26:05] ‫ולכן, אם פה אנחנו יכולים לעשות ‫כל מיני דברים בצורה מדויקת,
[01:26:05 - 01:26:07] ‫פה תמיד אנחנו נצטרך לעשות ‫כל מיני
[01:26:10 - 01:26:11] ‫קירובים של דרך השם.
[01:26:12 - 01:26:17] ‫זה רק כדי שתכירו את השמות. ‫עוד דרך לייצג את זה, ‫זה נקרא לפעמים פקטור גרפי.
[01:26:17 - 01:26:27] ‫אוקיי, אז אני דורג פה כמה מונחים,
[01:26:28 - 01:26:31] ‫ועוד מונח שאני אדרוג ‫שאנחנו נכבוש אותו בהמשך הקורס.
[01:26:37 - 01:26:44] ‫אחת מהסיבות של כל החלק הראשון הזה ‫של הקורס זה שתסתכלו ‫בכל מיני מונחים, ‫ואחר כך כשנסתכל על מודלים ‫שנעבוד איתם ממש,
[01:26:45 - 01:26:46] ‫תכירו ותראו מאיפה זה בא,
[01:26:46 - 01:26:48] ‫גם אם אנחנו לא נכנסים לעומק ‫של כל אחד מהם.
[01:26:49 - 01:26:52] ‫אז מה שאנחנו כן נתקל בו, ‫זה נקרא Energy Based Models.
[01:27:05 - 01:27:06] ‫מודל,
[01:27:07 - 01:27:11] מודל הסתברותי על X, ‫שהוא מוגדר ככה על ידי איזשהו נמול,
[01:27:12 - 01:27:15] ‫פול E בחזקת
[01:27:16 - 01:27:18] ‫כאיזושהי פונקציה ‫שהיא בדרך כלל בשלילה,
[01:27:20 - 01:27:21] ‫שהיא
[01:27:24 - 01:27:26] E של X זה בעצם האנרגיה של X,
[01:27:28 - 01:27:30] ‫זה מספר שיכול להיות
[01:27:32 - 01:27:33] חיובי ושחידי,
[01:27:34 - 01:27:35] ‫יכול להיות כל הדבר.
[01:27:36 - 01:27:41] ‫בעצם דרכו אנחנו מגדירים את ההתפלגות על X ‫על ידי ככה שאנחנו לוקחים את האקספוננט של זה,
[01:27:42 - 01:27:44] ‫שהופך את זה למספר שהוא תמיד חיובי.
[01:27:45 - 01:27:46] ‫ואז מנרמלים את זה,
[01:27:46 - 01:27:49] ‫ככה שהאינטגרר וכל הדבר הזה יהיה שווה חי.
[01:27:52 - 01:27:56] ‫אבל זה מאפשר לנו שבזמן המידול ‫אנחנו יכולים להתעלם מכל הדבר הזה,
[01:27:56 - 01:28:00] ‫ואחר כך אנחנו צריכים ללכת כל מיני דברים ‫כדי לדעת להתמודד עם זה.
[01:28:01 - 01:28:02] ‫אז אחת מהדרכים בעצם,
[01:28:06 - 01:28:09] ‫הרבה פעמים הפונקציית האנרגיה הזאת, ‫היא תהיה פשוט
[01:28:09 - 01:28:11] סכום של הרבה פקטורים שונים.
[01:28:12 - 01:28:14] ‫כמו למשל, בדוגמה הזאת יש לנו תמונה,
[01:28:14 - 01:28:19] ‫על כל פאצ' יש לנו איזה פונקציית אנרגיה כזאת, ‫שזה סוג של לוג ההסתברות על הפאצ' הזה.
[01:28:20 - 01:28:25] ‫אז הרבה פעמים אנחנו נכתוב פי של איקס, ‫שבחלקת תקודד,
[01:28:26 - 01:28:29] ‫אי במינוס סכום, על כל מיני פקטורים,
[01:28:30 - 01:28:34] ‫איזה שהם פונקציות של הפקטורים האלה.
[01:28:34 - 01:28:46] ‫אבל הסיבה שאנחנו צריכים לעשות ‫כל מיני קירובים עם הדבר הזה, ‫זה בגלל שאנחנו לא יודעים ‫לנרמל את זה.
[01:28:47 - 01:28:49] ‫אוקיי? אז למשל, אם אני רוצה להגיד ‫משהו שבתמונות,
[01:28:50 - 01:28:57] ‫אני רוצה שתמונות, שבאופן, ‫שיהיה הסתברות גבוהה ‫לזה ששני פיקסלים שכנים, ‫יש להם את אותו ערך,
[01:28:58 - 01:29:01] ‫בהסתברות נמוכה שיש להם ערך שונה,
[01:29:02 - 01:29:04] ‫אז אני יכול לכתוב את זה בצורה כזאת, אוקיי?
[01:29:04 - 01:29:05] ‫אז זה מגביל,
[01:29:07 - 01:29:09] ‫במרקוב נטרור כזה.
[01:29:11 - 01:29:14] ‫נגיד, יש לי פה את כל האדלים,
[01:29:15 - 01:29:17] ‫אני אומר, כל פיקסל שכן.
[01:29:19 - 01:29:24] ‫אם הפיקסלים האלה יש להם את אותו ערך, ‫נגיד שזה תמונות בינארי, שחור או לבן,
[01:29:24 - 01:29:28] ‫אם הפיקסלים האלה יש להם את אותו ערך, ‫אז האנרגיה היא נמוכה, זה טוב,
[01:29:28 - 01:29:31] ‫ואם יש להם ערך שונה, ‫האנרגיה היא גבוהה.
[01:29:32 - 01:29:35] ‫יש לי כאן את הפקטור הזה ‫על כל אחד מהאדג'ים האלה.
[01:29:37 - 01:29:39] ‫בסך הכול ההתפלגות שלי, ‫הההסתברות של תמונה,
[01:29:40 - 01:29:47] ‫זו המכפלה של כל הפקטורים האלה. ‫אם אני אסתכל בהצפוננט, ‫זה יהיה הסכום של כל הלוג ‫של הפקטורים האלה.
[01:29:49 - 01:29:50] ‫אבל אין לי דרך לנרמל את הדבר הזה.
[01:29:51 - 01:29:54] ‫הנה, אני לא יכול לספר את זה ‫בתור דג כזה,
[01:29:54 - 01:29:56] ‫כי יש לי פה מעגלים בעצם.
[01:29:58 - 01:29:58] בסדר?
[01:29:59 - 01:30:02] ‫אוקיי, אז זה מונחים שתסגרו. ‫מרקול נטוורקס,
[01:30:03 - 01:30:07] ‫פקטורים גרס לא כל כך הגדרנו, ‫אבל זו דרך אחרת לחשוב על זה.
[01:30:07 - 01:30:10] ‫אנרג'י בייסט מודל, והעניין הזה ‫שאנחנו בעצם ממלאים
[01:30:11 - 01:30:12] ‫איזושהי פונקציית אנרגיה,
[01:30:13 - 01:30:19] ‫בלי שזה יהיה הסתברות או משהו כזה. ‫זה פשוט אומר מה אנחנו רוצים שיקרה יותר, ‫מה אנחנו רוצים שיקרה פחות,
[01:30:20 - 01:30:23] ‫ואז הפונקציית ההתפלגות שלנו ‫מוגדרת ככה.
[01:30:23 - 01:30:27] ‫על ידי זה שאנחנו לוקחים ‫לצפונן של הפונקציית האנרגיה ‫ומנרמלים אותה.
[01:30:28 - 01:30:33] ‫ואנחנו בדרך כלל לא נוכל ‫לעשות את זה במוצרות מדויקת, ‫אני צריך לחשוב על דרכים לקרב לזה.
[01:30:34 - 01:30:34] ‫אוקיי,
[01:30:36 - 01:30:48] עכשיו אנחנו נדבר על דרכים לקרב. ‫אז מתי אנחנו יכולים למצוא EM? אמרנו, ‫כשאנחנו יכולים לחשב בצורה יעילה, ‫לפחות את הלוג של P, X ו-Z,
[01:30:48 - 01:30:53] ‫וגם את הפוסטריו של Z בינתי X. ‫אבל הרבה דברים ‫אנחנו לא יכולים לעשות E.
[01:30:53 - 01:30:56] ‫למשל, כשיש לנו מספר אקספוננציאלי ‫של קומפוננטות,
[01:30:57 - 01:31:02] ‫לגב של מיקשור מודלים, ‫יש לנו, נגיד, על כל פאצ' יש לנו מקשר אחר, ‫סך הכול,
[01:31:03 - 01:31:06] ‫אנחנו יכולים לכתוב ל-H של מיקשור מודלים, ‫מספר מאוד גדול של קומפוננטות,
[01:31:08 - 01:31:15] ‫כבר אז אנחנו לא יכולים לעשות EM, ‫כי יש לנו את השלב של החישוב ‫בספונסיביליטי, ‫אז אנחנו צריכים לעבור ‫על כל ה-K'ים כדי לנרמם אותם.
[01:31:16 - 01:31:17] ‫אנחנו לא יכולים לעבור על כל ה-K'ים.
[01:31:17 - 01:31:24] ‫עוד מקרה זה כשה-Z שלנו ‫בכלל משתנה רציף, ‫אז גם אנחנו לא יכולים לעשות את זה ‫בצורה יעילה.
[01:31:25 - 01:31:32] ‫אז מה עושים בדבר הזה? ‫אז אנחנו נדבר על שתי דרכים.
[01:31:37 - 01:31:41] ‫היום אנחנו נדבר על שיטות דגימה.
[01:31:48 - 01:31:56] ‫ואני מקווה שנביא לשיעור הבא, ‫אני מדבר על שיטות שמבוססות על אוטימיזציה ‫שנקראות וריאשיונל אינפלנציה.
[01:31:58 - 01:32:06] ‫זה שתי שיטות להתמודד עם זה ‫שיש לנו התפלגויות ‫שאנחנו לא יכולים לעבוד איתן, ‫וגם לא יכולים לעשות איתן ‫כדומה עם EM.
[01:32:17 - 01:32:18] ‫אוקיי, אז אנחנו...
[01:32:28 - 01:32:29] ‫אם ב-MC...
[01:32:30 - 01:32:39] ‫רואים להגיע לשיטה שנקראת MC-MC, ‫יש פה פעמיים MC. ‫MC אחד זה מרקו צ'יין, ‫וה-MC השני זה מונטקארו.
[01:32:40 - 01:32:43] ‫אז נדבר רגע על כל אחד מהם בנפרד, ‫ואז נשלב אותם.
[01:32:45 - 01:32:46] אז מונטקארו,
[01:32:47 - 01:32:48] ‫התקלתם בזה כבר, לא?
[01:32:51 - 01:32:53] ‫אתם יודעים מה זה מונטקארלו? ‫שיטות מונטקארלו?
[01:32:56 - 01:32:56] ‫אתם רואים?
[01:32:59 - 01:33:04] שיטות מונטקארלו זה שיטות לחשב ‫איזשהו חישוב נומרי ‫עם אלגוריתם הסתברותי,
[01:33:05 - 01:33:06] ‫זה באופן כללי,
[01:33:06 - 01:33:13] ‫ובדרך כלל הכוונה היא לחשב אינטגרל ‫על ידי שיטה הסתברותית ‫שדוגמת דגימות.
[01:33:14 - 01:33:17] ‫אז נגיד שאנחנו רוצים לחשב ‫איזשהו אינטגרל של איזושהי אינטגרליה.
[01:33:22 - 01:33:29] ‫אז אנחנו יכולים לכתוב את הפונקציה הזאת ‫בתור אינטגרל
[01:33:30 - 01:33:34] ‫שאתם יכולים לפרק מתוך ה-fx הזה ‫למשהו שהוא יהיה התפלגות
[01:33:35 - 01:33:35] של x,
[01:33:36 - 01:33:38] ‫ועוד מה שנשאר זה יהיה f'
[01:33:40 - 01:33:42] ‫התפלגות זה לא משהו שהאינטגרל עליו שווה 1.
[01:33:44 - 01:33:46] ‫אז אם נוציא מתוך f הזה ‫איזשהו משהו שהאינטגרל
[01:33:47 - 01:33:48] עליו שווה 1,
[01:33:50 - 01:33:52] ‫אז בעצם מה שכתוב כאן
[01:33:54 - 01:33:55] ‫זה תוחלת
[01:33:56 - 01:33:58] לפי p של x
[01:33:58 - 01:34:00] ‫של פונקציה ה-f'tג.
[01:34:07 - 01:34:08] ‫אוקיי, זו תוחלת הזאת.
[01:34:08 - 01:34:10] ‫בתוחלת אנחנו יכולים לקרב
[01:34:12 - 01:34:12] על ידי
[01:34:14 - 01:34:15] ‫הן דגימות.
[01:34:24 - 01:34:26] ‫הדגימות מגיעות מההתפלגות הזאת.
[01:34:32 - 01:34:35] ‫אוקיי, זו שיטת מונטיקרל, ‫אנחנו רוצים לחשב איזשהו אינטגרל,
[01:34:35 - 01:34:37] ‫אנחנו חושבים על האינטגרל הזה, ‫כותבים אותו בתור
[01:34:38 - 01:34:39] ‫זוכרת של משהו,
[01:34:39 - 01:34:40] ‫כי איזושהי התפלגות,
[01:34:41 - 01:34:45] ‫ואז אנחנו דוגמים נקודות ‫מתוך ההתפלגות הזאת ‫ומחשבים את הממוצע,
[01:34:45 - 01:34:47] ‫וזה מקרב לנו את האינטגרל.
[01:34:49 - 01:34:50] ‫לא נתקלתם בזה לפני?
[01:34:54 - 01:35:01] ‫אוקיי, אז זה באופן כללי ‫שכנות מונטיקרל, ‫משתמשים בזה למשל כדי לחשב שטחים ‫של צורות שקשה לחשב בצורה אנליתית.
[01:35:05 - 01:35:06] ‫משתמשים בזה כדי לחשב את פיי,
[01:35:06 - 01:35:11] ‫בתור היחס של הנקודות ‫שנופלות בתוך המעגל,
[01:35:13 - 01:35:14] ‫מדוגמים את הנקודות מתוך
[01:35:16 - 01:35:17] ‫הריבוע שחושב את המעגל.
[01:35:19 - 01:35:19] ‫אז
[01:35:20 - 01:35:26] באופן כללי זה דרך לחשב את האינטגרל הזה. ‫עכשיו, גם אם אנחנו מראש מגיעים ‫למצב שיש לנו פה כבר איזושהי התפלגות,
[01:35:27 - 01:35:28] ‫לכן אנחנו לא צריכים את השלב הראשון.
[01:35:30 - 01:35:34] ‫אוקיי? למשל, אנחנו כבר ראינו ‫שימוש במונטקרלו,
[01:35:35 - 01:35:35] ‫מישהו אחר אפוא?
[01:35:37 - 01:35:37] ‫אורס.
[01:35:39 - 01:35:40] ‫אתה עשינו את הקירוב הזה.
[01:35:43 - 01:35:44] ‫אתם רואים שם ראינו אותו כבר?
[01:35:46 - 01:35:48] ‫כשאנחנו עושים ‫מצימום לייקליות,
[01:35:51 - 01:35:52] MLE,
[01:35:53 - 01:35:58] ‫אז בעצם אנחנו מנסים לקרב את התוחלת,
[01:35:59 - 01:36:03] ‫הדברים זה שזה כמו לקרב תוחלת ‫לפי פי דאטה.
[01:36:03 - 01:36:07] ‫של הלוג
[01:36:08 - 01:36:10] ‫של ההסתברות שלנו.
[01:36:13 - 01:36:14] ‫זו הפונקציה שלנו.
[01:36:15 - 01:36:20] ‫אנחנו רואים את התוחלת שלה ‫לפי ההסתברות של הדאטה,
[01:36:21 - 01:36:22] ‫וזה שווה בערך
[01:36:24 - 01:36:30] ‫לקחת כמה דגימות מההתפלגות הזאת ‫שמייצרת את הדאטה.
[01:36:33 - 01:36:35] ‫נראה, נראה, נדבר לרנט.
[01:36:38 - 01:36:39] ‫זה יותר ממוצע של הדאטה.
[01:36:42 - 01:36:45] ‫כאשר XI מגיע מההתפלגות הזאת ‫של הדאטה.
[01:36:47 - 01:36:50] ‫ובעצם הדאטה-סט שאנחנו עושים, ‫הטריינינג-סט שאנחנו עושים.
[01:36:51 - 01:36:55] ‫זה בעצם שיטת מונתקה, ‫אבל כרצים לחשב את כל התוחלת הזאת, ‫אנחנו חישבנו רק
[01:36:58 - 01:37:00] ‫לממוצע על תנאי הדגימות ‫שיש לנו בטריינינג-סט.
[01:37:00 - 01:37:04] ‫זו דוגמה ראשונה. ‫זו דוגמה ראשונה. ‫זו דוגמה שנייה.
[01:37:09 - 01:37:18] ‫למשל אנחנו רוצים לחשב פי של איקס ‫במשתנה חבוי,
[01:37:18 - 01:37:20] ‫במודל עם משתנה חבוי.
[01:37:22 - 01:37:24] ‫אנחנו יודעים שפי של איקס ‫שווה
[01:37:27 - 01:37:29] למגרה לפי אפילו זה.
[01:37:30 - 01:37:31] ‫זה פי של איקס בהינתנדד.
[01:37:37 - 01:37:40] ‫זה מחוק ההסתברות השלימה,
[01:37:41 - 01:37:47] ‫איך ככה שהמודל שלנו ממודד, נכון? ‫על ידי איזשהו פרויור על המשתנה החבוי,
[01:37:48 - 01:37:50] ‫ולייקליות של איקס בהינתנדד.
[01:37:51 - 01:37:55] ‫ואם אנחנו רוצים לחשב את פי של איקס, ‫אנחנו יכולים לחשוב על זה, על פי של איקס ‫בתור
[01:37:56 - 01:37:57] האינטגרל הזה.
[01:37:57 - 01:38:00] ‫אבל זה שווה בערך
[01:38:02 - 01:38:04] ‫לממוצע של n בדימות
[01:38:07 - 01:38:08] ‫של הפונקציה הזו.
[01:38:10 - 01:38:12] ‫אנחנו דוגמים את z כפיים,
[01:38:13 - 01:38:16] ‫כשה z i נגידה מהפרעה היום.
[01:38:23 - 01:38:26] ‫למשל אנחנו רוצים לחשב, יש לנו נקודה, אנחנו רוצים לחשב את ה...
[01:38:27 - 01:38:30] ‫מה ההסתברות של הנקודה הזאת, ‫בהינתן המודל שלנו,
[01:38:31 - 01:38:34] ‫אנחנו יכולים לדגום מלא דגימות ‫מהפרעה היום של z,
[01:38:35 - 01:38:38] ‫לחשב בהינתן כל דגימה של z ‫מה ההסתברות של הנקודה, ולמצע את זה.
[01:38:42 - 01:38:42] ‫זו דרך,
[01:38:43 - 01:38:49] ‫לא כל כך יעילה בדרך כלל, ‫אנחנו צריכים, נצטרך הרבה דוגמאות של z ‫כדי שהדבר הזה יהיה מדויק,
[01:38:51 - 01:38:53] ‫אבל עדיין משתמשים בזה לפעמים ‫בשביל לעשות אבלואציה.
[01:38:54 - 01:38:59] ‫למדנו איזשהו מודל, ‫אנחנו רוצים לדעת עד כמה הוא טוב,
[01:38:59 - 01:39:01] ‫אנחנו נדגום הרבה דגימות מה-Latent,
[01:39:02 - 01:39:06] ‫ונחשב את ההסתברות של הדאטה שלנו, ‫ניתן כל הדגימות האלה של ה-Latent,
[01:39:07 - 01:39:07] ‫ונמצא
[01:39:08 - 01:39:09] ‫זה ייתן לנו
[01:39:11 - 01:39:13] איזשהו מדע דעה בקביש ה-X.
[01:39:24 - 01:39:26] ‫זה נקרא נולטיקלו.
[01:39:28 - 01:39:31] ‫אנחנו נותנים לב, ‫אפשר עכשיו לדבר לפתור איזשהו משערך
[01:39:34 - 01:39:36] ‫כי של X, שהוא בעצמו משתנה מקרי,
[01:39:37 - 01:39:39] ‫שהוא נדגם מאיזושהי התפלגות,
[01:39:40 - 01:39:44] ‫פונקציה של משהו שנדגם מאיזושהי התפלגות,
[01:39:44 - 01:39:46] ‫הדבר הזה באיזשהו משתנה מקרי.
[01:39:47 - 01:39:50] ‫אז נגיד שאנחנו רוצים לחשב, ‫הסיגנל שאנחנו רוצים לחשב, ‫נקרא לו רגע X,
[01:39:51 - 01:39:52] ‫אז לזה נקרא
[01:39:54 - 01:40:04] ‫S גובה, אוקיי, זה משערך של S. ‫אנחנו רוצים לראות עכשיו ‫תכונות של המשערך הזה, ‫אבל קודם כול התוחלת שלו,
[01:40:06 - 01:40:08] ‫מה התוחלת של S גובה?
[01:40:10 - 01:40:11] ‫אתה נוחש?
[01:40:16 - 01:40:19] ‫התוחלת של S גובה, ‫התוחלת של הדבר הזה,
[01:40:20 - 01:40:22] ‫אני יכול להכניס את התוחלת פנימה.
[01:40:24 - 01:40:26] ‫בואו ננסה לדבר על כאן כללים עכשיו.
[01:40:31 - 01:40:32] ‫נגמר ספציפית,
[01:40:33 - 01:40:37] ‫ננסה לדבר על הנקרא הכללי. ‫זה מה שאנחנו רוצים לחשב, S.
[01:40:53 - 01:41:23] ‫כן, כן, נעשה את זה ככה. ‫נגיד ש...
[01:41:23 - 01:41:25] ‫לדבר הזה אני קורא S, ‫זה מה שאני רוצה לחשב,
[01:41:26 - 01:41:28] ‫זה המשערך שלו בין ובין S גובה.
[01:41:30 - 01:41:32] ‫מה התוחלת של S גובה?
[01:41:33 - 01:41:34] ‫בתוחלת של S גובה אני יכול להכניס
[01:41:35 - 01:41:38] ‫את התוחלת לתוך
[01:41:40 - 01:41:40] הסכום הזה.
[01:41:42 - 01:41:43] ‫זה יהיה 1 חלקי n
[01:41:44 - 01:41:48] ‫סכום של התוחלת של הפונקציה.
[01:41:51 - 01:41:52] ‫התוחלת של הפונקציה
[01:41:53 - 01:41:55] ‫זה בדיוק כמו שאני רוצה לשערך.
[01:41:57 - 01:42:02] ‫זה פשוט S. ‫אז יש לי ממוצע של פונקציה כ-n,
[01:42:02 - 01:42:04] ‫של הרבה דברים שכל אחד מהם הוא S,
[01:42:06 - 01:42:10] ‫אז בסך הכול קיבלתי S. ‫אז התוחלת של המשערך S גובה
[01:42:11 - 01:42:18] ‫זה S, זה מה שרציתי לשערך. ‫איך נקרא משערך כזה? ‫שהתוחלת שלו זה מה שרציתי לשערך מראש? ‫-שהוא לא מוטה.
[01:42:18 - 01:42:23] ‫כן, זה משערך בלתי מוטה. ‫אז שיטת מוטה נותנת לנו משערך שהוא בלתי מוטה,
[01:42:24 - 01:42:26] ‫תכונה אחרת שמעניין איתו זה מה וריאנס.
[01:42:27 - 01:42:27] ‫זאת אומרת,
[01:42:29 - 01:42:31] ‫עד כמה אני קרוב ל-S באמת,
[01:42:32 - 01:42:39] ‫אני עושה הרבה דגימות, ‫זה משתנה מקרה, ‫יכול להיות שבתוחלת הוא S, ‫אבל הוא תמיד מאוד מאוד רחוק. זאת אומרת, פעם אחת הוא יהיה
[01:42:40 - 01:42:43] ‫עם פגיעה מאוד גדולה לכיוון אחד, ‫פעם אחת הוא יהיה פגיעה מאוד גדולה לכיוון השני.
[01:42:45 - 01:42:46] ‫אז מה הווריאנס של S. קודם כול?
[01:42:49 - 01:42:51] ‫אז הווריאנס של S. קודם כול, אם אני מכניס אותו לתוך הסכום,
[01:42:52 - 01:42:55] ‫קודם כול יש לי את ה-1 חלקי n, ‫אז הוכיח את ה-1 חלקי n בריבוע,
[01:42:57 - 01:42:59] ‫ואז אני מחליץ אותו לתוך הסכום.
[01:43:00 - 01:43:02] ‫יש לי את הווריאנס של כל אחד מהפגיעות האלה.
[01:43:09 - 01:43:12] ‫כשאני סוכן וריאנסים
[01:43:13 - 01:43:15] ‫של משתנים בלתי תלויים,
[01:43:16 - 01:43:20] ‫עוד אני מקבל n פעמים את הווריאנס, נכון? בסך הכול יש לי 1 חלקי n
[01:43:20 - 01:43:37] ‫כפול הווריאנס של F. ‫כן, אז ככל שיש לי יותר דגימות, ‫הווריאנס הולך וקטן.
[01:43:40 - 01:43:43] ‫לא, לא, אבל לפעמים צריך המון דגימות ‫כדי שיהיה וריאנס סביר שאפשר לבוא ממנו.
[01:43:45 - 01:43:47] ‫אוקיי, זה שיטות מונטי קו, באופן כללי.
[01:43:50 - 01:44:05] ‫אפשר לדבר על האמצעי השני, ‫שזה מונקוב צ'יין.
[01:44:06 - 01:44:08] ‫אני מדבר על משהו שנקרא ‫אימפורטן סמפלינג.
[01:44:09 - 01:44:11] ‫זה גם מונטי קלטנבו.
[01:44:20 - 01:44:37] ‫זה שיטה לחשב משערך כזה מונטי קארלו, ‫אבל כשאני לא יודע לדגום,
[01:44:39 - 01:44:41] ‫מההתפלגות שאני רוצה לדגום.
[01:44:42 - 01:44:44] ‫אז אני רוצה להגיד לחשב את החלק
[01:44:46 - 01:44:49] ‫לפי איזשהו b של x באיזושהי פונקציה.
[01:44:50 - 01:44:54] ‫אז לפי מנתקה הלאה רגיל, ‫הייתי יכול לקחת, לדגום הרבה דגימות מ-p,
[01:44:55 - 01:44:56] ‫ולחשב את הממוצע.
[01:44:57 - 01:45:00] ‫אבל יכול להיות שההתפלגות p הזאתי ‫היא התפלגות שאני לא יודע לדגום ממנה.
[01:45:03 - 01:45:04] ‫אז מה אני יכול לעשות?
[01:45:07 - 01:45:10] ‫במונטי קארלו, ‫אפשר לשים לב שאת הדבר הזה, ‫אפשר לכתוב אותו
[01:45:13 - 01:45:14] אה, פי של e
[01:45:17 - 01:45:19] dx, נכון? ‫זו ההגדרה של המתבוחלת.
[01:45:20 - 01:45:21] ‫ועכשיו אני יכול
[01:45:28 - 01:45:31] מה שאני רוצה, להכפיל במשהו, ‫לחלק במשהו.
[01:45:40 - 01:45:44] ‫זה תמיד נכון, ‫הכפלתי באיזושהי פונקציה של x ‫וחילקתי באותה פונקציה,
[01:45:45 - 01:45:47] ‫אבל אם הפונקציה הזאתי ‫היא גם התפלגות ב-x,
[01:45:48 - 01:45:51] ‫אז אני יכול עכשיו לחשוב על הדבר הזה ‫בתור תוכלת לפי q של x
[01:45:52 - 01:45:53] של כל השאר.
[01:45:55 - 01:45:58] ‫נכון? אז אני יכול לחשוב בתור תוכלת
[01:45:59 - 01:46:00] לפי q של x,
[01:46:02 - 01:46:06] ‫וכל מה שנשאר כאן, שזה t של x חלקי q של x,
[01:46:07 - 01:46:08] ‫כל f
[01:46:09 - 01:46:10] של x ב-x.
[01:46:12 - 01:46:13] ‫ומה זה אומר?
[01:46:13 - 01:46:16] ‫זה אומר שלפי מנתקה, ‫אבל עכשיו אני יכול לשערך את זה.
[01:46:18 - 01:46:20] ‫לפי ממוצע של נקודות.
[01:46:24 - 01:46:25] ‫1 עד n
[01:46:28 - 01:46:28] ‫אלא
[01:46:29 - 01:46:30] תוכלת גדולות.
[01:46:35 - 01:46:39] y, x, i, כש-x, i מגיע עכשיו מ-q של x.
[01:46:41 - 01:46:44] ‫אז בעצם אני יכול לשערך את התוכלת המקורית,
[01:46:44 - 01:46:46] ‫על ידי דגימות שהגיעו מהתפלגות אחרת.
[01:46:47 - 01:46:50] ‫למה ההתפלגות שלפיה אני רוצה ‫לחשב את התוכלת?
[01:46:51 - 01:46:53] ‫מה שאני צריך לעשות זה כל דגימה שיש לי,
[01:46:54 - 01:46:56] ‫קודם רק ה-eCD ממוצע של הפונקציה הזאת.
[01:46:57 - 01:47:01] ‫עכשיו אני צריך לתקן, לחשב איזשהו משקל אחר ‫לכל נקודה,
[01:47:02 - 01:47:06] ‫שבעצם מתקן את זה שלא לקחתי את הנקודה, ‫לא דגמתי את הנקודה הזאת לפי p, ‫אלא לפי q.
[01:47:07 - 01:47:11] ‫אני מכפיל ב-p ומחלק ב-q. זאת אומרת, אם דגמתי נקודה שלפי p
[01:47:12 - 01:47:16] ‫היה אמור להיות לה סיכוי מאוד קטן ‫שאני אדגום אותה, ‫ולפי q היה סיכוי גבוה,
[01:47:17 - 01:47:21] ‫אז אני אחלק אותה במספר קטן, ‫ניתן לה משקל קטן בממוצע שלה.
[01:47:22 - 01:47:26] ‫ולהפך גם, אם בעצם היה אמור להיות לה ‫סיכוי מאוד גבוה ב-p,
[01:47:28 - 01:47:31] ‫אבל ב-q היא קורית רק לעיתים נדירות, ‫ברגע שהיא קורית,
[01:47:32 - 01:47:36] ‫אז אני מכפיל אותה במספר מאוד גדול, ‫אני מחשיב אותה יותר בממוצע שלה.
[01:47:37 - 01:47:40] ‫זה נקרא שיטת important something, ‫זה גם מאוד שימושי.
[01:47:42 - 01:47:47] ‫שוב, אנחנו הרבה פעמים מדברים ‫על מודלים שאנחנו, קשה לנו לעבוד איתם,
[01:47:48 - 01:47:50] ‫והרבה פעמים קשה לנו לדגום ‫גם מהמודלים האלה,
[01:47:51 - 01:47:54] ‫ולכן זה חשוב שיהיה לנו דרך ‫בכל זאת לחשב תוחלת,
[01:47:55 - 01:47:57] ‫כשהדגימות הגיעו ‫מהתפלגות אחרת.
[01:47:59 - 01:48:01] ‫זה עוד...
[01:48:01 - 01:48:04] ‫האם פה זה מה שהיה סביב, פשוט?
[01:48:05 - 01:48:11] ‫כן, כן. פה התחלתי מזה כבר שזה תוחלת, ‫אז כבר קראתי לזה F.
[01:48:11 - 01:48:13] ‫אם זה ילכים אינטגרציה.
[01:48:27 - 01:48:30] ‫יש כאן איזו נקודה מעניינת ‫שנראה לי ששווה להזכיר.
[01:48:30 - 01:48:41] ‫אני קצת זורק עליכם מלא מושגים, ‫אני מקווה שזה מועיל בסופו של דבר, ‫וגם מוצלח, אז תוכלו אחר כך להסתכל ‫כשאנחנו ניתקל במושגים,
[01:48:41 - 01:48:42] ‫אם אחר כך.
[01:48:44 - 01:48:54] ‫עוד נקודה שנראה לי שהיא קצת... ‫שהיא מעניינת זה להגיד, אוקיי, ‫נגיד שאני יכול לדגום מכל Q, ‫מה ה-Q הכי טוב לדגום ממני?
[01:48:56 - 01:48:57] ‫מה זאת אומרת, הכי טוב?
[01:48:58 - 01:49:06] ‫זה אומר שהווריאנס של הדגימות שלי ‫יהיה כמה שיותר קטן, ‫של המשורת שלי יהיה כמה שיותר קטן, ‫שאני צריך כמה שפחות דגימות ‫כדי להגיע לווריאנס קטן, בסדר?
[01:49:08 - 01:49:09] ‫והתוחלת בכל מקרה תישאר,
[01:49:09 - 01:49:17] ‫זה יישאר בלתי מוטט, ‫היא תישאר עד כמה שיותר נכון,
[01:49:18 - 01:49:20] ‫אבל הווריאנס כן מושפע מ-Q.
[01:49:23 - 01:49:30] ‫אפשר לראות שה-Q ‫שממזער את הווריאנס.
[01:49:39 - 01:49:46] ‫זה מה שיהיה לי כתוב כאן, זה יהיה,
[01:49:47 - 01:49:53] ‫הווריאנס יהיה שווה ל-1 חלקי Kn,
[01:49:55 - 01:49:58] ‫הוא לוריאנס של PX,
[01:49:59 - 01:50:02] FX חלקי QX.
[01:50:05 - 01:50:06] מה ה-Q שמזער את הווריאנס הזה?
[01:50:07 - 01:50:09] ‫-Q שהוא כמה שיותר קרוב למה שיש במונה.
[01:50:10 - 01:50:13] ‫אם ה-Q בדיוק שווה למה שיש במונה,
[01:50:13 - 01:50:16] ‫אז מה שכתוב כאן, ‫או אפילו פרופורציונלי למה שכתוב במונה,
[01:50:17 - 01:50:18] ‫אז מה שכתוב כאן זה קבוע,
[01:50:20 - 01:50:21] ‫לכן הווריאנס שלו הוא 0.
[01:50:24 - 01:50:27] ‫זה Q שממזער את זה, ‫זה ה-Q שהוא פרופורציונלי לזה,
[01:50:27 - 01:50:28] ‫זה Q של X, זה התפלגות,
[01:50:29 - 01:50:31] A שווה לאיזשהו
[01:50:34 - 01:50:35] חלקי Z, כפול
[01:50:36 - 01:50:37] ‫P של X, F של X.
[01:50:41 - 01:50:48] ‫שם הוויליאנס של זה הוא אפילו 0, זאת אומרת, ‫אספיק דגימה אחת כדי לחשב את זה, ‫אבל זה לא מהות, כי לחשב את ה-Z של זה,
[01:50:49 - 01:50:50] ‫זה בעצם
[01:50:51 - 01:50:53] בדיוק לפתור את האינטגרל ‫שאנחנו צריכים לפתור בכפיל.
[01:50:55 - 01:50:59] ‫אבל עדיין, אנחנו נראה בכל מיני שיטות ‫שאנחנו מקרבים, כל מיני דברים,
[01:50:59 - 01:51:02] ‫שהרבה פעמים אנחנו נרצה שה-Q הזה ‫שאנחנו מחשבים עליו,
[01:51:03 - 01:51:04] ‫יהיהיה כמה שיותר קרוב,
[01:51:05 - 01:51:05] ‫ל...
[01:51:07 - 01:51:07] ‫ל...
[01:51:08 - 01:51:10] ‫ל... לאיבר הזה. ‫בדוגמה הזאתי,
[01:51:11 - 01:51:13] מה זה יוצא?
[01:51:16 - 01:51:18] ‫בדוגמה שתיים,
[01:51:24 - 01:51:25] ‫כל זה Q כוכב
[01:51:28 - 01:51:29] Q כוכב X,
[01:51:31 - 01:51:33] ‫ולדאי אחד חלקי זה,
[01:51:33 - 01:51:35] ‫P של...
[01:51:35 - 01:51:37] ‫שמה המשתמשת שלנו ל-Z, פי של Z,
[01:51:39 - 01:51:41] ‫אולי לא כדאי כולל עם Z כאן,
[01:51:42 - 01:51:44] ‫זה Z אחר נקרא לו.
[01:51:50 - 01:51:53] ‫זה פרופורציוני ל-P של Z,
[01:51:54 - 01:51:57] ‫כפול הפונקציה שאנחנו עושים על זה, את ה...
[01:51:58 - 01:52:00] ‫אתה זוכרת, שזה X בינתן Z.
[01:52:03 - 01:52:07] ‫איזה...
[01:52:08 - 01:52:09] איזה הסתברות
[01:52:09 - 01:52:13] ‫היא פרופורציונית ל-P של Z כפול P של X בינתן Z?
[01:52:15 - 01:52:15] ‫נכון, זה?
[01:52:17 - 01:52:19] ‫-P של Z בינתן Z.
[01:52:24 - 01:52:25] ‫בעצם אנחנו נראה גם, שוב פעם,
[01:52:25 - 01:52:29] ‫משהו שחוזר על פה כל הזמן, ה-Posterior הזה של ה-Latent,
[01:52:29 - 01:52:31] ‫בינתן הדאטה שאנחנו רואים,
[01:52:31 - 01:52:36] ‫זה משהו שאנחנו נתקלים בו הרבה. ‫גם ב-EM היו צריכים אותו כדי לחשב,
[01:52:36 - 01:52:40] ‫וגם כשאנחנו עושים שיטות דגימה, ‫הרבה פעמים יצא לנו שלקרב
[01:52:41 - 01:52:44] את הדבר הזה, ‫זה הכי טוב שאנחנו יכולים לעשות.
[01:52:45 - 01:52:48] ‫אם אנחנו יודעים לחשב את זה, ‫אז בדרך כלל הבעיות שלנו יפתרו.
[01:53:01 - 01:53:08] ‫אז עוד עכשיו דיברנו על למה אנחנו רוצים ‫להיות מסוגלים לדגום.
[01:53:09 - 01:53:11] ‫אנחנו יכולים, עם הדגימות האלה, ‫לחשב
[01:53:12 - 01:53:22] בעזרת שיטות מונטי קרלו ‫או אימפורטן סמפלינג, ‫לחשב כל מיני אינטגרלים או תוכלות ‫או התפלגויות שוליות,
[01:53:22 - 01:53:23] ‫כמו שיש לנו כאן,
[01:53:24 - 01:53:27] ‫אבל הרבה פעמים יהיה לנו קשה לדגום את הנקודות,
[01:53:29 - 01:53:30] מתוך ההתפלגויות האלה,
[01:53:30 - 01:53:34] ‫אנחנו צריכים איזו שיטה ‫שתעזור לנו לעשות את זה,
[01:53:34 - 01:53:40] ‫ו-mark of chain זו שיטה לייצר דגימות ‫מהתפלגויות שאנחנו לא יכולים לדגום מהן בצורה ישירה.
[01:53:41 - 01:53:46] ‫אז כשאומרים MCNC, ‫זאת אומרות שאנחנו משתמשים ב-mark of chain ‫כדי לייצר את הדגימה,
[01:53:46 - 01:53:50] ‫ואז משתמשים במונטי קרלו ‫כדי להשתמש בדגימה הזאתי, לחשב משהו.
[01:53:53 - 01:53:56] ‫אנחנו רוצים רגע לזכור את מה זה ‫-mark of chain.
[01:53:56 - 01:54:01] ‫התפלגתם בזה באיזשהו קורס?
[01:54:05 - 01:54:06] ‫אפשר את מרקוב?
[01:54:09 - 01:54:10] ‫זה באנגלית,
[01:54:11 - 01:54:13] ‫שתבינו איפה מגיע ה-NC, מרקולוב.
[01:54:17 - 01:54:20] ‫אז מרקוב chain זה באופן פלילי, כשאני,
[01:54:20 - 01:54:29] ‫יש לי תהליך שמייצר איקסים, ‫שמייצר איזה שהן דגימות,
[01:54:30 - 01:54:35] ‫כאשר ההסתברות של XT ‫בהינתן כולל מה שהיה קודם,
[01:54:36 - 01:54:39] ‫X1 עד XT מינוס 1,
[01:54:40 - 01:54:42] ‫יש לו את התכונה המרקובית.
[01:54:43 - 01:54:47] ‫כלומר, זה שווה להתפלגות של XT ‫והינתן רק ה-X האחרון.
[01:54:48 - 01:54:52] ‫זה תהליך מרקוב, תהליך בלי זיכרון, ‫הוא לא צריך
[01:54:53 - 01:54:55] ‫לזכור את כל ה-X'ים עד עכשיו, ‫רק את ה-X האחרון.
[01:54:57 - 01:55:01] ‫אם הדבר הזה שווה לזה, ‫אז זה תכונת המרקוביות,
[01:55:01 - 01:55:07] ‫זה תהליך מרקובי, ‫אם זה תהליך מזמן דיסקטי, ‫אז קוראים לזה שרשרת מרקוביות.
[01:55:09 - 01:55:10] ‫זו ההגדרה הראשונה.
[01:55:10 - 01:55:11] ‫ההגדרה השנייה
[01:55:14 - 01:55:16] ‫שנשתמש בה זה התפלגות סטציונרית.
[01:55:17 - 01:55:19] ‫זה שרשרת מרקוב.
[01:55:47 - 01:55:53] ‫ההגדרה ההתפלגות של הצד הבא, ‫שאני יכול לחשב אותו על ידי
[01:55:56 - 01:55:57] ‫ההתפלגות שהיה בצד המקודם,
[01:55:58 - 01:55:58] ‫היא
[01:56:01 - 01:56:02] X כפול.
[01:56:14 - 01:56:16] ‫אני מסתכל על כל ההקשורת של מה שהיה קודם.
[01:56:17 - 01:56:20] ‫נשאר
[01:56:45 - 01:56:45] מה זה אומר?
[01:56:45 - 01:56:51] ‫זאת אומרת שאם היה לי התפלגות קודם, ‫התפלגות בצד ה-T מינוס אחד, ‫ידעתי שההתפלגות שלי היא פי.
[01:56:53 - 01:56:57] ‫זאת אומרת, ההסתברות שאני אהיה ‫בכל אחד מההתכשירויות של X הייתה פי.
[01:56:58 - 01:57:00] ‫בצד הבא אני אשאר עדיין באותה הסתברות.
[01:57:02 - 01:57:08] ‫יש לי משהו שמגדיר לי ‫מה ההסתברות מעבר ‫בין כל X ל-T ל-X אחר,
[01:57:08 - 01:57:09] ל-X הבא.
[01:57:11 - 01:57:14] ‫אם אחרי הצד המעבר הזה ‫אני נשאר באותה התפלגות,
[01:57:14 - 01:57:15] ‫זה עוד אומר שההתפלגות היא סטציונלית.
[01:57:19 - 01:57:22] ‫אנחנו נראה דוגמה שזה יהיה יותר ברור.
[01:57:23 - 01:57:25] ‫עוד תכונה נקראת ארגודלט.
[01:57:30 - 01:57:31] ‫זה אומר ש...
[01:57:31 - 01:57:47] ‫אם לכל X ו-X תג, ‫שזה שני ערכים שונים ש-X יכול לקבל,
[01:57:49 - 01:57:50] ‫קיים
[01:57:52 - 01:57:53] איזשהו N גדול,
[01:57:55 - 01:57:55] ‫ככה
[01:57:55 - 01:58:03] ‫שלכל T ולכל N שגדול ל-N גדול הזה,
[01:58:05 - 01:58:06] ‫ההסתברות
[01:58:10 - 01:58:12] ל-X בזמן
[01:58:13 - 01:58:15] ‫T ועוד N
[01:58:22 - 01:58:23] ‫ההסתברות שזה שווה X תג.
[01:58:25 - 01:58:45] ‫הסתברות שאחרי N צעדים, ‫אם הייתי ב-X בזמן T, ‫הייתי במצב X,
[01:58:46 - 01:58:48] ‫אני יכול להגיע ל-X תג
[01:58:49 - 01:58:50] ‫אחרי מספר מסוים של צעדים,
[01:58:51 - 01:58:52] ‫בהסתברות שהיא גדולה מ-0.
[01:58:52 - 01:58:57] ‫זאת אומרת, עבור כל שני ערכים ‫שיכולים להיות לי, ל-X,
[01:58:59 - 01:59:10] ‫אין מצב שהדרך היא חסומה, ‫שכלומר ההסתברות 0 ‫להגיע אל זה אף פעם. ‫תמיד יש איזשהו מספר צעדים ‫שיגרום לי לאיזושהי הסתברות ‫להגיע כל אחד מהשונות.
[01:59:11 - 01:59:16] ‫בעצם כל רגע נתון, ‫כל האפשרויות עדיין פתוחות. ‫יש סיכוי להגיע ‫לכל האפשרויות האחרות,
[01:59:16 - 01:59:18] ‫יכול להיות שזה ייקח מספר מסוים של צעדים, ‫אבל
[01:59:19 - 01:59:19] הסיכוי הזה קיים.
[01:59:20 - 01:59:26] ‫זו תכונה זו נקראת תכונת הארגודיות. ‫זו תכונה שהיא תלויה ‫בהסתברות מעבר האלה.
[01:59:27 - 01:59:35] ‫למשל, אם יש איזושהי הסתברות מעבר ‫שאומרת שברגע שהגעתי למצב מסוים, ‫כבר אין סיכוי שאני אגיע ‫לסט מסוים של מצבים,
[01:59:35 - 01:59:36] ‫אז הארגודיות לא מתקיימת.
[01:59:38 - 01:59:40] ‫עדיין יכול להיות שיהיו פה ‫אפסים מדי פעם,
[01:59:40 - 01:59:45] ‫בהינתן שאחר כך אפשר ‫להגיע אליהם מנקודה אחרת, ‫שלא כל המסלולים חסומים.
[01:59:47 - 01:59:47] ‫אז זה נקרא ארגודיות.
[01:59:49 - 01:59:58] ‫זה בעצם המשפט שבגללו ‫אנחנו צריכים את כל זה.
[02:00:20 - 02:00:21] ‫אוקיי.
[02:00:24 - 02:00:30] ‫שנחרת מורקוביץ גודי.
[02:00:50 - 02:00:52] ‫באמת ש...
[02:01:09 - 02:01:11] ‫אם אנחנו מפעילים את השרשרת הזאת ‫כבר הרבה פעמים,
[02:01:13 - 02:01:18] ‫אנחנו לאט-לאט מתכנסים ‫להתפלגות הסטציונלית של השרשרת.
[02:01:20 - 02:01:25] ‫אוקיי, מה זה התפלגות סטציונלית? ‫זה התפלגות שאם אנחנו נמצאים בה, ‫אז אנחנו נשארים בה.
[02:01:25 - 02:01:28] ‫זאת אומרת, הדגימות האחרות ‫מגיעות מאותה התפלגות.
[02:01:37 - 02:01:40] ‫מה שהמשפט הזה אומר, ‫שלא משנה מאיפה התחלנו,
[02:01:41 - 02:01:44] ‫כל עוד השרשרת שלנו היא ארגודית, ‫זאת אומרת, אפשר להגיע ‫מכל מצב לכל מצב.
[02:01:45 - 02:01:49] ‫אז בסופו של דבר אנחנו נגיע ‫להתפלגות הסטציונלית.
[02:01:50 - 02:02:02] ‫איך אנחנו משתמשים בזה? ‫למה אנחנו צריכים את הדבר הזה? ‫אתם זוכרים, אנחנו רצינו בשביל ‫שיטות מונטה קרלו ‫לדגום מאיזושהי התפלגות.
[02:02:03 - 02:02:05] ‫לפעמים זו התפלגות ‫שקשה לנו לדגום ממנה,
[02:02:06 - 02:02:09] ‫אבל אולי אנחנו יכולים להגדיר ‫שרשרת מרקו,
[02:02:09 - 02:02:11] ‫שההתפלגות הזאת ‫תהיה התפלגות הסטציונרית שלה,
[02:02:13 - 02:02:14] ‫ואז כשנפיל את השרשרת הזאת,
[02:02:14 - 02:02:16] ‫אחרי מספר מסוים של צעדים,
[02:02:16 - 02:02:21] ‫אנחנו נגיע לדגימות ‫שאנחנו יכולים להניח ‫שהגיעו מההתפלגות הזאת.
[02:02:23 - 02:02:25] ‫זה הרעיון של MCMC.
[02:02:26 - 02:02:27] ‫זה רק MCMC,
[02:02:32 - 02:02:34] ‫ונגדיר במפקרת
[02:02:36 - 02:02:42] מרקו ככה שפית חיי איקס,
[02:02:46 - 02:02:47] ‫ההתפלגות.
[02:02:50 - 02:02:51] ‫אנחנו נמנע
[02:02:52 - 02:02:53] עודכים לדגום.
[02:02:59 - 02:03:00] ‫אנחנו נדגום עבור
[02:03:01 - 02:03:01] ‫יש ליום
[02:03:10 - 02:03:13] ‫כן, זה הרעיון בשיטות אנסים.
[02:03:16 - 02:03:41] ‫אוקיי, בואו נראה רק,
[02:03:42 - 02:03:44] ‫אני לא אוכיח למה הדבר המשפט הזה נכון, ‫אבל
[02:03:44 - 02:03:49] אני רק אראה לכם חצי הוכחה, חצי אינטואיציה,
[02:03:49 - 02:03:53] ‫למה זה מתקיים עבור מקרים שאיקס דיסטרטי.
[02:03:56 - 02:03:59] ‫איקס דיסטרטי סופי. אוקיי, אז עבור איקס,
[02:04:03 - 02:04:03] ‫אני מקבל
[02:04:04 - 02:04:05] סופר
[02:04:06 - 02:04:06] פי
[02:04:08 - 02:04:08] ‫בערכים.
[02:04:09 - 02:04:17] ‫אז נגיד הוא מקבל ערכים מ-1 עד M. ‫אז אפשר להגדיר
[02:04:21 - 02:04:22] ‫רקטרור,
[02:04:23 - 02:04:23] ‫ווי,
[02:04:25 - 02:04:27] ‫שהוא בעצם נראה ככה,
[02:04:28 - 02:04:28] ‫רקטרור,
[02:04:30 - 02:04:33] ‫פה יש את הסיכוי שאיקס שווה 1,
[02:04:35 - 02:04:37] ‫סיכוי שאיקס שווה 2,
[02:04:38 - 02:05:07] ‫אבל איך נראה, תראו שאיקס שווה L. ‫כן, אז יש פה סכום ה... ‫הדברים שלו צופיית שווה 1. אוקיי, איך תראה, ‫מפריצת מעברים עבור מתי משתנה מקרי כזה איקס, ‫שיש לו מספר סופי של ערכים, ‫ואז איך נראה, איך תראה פונקציית המעברים ‫שמעבירה אותנו מ-X1 ל-X הבא, השרשרת,
[02:05:07 - 02:05:08] ‫שזה יהיה מטריצה.
[02:05:09 - 02:05:12] ‫זה מטריצת מעברים.
[02:05:23 - 02:05:27] ‫בעצם האיבר ה-IJ שלה אומר,
[02:05:28 - 02:05:29] ‫אם הייתי ב-X
[02:05:32 - 02:05:33] J,
[02:05:34 - 02:05:36] מה ההסתברות שאני אעבור ל-XI?
[02:05:38 - 02:05:43] ‫אוקיי, אז איבר נגיד בצורה הראשונה ‫בעמידה השלישית,
[02:05:44 - 02:05:52] ‫להגיד ההסתברות ש-XP ועוד 1 יהיה שווה 1, ‫בהינתן ש-XP שווה 3.
[02:05:58 - 02:06:03] ‫טוב, אם אני, מה אני יכול לעשות עם הדבר הזה? ‫אני יכול עכשיו לחשב את P
[02:06:03 - 02:06:12] ‫של XT ועוד 1 בהינתן XT. ‫ומה זה יהיה שווה?
[02:06:18 - 02:06:20] ‫המכפלה של המטריצה הזאת
[02:06:21 - 02:06:26] ‫בווקטור שמתאים להתפלגות של T. ‫יש לי וקטור שיש בתוכו את ההסתברות,
[02:06:27 - 02:06:32] ‫שנמצא בזמן T בכל אחד מה-Xים הישרים. ‫אם אני אכפיל אותו במטריצה הזאת,
[02:06:33 - 02:06:36] ‫אני מקבל את ההתפלגות עבור הזמן הבא, ‫עבורתי ועוד אחד.
[02:07:03 - 02:07:05] ‫זמן השורות.
[02:07:13 - 02:07:16] ‫ועוד תמונה שיש למדיגת סופסטיות זה שה...
[02:07:18 - 02:07:26] ‫המשתנה המקרי הכי גדול, ‫כל המשתנים המקריים שווים 1. סליחה, ‫כל המשתנים המקריים גדולים מ-0, ‫והמשתנה המקרי הכי גדול שווה ל-1.
[02:07:28 - 02:07:29] ‫לא נתקלתם בזה בשום מקום?
[02:07:30 - 02:07:31] ‫-אלגוריתמים לא עשיתם.
[02:07:31 - 02:07:33] ‫כן.
[02:07:34 - 02:07:36] ‫למה עשית שם פעם שעוד? ‫-זה לא משנה.
[02:07:37 - 02:07:53] ‫המודל הטרנספורט זה V בזמן T. ‫-כן, זה שווה בעצם ל-V בזמן T ועוד אחד.
[02:08:01 - 02:08:19] ‫זאת ההתפלגות של X בזמן T ועוד אחד, ‫לא בהינתן.
[02:08:20 - 02:08:25] ‫השורה A זה בדיוק ההתפלגות של X, ‫T ועוד אחד בהינתן.
[02:08:31 - 02:08:42] ‫אוקיי, אז בגלל שהמטריצה הזאתי ‫יש לה ערכים עצמיים ‫שהם קטנים שווים 1, ‫חוץ מערך עצמי 1, שהוא שווה 1,000 יחידים גדול,
[02:08:42 - 02:08:58] ‫אז מה קורה כשאנחנו נפעיל ‫את המטריצה הזאת עוד פעם ועוד פעם? ‫שרשרת מרקוב בעצם היא שקולה ‫לגידה שאני כל כך מכפיל ב-A. ‫אז נתחיל מאיזשהו D, 0. ‫ואז אני מכפיל ב-A, ‫אני מקבל את ה-V בזמן אחד, ‫את ההתפלגות של ה-Xים שלי בזמן אחד.
[02:08:58 - 02:09:03] ‫אני מכפיל בעוד A, ‫אני מקבל את ההתפלגות בזמן אחד, אוקיי? ככה,
[02:09:04 - 02:09:20] ‫עד שאני מקבל את ה-B בזמן T. ‫בגלל שהערכים העצמיים כאן, ‫שכולם קטנים מאחד, ‫אז המחקדה של ה-A בעצמה ‫מדאיכה את הערכים העצמיים,
[02:09:21 - 02:09:22] ‫חוץ מהערך העצמי היחיד,
[02:09:23 - 02:09:24] ‫שהוא שווה 1,
[02:09:25 - 02:09:29] ‫הוא היחיד שנשאר בעצם.
[02:09:29 - 02:09:32] ‫ואז מה שאני מקבל זה שהדבר הזה מתכנס
[02:09:35 - 02:09:39] ל-A, T, V, 0,
[02:09:41 - 02:09:42] ‫אשר T,
[02:09:46 - 02:09:53] אוהב בינסוף, מתכנס ל-Vוקטור העצמי ‫שמתאיש לערך עצמי אחד,
[02:09:54 - 02:09:56] ‫שהוא בדיוק ההתפלגות הפרציונלית.
[02:09:59 - 02:10:01] ‫זה בעצם אחת מהדרכים למצוא
[02:10:02 - 02:10:05] את הערך העצמי ‫הגבוה ביותר של פונקציות.
[02:10:08 - 02:10:11] ‫למצוא את הווקטור העצמי ‫שמתאים לערך העצמי הכי גדול.
[02:10:13 - 02:10:16] ‫זו שיטה שמשתמשים בה, ‫מכפילים את המטריצה בעצמה ‫הרבה פעמים,
[02:10:18 - 02:10:21] ‫והדבר הזה אמור להתכנס לוקטור ‫שהוא הוקטור העצמי
[02:10:22 - 02:10:23] ‫שמתאים לערך העצמי הגדול ביותר.
[02:10:25 - 02:10:26] ‫זה נקרא ה-Power Method.
[02:10:28 - 02:10:29] ‫אבל זה גם מה שמראה,
[02:10:30 - 02:10:34] ‫לא הראינו פה את כל הפרטים, ‫שהמטריצה הזאת יש לה את הערכים העצמיים האלה,
[02:10:35 - 02:10:40] ‫אבל בהינתן שזה נכון, בעצם מה שקורה ‫זה שכשאנחנו מפעילים את המטריצה הזאת ‫יותר ויותר פעמים,
[02:10:40 - 02:10:43] ‫כל הכיוונים של הווקטור העצמיים ‫האחרים דורכים,
[02:10:44 - 02:10:46] ‫כי אנחנו מכפילים אותם כל פעם ‫במספר שהוא קטן או אחד.
[02:10:47 - 02:10:50] ‫הדבר היחיד שנשאר זה הווקטור ‫העצמי הזה ‫שהוא מוכפל כל פעם באחד.
[02:10:52 - 02:10:53] ‫לא משנה מאיפה התחלנו,
[02:10:54 - 02:11:00] ‫אנחנו תמיד בסופו של דבר, ‫כל מה שיישאר שם זה המרכיב הזה ‫שמתאים לווקטור העצמי הגדול ביותר,
[02:11:01 - 02:11:01] ‫של הערך העצמי.
[02:11:02 - 02:11:04] ‫זה בדיוק ההתפלגות הצטטיונית.
[02:11:06 - 02:11:10] ‫אוקיי, אז זה שיטת MCMC, ‫ואחת מהשיטות הכי...
[02:11:11 - 02:11:16] ‫זה בעצם משפחה של שיטות, אוקיי? ‫זה דרך לפתח שיטות,
[02:11:17 - 02:11:18] ‫ואנחנו צריכים,
[02:11:19 - 02:11:22] ‫בשביל שזה יהיה משמעותי, ‫אנחנו צריכים איזושהי דרך
[02:11:23 - 02:11:28] ‫לפתח שרשרת מרקוב, שההתפלגות ‫הסטציונרית שלה ‫היא התפלגות שמעניינת אותה.
[02:11:31 - 02:11:34] ‫אחת מהשיטות לעשות את זה ‫נקראת דגימת גיפס.
[02:11:46 - 02:12:15] ‫זה שיטת MCC. ‫האחר קל לדגום.
[02:12:17 - 02:12:23] ‫מ-P של XI בהינתן X
[02:12:24 - 02:12:27] ‫על מינוס I. ‫אז אם יש לנו X שהוא,
[02:12:27 - 02:12:28] יש לו הרבה ממדים,
[02:12:29 - 02:12:30] ‫אם אני יודע לדגום בקלות
[02:12:31 - 02:12:33] ‫את אחד מהממדים בהינתן כל השאר,
[02:12:36 - 02:12:38] ‫אז אפשר להשתמש בשיטת דגימת גיפס.
[02:12:40 - 02:12:43] ‫מתי זה קורה? למשל, בדוגמה הזאת שנתנו עכשיו,
[02:12:43 - 02:12:44] ‫בתחילת השיעור,
[02:12:44 - 02:12:47] ‫אם יש לי פרעה על פאצ'ים של תמונות,
[02:12:48 - 02:12:52] ‫אין לי פרעה טובה על כל התמונה, ‫אבל יש לי פרעה טובה בפאצ'ים של תמונות,
[02:12:52 - 02:12:54] ‫אבל בהינתן כמה פיקסלים, ‫אני יודע לדגום
[02:12:55 - 02:12:57] ביחסי ביעילות את הפיקסלים שלידם.
[02:12:59 - 02:13:00] ‫אני לא יודע לדגום את כל התמונות.
[02:13:01 - 02:13:04] ‫אז אני יכול להשתמש בשיטה הזאת ‫שנקראת דגימת גיפס.
[02:13:05 - 02:13:07] ‫השיטה הזאת עובדת ככה,
[02:13:08 - 02:13:12] ‫בעצם השרשרת מרקוב מוגדרת על ידי זה ‫שפשוט דוגמים.
[02:13:14 - 02:13:32] ‫מינוס I זה כל האינדקסים שהם לא I. ‫אני רוצה לכתוב את האלגורית, ‫הוא באמת מאוד קשוט.
[02:13:44 - 02:13:51] ‫אנחנו מתחילים את איקס
[02:13:53 - 02:13:55] ‫בפופעה טו-אייץ.
[02:14:00 - 02:14:05] ‫ואז אנחנו נקדם את התהליך ‫המרקובי לבא, או T
[02:14:08 - 02:14:09] ‫למחד ל-T גדול.
[02:14:09 - 02:14:16] ‫מבחורים באופן אקראי את איזשהו אינדקס,
[02:14:17 - 02:14:24] ‫שווי I. אם יש לנו איזשהו סדר רגיוני, ‫אז אולי לפעמים כדאי לעשות את זה בצורה מסודרת, ‫אבל אפשר לעשות את זה בצורה רנדומלית גם.
[02:14:26 - 02:14:30] ‫שווי זה רנדום I.
[02:14:34 - 02:14:37] אנחנו נתחיל, איקס זה הראשון שאנחנו לוקחים, נקרא לו איקס אפס.
[02:14:40 - 02:14:43] ‫ואז אנחנו דוגלים את איקס איי
[02:14:45 - 02:14:45] ‫בפי
[02:14:52 - 02:14:55] ‫בהתפלגות הזאת של איקס איי, ‫בהינתן איקס
[02:14:57 - 02:14:59] לא איי שווה
[02:15:00 - 02:15:02] ‫איקס לא איי שהיה לנו עד עכשיו,
[02:15:03 - 02:15:04] ‫איזו תוצאה T מינוס אחד.
[02:15:05 - 02:15:15] ‫בכל השאר אנחנו מעתיקים פשוט, ‫איקס לא I, T שווה ל-X
[02:15:17 - 02:15:17] לא I,
[02:15:19 - 02:15:20] ‫אינה נוסחת.
[02:15:25 - 02:15:29] ‫אז מה כתוב כאן? אנחנו מתחילים את איקס ‫בצורה רנדומלית, כלשהי,
[02:15:30 - 02:15:32] ‫ואז כמובן אנחנו בוחרים ‫את אחד מהממדים של איקס.
[02:15:33 - 02:15:39] ‫דוגמים אותו בהינתן המצב של השכנים שלו, ‫הוא המצב של כל השאר,
[02:15:41 - 02:15:46] ‫ומה שיצא לנו, אנחנו מחליפים ב-X שלנו,
[02:15:47 - 02:15:51] ‫ואז בוחרים אינדקס אחר ‫ודוגמים אותו ביחס לפגשה,
[02:15:53 - 02:15:57] ‫וככה הלאה עד שזה מתכנס.
[02:15:58 - 02:16:03] ‫-באמת, ברור שהדבר הזה ‫הוא תהליך ננקובי,
[02:16:04 - 02:16:10] ‫נכון? כי בכל צער רק מסתכלים ‫על המצב הנוכחי של ה-Xים שלנו, ‫לא על מה שקרה עד עכשיו.
[02:16:11 - 02:16:12] ‫כן, אז ברור, נזרום שזה ננקובי.
[02:16:20 - 02:16:21] ‫וגם ש...
[02:16:24 - 02:16:25] ‫אבל לראות ש-P של X,
[02:16:28 - 02:16:31] ‫פרופורציוני לכל הדברים האלה,
[02:16:32 - 02:16:38] ‫לכל ה... לכל ה... לכל הביטויים המותנים האלה, ‫של ההתפלגות המותנית,
[02:16:40 - 02:16:41] ‫זה התפלגות הסטציונלית.
[02:16:48 - 02:16:51] P של X הוא פרופורציוני,
[02:16:52 - 02:16:57] ‫המכפלה של כל ה-P של XI בהינתן X,
[02:16:57 - 02:16:58] ‫לא I.
[02:17:13 - 02:17:23] ‫כן, בעצם היה לנו... ידענו שאנחנו, ‫יש פה כל מיני ביטויים של התפלגות מותנית, ‫שאנחנו רוצים שיהיו בהתפלגות שלנו, ‫ואנחנו לא ידענו לכתוב את ההתפלגות הכללית,
[02:17:24 - 02:17:27] ‫שאנחנו לא יודעים למרמר אותה, כן? ‫זה כמו שראינו קודם שיש לנו מעללים,
[02:17:27 - 02:17:34] ‫למשל ב-Marcov Networks כזה, ‫או למשל יש לנו תמונה ‫שיש לנו פקטור על כל הפאצ'ים.
[02:17:35 - 02:17:42] ‫אין לנו דרך היררכית כזאת, ‫כמו שהיה לנו בשיעורים הקודמים של דב, ‫כזה לייצר את כל ההתפלגות. ‫יש לנו כל מיני מעגלים,
[02:17:43 - 02:17:45] ‫כל מיני דברים של מספרים פעמיים,
[02:17:45 - 02:17:49] ‫אבל עדיין אנחנו יכולים, ‫בהינתן כל הפיקסלים, ‫אנחנו יכולים להגיד ‫מה ההסתברות
[02:17:50 - 02:17:51] של פיקסל אחד.
[02:17:53 - 02:17:56] ‫אנחנו יודעים את כל ההתפלגויות המותנות, ‫אנחנו יודעים שההתפלגות שלנו היא איזשהו,
[02:17:57 - 02:17:59] ‫ביטוי שפורפורציוני ‫לכל ההתפלגויות המותנות האלה,
[02:18:00 - 02:18:03] ‫ואנחנו לא יודעים את המרמול, ‫אנחנו לא יודעים גם לדגום מההתפלגות הזאת.
[02:18:04 - 02:18:09] ‫בשיטת גיף נותנת לנו הבטחה ‫שבסופו של דבר, אם נעשה את התהליך הזה ‫בצורה איטרטיבית,
[02:18:10 - 02:18:11] ‫ה-X'ים שנקבל,
[02:18:12 - 02:18:16] ‫הם יגיעו מההתפלגות הזאת, ‫שאנחנו לא יודעים ‫לא לחשב ולא לדגום ממנה.
[02:18:18 - 02:18:23] ‫אם אנחנו רוצים אחר כך לעשות תוחלת, ‫לפי הדבר הזה אנחנו יכולים למצע את הפונקציה שלנו,
[02:18:24 - 02:18:28] ‫לפי ההדגימות שיצאו לנו קודם.
[02:18:29 - 02:18:38] ‫זה יהיה שיטת MCMC. ‫הבעיה עם זה זה שהרבה פעמים ‫יכול לקחת הרבה זמן ‫עד שאנחנו מגיעים
[02:18:39 - 02:18:42] ‫להתפלגות שאנחנו רוצים כאלה.
[02:18:43 - 02:18:45] ‫התהליך הזה שעד שאנחנו מגיעים,
[02:18:46 - 02:18:48] ‫ההתפלגות הסוציונרית ‫נקרא מיקסינג טיים,
[02:18:50 - 02:18:53] ‫וזה משהו שהוא מאוד מורכב, ‫והרבה פעמים אנחנו לא יודעים ‫להג'יד
[02:18:53 - 02:19:04] ‫כמה איטרציות צריך כדי להגיע להתפלגות הזאת. ‫אז מובטח לנו שבסופו של דבר זה יהיה נכון, ‫אבל הרבה פעמים יש פה עדיין קושי ‫שאנחנו לא יודעים כמה איטרציות צריך.
[02:19:05 - 02:19:08] ‫אנחנו גם לא יודעים כשהגענו למספר הזה, ‫אנחנו לא יודעים שהגענו אליו.
[02:19:10 - 02:19:17] ‫הרבה פעמים זה דורש יותר איטרציות ‫ממה שאנחנו יכולים.
[02:19:17 - 02:19:20] ‫יש כל מיני שיטות להתמודד עם זה, ‫לנסות לחשב את זה,
[02:19:21 - 02:19:24] ‫לפעמים מריצים במקביל כמה שרשראות,
[02:19:25 - 02:19:27] ‫לפעמים הרבה דגימות במקביל.
[02:19:28 - 02:19:34] ‫זה תחום מחקר שלם, ‫ואיך לעשות M.T.M.C יותר יעיל, ‫להבין את המיץ הזה, ‫לשלוט בו.
[02:19:36 - 02:19:42] ‫לא ניכנס לזה בכך, ‫אבל רק שתדעו שזה אחד מהתקעים העבריים.
[02:19:51 - 02:19:51] ‫יכול להיות
[02:19:54 - 02:19:55] ‫היא
[02:19:56 - 02:19:56] ‫האיטרציות
[02:19:58 - 02:19:59] ‫הוא נכניס להיות גדול מאוד.
[02:20:06 - 02:20:07] ‫אז אנחנו נכניסים.
[02:20:07 - 02:20:23] ‫למה לא בוחרים מדי ‫לומר לדבר נגיד כאילו, ‫אחד עד כאן אני אמרתי את זה, כאילו...
[02:20:23 - 02:20:26] ‫אז זה מה שאמרתי, ‫אם יש איזשהו סדר הגיוני,
[02:20:27 - 02:20:31] ‫אז לפעמים כדאי לעשות את זה, ‫אבל אתה אומר, לא חייב. ‫כדי שזה יהיה נכון, לא חייב.
[02:20:32 - 02:20:36] ‫אם יש איזשהו סדר שאולי הוא כן, ‫יש בו היגיון מסוים.
[02:20:37 - 02:20:42] ‫ואתה חושב שהוא יכול לעזור, ‫להוריד את המיצינג טיימן למשל, ‫אז כדאי להשתמש בו.
[02:20:42 - 02:20:46] ‫אבל כאילו אני יכול לעשות ‫כל הסדר שאני רוצה, ‫גם אם קובעו מראש ועובד, ‫ככה, ואתן...
[02:20:48 - 02:20:49] ‫זה צריך לעבור בכל החולקים.
[02:20:56 - 02:21:01] ‫טוב, זרקתי לכם היום הרבה מושגים, ‫בלי ממש סיפור מסגרת מאוד ומסודר.
[02:21:03 - 02:21:04] ‫אני חושב שהצלחתם להתמודד
[02:21:05 - 02:21:07] ‫ולספור קצת מהמושגים.
[02:21:08 - 02:21:16] ‫אם אנחנו נראה שבוע הבא עוד, ‫נדבר עוד טיפה על שיטת דגימה, ‫שנקראת לנג'אווין סמפלינק,
[02:21:17 - 02:21:23] ‫שמשתמשים גם ב-NNCMC ‫וגם בלנג'אווין משתמשים במודלי דיפיוז'ן,
[02:21:23 - 02:21:28] ‫שזה מודלית שאני מאוד מקווה ‫שנגיע אליו בסוף הקורס,
[02:21:29 - 02:21:30] ‫ובכלל,
[02:21:31 - 02:21:34] בעיקר לעשות אבלואציה ‫למודלים משתמשים בשיטות
[02:21:34 - 02:21:35] NNCMC,
[02:21:36 - 02:21:39] ‫אבל גם לפעמים בשביל ללמוד ‫את המודלים משתמשים בהם.
[02:21:41 - 02:21:45] ‫תשובר ואני אדבר על זה, ‫ואז נדבר קצת על השיטה השנייה ‫של מקצועות על אוטימיזציה,
[02:21:47 - 02:21:48] ‫כבר יש אינפרנס,
[02:21:49 - 02:21:54] ‫בזאת נסיים את הפרק התיאורטי, ‫שזרקתי לכם מראה מושגים.
[02:21:55 - 02:22:01] ‫באמת, כאילו זה בערך כל שיעור שעשינו, ‫אפשר לעשות עליהם ‫על שיעור בקורס שלם, סכום שלם,
[02:22:02 - 02:22:12] ‫אבל אני חושב שזה עוזר ‫שתכירו קצת בגדול, ‫ואפילו בכמה מקרים קצת יותר בקטן ומלוכלך ‫את המושגים ואת המשמעויות שלהם.
[02:22:13 - 02:22:15] ‫לפני שאנחנו עוברים לשלב של
[02:22:16 - 02:22:18] ‫המודלים שהם פתקסים על רשתות לימונים,
[02:22:18 - 02:22:22] ‫זה עוד היום.
[02:22:23 - 02:22:23] ‫תודה רבה.
[02:22:31 - 02:22:39] ‫לגבי יוסי, אני חושב שאתה מוזיקה.