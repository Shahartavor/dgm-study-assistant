[00:00:00 - 00:00:03] ‫טוב, אוקיי, יש הצלחתי.
[00:00:05 - 00:00:06] ‫בונו בראנד,
[00:00:07 - 00:00:08] מעניינים?
[00:00:10 - 00:00:11] ‫צריכים להגיש את התרגיל, את כולם?
[00:00:14 - 00:00:14] בסדר?
[00:00:15 - 00:00:18] ‫אני מקווה שהסתכלתם לפחות ‫על התרגיל הבא.
[00:00:20 - 00:00:27] ‫אז בתרגיל הבא יש פה פחות סעיפים, ‫זה בעצם לבנות את המודל עצמו, את ה-VDE, ‫אבל זה עוד להגיע לתוצאות די טובות.
[00:00:27 - 00:00:32] ‫כשאתם דוגמים ממנו אתם אמורים לראות ‫שאתם מזהים את הספרות.
[00:00:36 - 00:00:39] ‫כן, ולהראות גם עוד, יש שם עוד סעיף ‫שמדבר על
[00:00:43 - 00:00:48] ‫פרזנטיישן לרנינג, זאת אומרת, ‫להבין מה המודל הזה תופס ‫ואיך הוא בעצם תופס
[00:00:49 - 00:00:50] דאטה פוינטס שונים.
[00:00:51 - 00:00:57] ‫אחד מטוחים להראות את זה, ‫שראינו את זה בשיעור גם, זה לעבור מלראות איך המודל עובר
[00:00:57 - 00:01:01] ‫בצורה חלקה מאחד מהייצוגים ‫של אחת מהתמונות לתמונה אחרת.
[00:01:03 - 00:01:05] ‫אז זו המטרה של התרגיל.
[00:01:05 - 00:01:08] ‫והיום אנחנו נדבר על מודל חדש ‫שנקרא Normalizing Flows,
[00:01:09 - 00:01:13] ‫שגם עליו יהיה לכם תרגיל, ספרו לממוני של תורגם.
[00:01:16 - 00:01:22] ‫זה יהיה תרגיל, עכשיו התרגיל שנתתי זה 5, נכון? של VE, ‫התרגיל הזה יהיה תרגיל 6,
[00:01:23 - 00:01:33] ‫ואז כנראה שהתאריך שלו ‫זה יהיה כבר לקראת סוף הסמסטר, ‫ואז תקבלו את התרגיל האחרון ‫שיהיה על Diffusion Models, ‫שזה יהיה הנושא שלנו.
[00:01:35 - 00:01:39] ‫בשיעור הבא כנראה, ‫אולי נתחיל לנגוע בדברים ‫שקשורים אליו,
[00:01:40 - 00:01:43] ‫ואז יש לנו עוד שני שיעורים, נכון? ‫קשיב לנו את השיעור הזה ועוד שלושה,
[00:01:43 - 00:01:44] ‫אם לא טועה?
[00:01:45 - 00:01:47] ‫אז יש לנו שני שיעורים ‫נדבר על Diffusion.
[00:01:49 - 00:01:51] ‫אז היום Normalizing Flows,
[00:01:52 - 00:02:00] ‫אז נתחיל מקצת הקדמה, ‫לסדר איפה ה-Normalizing Flows יושב ‫ביחס למודלים האחרים שדיברנו עליהם,
[00:02:01 - 00:02:03] ‫ולאם אנחנו רוצים אותם,
[00:02:03 - 00:02:04] ‫המודלים האלה.
[00:02:05 - 00:02:11] ‫אחר כך נחזור קצת לרקע שעברנו עליו, ‫קצת בשיעור הראשון, אני חושב.
[00:02:13 - 00:02:18] ‫אז אחד זה הנוסחה הזאתי, ‫שהיא הבסיס בעצם של המודל הזה ‫שנדבר עליו היום, של Normalization Flow,
[00:02:18 - 00:02:21] ‫נוסחת החלפת משדרים,
[00:02:22 - 00:02:23] ‫קרינט ובריברס,
[00:02:24 - 00:02:35] ‫נחזור אליה, ובמקרה הרב-ממדי, ‫בעצם אנחנו צריכים להשתמש, ‫בשביל להחכם את זה, ‫אנחנו צריכים את שני הדברים האלה, ‫אז נזכיר כמה תחומות ‫שקשורות לדטרמיננטות ויעקוב ירנט.
[00:02:36 - 00:02:39] ‫אז זה חלק מהרקע, ‫ואז אנחנו נדבר על המודל עצמו,
[00:02:40 - 00:02:41] ‫נגדיר אותו,
[00:02:42 - 00:02:46] ‫מה הוא אומר, איך הוא מייצג את הדאטה, ‫איך מתבצעת הלמידה,
[00:02:47 - 00:02:50] ‫ובפועל, איך בונים את המודל,
[00:02:50 - 00:02:57] ‫זה יהיה על ידי שילוב של הרבה חלקים קטנים, ‫הרבה פונקציות קטנות, ‫שכל אחת מהן מ-Normalizing Flow.
[00:03:01 - 00:03:04] ‫אז החלק השני, אנחנו נדבר בעצם ‫על איך בפועל משתמשים בזה,
[00:03:05 - 00:03:08] ‫איזה סוג של מודלים בנו עם השיטה הזאת,
[00:03:09 - 00:03:09] ‫אותו תוצאות.
[00:03:16 - 00:03:19] ‫אז זה מודל שהוא יותר פשוט קונספטואלית מ-BAE,
[00:03:20 - 00:03:27] ‫שיש בו כל מיני חלקים ‫שהוא יותר יותר מורכבים, ‫יש בו את העניין הזה של וריאט אינפרנס שבו מורכב.
[00:03:28 - 00:03:30] ‫זה מודל שהוא יותר פשוט, להבנה,
[00:03:31 - 00:03:32] ‫אני חושב.
[00:03:34 - 00:03:41] ‫אוקיי, תזכורת? ‫-התחלתי את כל התזכורת שלי ‫לשקף אחד, אוקיי? מודל גנרטיבי, ‫אנחנו רואים שזה מודל הסתברותי,
[00:03:42 - 00:03:43] ‫שתופס את ההסתברות של הדאטה,
[00:03:44 - 00:03:48] ‫שהאאוטפוט שלו, הדאטה שאליו הוא מומן, ‫הוא רב-לממדי.
[00:03:48 - 00:03:50] ‫אנסה קראנו באופן פתרון ‫הבין-דנרטיבי,
[00:03:51 - 00:03:58] ‫ובעצם יש לנו איזה דאטה ‫שנוצר מאיזושהי התפלגות לא ידועה, פי של דאטה, ‫ואנחנו מחפשים את הפרמטרים ‫של ההתפלגות שלנו, פי תטא,
[00:03:59 - 00:04:01] ‫ככה שפי של תטא יהיה פי של פי של דאטה,
[00:04:01 - 00:04:06] ‫בואו נקבל איזה משהו כזה, ‫שאם אנחנו דוגמים ממנו, ‫האזורים עם ההסתברות הגבוהה,
[00:04:06 - 00:04:10] ‫זאת אומרת שאם נקבל שם הרבה דגימות, ‫זה אזורים שמתאימים לתמונות שאנחנו אוהבים,
[00:04:11 - 00:04:16] ‫ואזורים עם הפלגות נמוכה, ‫זה משהו שאנחנו לא רוצים לראות אף פעם. ‫שאנחנו דוגמים, זה יהיה תמונות
[00:04:17 - 00:04:20] ‫שהן מבחינתנו לא תמונות טבעיות,
[00:04:21 - 00:04:23] ‫לא מייצגות את הדאטה-סט ‫שעליו אימנו את הדאטה.
[00:04:24 - 00:04:28] ‫בעצם רוב האזור אמור להיות משהו כזה, ‫חלק קטן מאוד
[00:04:31 - 00:04:35] ‫מכל האפשרויות של סידור פיקסלים ‫אמור לראות לנו כמו תמונה טבעית.
[00:04:36 - 00:04:40] ‫ואנחנו יכולים להשתמש בזה כדי, ‫ברגע שיש לנו דאט כזה, ‫כדי לייצר דאטה, לתגוע ממנו,
[00:04:41 - 00:04:52] ‫לעשות פרזנטיישן לרנינג, ‫זאת אומרת לייצג את הדאטה ‫באיזושהי דרך אחרת, ‫שבמקום סתם להגיד ‫מה הערך בכל פיקסל, ‫היא תופסת איזה משהו ‫אולי קצת יותר קרוב ‫לפקטורים המעניינים בדאטה שלנו,
[00:04:54 - 00:04:57] ‫שאנחנו יכולים להשתמש בזה ‫בתור הבסיס, למשל, לדברים אחרים,
[00:04:57 - 00:05:08] ‫או לעשות, בגלל שזה מודל הסתברותי ‫שאנחנו יכולים לחשב את הוודאות עליו, ‫אנחנו יכולים להשתמש בו ‫כדי להחליט אחר כך ‫כל מיני החלטות ‫שלא רק מסתמכות על פרזיקציה אחת, ‫אלא על זה שאנחנו יודעים
[00:05:08 - 00:05:13] ‫מה ההסתברות להסתברות ‫לאפשרויות שונות.
[00:05:13 - 00:05:21] ‫ובאופן כללי, לעשות איתו ‫כל מיני דברים ‫שאפשר לקרוא להם פה ‫בליסטיק אינפרנס, ‫וראיתם דוגמאות בתרגיל האחרון, ‫בעצם צריכים לעשות כל מיני,
[00:05:21 - 00:05:34] ‫היה לכם כל מיני שאלות, ‫בגלל שזה היה מודל הסתברותי ‫היה אפשר לענות על השאלות האלה. ‫סתם היה מודל שמוציא output ‫בדיוק לפי ה-Task שאימנו אותו, ‫אז לא הייתם יכולים לעשות ‫כל מיני דברים מוזרים כאלה עם המודל.
[00:05:37 - 00:05:47] ‫אוקיי, אז זה מודלים עמוקים. ‫אז בואו נזכר בשני סוגי המודלים ‫שראינו עד עכשיו, ‫ואני מתכוון למודלים העמוקים. ‫הסיבה שאנחנו עובדים ‫עם דברים עמוקים, ‫כי אנחנו רוצים לתפוס
[00:05:48 - 00:05:50] ‫הסתברות כזאת מאוד מורכבת.
[00:05:50 - 00:05:53] ‫וכל המודלים שאנחנו יכולים ‫לעבוד איתם בצורה ישירה,
[00:05:54 - 00:05:58] ‫כמו גאוסיאנים, התפלגות אוליפורמית, ‫או אפילו תערובת של גאוסיאנים,
[00:05:58 - 00:06:04] ‫לא מספיק בשביל לתפוס ‫ההתפלגות המורכבת הזאת ‫של תמונות למה שאנחנו רוצים.
[00:06:05 - 00:06:12] ‫אוקיי, ראינו שני סוגים של מודלים ‫שמבוססים על רשתות עמוקות,
[00:06:13 - 00:06:16] ‫שמבחורה מאפשר להן ‫לתפוס התפלגויות יותר מורכבות.
[00:06:17 - 00:06:20] ‫אז הראשונה זה מה שעשיתם בתרגיל עכשיו, ‫אוטו-רגרסיב.
[00:06:20 - 00:06:23] ‫מודלס, הדאטה שלנו פי של איקס, ‫איקס הוא מממד גבוה.
[00:06:24 - 00:06:29] ‫סתם ככה אנחנו לא יכולים ‫לחשב את כל האפשרויות, ‫אז אמרנו בואו נפרק אותו ‫לפי כלל השרשרת,
[00:06:30 - 00:06:33] ‫פי איזשהו סידור, פי של איקס איי תלוי ‫בכל הקודמים, נכון?
[00:06:34 - 00:06:35] ‫ואז אמרנו בזה, יש שתי דרכים...
[00:06:38 - 00:06:44] זה כשלעצמו לא עוזר לנו, ‫אבל אם אנחנו תופסים ‫כל אחד מהמודלים האלה ‫באיזושהי צורה יעילה,
[00:06:44 - 00:06:47] ‫זה יכול לעזור לנו. למשל, אנחנו יכולים ‫להשתמש בזה בתכונה מרכובית,
[00:06:48 - 00:06:56] ‫שכל פיקסל תלוי רק בחלק, ‫או מה שאנחנו עשינו זה ‫שכל פיקסל תלוי בכל הקודמים, ‫אבל עם איזושהי פרמטריזציה מסוימת,
[00:06:57 - 00:07:02] ‫או שהיא משותפת לכל הפיקסלים, ‫זה מאפשר לנו לעשות את זה ‫בצורה יעילה.
[00:07:05 - 00:07:08] ‫יכולים להיות שם הרבה פרמטרים, נכון? ‫בארצת שלכם היו הרבה פרמטרים, ‫אבל זה עדיין הרבה פחות,
[00:07:09 - 00:07:11] ‫כדי פשוט לתפוס את כל האפשרויות ‫שיש בדאטה.
[00:07:13 - 00:07:17] ‫אז אנחנו כן יכולים לעבוד ‫עם מיליוני פרמטרים, אבל...
[00:07:17 - 00:07:23] ‫לא עם 256 של בית חזקת מיליון ‫של מספר הפיקסלים מאוד.
[00:07:24 - 00:07:26] ‫זה 700 של הרבה יותר מדיון.
[00:07:27 - 00:07:29] ‫אוקיי, אז זו הייתה הגישה הראשונה,
[00:07:29 - 00:07:35] ‫והיתרונות פה זה שאנחנו בעצם ‫יכולים לחשב את ה-Lightlyhood.
[00:07:36 - 00:07:38] ‫ה-objective function שלנו במודלים האלה,
[00:07:39 - 00:07:40] ‫הטבעי, זה פשוט Lightlyhood,
[00:07:41 - 00:07:42] ההסתברות של הטונר.
[00:07:43 - 00:07:45] אנחנו רוצים לעשות מקסימום Lightlyhood,
[00:07:45 - 00:07:51] ‫פה ראינו שזה דרך לקרב ‫את פי של תטא לפי של דאטה, ‫זה שקול ל-KL Divergence ‫בין פי של תטא לפי של דאטה.
[00:07:53 - 00:07:58] ‫אז במודל הזה אנחנו יכולים לחשב ‫את ה-IQ בצורה ישירה,
[00:07:59 - 00:08:03] ‫אנחנו יכולים לעשות לאופטימיזציה, ‫אנחנו יכולים לעשות ‫למצוא את ה-Teta הכי טוב,
[00:08:03 - 00:08:06] ‫אז זה המתרונות.
[00:08:06 - 00:08:09] ‫החסרונות זה שיש פה איזה משהו קצת מוזר,
[00:08:09 - 00:08:14] ‫אנחנו אומרים שפיקסל מסוים תלוי ‫רק בפיקסלים הקודמים,
[00:08:14 - 00:08:15] ‫לפי איזשהו סידור.
[00:08:16 - 00:08:18] ‫לא נשמע כמו איזה משהו שהוא סביר,
[00:08:19 - 00:08:23] ‫מבחינת איך שהדאטה באמת נוצר, ‫אנחנו אולי מפוספסים ‫כאן איזה משהו במודל הזה,
[00:08:23 - 00:08:24] ‫בפרמטריזציה הזאת.
[00:08:25 - 00:08:26] וגם
[00:08:29 - 00:08:34] סתם דוגמה, למה שאנחנו מפספסים, ‫אז ראיתם בתרגיל שיש סוג של שאלות
[00:08:34 - 00:08:37] ‫שהן נורא קל לענות עם המודל הזה,
[00:08:37 - 00:08:40] ‫בסוג אחר שנורא קשה, למרות שזה לכאורה,
[00:08:40 - 00:08:43] אין סיבה שחצי עליון,
[00:08:43 - 00:08:44] ‫בהינתן החצי התחתון,
[00:08:45 - 00:08:48] ‫יהיה בעיה מאוד שונה ‫מהחצי התחתון ומהחצי העליון.
[00:08:50 - 00:08:53] ‫מבחינת איך שהדאטה נוצר, ‫מבחינת ההתפלגות האמיתית של הדאטה,
[00:08:53 - 00:08:55] ‫זה אמור להיות שקול, הדבר הזה.
[00:08:55 - 00:08:58] ‫אבל בגלל ספציפית הפרמטריזציה שלנו,
[00:08:58 - 00:09:04] ‫אנחנו מפספסים את העניין הזה, ‫ואנחנו מחליטים שכיוון אחד ‫הוא הכיוון הטבעי, ‫וכיוון אחר הוא כיוון שנורא קשה לעשות.
[00:09:06 - 00:09:10] אז זה חיסרון, חיסרון אחר זה שזה פשוט איטי לייצר פדימות.
[00:09:10 - 00:09:14] ‫אחת מהמשימות שהיינו רוצים ‫במודלים גיארטיביים זה לייצר דגימות,
[00:09:15 - 00:09:19] ‫והמודל הזה הוא לא כל כך טוב בשביל זה, ‫במובן הזה שזה פשוט איטי.
[00:09:22 - 00:09:26] ‫וגם אין לנו פה איזה משהו טבעי ‫שאומר לנו מהי סוג של הדאטה
[00:09:27 - 00:09:29] ‫שאולי תופס את הדברים המעניינים,
[00:09:30 - 00:09:31] ‫וקשור לנקודה הראשונה שאמרתי.
[00:09:33 - 00:09:39] ‫אוקיי, זה היה מודל אחד. ‫מודל שני זה Variational Auto-Encoder, ‫שזה סוג של latent variable model, נכון? ‫אז יש לנו את זה,
[00:09:40 - 00:09:41] משתנה חבוי שאנחנו לא רואים,
[00:09:42 - 00:09:45] ‫ויש לנו התפלגות עליו ‫שנקראת ה-Prior.
[00:09:45 - 00:09:47] ‫יכול להיות שזו התפלגות עם פרמטרים,
[00:09:47 - 00:09:50] ‫אבל פה בדוגמאות שדיברנו, ‫וגם בתרגיל שלכם,
[00:09:51 - 00:09:53] ‫אז ה-Prior הזה הוא בלי פרמטרים, ‫אנחנו פשוט מניחים שזה גרסיין.
[00:09:55 - 00:10:00] ‫ויש לנו משהו שהוא נלמד, ‫זה בעצם שנקרא Decoder בתרגיל שלכם,
[00:10:01 - 00:10:03] ‫שאומר לנו מה ה-X בהינתן Z, ‫ופהוא יש לנו את הפרמטרים,
[00:10:04 - 00:10:08] ‫והוא ממפה לנו את Z בהינתן X בהינתן Z.
[00:10:10 - 00:10:20] ‫גם פה, לא רואים את זה כאן, ‫אבל בגלל ש-X הוא מימד גבוה, ‫בדרך כלל מה שקורה בהתפלגות הזאת, ‫היא מניחה שכל ה-Mימדים של X'ים ‫הם בלתי תלויים, ויינתן Z.
[00:10:22 - 00:10:25] ‫זה הדרך בעצם שאנחנו מתגברים ‫על עניין ה...
[00:10:28 - 00:10:29] ‫להעלת המימד.
[00:10:30 - 00:10:31] ‫אוקיי, מה היתרונות של המודל הזה?
[00:10:32 - 00:10:38] ‫אז יחסית פשוט, ‫יש לנו, אם יש לנו מודל כזה פשוט, ‫מודל כזה פשוט, ‫יש לנו דרך בעצם לשלב
[00:10:39 - 00:10:43] את שניהם ולייצר את ההתפלגות המורכבת. ‫זה יכול להיות קלוסיאן בהינתן Z,
[00:10:44 - 00:10:44] ‫זה יכול להיות קלוסיאן,
[00:10:46 - 00:10:48] ‫ויחד זה יכול להיות משהו מאוד מורכב.
[00:10:53 - 00:10:58] ‫לדגום מזה זה קל, אוקיי? ‫פשוט דוגמים Z,
[00:10:59 - 00:11:00] זה מה שאתם יכולים למשוך שם בתרגיל,
[00:11:01 - 00:11:02] ‫דוגמים התפלגות קלוסיאנית Z,
[00:11:03 - 00:11:06] ‫ומריצים את זה במודל הזה, ‫שזה נותן לנו תוחלת,
[00:11:07 - 00:11:14] ‫כי נותן Z נותן לנו תוחלת של X, ‫ואולי וריאנס גם של X, ‫לתרגיל אני ביקשתי מכם ‫שהווריאנס יהיה פשוט פגוע, הווריאנס הזה,
[00:11:15 - 00:11:22] ‫ואז דוגמים מהדבר הזה, ‫כמו שפשוט, בדרך כלל, ‫מחזירים פשוט את התוחלת של הדבר הזה, ‫זה כאילו הדגימה שלו.
[00:11:23 - 00:11:25] ‫זה קל מאוד לעשות את הדגימה,
[00:11:27 - 00:11:28] ‫אני מקווה שזה ילך להיות קל.
[00:11:29 - 00:11:30] החסרונות
[00:11:31 - 00:11:35] זה שבעצם אין לנו כבר גישה ישירה ‫ללוגלייפיות,
[00:11:36 - 00:11:41] ‫אנחנו לא יכולים לחשב את זה ‫בצורה מדויקת ונכונה,
[00:11:41 - 00:11:46] ‫אבל אנחנו יכולים לעשות איזשהו קירוב, ‫נכון? אז בעצם כל החסרונות ‫זה סביב הנקודה הזאת,
[00:11:46 - 00:11:50] ‫אנחנו עושים איזשהו קירוב, ‫זה הקירוב ה-Varational Bound, ה-Elbo,
[00:11:50 - 00:11:51] ‫שהוא
[00:11:54 - 00:11:56] חסם ללוגלייפיות,
[00:11:57 - 00:11:59] ‫ואנחנו עובדים מטוב עם כל מיני גלייפיות,
[00:11:59 - 00:12:04] ‫ולא ברור שזה נותן לנו מודל ‫שיש לו אלבו יותר טוב משני,
[00:12:04 - 00:12:08] ‫לא ברור שהלוגלייפיות שלו יותר טוב, ‫אבל יכול להיות שרק החסם שלו היה יותר טוב.
[00:12:09 - 00:12:15] ‫אז כל מיני קשיים שמביאים מזה, ‫שאין לנו בעצם דרך לחשב את הלייקים ‫בצורה מדויקת.
[00:12:17 - 00:12:19] ‫הבעיה הזאת היא שקולה, ‫לבעיה של בעצם לחשב את ה-Light
[00:12:21 - 00:12:25] ‫של נקודה, הקושי זה בעצם, ‫אם אתם יכולים לחשב את ה-Posterior,
[00:12:26 - 00:12:27] ‫זה שקול.
[00:12:27 - 00:12:33] ‫זה ראינו בהרצאות האלה, ‫היו לנו בעצם שתי הרצאות, ‫על Variational inference, ‫אחת יותר תיאורטית,
[00:12:34 - 00:12:38] ‫ואחת על VAAE. ‫אוקיי,
[00:12:38 - 00:12:43] ‫אז ראינו ששני המודלים האלה, ‫אז השאלה שאנחנו שואלים,
[00:12:43 - 00:12:45] ‫האם בכל זאת אנחנו יכולים לעבוד ‫עם מודל כזה,
[00:12:46 - 00:12:47] עם משתנה חבוי,
[00:12:48 - 00:12:51] ‫אבל שכן יהיה לנו דרך ‫לחשב את הלוגלייפיות בצורה מדויקת.
[00:12:55 - 00:13:01] זאת השאלה שאנחנו שואלים היום, ‫או איך אפשר לבנות את המיפוי הזה,
[00:13:01 - 00:13:04] ‫ככה שכן נוכל לחשב את הדבר הזה ‫בצורה מדויקת.
[00:13:05 - 00:13:09] ‫אז ראינו כבר דוגמה לזה שהיינו ‫יכולים לחשב את זה
[00:13:10 - 00:13:10] בצורה מדויקת,
[00:13:11 - 00:13:14] ‫זה היה תערובת גאוסיאנים GMM,
[00:13:14 - 00:13:16] ‫בעצם זה היה מודל שהיה נראה ככה.
[00:13:16 - 00:13:18] ‫זה היה מאוד פשוט, היה לו,
[00:13:19 - 00:13:23] ‫הוא היה פשוט מספר נגיד בין אפס, ‫בין אחד למאה, כן? זה היה פשוט
[00:13:24 - 00:13:29] 100 גאוסיאנים שונים, ‫אז Z היה מספר דיסקרטי, ‫בין אחד למאה,
[00:13:30 - 00:13:38] ‫ופה היה לנו לכל אחד מהם, ‫זה היה יכול להיות מאוד מורכב, ‫זה אפילו היה יכול להיות פשוט גאוסיאן אחר ‫לכל אחד מהמספרים האלה של Z,
[00:13:39 - 00:13:41] ‫וראינו שזה אנחנו יכולים ללמוד בצורה,
[00:13:41 - 00:13:43] ‫לחשב בצורה יעילה,
[00:13:44 - 00:13:46] ‫אבל מה אם אנחנו רוצים שבכל זאת, ‫שיהיה פה משהו יותר מורכב?
[00:13:47 - 00:13:48] ‫שה Z יהיה נגיד מספר רציף,
[00:13:49 - 00:13:53] ‫ושיהיה לנו כאן משהו שיכול להיות יותר מורכב ‫רק 100 גאוסיאנים.
[00:13:54 - 00:13:57] אז מה האילוצים בעצם שיידרמו לדבר הזה,
[00:13:57 - 00:14:02] ‫להיות קוראים לזה trackdable, זאת אומרת שאנחנו יכולים ‫לחשב את זה בצורה יעילה.
[00:14:05 - 00:14:08] ‫אז התשובה היא שיש משפחה של מודלים
[00:14:13 - 00:14:15] ‫של דיקודרים כאלה שמאפשרים את הדבר הזה,
[00:14:16 - 00:14:19] ‫וזה בעצם המרכיב של נורא לגן פלור, ‫שאני מדבר עליו.
[00:14:20 - 00:14:23] בעצם המטרה שלנו זה לבנות מודלים ‫עם לייטלן ברייבל,
[00:14:23 - 00:14:27] ‫אבל שאפשר לחשב עליהם את הלוגי רייטי בצורה יעילה.
[00:14:28 - 00:14:38] ‫ומה יהיה האפיסרון? ‫אפיסרון יהיה שהדבר הזה, ‫בניגוד ל-VAE, שהדיקודר כאן היה יכול להיות ‫פשוט רשת נוירונים בלי שום אילוצים,
[00:14:38 - 00:14:40] ‫עד שהאינפוט שלה לא נכון ‫והארטבל שלה לא נכון,
[00:14:41 - 00:14:42] ‫כאן יהיו לנו כל מיני אילוצים.
[00:14:43 - 00:14:45] ‫זה יהיה בעצם האפיסרון.
[00:14:48 - 00:14:55] אוקיי, אז כן, בואו נדבר רגע על הרקע ‫של מה שאנחנו צריכים.
[00:14:56 - 00:14:59] ‫אנחנו רוצים לעבור מהתפלגות פשוטה ‫להתפלגות מורכבת.
[00:15:00 - 00:15:00] ‫אז יהיה לנו
[00:15:05 - 00:15:07] ‫אנחנו רוצים שיהיה לנו כבר לחשב בצורה...
[00:15:10 - 00:15:10] ‫כל זה אמרתי.
[00:15:15 - 00:15:18] ‫אז זה למשל דוגמה למודלים ‫שאנחנו כבר ראינו,
[00:15:18 - 00:15:20] ‫שאנחנו יכולים לחשב בצורה יעילה את הכול.
[00:15:21 - 00:15:24] ‫מיקשר זה אפילו מודל עם לייטנט ברייבל,
[00:15:24 - 00:15:25] ‫אפשר לחשוב עליו יותר מודל עם לייטנט ברייבל.
[00:15:26 - 00:15:30] ‫אבל זה עדיין מודלים יחסית פשוטים. ‫אנחנו רוצים משהו קצת יותר מורכב
[00:15:33 - 00:15:36] ‫קומפלקסט, נס רגישות. ‫אנחנו רוצים משהו שהוא קצת יותר מורכב.
[00:15:38 - 00:15:41] ‫אוקיי, אז הרעיון הוא שאנחנו בעצם,
[00:15:41 - 00:15:44] ‫מה שאנחנו צריכים לעשות זה שהמודל הזה,
[00:15:47 - 00:15:49] ‫שמבטא אותנו מ-z ל-x,
[00:15:50 - 00:15:51] ‫הוא צריך להיות הפיך.
[00:15:52 - 00:15:54] ‫עם המודל שמבטא אותנו מ-z ל-x הוא הפיך,
[00:15:54 - 00:15:56] ‫זאת אומרת, כשאנחנו יכולים לחזור מ-x ל-z,
[00:15:58 - 00:16:03] ‫אז אנחנו נוכל לחשב את ה...
[00:16:04 - 00:16:09] ‫תראו שעוד קצת קבוצים, אבל בעיקרון, ‫אנחנו נוכל לחשב את הלייטנט.
[00:16:10 - 00:16:13] ‫סתם אינטואיציה כזאת ראשונית, ‫זה שאנחנו נכנסים לדברים.
[00:16:13 - 00:16:14] ‫מה הבעיה במודלים האלה?
[00:16:15 - 00:16:17] ‫אנחנו צריכים לחשב את האינטגרל הזה, נכון?
[00:16:17 - 00:16:19] ‫להינתן x, אנחנו לא יודעים איזה z
[00:16:20 - 00:16:22] יצר אותו, ‫אנחנו צריכים לדאוג את כל ה-z'ים.
[00:16:22 - 00:16:27] ‫כל z יש לו הסתברות אחרת ‫מייצר את ה-x שראינו,
[00:16:28 - 00:16:31] ‫אנחנו צריכים כל פעם להתחשב ‫בכל ה-z'ים של האינטגרל הזה.
[00:16:32 - 00:16:34] ‫אם אנחנו יודעים למפות חזרה את z מ-x,
[00:16:34 - 00:16:36] ‫אבל אנחנו לא צריכים לחשב את האינטגרל הזה.
[00:16:37 - 00:16:38] ‫אנחנו יודעים מה ה-z שיצר את ה-x.
[00:16:40 - 00:16:43] ‫זו האינטואיציה המהירה,
[00:16:43 - 00:16:45] ‫ובעצם הדבר הזה זה בדיוק מה
[00:16:46 - 00:16:56] ‫שפורמל על ידי המשוואה הזאת של... ‫הבעיה הזאת של נקראת Change of Variable. ‫אנחנו נסתכל רגע במימד אחד.
[00:16:57 - 00:16:58] ‫אם ה-z שלנו,
[00:16:59 - 00:17:01] ‫שזה בעצם המשתנה החבוי,
[00:17:02 - 00:17:05] ‫אז הוא משתנה אוניפורמי בין 0 ל-2.
[00:17:06 - 00:17:15] ‫יש לנו התפלגות p-z, ‫זו התפלגות אוניפורמית בין 0 ל-2. ‫כל הערכים הרציפים בין 0 ל-2 בהסתברות שווה.
[00:17:16 - 00:17:18] ‫אז מה ההסתברות שהוא כתבי 1?
[00:17:20 - 00:17:23] ‫או יותר נכון, צריך להגיד את זה, ‫מה p של 1, pz של 1?
[00:17:24 - 00:17:28] ‫נכון, נכון, כי זה לא הסתברות, ‫אמרתי את זה לא נכון, זה ה-pdf של 1.
[00:17:31 - 00:17:34] ‫אנחנו צריכים שהאינטגרל ‫של הדבר הזה יהיה של 1, נכון?
[00:17:35 - 00:17:37] ‫מכל ההתפלגות הזאת.
[00:17:37 - 00:17:39] ‫אז זה נראה ככה.
[00:17:40 - 00:17:51] ‫זה נראה ככה, בין 0 ל-2. ‫אז כדי שכל הדבר הזה יהיה של 1, ‫אז הגובה של זה צריך להיות חצי,
[00:17:52 - 00:17:52] ‫נכון?
[00:17:53 - 00:17:55] ‫אז זו התפלגות אוניפורמית, אוקיי, זה חצי.
[00:17:56 - 00:17:59] ‫עכשיו, נגיד שיש לנו משתנה ‫מקלי חדש שנקרא לו x,
[00:18:00 - 00:18:02] ‫והוא הפונקציה שממפה בין z ל-x,
[00:18:03 - 00:18:04] ‫זה 4 כפול z.
[00:18:05 - 00:18:07] ‫אז x תמיד שווה 4 כפול z.
[00:18:07 - 00:18:13] ‫וילך לנו עכשיו את ה-density של x,
[00:18:14 - 00:18:18] ‫אנחנו לא יודעים אותו, אוקיי? ‫אנחנו שואלים מה ה-density של x בנקודה 4 למשל.
[00:18:21 - 00:18:24] ‫זו הדרך הנאיבית לחשב את זה, ‫זה להגיד, אוקיי,
[00:18:24 - 00:18:26] ‫p של x שווה 4,
[00:18:26 - 00:18:29] ‫זה כמו ההסתברות שהאירוע של x שווה 4,
[00:18:29 - 00:18:31] ‫שזה כמו ש-4z,
[00:18:31 - 00:18:32] 4 כפול z שווה 4,
[00:18:33 - 00:18:36] ‫שזה אנחנו בעצם אומרים ש-z שווה 1,
[00:18:36 - 00:18:38] ‫וזה אנחנו יודעים קודם שזה חצי,
[00:18:39 - 00:18:42] ‫אוקיי? אבל זה כמובן לא נכון, אוקיי? ‫מה לא נכון פה?
[00:18:44 - 00:18:46] איפה רימיתי? ‫מ-4z שווה 4,
[00:18:46 - 00:18:48] 4,z שווה 1. ‫פה?
[00:18:49 - 00:18:57] לא, אני חושב שזה דווקא בסדר. ‫זאת אומרת, האירוע ש-4z שווה 4, ‫זה בדיוק אותו אירוע ‫כמו z שווה 1.
[00:19:06 - 00:19:12] ‫נכון, נכון, נכון, קשור לנגזרת. ‫אז המעבר הראשון פה הוא לא נכון.
[00:19:13 - 00:19:15] ‫מה כתוב כאן? ‫כבר כתוב ההסתברות
[00:19:15 - 00:19:17] ‫ש-x שווה ל-4.
[00:19:18 - 00:19:21] ‫כאילו, המאורע ש-x שווה ל-4. ‫מה ההסתברות ש-x שווה ל-4?
[00:19:21 - 00:19:32] ‫-0. ‫-0, נכון? הדבר הזה זה לא ההסתברות ‫ש-x שווה ל-4, כן? ‫אנחנו כותבים כל מיני דברים פה כבר בקיצור, ‫אז אנחנו לא כל כך זוכרים ‫את כל ההסתברות שמאחורי זה.
[00:19:32 - 00:19:33] זה ה-PDF
[00:19:34 - 00:19:39] ‫של x בנקודה 4, זאת אומרת, ‫זאת אומרת, זה הנגזרת של ההתפלגות
[00:19:42 - 00:19:43] הדמודטיבית, איך קוראים לזה?
[00:19:44 - 00:19:46] ה-CDF,
[00:19:47 - 00:19:48] קוראים לזה גם צפיפות ההסתברות,
[00:19:49 - 00:19:50] ‫כן, CDF,
[00:19:51 - 00:20:00] התפלגות שאומרת מה ההסתברות ‫ש-x קטן מאיזשהו ערך, נכון? ‫אז זה הנגזרת של הדבר הזה בנקודה 4. ‫קוראים לזה גם צפיפות ההסתברות,
[00:20:01 - 00:20:01] ‫זה באיזה קצב
[00:20:02 - 00:20:06] ‫ההסתברות גדלה מקצת פחות 4 ‫לקצת יותר מ-4.
[00:20:07 - 00:20:10] ‫אז זה לא שווה לזה, אוקיי?
[00:20:11 - 00:20:15] כל המעבר פה הוא נכון, ‫אבל בעצם שני המעברים האלה בחזרה,
[00:20:16 - 00:20:19] ‫מהסתברות, בחזרה לצפיפות, הם לא נכונים.
[00:20:21 - 00:20:25] ‫ובעצם מה שצריך לתקן זה שהקצב עצמו ‫גם משתנה, נכון?
[00:20:25 - 00:20:28] ‫זה לא רק שאנחנו יודעים ‫לאיזה מקום אנחנו חוזרים.
[00:20:29 - 00:20:33] ‫הנקודה הזאת היא מופתה לנקודה אחרת, ל-4.
[00:20:35 - 00:20:36] ‫אנחנו בעצם יודעים לחזור לכאן,
[00:20:37 - 00:20:39] ‫אבל גם לא רק הנקודות מופו,
[00:20:39 - 00:20:42] ‫אלא גם הקצב של השינוי של ההתפלגות.
[00:20:42 - 00:20:45] ‫איך נראה ה-CDF של זה, ‫ההתפלגות הרפורמית?
[00:20:50 - 00:20:53] ‫הקו ישר רחוק, הוא עולה בקצב אחיד,
[00:20:53 - 00:20:56] ‫מ-0 ל-1, נכון?
[00:20:56 - 00:20:58] ‫אז הנגזרת של זה, זה פשוט זה.
[00:20:59 - 00:21:04] ‫עכשיו, כשאני ממפה את הכול ל-0 עד 8,
[00:21:05 - 00:21:08] ‫אז לא רק זה מתרחב, ‫אבל גם הדבר הזה, הקצב משתנה.
[00:21:09 - 00:21:10] ‫כן, חייב לרדת.
[00:21:11 - 00:21:14] ‫זאת אומרת, הקצב שבו ההתפלגות מצטברת,
[00:21:14 - 00:21:15] ‫הוא יותר נמוך.
[00:21:17 - 00:21:21] ‫אז צריך להתייחס לזה ‫כשאנחנו עושים את החישוב.
[00:21:26 - 00:21:27] ‫תכף נדבר על זה עוד קצת.
[00:21:31 - 00:21:33] ‫הנוסחה היא בעצם לנוסחה הזאת.
[00:21:34 - 00:21:36] ראינו את זה, ‫ואתם ברוכים שראינו את זה בשיעור ראשון?
[00:21:37 - 00:21:38] ‫לא, בסדר, שוב.
[00:21:40 - 00:21:41] ‫נדבר על זה גם מספיק היום.
[00:21:42 - 00:21:46] ‫אז במימד אחד זה נראה ככה, ‫אם יש לנו התפלגות...
[00:21:48 - 00:21:50] יש לנו Z,
[00:21:52 - 00:21:55] זה המשפנה הראשון שלנו, X הוא פונקציה של Z,
[00:21:56 - 00:21:56] ‫והיא, ב-F,
[00:21:57 - 00:21:59] ‫הפונקציה הזאת היא מונוטונית,
[00:22:00 - 00:22:02] ‫שההפכי שלה הייתה קרמת ה-H של X,
[00:22:04 - 00:22:08] ‫אז ההסתברות, הצפיפות של ההסתברות
[00:22:09 - 00:22:13] של X שווה לצפיפות של ההסתברות של ה-Z ‫שהתאים ל-X הזה,
[00:22:13 - 00:22:16] ‫זה ה-H של X, אנחנו חוזרים חזרה על ה-Z,
[00:22:16 - 00:22:19] ‫חברה על ה-Z שממנו אנחנו יודעים ‫שנוצר ה-X,
[00:22:20 - 00:22:21] ‫בודקים מה הייתה הצפיפות שם,
[00:22:22 - 00:22:25] ‫אבל את זה אנחנו צריכים לתקן על ידי בכמה
[00:22:26 - 00:22:27] ‫מתחנו את הפונקציה בעצם,
[00:22:28 - 00:22:30] ‫בנקודה הזאת, שזה בדיוק ה...
[00:22:31 - 00:22:32] ‫התאי המופקר הזה,
[00:22:35 - 00:22:38] ‫התאי הזה זה נגזרת, ‫זה הנגזרת של ה-H של X,
[00:22:39 - 00:22:42] ‫זה בערך מוחלט, זה לא כל כך משנה ‫איזה פונקציה שיורדת או פונקציה שיורדת,
[00:22:43 - 00:22:44] ‫זה משנה איזה כמה זה מונקח.
[00:22:46 - 00:22:48] ‫אוקיי, אז אין נוסחה למימד אחד,
[00:22:48 - 00:22:52] ‫וזה מתאים למה שראינו כאן, נכון? ‫ראינו שזה שמינית,
[00:22:52 - 00:22:58] ‫היא הנגזרת של ההופכי של 4 כפול Z,
[00:22:59 - 00:23:02] ‫שזה רבע כפול X, ‫נגזרת של Z זה פשוט רבע,
[00:23:02 - 00:23:03] ‫אנחנו צריכים להכפיל את החצי ברבע.
[00:23:12 - 00:23:13] ‫את זה אמרנו.
[00:23:14 - 00:23:16] ‫אוקיי, מה אם המקרה הכללי? ‫אז במקרה הכללי זה נראה ככה,
[00:23:17 - 00:23:17] ‫אוקיי? יש לנו פונקציות,
[00:23:18 - 00:23:20] ‫זה זה מימד N,
[00:23:20 - 00:23:22] ‫ו-X הוא גמי ממימד N,
[00:23:23 - 00:23:28] ‫ויש לנו איזושהי... ה-X הוא מוגדר ‫על ידי איזושהי פונקציה מ-RN ל-RN,
[00:23:29 - 00:23:30] ‫ויש לנו מפה אותנו מ-Z מ-X,
[00:23:31 - 00:23:32] ‫ויש לפונקציה הזאת הופכית,
[00:23:32 - 00:23:33] S במינוס אחד.
[00:23:34 - 00:23:37] ‫אז שוב, בהינתן שקיבלנו איזשהו X, ‫נגיד ש-X זה התמונה שלנו,
[00:23:37 - 00:23:41] ‫אנחנו יודעים להגיד מה ה-Z שיצר את ה-X הזה.
[00:23:41 - 00:23:44] ‫אנחנו יכולים לחזור חזרה ל-Lacent והיכולת שלנו Z,
[00:23:45 - 00:23:45] ‫זו פונקציה הופכית.
[00:23:46 - 00:23:51] ‫במקרה שיש לנו פונקציה הופכית, ‫אנחנו יכולים לחזור חזרה, ‫ואז אנחנו יודעים להגיד מה ההסתברות,
[00:23:52 - 00:23:53] ‫מה צפיפות ההסתברות ב-X,
[00:23:54 - 00:23:59] ‫אז הצפיפות שווה לצפיפות ב-Z המתאים, ‫חוזרים חזרה פה על Z, זה ה-X נכון ב-X,
[00:24:00 - 00:24:01] ‫אבל צריך לתקן את זה.
[00:24:02 - 00:24:06] ‫עכשיו, פה התיקון נראה קצת יותר מורכב, ‫בגלל שזו פונקציה רב-ממדית,
[00:24:07 - 00:24:09] ‫בעצם במקום נגזרת,
[00:24:10 - 00:24:11] ‫אנחנו צריכים להסתכל על
[00:24:12 - 00:24:15] מטריצת הנגזרות, ‫זה מה שנקרא יעקוביאן,
[00:24:16 - 00:24:20] ‫אז יש לנו בעצם, אפשר לחשוב על זה ‫בתור n פונקציות,
[00:24:22 - 00:24:26] ‫שלכל אחת מהן יש n משתנים, ‫אז אנחנו יכולים לגזור ‫את כל אחת מה-n פונקציות האלה
[00:24:28 - 00:24:30] ‫ביחס לכל אחד מה-n משתנים,
[00:24:30 - 00:24:32] ‫אנחנו קובלים n על n מספרים,
[00:24:33 - 00:24:35] ‫זה בונה לנו את המטריצה הזאת,
[00:24:35 - 00:24:36] ‫שנקראת יעקוביאן,
[00:24:37 - 00:24:42] ‫והדרך שאנחנו נתקן פה זה ‫על ידי פשוט הדלטרמיננטה ‫של המטריצות.
[00:24:43 - 00:24:48] ‫בשביל שוק יש גם ערך מוחלט, ‫כן היא פחות לנו אינטואיציה ‫שמתחילה או מגדילה את
[00:24:50 - 00:24:51] ‫המרחב.
[00:24:51 - 00:24:54] ‫אז תכף נסתכל גם קצת אינטואיציה ‫על הדבר הזה.
[00:24:56 - 00:25:01] ‫שתי הערות קודם, ‫אחד זה ששימו לב ש-X ו-Z ‫צריכים להיות באותו מימד,
[00:25:02 - 00:25:04] ‫זה כבר אילוץ שלא היה לנו קודם,
[00:25:05 - 00:25:06] ‫כשדיברנו על V-A-E,
[00:25:07 - 00:25:10] ‫Z היה יכול להיות משהו, ‫אפילו לא נראית כמו תמונה,
[00:25:11 - 00:25:15] ‫איזשהו וקטור, ‫ו-X היה כל הפיקסלים שפתחו לב.
[00:25:15 - 00:25:18] ‫אז פה אנחנו, כדי שזה יהיה הפיכה, ‫אנחנו צריכים שזה יהיה באותו מימד.
[00:25:21 - 00:25:24] ‫בעוד נקודה שהדטרמיננטה הזאת,
[00:25:25 - 00:25:29] ‫הדטרמיננטה של הדבר הזה, ‫זה מטריצה.
[00:25:33 - 00:25:36] ‫אוקיי, כמו חברי זה לא מטריצה, ‫אבל תכף אנחנו נראה ש...
[00:25:41 - 00:25:48] ‫דילגתי פה איזה שלב, ‫אבל ה-F במינוס אחד, אמרנו שזה מטריצה, ‫המטריצה האופכית של זה,
[00:25:48 - 00:25:50] ‫זה גם הנגזרת של הפונקציה האופכית,
[00:25:51 - 00:25:58] ‫ויש כלל שבדטרמיננטות, ‫הדטרמיננטה של המטריצה האופכית, ‫זה שווה לאחד חלקי הדטרמיננטה של המטריצה.
[00:25:59 - 00:26:01] ‫אז אני יכול באותה מידה לחשב פה,
[00:26:02 - 00:26:05] ‫לא לקחת את הפונקציה האופכית, ‫אלא לקחת את הפונקציה עצמה,
[00:26:05 - 00:26:08] ‫לחשב את היעקוביאן שלה,
[00:26:09 - 00:26:16] ולחשב את הדרמיננטה שלו ואת זה צוברים עם מסכן, אוקיי זה מספר ודרמיננטה זה מספר
[00:26:16 - 00:26:18] ופשוט לחלק במספר במקום להכביל
[00:26:19 - 00:26:21] ותכף נראה קצת יותר בפירוט.
[00:26:24 - 00:26:29] אוקיי זו הוכחה שהיה לנו בשיעור הראשון אני לא אכנס לזה אני רוצה היום לדבר קצת יותר על אינטואיציה אני רוצה להסתכל על ההוכחה
[00:26:30 - 00:26:37] שעברנו עליה אז יכול להסתכל בשיעור הראשון אבל ההוכחה היא מה שאמרתם פה בגדול אנחנו
[00:26:37 - 00:26:43] צריכים לעבור להתייחס ל-CDF לא להישאר במוצע ב-PDF שם אנחנו לא יודעים
[00:26:44 - 00:26:50] לדבר על מאורעות שקולים מבחינת ההסתברות אז אנחנו צריכים מה-PDF לעבור ל-CDF
[00:26:51 - 00:27:02] ואז שם לעשות את כל המעבר בין X ל-Z ואז לחזור ל-PDF זה מה שיש פה פה זה עם X ו-Y אנחנו עוברים מההתפלגות ל-X להתפלגות של Y
[00:27:03 - 00:27:05] ל-CDF פי גדול כאן זה ה-CDF
[00:27:06 - 00:27:07] ואז אנחנו גוזרים שוב פעם
[00:27:11 - 00:27:11] את מה שראינו
[00:27:15 - 00:27:17] בואו נדבר עוד קצת על אינטואיציה
[00:27:20 - 00:27:24] נחזור רגע למימד 1, במימד 1 אנחנו יכולים לחשוב על ה...
[00:27:25 - 00:27:34] בעצם אנחנו צריכים לגרום לאינטגרל שלנו להיות 1 נכון? והאינטגרל אפשר לחשוב עליו בתור סכום של
[00:27:35 - 00:27:38] מלבנים פה אנחנו יכולים לחלק את הפונקציה שלנו להרבה
[00:27:38 - 00:27:41] מלבנים שכל אחד מהם הגודל שלו הולך ופטרנט,
[00:27:42 - 00:27:49] נכון? אז אפשר לחשוב על האינטגרל בתור הגבול של ספור המלבנים האלה כשאנחנו מקטינים את גדולי המלבנים
[00:27:49 - 00:27:55] ואז השאלה מה קורה כשאנחנו עברנו ממשתנה אחד למשתנה אחר
[00:27:55 - 00:28:02] אז פתאום בעצם הגדלנו או הקטנו את המלבן שאנחנו עובדים איתו בזמן שאנחנו מקטינים את המלבנים
[00:28:02 - 00:28:08] פתאום אנחנו מסתכלים על המלבנים של ה... אחרי הדשף היה לנו את הפונקציה
[00:28:08 - 00:28:16] אז המלבנים האלה פתאום גדלו או קטנו אז בדיוק בכמה הם גדלו או קטנו זה הפרמזר שאנחנו צריכים לתקן בו
[00:28:18 - 00:28:23] גם אפשר לחשוב על זה ככה במונחים כאלה של קלקולוס כזה של dx וה dz'ים
[00:28:24 - 00:28:26] ואז אם יש לנו את האינטגרל על dz המקורי
[00:28:27 - 00:28:30] תמיד אנחנו חשובים להכפיל ולחלק ב-vx
[00:28:30 - 00:28:34] אנחנו רוצים בסופו של דבר לעשות את האינטגרל לפי x, פשע dx בחוץ
[00:28:34 - 00:28:42] אז זה בעצם נשאר לנו כאן את היחס הזה בין z ל-dx שזה בדיוק הנגזרת של הפונקציה שאנחנו
[00:28:44 - 00:28:45] מיפינו איתה
[00:28:46 - 00:28:48] בין x לזה, לא הפוך בין z ל-z
[00:28:49 - 00:28:50] לא בין x לזו
[00:28:52 - 00:28:58] אוקיי אז זה היה בממד אחד והדבר הזה הוא גם אפשר לחשוב עליו כמה ממדים, אוקיי?
[00:28:58 - 00:29:00] בוא נסתכל רגע בדו-ממד
[00:29:01 - 00:29:03] בדו-ממד אם יש לנו מטריצה
[00:29:05 - 00:29:08] אפשר לחשוב על מטריצה שממפה לנו וקטור, אוקיי?
[00:29:09 - 00:29:11] בעצם אם אנחנו חושבים על ארבע הנקודות האלה
[00:29:12 - 00:29:15] אנחנו מפעילים על הנקודות האלה את הפונקציה הזאת
[00:29:16 - 00:29:23] אז הנקודות האלה ימופו בצורה כזאת שאנחנו נקבל עכשיו מקבילית, כמובן אם היה לנו קודם ריבוע
[00:29:24 - 00:29:26] בעצם אנחנו נקבל את המזגילית הזאת
[00:29:27 - 00:29:30] והשטח של מגבילית הוא יכול לחשב אותו ככה
[00:29:31 - 00:29:32] a, d פחות b, c
[00:29:33 - 00:29:36] שזה בדיוק הדטרמיננטה
[00:29:37 - 00:29:41] של הפונקציה הזאת, אוקיי? בדיוק לממד a, b פחות c,
[00:29:42 - 00:29:44] אוקיי? זה לדטרמיננטה של דו-ממד
[00:29:48 - 00:29:53] מי שלא רואה למה הדבר הזה זה השטח של זה, זה לא לגמרי מיידי אבל אפשר, יש פה פורים
[00:29:54 - 00:29:57] שמתי לכם כאן כזה שמראה את זה
[00:29:58 - 00:30:03] אוקיי? וזה בדיוק הדטרמיננטה. אז הדטרמיננטה בדיוק אומרת, תחשבו שקודם אמרנו בכמה הגדלנו את
[00:30:05 - 00:30:14] המלבנים האלה כשאנחנו עושים את האינטגרל, ברב-ממד זה לא רק האורך של המלבן אלא זה ממש אנחנו צריכים לחשב את הנפח של איזשהו
[00:30:16 - 00:30:20] משהו נגיד בדו-ממד, אנחנו צריכים לחשב את הנפח על פני איזשהו שטח
[00:30:21 - 00:30:24] אנחנו לפעמים מחלק את השטח הזה לקוביות כאלה,
[00:30:25 - 00:30:31] הבסיס של כל קובייה זה איזשהו ריבוע כזה ובעצם כשאנחנו מפעילים איזושהי פונקציה לינארית
[00:30:32 - 00:30:37] על ה-Z שלנו אז הקוביות האלה הופכות להיות מן מגביליות כאלה
[00:30:38 - 00:30:41] אנחנו צריכים לתקן בכמה הגודל של הקובייה,
[00:30:42 - 00:30:46] כמה כל קובייה הפכה למגבילית, זה בדיוק הדטרמיננטה של הפונקציה לינארית
[00:30:48 - 00:30:50] אוקיי אז זו הסיבה שהדטרמיננטה מופיעה שם
[00:30:51 - 00:30:54] אבל עכשיו דיברנו על מטריצות
[00:30:57 - 00:31:00] אותו דבר קורה בממדים יותר גרועים אוקיי אז
[00:31:01 - 00:31:03] סתם איזה מחשב יצת ממד ורב ממד
[00:31:05 - 00:31:07] האינטואיציה תופסת
[00:31:09 - 00:31:14] אז דיברנו למה צריכים דטרמיננטה, עכשיו למה האינטואיציה הזאת אם A
[00:31:15 - 00:31:17] לינארי עדיין תופסת
[00:31:19 - 00:31:19] אז
[00:31:21 - 00:31:26] מה שכתוב כאן זה לא שכבר אמרנו,
[00:31:27 - 00:31:29] השאלה מה קורה כשהפונקציה היא לא לינארית
[00:31:30 - 00:31:34] עוד שקף על מה שקורה כשזה לינארי, אוקיי?
[00:31:35 - 00:31:36] זה מה שאמרנו
[00:31:37 - 00:31:46] אני אמרתי את זה יחסית מהר נראה לי האינטואיציה יש לכם, המטרה כאן זה לא להבין בדיוק את המספורת אלא יותר לתפוס את האינטואיציה של הדבר הזה
[00:31:46 - 00:31:50] מי שרוצה להסתכל, זה בדיוק מה שקורה עם הפונקציה הזאת שהפעלנו על זה היא
[00:31:50 - 00:31:52] באמת הייתה פונקציה לינארית פשוט היינו מכפילים ל-A,
[00:31:53 - 00:31:56] אבל בדיוק מה שאמרנו קורה, צריכים להכפיל בדטרמיננטה
[00:31:57 - 00:32:02] של ה-A או לחלק בדטרמיננטה של ה-A שזה כמו להכפיל את הדטרמיננטה של הפריצה ההופכית ל-A
[00:32:04 - 00:32:10] אבל אנחנו עכשיו מדברים על משהו שהוא באופן טרני, הפונקציה שלנו לא תהיה לינארית,
[00:32:10 - 00:32:11] אנחנו רוצים
[00:32:11 - 00:32:18] מה שרצינו זה למצוא איזושהי פונקציה מורכבת שלוקחת אותנו מהתפלגות יחסית פשוטה להתפלגות מורכבת
[00:32:19 - 00:32:20] הפונקציה הזאת כנראה לא תהיה לי נוטרית
[00:32:21 - 00:32:28] אבל עדיין כשאנחנו מחשבים את האינטגרז של הדבר הזה אנחנו יכולים לחשוב על כל
[00:32:36 - 00:32:39] קטע קטן שאנחנו עושים,
[00:32:39 - 00:32:42] אנחנו יכולים לחשוב בתור ליניאריזציה של הפונקציה הזאת
[00:32:43 - 00:32:48] עוד דרך אחרת לחשוב על זה זה שבעצם אנחנו חושבים את הנגזרת של הדבר הזה שזה שקול
[00:32:48 - 00:32:53] למטריצה אחת, אוקיי? למטריצה היעקוביאנית שלנו זה בעצם המטריצה שבה אנחנו,
[00:32:54 - 00:32:56] זה הליניאריזציה של הדאטה שלנו,
[00:32:57 - 00:33:05] אוקיי? זה נובע מטור טיילר, הנגזרת הראשונה זה בעצם המקדם אם היינו מתייחסים לפונקציה בתור פונקציה ליניארית,
[00:33:06 - 00:33:07] אז במקום
[00:33:08 - 00:33:13] באופן כללי אנחנו יכולים לחשוב למרות שהפונקציה היא לא ליניארית,
[00:33:13 - 00:33:15] ‫שאנחנו יכולים לחשוב עליה בתור...
[00:33:16 - 00:33:19] ‫בכל מקום אנחנו מכפילים ‫באיזושהי פונקציה.
[00:33:21 - 00:33:23] ‫עושים טרנספורמציה לינארית,
[00:33:23 - 00:33:26] ‫והמטריצה הזאת שאנחנו מכפילים בה ‫היא היעקוביאן, אוקיי? היא הנגזרת
[00:33:27 - 00:33:28] באותה נקודה.
[00:33:29 - 00:33:35] ‫אז מקומית אנחנו יכולים לחשוב ‫על הדבר הזה בתור פונקציה לינארית.
[00:33:36 - 00:33:38] ‫איך נראה יעקוביאן? ‫שוב, זו ההגדרה של יעקוביאן.
[00:33:40 - 00:33:43] ‫אז זה מטריצה, ‫כל איבר במטריצה זה נגזרת של
[00:33:44 - 00:33:49] ‫כל שורה זה בעצם ל-output שונה ‫של הפונקציה הזאת. אוקיי? ‫הפונקציה הזאת לוקחת אותנו מ...
[00:33:50 - 00:33:57] ‫- אז כאן זה מוגדר מהפונקציה על איקס, אוקיי? ‫אז הפונקציה מ-x ל-y או ל-z, ‫אם אנחנו חושבים על הכיוון ההפוך.
[00:33:59 - 00:34:06] ‫אז כל שורה כאן זה ה-output. ‫output שונה בפונקציה הזאת, ‫זו פונקציה שהoutput שלה הוא רב ל-v,
[00:34:07 - 00:34:11] ‫כל שורה זה מתייחס ל-output אחר ‫וכל עמודה זה מתייחס ל-input אחר.
[00:34:11 - 00:34:14] אנחנו גוזרים אאוטפוט של הפונקציה באינפוט, אז
[00:34:15 - 00:34:16] מקבלים את המטריצה הזאת.
[00:34:21 - 00:34:28] אוקיי, שוב פעם יש את האינטואיציה הזאת שאמרתי קודם, אנחנו יכולים לחשוב שמקומית אנחנו מיפינו את כל אחד מהמקומיים שלנו,
[00:34:30 - 00:34:33] מהשטחים שלנו, לאיזושהי מקבילית
[00:34:34 - 00:34:36] וזה בדיוק מה שהקוביאן תופס.
[00:34:38 - 00:34:40] אוקיי, אז זה מה שאנחנו מקבלים
[00:34:41 - 00:34:49] באופן כללי, אוקיי, אבל יש לנו חוזר וזה לא מה שאמרתי מקודם, אוקיי, כשיש לנו פי של x אנחנו יכולים לשלב
[00:34:49 - 00:34:50] את פי של
[00:34:53 - 00:34:54] פי של z
[00:34:55 - 00:34:59] עבור ה-z שיצא את ה-x, אנחנו חוזרים חזרה, מפעילים את f במינוס 1
[00:35:01 - 00:35:04] אני אחזור חזרה, אני אחזור על הכל, כי בילגתי פה.
[00:35:04 - 00:35:08] אם יש לנו z, זה המשתנה החבוי שלנו, x זה איזושהי פונקציה
[00:35:09 - 00:35:09] של z
[00:35:11 - 00:35:19] אבל העילות שלנו עכשיו על ה-f זה שהיא הפיכה, אוקיי, שאנחנו יכולים לחזור חזרה מ-z ל-x אז יש לנו,
[00:35:19 - 00:35:21] אם יש לנו את f, יש לנו גם את f מינוס 1
[00:35:22 - 00:35:24] ועכשיו אנחנו רוצים לחשב את הצפיפות של x
[00:35:25 - 00:35:26] אז זה יהיה פשוט
[00:35:27 - 00:35:31] הצפיפות של z, שזה למשל גאוסיאן או משהו שאנחנו יודעים לחשב,
[00:35:32 - 00:35:40] אבל באיזה מקום? במקום של f מינוס 1x אנחנו חוזרים חזרה מ-x ל-z שמתאים לו ואנחנו מתקנים את זה בדבר הזה, אוקיי,
[00:35:40 - 00:35:50] שוב שאומרים לו כאן ערך מוחלט של דטרמיננטה של היעקוביאנט של f מינוס 1 או שאפשר לחלק בערך מוחלט של דטרמיננטה של היעקוביאנט של f
[00:35:51 - 00:35:51] באותו דבר.
[00:35:54 - 00:35:55] אוקיי,
[00:35:57 - 00:35:57] אז
[00:35:57 - 00:35:59] זה היה הרקע של יעקוביאנטות,
[00:36:00 - 00:36:03] דטרמיננטות ומוסחת
[00:36:07 - 00:36:07] ה-changer variables.
[00:36:09 - 00:36:09] שאלות על זה?
[00:36:10 - 00:36:14] אוקיי, אז מה זה Normalizing Flows?
[00:36:15 - 00:36:17] Normalizing Flows הרעיון הוא כזה,
[00:36:20 - 00:36:25] תעשו בדיוק את מה שאני חושב שאתם הבנתם כבר ממה שהסברנו, יש לנו
[00:36:26 - 00:36:27] latent z,
[00:36:27 - 00:36:29] יש לנו x,
[00:36:29 - 00:36:34] אנחנו נבנה את המיפוי הזה בין z ל-x בתור איזושהי פונקציה הפיכה,
[00:36:35 - 00:36:37] אנחנו נדאג שהפונקציה הזאת תמיד הפיכה.
[00:36:40 - 00:36:47] מה שאומר שזה יהיה פונקציה עם איזה שהם פרמטרים, למשל איזושהי רשת מיורונים או משהו שיש בתוכו איזה רשת מיורונים,
[00:36:48 - 00:36:54] אבל אנחנו צריכים לבנות את זה ככה שעבור כל פרמטר תטא אנחנו יכולים לחשב גם את הכיוון האחרון.
[00:36:56 - 00:36:58] אם אנחנו יכולים ללכת מזה ב-x אנחנו לא יכולים ללכת פזרה גם
[00:36:59 - 00:37:00] מ-x לזה.
[00:37:01 - 00:37:03] זה הרעיון ב-Normalizing Flows.
[00:37:11 - 00:37:16] ובעצם זה מאפשר לנו לעשות למידה עם מקסימום לייקליות או מקסימום לוג לייקליות.
[00:37:17 - 00:37:19] אז איך אנחנו עושים את זה? שוב אנחנו לוקחים,
[00:37:20 - 00:37:22] מה זה מקסימום לייקליות? אנחנו לוקחים דאטה
[00:37:23 - 00:37:24] מהטרנינג סט שלנו,
[00:37:25 - 00:37:25] נכון?
[00:37:26 - 00:37:27] מחשבים את ה-likelyיות,
[00:37:28 - 00:37:32] יש לנו ניחוש נוכחי של הפרמטרים שלנו,
[00:37:32 - 00:37:33] עם תטא,
[00:37:33 - 00:37:35] אנחנו מחשבים את ה-likelyיות
[00:37:36 - 00:37:38] של הנכודות האלה, שוב,
[00:37:39 - 00:37:39] לקחנו מהדאטה,
[00:37:41 - 00:37:46] ומחשבים את הגרדיאנט של זה, נכון? ועושים gradient descent, מנתקנים את הפרמטרים תטא.
[00:37:47 - 00:37:50] ובשביל זה אנחנו צריכים להיות מסוגלים לחשב את ה-likely של הנקודה שם.
[00:37:51 - 00:37:51] אז איך הם מחשבים?
[00:37:51 - 00:37:54] זה בדיוק לפי הנוסחה שראינו קודם, יש לנו איזושהי
[00:37:56 - 00:37:57] נקודה מהדאטה-סט שלנו, x,
[00:37:58 - 00:38:00] אנחנו מחשבים את ה-likelyיות
[00:38:01 - 00:38:03] ולפי זה שאנחנו מחזירים חזרה את x,
[00:38:03 - 00:38:07] לפי הפרמטרים שיש לנו כרגע תטא אנחנו מחזירים חזרה את x למרחב של z,
[00:38:08 - 00:38:09] בודקים מהי ההסתברות,
[00:38:10 - 00:38:10] של ה-z הזה,
[00:38:12 - 00:38:14] ומתקנים בדבר הזה.
[00:38:21 - 00:38:25] אוקיי, אז מה אנחנו צריכים לשים לב?
[00:38:26 - 00:38:28] אם אנחנו משווים את זה נגיד ל-BAE,
[00:38:29 - 00:38:30] אז אוקיי,
[00:38:32 - 00:38:33] אני קודם את זה נותן לנו,
[00:38:33 - 00:38:39] לא כתבתי גם על מה אנחנו צריכים לשים לב, אנחנו צריכים לשים לב שה-F באמת יכולה להיות הפיכה, בניגוד ל-BAE שלא היינו צריכים לזה,
[00:38:40 - 00:38:41] זה יהיה חלק,
[00:38:42 - 00:38:42] זה יהיה המשך,
[00:38:43 - 00:38:46] בעצם כל המשך ההוצאה זה איך אנחנו גורמים ל-F תהיה הרבה הפיכה,
[00:38:48 - 00:38:50] ושימו לב שבעצם זה מאפשר לנו,
[00:38:50 - 00:38:53] בניגוד ל-BAE, לחשב את ה-likelyיות בצורה ישירה,
[00:38:55 - 00:38:58] אין לנו יותר את ה-albo וכל הבלאגן הזה, הקירובים,
[00:38:59 - 00:39:02] אבל אנחנו צריכים גם, לא רק ש-F,
[00:39:03 - 00:39:04] אף אחד כתבתי את זה, אנחנו צריכים ש...
[00:39:05 - 00:39:06] לא כתבתי את זה,
[00:39:07 - 00:39:09] צריכים שאמרנו ש-F תהיה הפיכה,
[00:39:09 - 00:39:13] וגם עוד אילוץ זה ש-X ו-Z צריכים להיות באותו מימד.
[00:39:15 - 00:39:19] זה בעצם המחיר שאנחנו נשלם ביחס ל-BAE.
[00:39:20 - 00:39:23] ואם אנחנו לא היה לנו את האימוצים האלה,
[00:39:23 - 00:39:26] אבל לא יכולנו לחשב את ה-AE בצורה מדויקת,
[00:39:27 - 00:39:29] עכשיו אנחנו נוכל לחשב את ה-AE בצורה מדויקת,
[00:39:29 - 00:39:31] אבל אין לנו את האימוצים האלה.
[00:39:32 - 00:39:34] מעניין שנמצאית ידועה,
[00:39:35 - 00:39:39] הוא לא ידוע, אבל הוא התפלגות פשוטה.
[00:39:40 - 00:39:45] כי אנחנו צריכים פה לחשב את ההסתברות של זה, נכון? אז חזרנו אחר כך להגיד שאם זה באמת היה האוסיאן,
[00:39:48 - 00:39:51] ואז הלכנו למרחב של התמונה שלנו,
[00:39:52 - 00:39:54] שהוא הדבר המספק הזה שראינו פה.
[00:39:56 - 00:40:01] קיבלנו, בהינתן תמונה שהיא באה מהדאטה-סט ואנחנו מבינים שהיא מגיעה
[00:40:02 - 00:40:02] בקריאות הזאת,
[00:40:03 - 00:40:05] אז אנחנו יכולים לחזור איתה כאן.
[00:40:06 - 00:40:06] הגענו לכאן,
[00:40:06 - 00:40:09] אנחנו יודעים לחשב מה ההסתברות של הנקודה הזאתי בזה.
[00:40:10 - 00:40:10] זה יהיה הערך
[00:40:11 - 00:40:12] שיופיע כאן.
[00:40:12 - 00:40:16] אבל את זה אנחנו צריכים לתקן גם בדטרמיננטה של ה-Covian
[00:40:17 - 00:40:17] ולפונקציה הזאת.
[00:40:19 - 00:40:21] זה לא כאילו בעימק של תמונה.
[00:40:22 - 00:40:23] כן, זה בדיוק העברית שאמרתי קודם.
[00:40:24 - 00:40:26] העלו אותך במלצינס, שזה חייב להיות בממד של התמונה,
[00:40:26 - 00:40:29] זה בגלל שאנחנו צריכים ש-F תהיה עתיכה.
[00:40:29 - 00:40:33] זה לא כאילו חיסרון ממש גדול בשביל מה דיבור עכשיו?
[00:40:33 - 00:40:40] זה חיסרון גדול, זאת אומרת, אין לזה פתרון דרך השיטה הזאת, יש כל מיני שילובים של השיטה הזאתי,
[00:40:41 - 00:40:44] אתה יכול ללכת ללכת איתן פלואו על ה-Laytend של BAE למשל,
[00:40:44 - 00:40:46] זה נראה אפשר לשלם אותם.
[00:40:48 - 00:40:49] יש כל מיני דברים שאפשר לשלם אותם.
[00:40:50 - 00:40:53] אבל השיטה הזאת בכלל עצמה היא לא מאפשרת את הממשלה.
[00:41:03 - 00:41:04] אוקיי.
[00:41:06 - 00:41:11] אז בעצם מה אנחנו צריכים לעשות כדי לבנות את ה-F הזאת? אנחנו צריכים ש-F תהיה הפיכה,
[00:41:12 - 00:41:15] כדי שנתחל להתחיל את הנוסחה הזאת,
[00:41:16 - 00:41:17] אבל אנחנו גם רוצים שהיא תהיה
[00:41:19 - 00:41:21] אקספסיב, קוראים לזה פלקסיבל,
[00:41:22 - 00:41:29] פרופלקס, אנחנו לא רוצים שהיא תהיה איזה משהו כזה פשוט, כי כל המטרה שלנו זה לעבור מדרסיין למשהו נורא נורא מורכב, ההתפלגות של תמונות.
[00:41:30 - 00:41:32] זה צריך להיות משהו נורא נורא שמאפשר לנו לעבור
[00:41:32 - 00:41:34] ממשהו פשוט למשהו נורא מורכב.
[00:41:35 - 00:41:37] זה סתם יהיה מציחה ליניארית,
[00:41:37 - 00:41:38] זה מקבל גאוסיאן,
[00:41:39 - 00:41:42] אתם לא יודעים שאם אני מקבל פונקציה ליניארית לגאוסיאן זה גם יהיה גאוסיאן.
[00:41:43 - 00:41:48] אני עושה כל מיני דברים אחרים, זה עדיין נשאר פשוט, אני רוצה שזה יהפוך את זה למשהו נורא מורכב,
[00:41:48 - 00:41:49] שנראה כמו התפלגות של תמונות.
[00:41:52 - 00:41:56] אנחנו רוצים את זה ואת זה, אנחנו גם רוצים שנוכל עדיין לחשב
[00:41:56 - 00:41:58] את הדברים בצורה יעילה,
[00:41:59 - 00:42:02] בעיקר מה שיש לנו שם לחשב זה יעקביאן ו...
[00:42:02 - 00:42:03] ‫דטרמיננטה.
[00:42:05 - 00:42:07] ‫אנחנו חושבים שזה יהיה אפשר ‫לחשב את זה בצורה יעילה.
[00:42:10 - 00:42:15] ‫אז הגישה בדרך כלל ב-Normalizing flow ‫זה להתחיל באמת ‫מהתפלגות מאוד פשוטה, מוגרסיאן,
[00:42:16 - 00:42:21] ‫ואז לא לעשות טרנספורמציה אחת כזאת, ‫אלא פשוט לעשות הרבה טרנספורמציות,
[00:42:21 - 00:42:27] ‫שכל אחת מהן היא יחסית פשוטה. זאת אומרת, ‫כל אחת מהן מקיימת את זה ואת זה,
[00:42:28 - 00:42:30] ‫אבל לא את זה, לא את מיני שניים,
[00:42:31 - 00:42:32] ‫אבל בגלל שאנחנו נעשה הרבה כאלה,
[00:42:32 - 00:42:35] ‫בסך הכול, זה ייתן לנו משהו ‫שהוא כן מורכב.
[00:42:36 - 00:42:48] ‫לא ייתן אוכלוסייה 2. ‫-איך אתה מדבר עכשיו נעשה הרבה ‫כאלה ספציות מיליארדים, או שהם עושים את זה? ‫-זה לא יהיה לי לאט, ‫אנחנו נראה כל מיני, יש כל מיני פגישות לעשות את זה, ‫אבל זה יהיה הרעיון ש...
[00:42:49 - 00:42:51] ‫בעצם אנחנו נעבור מזה ל...
[00:42:51 - 00:42:52] ‫אם...
[00:42:54 - 00:42:54] ‫מרחקתי.
[00:42:57 - 00:42:58] ‫או מרחקתי.
[00:42:59 - 00:43:00] ‫גם כל כך מרחקתי.
[00:43:00 - 00:43:03] ‫אנחנו נעבור מזה ל...נגיד, יש לנו בהתחלה,
[00:43:04 - 00:43:08] ‫כשאני אחפש, יש כל מיני סרטונים כאן, ‫אני אהיה גאוסיאן ‫לאיזה משהו שהוא קצת יותר מורכב,
[00:43:09 - 00:43:13] ‫ואז אני רואה כאילו שהוא קצת יותר מורכב, ‫לאט לאט זה יגיע למשהו הרבה קבוצה.
[00:43:15 - 00:43:17] ‫כל אחד מהם אנחנו נדע לעשות, ‫כל אחד מהם יהיה אפילו,
[00:43:18 - 00:43:19] ‫נדעשות את הדבר הזה.
[00:43:20 - 00:43:24] ‫אז סך הכול זה יגדיר לנו גם ‫פונקציה שאנחנו יכולים ‫לעבור מכאן, חזרה עד ל...
[00:43:27 - 00:43:29] ‫נראה איך נראה איך נראה איך נראה את זה.
[00:43:29 - 00:43:34] ‫-אם אתה אומר שהגאוסיאן כאילו ‫אתה מסתכל על חצי שלנו ולא מתחתיב.
[00:43:37 - 00:43:37] ‫נפתור את זה להפיך.
[00:43:38 - 00:43:39] ‫הגאוסיאן הוא לא הפיך,
[00:43:40 - 00:43:41] ‫זה לגבי יש בה שצייה אפשרויות.
[00:43:42 - 00:43:43] ‫זה לא היסטורנית.
[00:43:43 - 00:43:48] ‫אז לא רק נראה למה אתה מתכוון ‫שהגאוסיאן הפיכה. הדבר שצריך להיות הפיך זה הפונקציה ‫שלוקחת אותנו אם היא תשתנה
[00:43:48 - 00:43:50] ‫מאחד למשתנה אחר.
[00:43:50 - 00:43:57] ‫אם אתה מסביר את הרבה פונקציה ‫אם אתה מתכוון להשתנה מגאוסיאן או? ‫-לא, אני אומר שאני מתחיל מגאוסיאן, ‫ההתפלגות של Z שלי תהיה גאוסיאן,
[00:43:57 - 00:43:59] ‫ואז אני מפעיל כל מיני פונקציות,
[00:44:00 - 00:44:01] ‫שכל אחת מהן,
[00:44:01 - 00:44:05] זאת אומרת, הרבה פונקציות, ‫שרשרת של פונקציות כאלה, ‫וכל אחת נמצאה להיות הפיכה.
[00:44:15 - 00:44:17] ‫אוקיי, זה בדיוק מה שכתוב כאן.
[00:44:17 - 00:44:20] ‫אז אנחנו בעצם נתחיל מגאוסיאן,
[00:44:22 - 00:44:26] ‫אנחנו נקרא לזה Z0. ‫אנחנו נצביע לנו עכשיו הרבה ZDים.
[00:44:28 - 00:44:33] ‫אז Z0, שזה הלייטנד באמת שלנו,
[00:44:34 - 00:44:35] ‫הוא יהיה פשוט גאוסיאן.
[00:44:36 - 00:44:37] ‫ואז אנחנו נפעיל,
[00:44:37 - 00:44:40] ‫אז פה יש כתוב M סימבל אימברטיבלן טרנספורמיישן,
[00:44:41 - 00:44:43] ‫שכל פעם אנחנו,
[00:44:44 - 00:44:46] שבסוף, אחרי שנגיע ל-ZM,
[00:44:47 - 00:44:51] ‫זה מה שיהיה הדאטה שלנו, ‫התפלגות על הדאטה, אוקיי? ‫אז אנחנו נהיה ב-X.
[00:44:52 - 00:44:58] ‫אז בעצם הרכבה של הרבה פונקציות. ‫ZM, שזה X שלנו, מרחב של הדאטה,
[00:44:59 - 00:45:01] ‫יהיה פשוט מורכב מהרבה פונקציות,
[00:45:04 - 00:45:09] ‫שמתחילות מ-Z0, סליחה שעד שם הגיעו לזדל.
[00:45:13 - 00:45:16] ‫אז הסימן הזה זה הרכבה שאפשר ‫לפונקציה של פונקציה של פונקציה של פונקציה.
[00:45:17 - 00:45:31] ‫אוקיי, אז עכשיו, האם אנחנו יכולים ‫להשתמש בצורה, נגיד שאנחנו יודעים לחשב ‫עבור כל אחד מהאפים האלה את ה-Change of Variable פורמולה, ‫האם אנחנו יכולים לחשב ‫את ה-Change of Variable פורמולה ‫להכול, סך הכול? זאת אומרת,
[00:45:32 - 00:45:36] מה-X מהפונקציה הראשונית שלנו ‫עד הפונקציה האחרונה?
[00:45:37 - 00:45:38] יש לי מרחק על זה, אני רוצה לצלוב.
[00:45:47 - 00:45:52] ‫אז יש לי את Z0, ‫נעשה את Z1 נעמד, הוא גם אוסיאן.
[00:45:55 - 00:45:58] ‫הוא ממוטל איזה X אחר, ‫עד Z1, לא יודע,
[00:45:59 - 00:46:01] ‫לא יותר מדי מורכב,
[00:46:03 - 00:46:06] ‫1 מישהו מ-2 עד ZM,
[00:46:08 - 00:46:08] ‫נפלא גם X,
[00:46:10 - 00:46:13] ‫שהוא כבר ממש מורכב, אוקיי? ‫זה ההתפגגות על הדאטה שלי.
[00:46:14 - 00:46:18] ‫אבל כל אחד מהם, אני יודע לחשב את ה...
[00:46:32 - 00:46:36] ‫נכון, אבל גם אני יודע לחשב את הוייקוורט, ‫אני מניח שבכל אחד מהם ‫אני יודע לחשב את ה-Pinverse.
[00:46:36 - 00:46:45] ‫וגם לחשב עבור כל אחד מהם ‫את היאקוביאן ואת ה-Determיננטה.
[00:46:46 - 00:46:49] ‫אני מניח שכל אחד מהאפים האלה ‫שאני יודע לחשב את הדברים האלה.
[00:46:50 - 00:46:55] ‫ראינו דוגמה של לינארי. ‫לינארי זה לא הולך להיות מעניין, ‫כי גם כל אחד מהם הוא לינארי, ‫אז סך הכול זה הכול יהיה לינארי.
[00:46:56 - 00:47:00] ‫אבל כשזה יישב לכם בראש, ‫נגיד שהכול לינארי, ‫אז אני יודע לחשב את ה-Determיננטה,
[00:47:00 - 00:47:01] ‫זה פשוט ה...
[00:47:02 - 00:47:05] סליחה, את היעקוביאן, ‫זה פשוט הפריצה עצמה,
[00:47:06 - 00:47:07] ‫והDetermיננטה הזאת ‫היא היתרמיננטה של מטריצה.
[00:47:09 - 00:47:15] ‫אז איך אני מחשב מהרבה חלקים כאלה ‫את המשוואה להכול?
[00:47:16 - 00:47:19] ‫אני פשוט יכול לשרשר את הכול גם בחישוב.
[00:47:20 - 00:47:24] ‫אז אני יכול בעצם ה-Determיננטה,
[00:47:25 - 00:47:28] ‫הלטרמיננטה של פונקציה, ‫של פונקציה, של פונקציה,
[00:47:28 - 00:47:29] ‫זה כמו,
[00:47:31 - 00:47:33] לא יותר נכון, סליחה, איה היעקוביאן, של פונקציה,
[00:47:34 - 00:47:35] של פונקציה, של פונקציה,
[00:47:35 - 00:47:37] ‫מקהל השרשרת של הגזירה,
[00:47:39 - 00:47:42] ‫זה יהיה פשוט היעקוביאנט של כל אחד, מכפלת...
[00:47:43 - 00:47:45] ‫זה יהיה מכפלת כל היעקוביאנטים.
[00:47:46 - 00:47:48] ‫אז מכפלת כל היעקוביאנטים,
[00:47:48 - 00:47:50] ‫אני יכול גם מכפלה של...
[00:47:51 - 00:47:53] ‫טרמיננטה של מכפלה של מטריצות,
[00:47:53 - 00:47:56] ‫כמו המכפלה של הבטרמיננטות.
[00:47:56 - 00:48:00] ‫בקיצור, אני יכול לחשב ‫את הדבר הזה לכל אחד בפני עצמו,
[00:48:01 - 00:48:04] ‫והעבר תיקון הזה, ופשוט להכפיל את כולם.
[00:48:05 - 00:48:09] ‫אוקיי, אז יש לי... בהינתן שקיבלתי תמונה, ‫כשאני מגיע מכאן,
[00:48:10 - 00:48:12] ‫אני חוזר איתה חזרה עד ל-z0,
[00:48:13 - 00:48:15] ‫בודק מה ההסתברות כאן ב-z0,
[00:48:16 - 00:48:17] ‫אוקיי, זה נפל לי כאן,
[00:48:18 - 00:48:19] ‫לדע מה ההסתברות שלה,
[00:48:20 - 00:48:21] ‫זה הדבר הזה,
[00:48:22 - 00:48:25] ‫ואני צריך לתקן בדטרמיננטה של זה, ‫דטרמיננטה של זה,
[00:48:26 - 00:48:31] ‫כן, זה האיבר תיקון שיש בו.
[00:48:35 - 00:48:35] ‫ברור?
[00:48:37 - 00:48:39] ‫אוקיי, אז זו דוגמה
[00:48:44 - 00:48:46] ‫לאחת הדוגמאות הראשונות שעשו את זה,
[00:48:48 - 00:48:49] ‫במאמר מ-2015,
[00:48:52 - 00:48:55] ‫שעשה את זה ‫עם טרנספורמציות שנראות ככה.
[00:48:55 - 00:48:57] ‫אז זה היה ה-F בכל
[00:48:59 - 00:49:01] ‫בכל איטרציה,
[00:49:02 - 00:49:02] ‫בכל אחת
[00:49:05 - 00:49:09] ‫המעברים האלה, היה לנו F, ‫שזה היה הפרמטריזציה שלנו.
[00:49:11 - 00:49:13] ‫מה רואים כאן?
[00:49:13 - 00:49:17] ‫אנחנו לוקחים את ה-z, ‫שזה ה-z המוכיחי שלנו,
[00:49:18 - 00:49:22] ‫מכפילים אותו עם וקטור, ‫אין מכפילים, משהו בלמד גבוה,
[00:49:23 - 00:49:28] ‫ביא איתו מכפלה פנימית ‫עם וקטור של פרמטרים, ‫שאתם אנחנו נלמד, W,
[00:49:28 - 00:49:29] ‫מוסיפים איזשהו,
[00:49:30 - 00:49:33] ‫זה אינטרנטיין סקלר, אוקיי? ‫מוסיפים איזשהו
[00:49:35 - 00:49:37] עוד איזה סקלר, זה גם פרמטר נלמד.
[00:49:38 - 00:49:40] ‫מכפילים איזושהי פונקציה ‫לא ליניארית,
[00:49:41 - 00:49:41] ‫הפיכה אבל.
[00:49:43 - 00:49:46] ‫יש לדוגמה שהם משתמשו ‫תן ה-h כזה,
[00:49:47 - 00:49:49] ‫חוץ לשני ה-S ועוד אחריו.
[00:49:51 - 00:49:52] ‫אז קוראים כזה.
[00:49:56 - 00:49:59] ‫בזה גם מכפילים בעוד איזשהו מספר,
[00:49:59 - 00:50:00] ‫מוסיפים עוד מספר.
[00:50:06 - 00:50:10] ‫סליחה, כן, זה וקטורים, ‫כי סך הכול זה צריך להיות וקטור.
[00:50:11 - 00:50:12] ‫הדבר הזה זה סקלר,
[00:50:12 - 00:50:13] ‫את זה אנחנו מכפילים בווקטור,
[00:50:14 - 00:50:20] ‫מוסיפים איזשהו וקטור, ‫אז סך הכול אנחנו מקבלים וקטור. ‫או, צריכים למכות את זה חזרה למספר במדינת גדול.
[00:50:21 - 00:50:26] ‫בעצם אנחנו עוברים לסקלר, ‫עושים שם משהו לא ליניארי, ‫וחזרים חזרה לוקטור.
[00:50:28 - 00:50:35] ‫והתוצאה נראית ככה, ‫אז זה בדיוק המעברים האלה ‫בין פונקציות שונות,
[00:50:35 - 00:50:39] ‫אז הדוגמה כאן זה כשכל המשתנים ‫הם בדו-מימד.
[00:50:40 - 00:50:41] ‫אז גם אני צייפתי את זה ‫בחד-מימד,
[00:50:42 - 00:50:44] ‫אז פה זה בדו-מימד,
[00:50:44 - 00:50:52] ‫וזה מתחיל מהתפלגות אוניפורמית, ‫אז זו התפלגות של Z0. ‫זאת אומרת שכל הנקודות ‫הדו-מימדיות,
[00:50:53 - 00:50:54] ‫יש להן את אותה הסתברות.
[00:50:56 - 00:50:58] ‫עכשיו, כשהפענו את הדבר הזה פעם אחת,
[00:50:59 - 00:51:00] ‫מאיזשהם פרמטרים,
[00:51:01 - 00:51:04] ‫אז קיבלנו משהו שנראה ככה, ‫פתאום יש איזה קו כאן באמצע,
[00:51:04 - 00:51:07] ‫שההסתברות שלו היא נמוכה.
[00:51:09 - 00:51:12] ‫שני הצדדים שפה, ‫הם כן חזרו להיות הסתברות דברה.
[00:51:13 - 00:51:19] ‫ואז כשמפעילים את זה עוד פעם, ‫אז מקבלים עוד אזור כזה ‫שפתאום ירדה לו ההסתברות.
[00:51:20 - 00:51:23] ‫וכשמפעילים את זה עשר פעמים, ‫מקבלים גם משהו כזה כבר.
[00:51:24 - 00:51:25] ‫כבר נראה קצת יותר מורכב.
[00:51:27 - 00:51:30] ‫יש כאן כבר, בניגוד למשל, ‫יואשן מיץ של מודל,
[00:51:31 - 00:51:33] ‫שהמורכבות עולה בצורה לינארית,
[00:51:33 - 00:51:38] ‫כאן יש פה כל מיני חיתוכים, ‫חלק מזה עולה בצורה אקספוננציאלית.
[00:51:39 - 00:51:43] ‫זה קצת יותר, יכול יותר מהר ‫להגיע להתפלגויות מיניות.
[00:51:44 - 00:51:47] ‫איזה שאלה מקבלים מתחזות קו ‫כשמגלים עם אותם פרמטרים?
[00:51:48 - 00:51:51] ‫לא, זה לא אותם פרמטרים. ‫לכל איטואציה יש פרמטרים אחרים.
[00:51:57 - 00:52:00] ‫תקפתי את זה כבר, ‫אבל הלמידה, בעצם איך היא מתבצעת?
[00:52:00 - 00:52:04] ‫-אנחנו לא חושבים. ‫היום פה את הנוסחה ‫של החישוב של הלייקליות,
[00:52:05 - 00:52:07] ‫נכון? ‫אנחנו חושבים את הלייקליות.
[00:52:08 - 00:52:11] ‫עכשיו, את הדבר הזה ‫אנחנו צריכים לגזור לפי תטא,
[00:52:13 - 00:52:18] ‫נכון? ‫ולחשב את הצעד של עדכון של תטא.
[00:52:19 - 00:52:20] ‫אנחנו רוצים למקסן את הלייקליות.
[00:52:24 - 00:52:26] ‫אז את תטא אפשר להגיע ‫אותו כל מיני דרכים,
[00:52:26 - 00:52:34] ‫אין לנו בעיה לחשב את הדבר הזה ‫כשיש לנו תטא שונה ‫לכל פונקציה.
[00:52:34 - 00:52:39] ‫אנחנו יכולים לחשב את הפורוורד הזה ‫ולחשב את המגזרת שלו,
[00:52:40 - 00:52:41] ‫ולנגזור לפי תקווה.
[00:52:42 - 00:52:46] ‫אז נגזר אחד לרשת אחרת, ‫ולגזר אחד לרשת אחרת, ‫ולגזר שניים לרשת אחרת.
[00:52:47 - 00:52:49] ‫אני לא יודע אם הייתי קוראים לזה רשת פה,
[00:52:51 - 00:52:52] ‫שאת הפונקציה הזאת.
[00:52:53 - 00:52:54] ‫זה נראה לי קצת כמו שכבה ברשת.
[00:52:57 - 00:53:03] ‫אתם עושים מקרים שנגיד, ‫לא רק הפרמטרים שונים, ‫גם הארכיטקטורה שונה, כאילו, מה המשנה?
[00:53:05 - 00:53:08] ‫אני יכול לדבר היום על מקרים ‫שהארכיטקטורה שונה,
[00:53:09 - 00:53:10] ‫יכול להיות שיש דברים כאלה.
[00:53:12 - 00:53:12] ‫-איך יודעים כמה...
[00:53:14 - 00:53:14] ‫כמה לעשות?
[00:53:15 - 00:53:23] ‫זה משהו שמשחקים איתו, ‫יש כל מיני שיטות לעשות.
[00:53:24 - 00:53:26] ‫לאן אין לי תשובה ברורה לגבי.
[00:53:30 - 00:53:38] ‫בדוגמאות של... ‫-פה זה היה דסר, אני חושב שרוב הפלואוס ‫שמדברים עליהם, ‫חוץ מאולי מה שנדבר על סוף השיעור, ‫זה נגיד סדר גודל של 100 איתרציות.
[00:53:42 - 00:53:51] ‫אנחנו בעצם נראה, בסוף הקורס, ‫אנחנו נדבר על דיפיוזן מורלז, ‫שגם אפשר לחשוב עליו ‫בתור איזשהו סוג של גורלגן פלואו, ‫שם סדר גודל של האיתרציות זה 1,000.
[00:53:54 - 00:53:57] ‫שם הפרמטרים שותפים, כן.
[00:53:59 - 00:54:02] ‫כן, הם שותפים, אבל הם יודעים,
[00:54:02 - 00:54:04] ‫הם עדיין יכולים להשתנות בכל מקום.
[00:54:06 - 00:54:08] ‫הם יודעים איפה אתה נמצא, ‫הם לא בדיוק שותפים.
[00:54:15 - 00:54:19] ‫אוקיי, אז יש פה את הרמה של הנוסחה ‫של לחשב את זה, ‫אז אפשר לחשב את זה בצורה יעילה,
[00:54:20 - 00:54:23] ‫גם כאשר המימד הוא יחסית גבוה,
[00:54:23 - 00:54:24] ‫אוקיי, בקטע הזה,
[00:54:27 - 00:54:30] ‫היא בעצם החישוב הזה,
[00:54:31 - 00:54:32] ‫אני רוצה להודות לך,
[00:54:34 - 00:54:36] ‫הקטע הזה, זה לא בעיה, פשוט נכון,
[00:54:37 - 00:54:39] ‫אני צריך לחשב את החישוב אחורה.
[00:54:45 - 00:54:48] ‫אני פה את ה-f למינוס 1.
[00:54:53 - 00:55:05] ‫אה, יכול להיות שזה זה.
[00:55:05 - 00:55:24] איך מחשבים את ההפכי של זה? ‫אני עכשיו לא רואה את זה.
[00:55:36 - 00:55:36] ‫טוב,
[00:55:42 - 00:55:46] צריך להסתכל, כי אני לא רואה עכשיו ‫איך מפרידים בין החלק הזה לחלק הזה.
[00:55:47 - 00:55:48] ‫אני אסתכל על זה,
[00:55:48 - 00:55:49] ‫לאחרי ההפסקה אני אראה.
[00:55:50 - 00:55:54] ‫נראה, אנחנו צריכים בעצם לדעת ‫דרך לחזור חזרה מ-X ל-Z,
[00:55:54 - 00:55:55] ‫אוקיי?
[00:55:55 - 00:55:57] ‫אז נורא לי הדרך, אני לא רואה את זה כרגע,
[00:55:59 - 00:56:01] ‫וגם דרך לחשב את הדטרמיננטה,
[00:56:02 - 00:56:02] אוקיי?
[00:56:03 - 00:56:05] ‫אז איך אנחנו מחשבים את הדטרמיננטה ‫של הדבר הזה?
[00:56:05 - 00:56:09] ‫אז פשוט בגלל שהכל כאן הוא לינארי, ‫חוץ מ...
[00:56:10 - 00:56:14] סליחה, צריכים לחשב את היעקוביאן ‫ואת הדטרמיננטה. הנגזרת ואת הדטרמיננטה.
[00:56:15 - 00:56:16] ‫אז הנגזרת זה מה שכתוב כאן בפנים,
[00:56:17 - 00:56:22] ‫זה לפי חוקי נגזרת של מכפלות וקטוריות.
[00:56:24 - 00:56:25] ‫הנגזרת של הדבר הזה,
[00:56:26 - 00:56:28] זה כמו שכתוב כאן,
[00:56:29 - 00:56:29] אוקיי?
[00:56:30 - 00:56:32] ‫ובטרמיננטה של הדבר הזה,
[00:56:34 - 00:56:38] ‫זה מטריצה, ‫זה היה פוגיה המבוקרית,
[00:56:39 - 00:56:43] ‫אבל בטרמיננטה של זה, ‫אפשר לכתוב אותה בצורה שהיא אהמ...
[00:56:45 - 00:56:49] ‫אפשר, היא שקולה למשהו שהוא, ‫לאיזה חישוב שהוא סקלארי.
[00:56:50 - 00:56:52] ‫אז אם הוא יד כאן נשארו מכפלה חיצונית,
[00:56:53 - 00:56:55] ‫אפשר להפוך את זה כאן למכפלה פנימית.
[00:56:56 - 00:56:58] ‫כשיש לנו בטרמיננטה של משהו ‫שהוא מכפלה חיצונית,
[00:56:59 - 00:57:02] ‫אז זה שקול לפשוט המכפלה הפנימית.
[00:57:03 - 00:57:07] ‫וגם לרשום פה יש פרופסים ‫מטריקס לטרמיננט, נאמר,
[00:57:07 - 00:57:11] ‫פשוט דרך יעילה לחשב את ‫מטרמיננטה של דברים ‫שנראים בצורה הזאת,
[00:57:11 - 00:57:16] ‫זה פשוט החישוב הזה. ‫והחישוב הזה הוא נמצא סקלאר במספר הזה.
[00:57:19 - 00:57:26] ‫לא ניכנס למה, ‫מי שרוצה להסתכל על ‫מטריקס לטרמיננט, למה? ‫זו בדיוק הסיבה שהם בחרו בצורה הזאת, ‫כי אפשר לחשב בצורה יעילה
[00:57:27 - 00:57:28] את ה-Vetרמיננטה שלנו.
[00:57:28 - 00:57:33] ‫אז אנחנו צריכים גם להיות מסוגלים ‫לחשב את ההופכי של הפונקציה,
[00:57:33 - 00:57:37] ‫וגם לחשב בצורה יעילה ‫את הדטרמיננטה של היאקוביאנט שלנו.
[00:57:38 - 00:57:40] ‫זה בעצם מה ש...
[00:57:41 - 00:57:44] ‫כללים, כשאנחנו רוצים לעשות ‫בזיינינג בפעולות פונקציה.
[00:57:47 - 00:57:54] ‫יש פה עוד כל מיני אימוצים, ‫כדי שזה ה-TAMH שלהם יהיה חוקי, ‫הם צריכים שהפרמטרים יהיו בין מוס אחד לאחד, ‫כל מיני דברים קריטים.
[00:57:55 - 00:58:04] ‫זו הייתה הדוגמה הראשונית, ‫אני לא נכנס כל כך לפרטים, ‫כי זה פחות מעניין. ‫זו הייתה הדוגמה הראשונה, ‫או אחת משתי הדוגמאות ‫הרוגמאות
[00:58:05 - 00:58:09] ‫הריטרת השנייה של הגישה, ‫של normalizing flows,
[00:58:09 - 00:58:14] ‫וזה הדוגמה של מה שיוצא, ‫ראינו את הדוגמה הזאת ‫להתפלגות אוניפורמית, ‫אבל אפשר להתחיל ‫מהתפלגות גרסיאנית,
[00:58:15 - 00:58:17] ‫וזה גם מפלק את המרחב בצורה כזאת,
[00:58:17 - 00:58:21] ‫שכל מה שנשאר הוא כאילו נהיה קצת יותר
[00:58:22 - 00:58:30] ‫מאוסיאנית, כן, מעוגל ודוער, ‫בניגוד לפה שכל אזור כאילו נשאר אוניפורמי בעצמו.
[00:58:32 - 00:58:35] ‫אוקיי, זאת הייתה הדוגמה הראשונה.
[00:58:41 - 00:58:47] ‫אז בואו נעשה רגע איזה סיכום ‫של מה שהיה לנו עד עכשיו, ‫ואז נראה עוד כל מיני דוגמאות של מודלית.
[00:58:49 - 00:58:51] ‫קודם כול, למה קוראים לזה normalizing flows?
[00:58:52 - 00:58:56] ‫אז זה normalizing, ‫כי בעצם כל שינוי בהתפלגויות,
[00:58:58 - 00:58:59] ‫אנחנו מנרמלים אותו,
[00:59:00 - 00:59:08] ‫אנחנו יודעים איך לחשב את ההתפלגות, ‫אנחנו יודעים לחשב את המקדם הזה ‫שאנחנו צריכים לתקן בו,
[00:59:08 - 00:59:10] ‫זה ה-normalizing שיש שם,
[00:59:10 - 00:59:17] ‫וה-flow זה בגלל שאנחנו עושים את זה ‫עם הרבה אינטרציות כאלה, ‫אפשר לחשוב על התפלגות, ‫זה מתחיל מהתפלגות כזאת דיברסיאנית,
[00:59:17 - 00:59:22] ‫לאט לאט ההתפלגות הזאת ‫זורמת לסט של התפלגויות ‫עד שהיא מגיעה להתפלגות
[00:59:24 - 00:59:24] ‫הנכונה של הדאטה.
[00:59:26 - 00:59:27] ‫בטח, זו סיבה שקוראים לזה normalize it.
[00:59:29 - 00:59:34] ‫ואמרנו, היוצאים שלנו, ‫אנחנו צריכים שכל הטרנספורמציות האלה יהיו הפיכות,
[00:59:35 - 00:59:40] ‫כל הממדים צריכים תמיד להישאר ‫באותו מימד, ‫של המימד של הדאטה שלנו, של התמונה,
[00:59:43 - 00:59:45] ‫ואנחנו עושים מרכיבים, ככה,
[00:59:45 - 00:59:47] ‫את הפונקציות הפשוטות אחת על השנייה.
[00:59:49 - 00:59:54] ‫זה מאפשר לנו לחשב את ה-Loglikelihood ‫בצורה ישירה עם הנוסחה הזאת.
[00:59:55 - 00:59:57] ‫זו גם הנוסחה של הלוג, זה ה-Loglikelihood,
[00:59:58 - 01:00:03] ‫להינתן פונק והינתן תמונה, ‫היא נותנת X, ‫אני יודע לחשב את ההסתברות שלה ‫תחת המודל שלי.
[01:00:04 - 01:00:05] ‫זו הנוסחה הזאת,
[01:00:05 - 01:00:14] ‫וזה מאפשר לי ללמוד, ‫כי אני עושה פשוט ‫מקסימום לייטיות של הדבר הזה, ‫כן, אני רוצה את הפרמטרים שימקסמו ‫את הנוסחה הזאתי עבור כל הדאטה ‫שיש פונקציות.
[01:00:16 - 01:00:18] ‫כן, בדיוק כמו שעשיתם בתרגיל.
[01:00:20 - 01:00:26] ‫נכון, שם תוך כדי לא כתבתם את הקוד ‫שלוקח את הדאטה מהטרניציה ועושה רצוננטיות, ‫אבל זה מה שהיה שם, נכון? ‫לוקח עוד פעם
[01:00:26 - 01:00:30] תמונה מהדאטה, ‫מחשב את ה-Ligel לפי המודל,
[01:00:31 - 01:00:40] ‫ועושה gradient descent, נכון? ‫וכל החבילות האלה של Deeper, ‫הם נותנים לנו לעשות את הדבר הזה ‫בצורה אוטומטית. ‫אנחנו רק צריכים לדעת לחשב את ה-Fורוורד, ‫בצורה כזאת שהכול יהיה גזיר,
[01:00:41 - 01:00:45] ‫ולהגיד לו, תעשה אופנימיזציה ‫עם SDD או עם אדם.
[01:00:47 - 01:00:54] ‫אוקיי, אז זה הסיכום. ‫עוד דבר חשוב, אנחנו יכולים לדגום ‫גם עכשיו מההתפלגות הזאת.
[01:00:55 - 01:00:56] ‫איך אנחנו דוגמים?
[01:00:58 - 01:01:01] ‫אנחנו פשוט דוגמים מהגאוסיאן הזה, של זה,
[01:01:01 - 01:01:02] ‫יכולים לדגום מגאוסיאן,
[01:01:03 - 01:01:04] ‫ואם מפיינים את הפונקציה הזאת,
[01:01:05 - 01:01:09] ‫אתם מקבלים דהימה מההתפלגות הזאת.
[01:01:11 - 01:01:14] ‫אוקיי, אז זה הדרך שאנחנו דוגמים,
[01:01:14 - 01:01:20] ‫ואנחנו גם יודעים לעשות ‫Latent representation, זאת אומרת, ‫אנחנו יודעים לחזור מאיזושהי תמונה ‫להגיד מה ה-Z שמתאים לה.
[01:01:22 - 01:01:23] איך אנחנו עושים את זה?
[01:01:24 - 01:01:26] ‫מפעים פונק ומפעילים את הפונקציות ההפוכות.
[01:01:28 - 01:01:32] ‫זה בעצם אפשר לעשות הכול, ‫כל מה שאנחנו רוצים לעשות ‫עם מודל גנרטיבי,
[01:01:33 - 01:01:35] ‫אנחנו יכולים לעשות ‫עם normalizing floors.
[01:01:35 - 01:01:38] ‫ה-catch הוא שיש לנו איזשהו איזוץ ‫על מה ה...
[01:01:39 - 01:01:41] ‫איך הטרנספורמציות האלה נראות.
[01:01:42 - 01:01:45] ‫בדוגמה שראינו קודם, ‫זה היה משהו מאוד פשוט, הדבר הזה.
[01:01:47 - 01:01:50] ‫אנחנו נראה עכשיו דוגמאות ‫לדברים קצת יותר מורכבים ‫שעדיין מאפשרים,
[01:01:52 - 01:01:56] מאפשרים לעשות את כל הדברים האלה,
[01:01:56 - 01:02:01] ‫וגם לתפוס התפלגויות מעניינות ‫ולהגיע אפילו לתוצאות טובות על תמונות.
[01:02:03 - 01:02:05] ‫אוקיי, אז בואו נעשה הפסקה ‫ואז נמשיך.
[01:02:08 - 01:02:11] ‫אם אפשר בין המעברים האלה, ‫איך זה גרלות במי-פס?
[01:02:12 - 01:02:14] ‫אז זה אילן לא עובר. ‫-אבל אז זה לא עפיל.
[01:02:17 - 01:02:19] ‫בוא, אני חושב שאתה עולה ויורד את הרשימה.
[01:02:30 - 01:02:33] ‫תראה, שנייה, אני רק בדיוק ‫הבטחתי שאני אמצא את ה...
[01:02:34 - 01:02:34] ‫עוזרים ב-40.
[01:02:35 - 01:02:37] ‫כן, אפשר.
[01:02:37 - 01:02:38] ‫סלאפ. בבקשה.
[01:03:07 - 01:03:08] ‫-כן, תודה רבה.
[01:03:37 - 01:03:38] ‫-כן, תודה רבה.
[01:04:07 - 01:04:08] ‫-כן, תודה רבה.
[01:04:37 - 01:04:38] ‫-כן, תודה רבה.
[01:05:07 - 01:05:08] ‫-כן, תודה רבה.
[01:05:37 - 01:05:38] ‫-כן, תודה רבה.
[01:06:07 - 01:06:08] ‫-כן, תודה רבה.
[01:06:37 - 01:06:38] ‫-כן, תודה רבה.
[01:07:07 - 01:07:08] ‫-כן, תודה רבה.
[01:07:37 - 01:07:38] ‫-כן, תודה רבה.
[01:08:07 - 01:08:08] ‫-כן, תודה רבה.
[01:08:37 - 01:08:38] ‫-כן, תודה רבה.
[01:09:07 - 01:09:08] ‫-כן, תודה רבה.
[01:09:37 - 01:09:38] ‫-כן, תודה רבה.
[01:10:07 - 01:10:08] ‫-כן, תודה רבה.
[01:10:37 - 01:10:38] ‫-כן, תודה רבה.
[01:11:07 - 01:11:08] ‫-כן, תודה רבה.
[01:11:37 - 01:11:38] ‫-כן, תודה רבה.
[01:12:07 - 01:12:08] ‫-כן, תודה רבה.
[01:12:37 - 01:12:38] ‫-כן, תודה רבה.
[01:13:07 - 01:13:08] ‫-כן, תודה רבה.
[01:13:37 - 01:13:38] ‫-כן, תודה רבה.
[01:14:07 - 01:14:08] ‫-כן, תודה רבה.
[01:14:37 - 01:14:38] ‫-כן, תודה רבה.
[01:15:07 - 01:15:08] ‫-כן, תודה רבה.
[01:15:37 - 01:15:38] ‫-כן, תודה רבה.
[01:16:07 - 01:16:08] ‫-כן, תודה רבה.
[01:16:37 - 01:16:38] ‫-כן, תודה רבה.
[01:17:07 - 01:17:16] ‫-כן, תודה רבה.
[01:17:37 - 01:17:39] ‫אממ...
[01:17:58 - 01:18:03] ‫אנחנו רוצים למנות מודלים ‫שמנה יותר כמה שיותר מורכבים,
[01:18:04 - 01:18:08] ‫אוקיי? אז מה האתגר שיש לנו?
[01:18:09 - 01:18:14] ‫אנחנו צריכים בעצם שאנחנו נוכל ‫בצורה עילה לחשב את המיפוי מ-X ל-Z,
[01:18:15 - 01:18:17] ‫כדי שנוכל לחשב לייקיות,
[01:18:18 - 01:18:19] ‫יש לי דאטה, עכשיו מקבלים.
[01:18:20 - 01:18:25] ‫אנחנו נרצה שהכיוון הפוך ‫גם יהיה מ-Z ל-X,
[01:18:25 - 01:18:26] ‫כדי שנוכל לדגום.
[01:18:27 - 01:18:34] ‫אממ... ואנחנו רוצים שנוכל לחשב ‫את האיבר הזה של ה...
[01:18:35 - 01:18:35] ‫של ה...
[01:18:36 - 01:18:39] ‫תיקון, נכון? זאת אומרת שגם החישוב ‫של ה...
[01:18:39 - 01:18:41] ‫לטרמיננטה של היעקביאן צריכה להיות יעילה.
[01:18:43 - 01:18:45] ‫יעקביאן זה, אם הדאטה שלנו,
[01:18:46 - 01:18:48] ‫זה גודל, אם הוא תמונה, נגיד, ‫אז יש לנו
[01:18:49 - 01:18:50] טרפי פיקסלים,
[01:18:51 - 01:18:53] ‫יעקביאן זה כבר n על n.
[01:18:54 - 01:18:58] ‫כן, זה משהו כבר די גדול,
[01:18:59 - 01:19:02] ‫ולחשב את הדטרמיננטה של זה ‫באופן כללי, זה n בשלישית.
[01:19:03 - 01:19:06] ‫זה כבר די מהר מגיע ‫למשהו שהוא לא יעיל.
[01:19:08 - 01:19:13] ‫אנחנו נרצה שכל אחד מהאיברים האלה, ‫נוכל לחשב את ה...
[01:19:14 - 01:19:16] ‫את היעקביאן וגם את ה...
[01:19:16 - 01:19:23] ‫לא יכול לחשב את היעקביאן, ‫אבל אחרי הדטרמיננטה של היעקביאן, ‫אנחנו נרצה להיות סוגלים ‫לחשב את זה בצורה עילה. ‫כמו שראינו בדוגמה הקודמת,
[01:19:24 - 01:19:26] ‫שהיה אפשר לעשות על ידי מכפלה,
[01:19:27 - 01:19:30] ‫היו שם רק מכפלות פנימיות, ‫זאת אומרת,
[01:19:31 - 01:19:33] ‫כמה מכפלות בגודל n,
[01:19:33 - 01:19:34] ‫זה היה סדר גודל של n,
[01:19:35 - 01:19:37] ‫אז צריך לחשב את הליטרמיננטה ‫של היעקביאן.
[01:19:38 - 01:19:41] ‫אז שהיום אנחנו יכולים לבנות ‫דברים קצת יותר מורכבים,
[01:19:41 - 01:19:43] ‫שעדיין נותנים לנו את ה...
[01:19:45 - 01:19:45] ‫את החישוב היעיל הזה.
[01:19:48 - 01:19:52] ‫אז למשל, אחד מהרעיונות המרכזיים ‫שנראה עכשיו,
[01:19:52 - 01:19:58] ‫זה שאם אנחנו מאלצים את היעקביאן ‫להיות אלכסוני,
[01:19:59 - 01:20:01] ‫או אפילו לא אלכסוני, ‫הוא יכול להיות
[01:20:06 - 01:20:10] שולשי, זאת אומרת שכל האיבר ‫שהוא מעל אלכסון, למשל, הוא אפס,
[01:20:11 - 01:20:12] ‫או מתחת לאלכסון, הוא אפס,
[01:20:13 - 01:20:18] ‫אז הדטרמיננטה היא פשוט ‫מכפלת האיברים באלכסון,
[01:20:19 - 01:20:21] ‫כל שאר האיברים מתאפסים,
[01:20:21 - 01:20:23] ‫כי הם מגיעים באחד מהאפסים,
[01:20:24 - 01:20:25] ‫ולכן החישוב הזה הוא יעיל.
[01:20:25 - 01:20:32] ‫גם אנחנו לא צריכים לחשב ‫את כל היעקביאן במקרה הזה, ‫הספיק לנו רק לחשב את האיברים ‫על האלכסון של היעקביאן,
[01:20:33 - 01:20:35] ‫ואז הדטרמיננטה של זה, ‫זה פשוט יהיה המכפלה של המתארמינט.
[01:20:36 - 01:20:40] ‫זה יאפשר לנו לעשות חישובים ‫ב-O של N חישובים.
[01:20:43 - 01:20:43] אוקיי?
[01:20:44 - 01:20:50] אז אפשר עכשיו, בדרכים אחרות אולי, ‫לבנות משהו שהוא יעיל חישובית,
[01:20:50 - 01:20:52] ‫אבל זו אחת מהדרכים הפופולריות.
[01:20:54 - 01:20:56] ‫כן, עונה לדוגמאות.
[01:20:58 - 01:21:08] ‫כל זה אמרתי, יש פה גם בזה, ‫אוקיי, אם היעקביאן שלנו הוא באופן כדי ככה, ‫אבל אם יש לנו החצי המשולשי הזה, ‫הוא אפס,
[01:21:10 - 01:21:17] ‫אז המטרמיננט הזה פשוט יהיה ‫המכפלות של האיברים כאן, ‫שכל אחד מהם זה המגזרת ‫של הממד הראשון של הארטפורט,
[01:21:17 - 01:21:19] ‫לפי הממד הראשון של הארטפורט, ‫לפי השני בארטפורט,
[01:21:20 - 01:21:22] ‫יש לו בדיוק, כן, נשמעות אין כאלה.
[01:21:25 - 01:21:27] ‫אוקיי, אז אחד המודלים,
[01:21:27 - 01:21:30] ‫או אחד המודל הראשון שעשה את זה, ‫נקרא NICE,
[01:21:31 - 01:21:33] ‫לא לינארי-דפיינג קומפורנט אסטימיישן,
[01:21:34 - 01:21:36] ‫בארטפורט ה-14.
[01:21:41 - 01:21:43] בעצם המודל הזה הוא גם בנה,
[01:21:44 - 01:21:47] ‫אחרי השיטה הזאתי, ‫אבל כשאנחנו מתחילים עם משהו פשוט,
[01:21:48 - 01:21:49] מתחילים עם נוסיין,
[01:21:50 - 01:21:51] ‫אז אנחנו מרכיבים הרבה פונקציות,
[01:21:52 - 01:21:57] ‫שכל אחת מהן היא הביטרמיננטה של היעקביאן שלה, ‫היא משולשית.
[01:21:59 - 01:22:00] אוקיי, אפשר לחשוב בצורה יעילה.
[01:22:01 - 01:22:07] הדרך שהם עשו את זה, ‫זה על ידי בעצם הגדרה ‫של שני שכבות כאלה,
[01:22:09 - 01:22:14] ‫אדיטיב קפלינג ליירס ומסקיילינג ליירס, ‫שמאלץ
[01:22:14 - 01:22:16] את היעקביאן להיות משולשית.
[01:22:17 - 01:22:18] אוקיי, אז תכף נראה. זה קורה.
[01:22:18 - 01:22:20] ‫אז איך זה מוגדר?
[01:22:21 - 01:22:21] ‫זה מוגדר ככה.
[01:22:24 - 01:22:31] ‫הניפוי מוגדר ככה, ‫אנחנו קודם מחלקים את הדאטה שלנו ‫לשני חלקים נפרדים,
[01:22:32 - 01:22:35] ‫אז אם הדאטה כאן מוגדר ‫בכלו מימד M,
[01:22:36 - 01:22:40] ‫אז עד D זה החלק הראשון של הדאטה,
[01:22:41 - 01:22:45] ‫מ-D ועונק אחד עד N זה החלק השני. ‫כן, כשגור לתמונה, ‫אז יש לנו את החצי העליון והחצי התחתון.
[01:22:46 - 01:22:49] ‫זה לא חייב להיות רצאים כשגורים.
[01:22:50 - 01:22:55] ‫אבל אני אגיד שכן. ‫אוקיי, אז יש לנו את החלוקה הזאת, ‫ואז המיפוי
[01:22:55 - 01:22:58] מ-Z ל-X או מ-Z לשכבה הבאה,
[01:22:59 - 01:23:00] ‫הולך ככה.
[01:23:01 - 01:23:03] ‫אנחנו, את החלק הראשון של הדאטה,
[01:23:04 - 01:23:06] ‫פשוט מעתיקים כמו שהוא.
[01:23:08 - 01:23:11] ‫אוקיי, אז זה טוב כך מעניין.
[01:23:12 - 01:23:13] זה מה שעושים.
[01:23:14 - 01:23:21] ‫אוקיי, אז יש פה את הדאטה, ‫זה מתוך המאמר עצמו בין X עם ל-Y.
[01:23:22 - 01:23:26] ‫ה-X ממופה לשכבה הבאה, ל-Y, פשוט שווה,
[01:23:27 - 01:23:28] ‫והחלק השני,
[01:23:30 - 01:23:30] ‫בוא נראה ככה,
[01:23:31 - 01:23:36] ‫ה-X זה ה-output של החלק השני, ‫שווה ל-input של החלק השני.
[01:23:37 - 01:23:39] ‫זה גם ממופה כמו שהוא,
[01:23:40 - 01:23:44] ‫אבל מוסיפים לו גם פונקציה מסוימת,
[01:23:45 - 01:23:47] ‫כלומר, נקראת כאן M של החלק הראשון.
[01:23:49 - 01:23:50] ‫אוקיי?
[01:23:50 - 01:23:54] ‫הפונקציית M הזאתי היא רשת ניאורונים ‫בלי אילוצים.
[01:23:54 - 01:23:57] ‫כאילו, יש לה אילוצים שה-input שלה ‫צריך להיות בגודל הזה,
[01:23:58 - 01:24:00] ‫ה-output שלה צריך להיות בגודל הזה.
[01:24:00 - 01:24:02] ‫לא חייב להיות שווים הגדולים האלה,
[01:24:02 - 01:24:08] ‫אבל בדרך כלל תחשבו על זה ‫שזה חצי לחצי, אוקיי? אז חצי זה אותו גודל, ‫ה-input צריך להיות אותו גודל כמו ה-output,
[01:24:09 - 01:24:12] ‫ואין אילוצים אחרים על הרשת.
[01:24:14 - 01:24:18] ‫אוקיי, אז זה ככה מוגדר השכבל הזאת.
[01:24:21 - 01:24:22] ‫האם זה הפיך?
[01:24:23 - 01:24:37] ‫לא רואים את זה כנראה, אבל איך זה הפיך?
[01:24:40 - 01:24:49] ‫אם אני אתן לכם את X, ‫בוא נסתכל על דילמה, ‫אני אתן לכם את Y, ‫איך אתם חושבים בחזרה את X. ‫הפונקציה הזאת היא, היא לא הפיכה.
[01:25:02 - 01:25:04] ‫אם יש לי התאחדת חלק כזה,
[01:25:05 - 01:25:07] ‫האם אני יכול לחזור לחלק כזה?
[01:25:07 - 01:25:11] ‫-כן, זה שווה, נכון? ‫אז מפה אני יודע לחזור לפה,
[01:25:12 - 01:25:13] ‫אז אני יודע לחזור.
[01:25:13 - 01:25:19] ‫אז אתה יכול לחשב את ה-output של הרשת הזאת,
[01:25:21 - 01:25:23] ‫נכון? והדבר הזה, זה שווה לזה,
[01:25:23 - 01:25:24] ‫זה עוד ה-output של זה.
[01:25:25 - 01:25:28] ‫אני מחסיר את ה-output של זה, ‫ואז אני מקבל את זה.
[01:25:28 - 01:25:30] ‫אז זו הדרך שבה אני יכול לחזור אחורה.
[01:25:31 - 01:25:32] ‫אז זה הפיך.
[01:25:33 - 01:25:34] ‫אז יש פשוט כל
[01:25:37 - 01:25:43] ‫אז זו הייתה להם שפה קצת שונה, ‫הם בכלל, אני חושב שהם במאמר הזה,
[01:25:44 - 01:25:48] ‫אבל לפני שהמציאו את המונח ‫נורמלייזינג פלואו, ‫הם קצת הגדירו דברים בצורה קצת שונה,
[01:25:49 - 01:25:53] ‫אבל בפועל זה היה נורמלייזינג פלואו, ‫והם הגדירו את זה שזה...
[01:25:54 - 01:25:56] ‫הם קראו לחלק הזה שעובר כמו שהוא, כי?
[01:25:57 - 01:25:59] ‫לא יודע, אני לא זוכר כבר את המאמר בדיוק למה זה...
[01:26:00 - 01:26:05] ‫הם קראו לחלק הזה כי, ‫לחלק השני שעובר כמו שהוא גם, פליים,
[01:26:05 - 01:26:09] ‫ואז זה בעצם סוג של קוד כזה ‫שצריך לפענח,
[01:26:10 - 01:26:13] ‫אבל יש לך את המפתח כדי לפענח בקודם.
[01:26:15 - 01:26:16] ‫טוב, אז כן,
[01:26:20 - 01:26:24] ‫כבר לא כל כך תופס השמות האלה, ‫בסופה של נורמלייזינג פלואו, ‫לא משתמשים בזה.
[01:26:27 - 01:26:29] ‫אוקיי, אז אנחנו יודעים לחשב את ה-forward,
[01:26:31 - 01:26:34] ‫את ה-forward, אנחנו יודעים לחשב את ה-inverse,
[01:26:34 - 01:26:35] ‫של שני דברים שרצינו.
[01:26:36 - 01:26:40] ‫יש פה איזושהי קומפוננטה ‫שהיא מאוד מסובכת,
[01:26:40 - 01:26:42] ‫רשת כזאתי שיכול להיות ‫כל מה שאנחנו רוצים.
[01:26:43 - 01:26:43] זה טוב.
[01:26:44 - 01:26:45] מה עוד אנחנו צריכים?
[01:26:53 - 01:26:55] מה עוד אנחנו צריכים כדי להשתמש בדבר הזה?
[01:26:56 - 01:26:59] ‫כדי לחשב את ה-LineQ, ‫כדי לחשב את ה-LineQ, ‫מה אנחנו צריכים?
[01:27:01 - 01:27:02] צריכים לדעת לחזור אחורה, וגם
[01:27:03 - 01:27:08] ‫לתקן, לחשב את ה-LineQ, ‫הדטרמיננטה של היעקוביאן.
[01:27:08 - 01:27:11] ‫אז בואו נראה אם אנחנו יודעים ‫לחשב את היעקוביאן.
[01:27:13 - 01:27:16] ‫אם צריך להתחיל לגזור, ‫לפי הרשת הזאת זה יכול להיות מסובך,
[01:27:17 - 01:27:19] ‫אבל בואו נראה. ‫אז האם...
[01:27:20 - 01:27:20] ‫פרקטיבלד?
[01:27:21 - 01:27:21] ‫עוד תשובה?
[01:27:23 - 01:27:24] ‫כן.
[01:27:24 - 01:27:25] ‫בואו נראה למה.
[01:27:26 - 01:27:27] ‫אז היעקוביאן נראה ככה.
[01:27:30 - 01:27:31] ‫מה יש לנו כאן?
[01:27:32 - 01:27:34] ‫האיבר ה... החצי הראשון,
[01:27:35 - 01:27:36] ‫מה הנגזרת שלו?
[01:27:37 - 01:27:42] ‫כבר זה מה הנגזרת של זה לפי זה,
[01:27:44 - 01:27:50] ‫זה פשוט ה-identity, פשוט מעתיקים את זה. ‫אז הנגזרת של זה לפי זה זה I.
[01:27:51 - 01:27:52] ‫אוקיי? אז זה ה...
[01:27:54 - 01:27:56] ‫הבלוק הראשון שיש כאן, זה I.
[01:27:58 - 01:28:00] אוקיי? מה... אולי אני אחזור לכאן.
[01:28:01 - 01:28:01] ‫עושה את זה בגלוח.
[01:28:02 - 01:28:02] ‫טוב.
[01:28:07 - 01:28:08] ‫אז שאני נותן את החלק הראשון.
[01:28:10 - 01:28:11] ‫אם צריך עד ה-D,
[01:28:12 - 01:28:13] ‫אם צריך עד ה-D,
[01:28:14 - 01:28:15] ‫נותן את החלק נותן סדפן Z.
[01:28:18 - 01:28:19] ‫וואי, וזה...
[01:28:24 - 01:28:28] זה לפי הציור, כי זה הפוך. בעצם הוא ציור X ו-Z ו-Y זה E.
[01:28:31 - 01:28:47] ‫אוקיי, אז הנגזרת של זה לפי זה, ‫כלומר שזה פשוט I, נכון?
[01:28:48 - 01:28:52] ‫מה הנגזרת של זה לפי זה?
[01:28:53 - 01:28:53] ‫אפס.
[01:28:53 - 01:28:57] ‫אפס, נכון? זה לא תלוי בזה בכלל. ‫אז כל הבלוק הזה הוא אפס.
[01:28:58 - 01:28:59] ‫עכשיו,
[01:29:00 - 01:29:04] ‫עוד בלוק שהוקל זה זה לפי זה,
[01:29:05 - 01:29:10] ‫נכון? הפונקציה ה-Y הזה הוא פשוט סכום של X
[01:29:12 - 01:29:14] ‫השני ואיזושהי פונקציה של X הראשון.
[01:29:15 - 01:29:16] ‫אז אם אני גוזר לפי ה-X השני,
[01:29:16 - 01:29:19] ‫יש לי את הסכום והדבר השני והסכום נופל,
[01:29:20 - 01:29:22] ‫הדבר הזה נופל, והסכום נופל.
[01:29:22 - 01:29:25] ‫הדבר הזה נופל, נשאר לי רק נגזרת של זה לפי Z, ‫זה שוב פעם
[01:29:26 - 01:29:27] עקה עקה ושוב פעם I.
[01:29:30 - 01:29:33] ‫וכאן, מה יש לי?
[01:29:34 - 01:29:36] ‫כאן יש לי מגזרת דרך הפונקציה, אוקיי?
[01:29:37 - 01:29:38] ‫אבל זה לא כל כך מעניין אותי.
[01:29:43 - 01:29:45] ‫למה זה לא מעניין אותי? ‫כי ברגע שיש לי מטריצה שהיא משולשית,
[01:29:49 - 01:29:51] ‫אז אני לא צריך לחשב את הדבר הזה ‫כדי לחשב את הבטרמיננטה.
[01:29:52 - 01:29:53] ‫הספיק לי רק המחקלות באלכסון,
[01:29:54 - 01:29:55] ‫במקרה הזה הן פשוט שוות אחד.
[01:29:56 - 01:30:02] ‫זה נקרא גם, ‫הדבר הזה נקרא ‫Volume Preserving Transformation.
[01:30:03 - 01:30:09] ‫כי הסקאלה הזאת שאנחנו מכפילים בה ‫היא תמיד שווה אחד. ‫בגלל זה אנחנו לא צריכים לעשות ‫את התיקון הזה.
[01:30:11 - 01:30:14] ‫אז כשאנחנו רוצים לחשב את ה-likeיות של X,
[01:30:14 - 01:30:18] ‫אנחנו חוזרים אחורה לזל, ‫תוקים מה ה-likeיות של X, בסדר, זה ה-likeיות שלי.
[01:30:25 - 01:30:31] ‫-אוקיי, זה ברור? ‫אז זה פשוט הטריק.
[01:30:32 - 01:30:41] ‫אז זה השכבות הראשונות ‫שנקראות Additive Coupling Layers, ‫כי יש את הקומפונטות, ‫יש את העניין הזה של שני חלקים,
[01:30:42 - 01:30:43] ‫שמחברים אותם אחד לשני.
[01:30:45 - 01:30:49] ‫ויש עוד משהו שהוא, כדי שזה לא יהיה, ‫יכול להיות שכן רוצים שיהיה שינוי
[01:30:50 - 01:30:53] ‫בפתיחה או קיבוץ של ההתפלגות,
[01:30:54 - 01:30:55] ‫אז כן רוצים לעשות משהו ‫שעושה סקיילינג,
[01:30:56 - 01:31:00] ‫כלומר זה ריסקיילינג ליירס, ‫והם פשוט מכפילים את ה-Z,
[01:31:01 - 01:31:03] ‫במשהו פשוט ליניארי, ‫הם מכפילים באיזושהי סקאלה,
[01:31:04 - 01:31:05] ‫והנגזרת של זה היא גם פשוטה,
[01:31:06 - 01:31:13] ‫הוא פשוט צריך לחלק במה שראינו ‫בדיוק קודם כשתירמנו ‫את ההתפלגות האוניפורמית
[01:31:14 - 01:31:16] ‫בין 0 ל-0 ל-0 ל-8.
[01:31:18 - 01:31:19] ‫-זה גם קל לחשב.
[01:31:19 - 01:31:20] ‫ואז אפשר לשלב
[01:31:24 - 01:31:27] ‫שכבות כאלה ושכבות כאלה ‫שעושים את ה-forward הזה,
[01:31:28 - 01:31:32] ‫והכול אנחנו יודעים לחשב את ה-inverse, ‫והכול אנחנו יודעים לחשב את הדטרמיננטה ‫בצורה יעילה,
[01:31:32 - 01:31:39] ‫ויש לנו כבר משהו שיכול לבנות ‫משהו די מורכב, ‫כי יש באמצע את הרשת ניורונים הזאת ‫שיכולה לעשות כל מיני דברים מורכבים.
[01:31:41 - 01:31:43] ‫עוד משהו שלא נכנסתי אליו פה,
[01:31:43 - 01:31:46] ‫אבל מה עושים, ‫השאלה איך עושים את החלוקה הזאתי
[01:31:47 - 01:31:48] ‫מול שני החצאים.
[01:31:48 - 01:31:52] ‫אפשר לעשות כל מיני טריקים ‫שבכל השכבה עושים חלוקה אחרת.
[01:31:53 - 01:31:59] ‫פעם עושים חצי-חצי, ‫פעם עושים שני רבעים, ‫מדלגים ביניהם.
[01:32:00 - 01:32:02] ‫כן, יש כל מיני טריקים ‫של לעשות כזה,
[01:32:02 - 01:32:04] ‫לחלק את התמונה ל...
[01:32:06 - 01:32:11] ‫הפכתי לשים פה, רציתי, ‫יש במאמר שלהם, ‫כל מיני checker words כאלה, ‫של שופר לבד,
[01:32:12 - 01:32:14] ‫מה נכנס לחלק הראשון, ‫מה נכנס לחלק השני,
[01:32:15 - 01:32:17] ‫ובכל שכבה עושים את זה בצורה אחרת,
[01:32:17 - 01:32:22] ‫ובסקאלה אחרת. ‫אז זאת אומרת, סקאלה אחת ‫ששני הדברים האלה נכנסים
[01:32:23 - 01:32:25] ‫בחצי הראשון, ‫ושני הדברים האלה בחצי השני.
[01:32:26 - 01:32:27] ‫אחר כך מחלקים את זה ליותר,
[01:32:28 - 01:32:29] ‫ואז יש את האזור ה...
[01:32:30 - 01:32:32] ‫האחרונה, אם לא עכשיו אף מאת כזה,
[01:32:32 - 01:32:34] ‫ארבע על ארבע, נגיד שמונה על שמונה.
[01:32:35 - 01:32:39] ‫ככה מחלקים בכל שכבה, ‫בעצם התמונה עושה, ‫מסתכלת על רזולוציות אחרות של ה...
[01:32:40 - 01:32:42] של התמונה,
[01:32:43 - 01:32:46] ‫והרשת נורא אני יכולה לעשות ‫כל מיני דברים מעניינים ‫ברזולוציות אחרות.
[01:32:48 - 01:32:49] ‫אבל כל פעם במשרד זה ‫רק על חצי מהתמונה.
[01:32:50 - 01:32:57] ‫יש גם תורים שפעם עושים את זה ‫החצי הראשון, פעם עושים את זה ‫החצי השני, ‫כדי שהרשת תוכל לעבוד ‫על את כל החלקים של התמונה.
[01:32:57 - 01:33:00] ‫-כן, זה גם סוג של אמנות כזאת, ‫איך הם בונים את השכבות האלה.
[01:33:02 - 01:33:10] ‫אוקיי, אז זה מ-2014, ‫אז הם גם, הדאטה, ב-2014, ‫הסוג הדאטה שעבדו עליו, ‫זה היה אמניסט וכל מיני תמונות קטנות כאלה.
[01:33:10 - 01:33:24] ‫אז הם כבר הראו תוצאות נכבדות באמניס, ‫זה היה איזה דאטה סט של פנים, ‫עוד לפני שעבדו פה עם סלב A, ‫או לפעמים אתם מכירים, ‫דיברנו עליו סלב A, ‫זה דאטה סט של פנים קצת יותר מעניין.
[01:33:27 - 01:33:29] ‫זה משהו ראשוני כזה של איזשהם פנים.
[01:33:30 - 01:33:33] ‫אז זה היה סוג התוצאות שהם קיבלו.
[01:33:35 - 01:33:38] ‫זה כשהם ניסו לעבוד ‫עם דאטה קצת יותר מורכב, ‫אבל זה סיפאר 10,
[01:33:38 - 01:33:43] ‫אבל לא נראה כל כך טוב, ‫לא נצליח אולי להגיע תוצאות כאלה רגבות.
[01:33:44 - 01:33:50] ‫זה דאטה סט, גם עמוד של ה-SVHN, ‫זה גם דאטה סט של ספרות,
[01:33:51 - 01:33:55] ‫אבל ספרות שבמקום להשיג, ‫באמניסט, ‫אתם יודעים איפה השיגו את הספרות?
[01:33:57 - 01:33:57] ‫זה דאטה של אמניסט?
[01:34:00 - 01:34:05] ‫כן, אבל מאיפה זה? ‫זה היה ממערכת של הדואר, ‫אז אני חושב שהתרתמו על צ'קים או משהו כזה,
[01:34:06 - 01:34:07] ‫אז הם אספו את זה משם.
[01:34:08 - 01:34:12] ‫ופה זה דאטה שאספו מ-Street View של גוגל,
[01:34:12 - 01:34:14] ‫הם מספרים של בתים.
[01:34:17 - 01:34:19] ‫סתם כל מיני דרכים לאסוף בצורה יעילה,
[01:34:19 - 01:34:26] ‫ספרות באיזשהו גבעון, ‫אם אתה רק כאן יצא לתפוס גבעון קצת יותר רחוק. ‫אתם לא רואים את הדאטה סט, ‫כי זה דגימות שהיה במודל,
[01:34:27 - 01:34:29] ‫שאומן על הדאטה סט הזה, ‫והדגימות כאן לא כל כך טובות.
[01:34:31 - 01:34:35] ‫בוא נסתכל על ה-SVHN, שזה סטרייט-ויו פאוס נאמברס.
[01:34:38 - 01:34:49] ‫אוקיי, אז זה היה הניסיון הראשון, ‫שכבר היה נראה די מבטיח, ‫אבל לא יחסית לשנה הזאתי, ‫ואז אני חושב ששנתיים אחרי זה,
[01:34:49 - 01:34:55] ‫יצאה בעצם הרחבה על העניין הזה, ‫שנקרא RealMVP,
[01:34:56 - 01:34:58] ‫זה Non-Volume Preserving,
[01:34:59 - 01:35:02] ‫ה-Real זה כאילו מספר רצית, סטרייט מספר רצית,
[01:35:02 - 01:35:11] ‫כך עוד להעביר ל-NVP, ‫ופה ה-Fורד הוא קצת שונה, אוקיי? ‫אז מה יש פה? ‫אז שוב פעם, יש את החלוקה הזאתי לשני חלקים,
[01:35:12 - 01:35:13] ‫והחלק הראשון
[01:35:14 - 01:35:16] פשוט עובר לחלק השני כמו שהוא,
[01:35:17 - 01:35:19] ‫אבל הדרך שעושים את ה...
[01:35:20 - 01:35:24] ‫הדרך שעושים מניפולציה ‫לחצי השני של הדאטה הראשונה,
[01:35:26 - 01:35:31] ‫גם לוקחים את החלק השני, ‫אבל במקום פשוט לעוש... ‫אם לא היה את הדבר הזה, ‫זה פשוט היה אותו דבר כמו קודם,
[01:35:32 - 01:35:34] ‫לבד הזה מוסיפים אותו לזה.
[01:35:36 - 01:35:39] ‫אז פה ההבדל הוא שיש עוד איזושהי מכפלה
[01:35:42 - 01:35:44] ‫במשהו שהוא גם
[01:35:45 - 01:35:47] ‫תוצאה של החלק הראשון של הדאטה.
[01:35:52 - 01:35:54] ‫אבל אתם מבינים מה קורה כאן?
[01:35:55 - 01:35:58] ‫החלק החצי הראשון, ‫גם מחלקים את הדאטה על שני חלקים,
[01:35:58 - 01:36:01] ‫שצריך שוב לחלק את זה ‫בכל מיני דרכים טוב כמוהו.
[01:36:02 - 01:36:04] ‫החלק הראשון עובר כמו שהוא,
[01:36:04 - 01:36:05] ‫החלק השני,
[01:36:06 - 01:36:08] ‫קודם כול, מוסיפים לו
[01:36:12 - 01:36:14] ‫או קצמפטואלית, קודם כול מסתכלים על
[01:36:15 - 01:36:17] ‫איזושהי טרנספורמציה ‫של החלק הראשון,
[01:36:18 - 01:36:22] ‫ולוקחים את החלק השני ומכפילים אותו ‫בטרנספורמציה של החלק הראשון.
[01:36:30 - 01:36:31] איך הופכים את הדבר הזה?
[01:36:31 - 01:36:33] ‫אז גם, אם זה די דומה למה שהיה קודם,
[01:36:34 - 01:36:34] ‫אפשר,
[01:36:35 - 01:36:37] ‫אנחנו יודעים את החלק הראשון, נכון?
[01:36:37 - 01:36:40] ‫אז אנחנו יודעים לחשב מה התווסף לנו,
[01:36:40 - 01:36:41] ‫ולהוריד אותו,
[01:36:42 - 01:36:45] ‫ואנחנו יודעים לחשב במה הכפלנו, ‫ולחלק בו.
[01:36:47 - 01:36:49] כן, זה מה שקורה כאן. אנחנו מורידים,
[01:36:50 - 01:36:52] ‫החלק הראשון הוא מתרגם למה שהיה פשוט,
[01:36:52 - 01:36:57] ‫והחלק השני, אנחנו מורידים ממנו את ה...
[01:36:58 - 01:37:00] ‫מה שה-neural network נתן על החלק הראשון,
[01:37:01 - 01:37:04] ‫ומחלקים את מה שה-neural network ‫במקום לעשות X
[01:37:05 - 01:37:08] של זה, זה X של מינוס זה, כן? ‫אנחנו מחלקים ב-T.
[01:37:10 - 01:37:10] נכון?
[01:37:13 - 01:37:16] שוב, אנחנו צריכים לדעת לחשב את ה-Forout ‫ואת ה-Inverse, אבל גם אנחנו צריכים לחשב
[01:37:17 - 01:37:19] ‫את הדטרמיננטה של היעקוביאן.
[01:37:22 - 01:37:23] טוב, אנחנו מחשב פה את היעקוביאן של היעקוביאן.
[01:37:24 - 01:37:25] ‫אז הכול אותו דבר,
[01:37:25 - 01:37:28] חוץ מזה שהחלק השני, ‫במקום שיהיה כאן I,
[01:37:29 - 01:37:31] ‫אנחנו הכפלנו באיזשהו מספר.
[01:37:32 - 01:37:34] ‫כל איבר הוכפל באיזשהו מספר,
[01:37:34 - 01:37:37] ‫המכפלה כאן היא מכפלה נקודתית,
[01:37:38 - 01:37:38] ‫איבר-איבר,
[01:37:39 - 01:37:39] אוקיי?
[01:37:40 - 01:37:45] בעצם כל מימד הוכפל באיזשהו משהו ‫שהוא פונקציה של החצי הראשון,
[01:37:46 - 01:37:46] ‫אבל אני יודע לחשב אותו,
[01:37:47 - 01:37:49] ‫פשוט יש כאן משהו שהאלכסון שלו,
[01:37:49 - 01:37:50] ‫במקום שיהיה אחדות,
[01:37:51 - 01:37:54] ‫יש את ה-output של הרשת הזאת, ‫את האקספוננט של ה-output של הרשת.
[01:37:56 - 01:38:01] אוקיי? זה שוב, זה לא בעיה, ‫כי אני יודע לחשב את הדבר הזה, זה בין מימד n,
[01:38:01 - 01:38:02] ‫סדר גודל או חצי n,
[01:38:04 - 01:38:06] ‫והדטרמינציה תהיה פשוט ‫המכפלה של הדברים האלה.
[01:38:07 - 01:38:11] ‫אם אני מכניס את זה לתוך האקס, ‫זה יהיה פשוט סכום של ה-outputים ‫של הרשת הזאת,
[01:38:12 - 01:38:14] ‫והאקספוננט של זה, ‫זה יהיה הדטרמינציה.
[01:38:16 - 01:38:18] אוקיי? זה גם יעיל. ‫פשוט לא אחדות, זה מספרים אחרים.
[01:38:19 - 01:38:21] ‫זה אין מספרים, ואני יכול לחשב אותם.
[01:38:25 - 01:38:27] זה הטריק הגדול שלהם.
[01:38:27 - 01:38:30] ‫יכול להיות שהם, אני עכשיו לא זוכר,
[01:38:30 - 01:38:46] ‫אני חושב שהם הוסיפו גם את העניין הזה ‫של כל מיני חלוקות מעניינות יותר של הדאטה וזה, ‫כבר איזה שנתיים אחרי זה, ‫שידעו לעבוד עם הרשתות יותר מתוחכמות, ‫אני חושב שהרשתות עצמן ‫היו יותר מעניינות ‫והיהן דאטה סטים יותר מעניינים, ‫אז הם הצליחו להגיע ‫לתוצאות כבר הרבה יותר טובות.
[01:38:48 - 01:38:59] אולי אתם רואים משם, ‫אבל את זה הדאטה הראשונה זו סיפה, ‫אז פה רואים קצת דברים ‫שנראים כמו אובייקטים ‫או סוג הדברים שרואים שם.
[01:39:00 - 01:39:02] ‫אני חושב שמשמאל זה הדאטה ‫וממימין זה דגימות.
[01:39:04 - 01:39:04] ‫אתה רואה?
[01:39:07 - 01:39:09] ‫זה הדאטה של סיפה, ‫יש פה כל מיני דברים.
[01:39:11 - 01:39:12] ‫זה תמונות מאוד קטנות,
[01:39:13 - 01:39:14] ‫אבל מאוד מוגוונות,
[01:39:14 - 01:39:21] ‫וכאן יש, לא רואים, ‫אי אפשר לזהות איזה אובייקציה, נגיד, ‫אבל הסטטיסטיקות כבר נראות די טוב,
[01:39:22 - 01:39:25] ‫זה רק המבנה שיש בתמונה טוב.
[01:39:26 - 01:39:28] ‫הידע הדאטה של פנים זה הרבה יותר קט, ‫כי המבנה הוא תמיד,
[01:39:29 - 01:39:31] ‫רק צריך לשנות כל מיני
[01:39:32 - 01:39:35] ‫כל מיני תכונות של הפנים,
[01:39:36 - 01:39:37] ‫זה כבר נראה די טוב.
[01:39:38 - 01:39:40] ‫פה יש גם דאטה סט שהוא קצת יותר מוגבל מפה,
[01:39:42 - 01:39:44] ‫אני חושב שזה רק דאטה סט של,
[01:39:46 - 01:39:47] ‫אני חושב שזה בדרומים,
[01:39:49 - 01:39:51] ‫כל מיני חדרי סדרים של חדרי סטרים.
[01:39:57 - 01:39:58] אוקיי, ברגע שיש לנו מודל כזה, שאומרים, טוב,
[01:39:58 - 01:40:03] ‫אז אפשר לעשות דברים כמו שעכשיו ‫יש לכם בתרגיל שתקבלו ב-DAE,
[01:40:04 - 01:40:07] ‫שאתם בעצם לוקחים נקודה, לוקחים תמונה,
[01:40:08 - 01:40:11] ‫מרפאים אותה בחזרה למרחב
[01:40:12 - 01:40:13] ‫למשתמש רבוי Z,
[01:40:14 - 01:40:18] ‫ושם עושים כל מיני משחקים. ‫אז המשחקים שיש פה זה לעבור,
[01:40:19 - 01:40:20] ‫לקחו את התמונה הזאת,
[01:40:21 - 01:40:23] ‫מצאו את ה-Z שלה,
[01:40:24 - 01:40:25] ‫במקרה הזה זה פשוט,
[01:40:26 - 01:40:28] ‫יש Z אחד שהוא אחראי על התמונה הזאת פה,
[01:40:28 - 01:40:29] ‫ויש מיפוי חזרה מהתמונה ל-Z,
[01:40:30 - 01:40:32] ‫לקחו את התמונה הזאת, ‫מצאו את ה-Z שלה,
[01:40:33 - 01:40:36] ‫ואז עשו אינטרפולציה ליניארית ‫בין כל ה-Z'ים.
[01:40:38 - 01:40:41] ‫כן, יש לנו וקטור אחד כאן, ‫יש לנו וקטור אחד כאן, אנחנו יכולים למצוא את המסלול,
[01:40:42 - 01:40:45] ‫הקו שמחבר בין שני הוקטורים האלה, ‫לחלק אותו ל...
[01:40:46 - 01:40:49] יש פה עשרה קטעים שווים,
[01:40:51 - 01:40:57] ‫וכל אחד הוא Z אחר, ועכשיו, עבור כל Z שיש כזה, ‫לעשות את המיפוי קדימה, ‫ולייצר תמונה עצתה. בבקשה.
[01:40:59 - 01:41:04] ‫כן, אז רואים כאן סוג של מעבר חלק ‫בין תמונה כזאתי, ‫מפנים כאלה ופנים כאלה.
[01:41:05 - 01:41:07] ‫אם היינו עושים את זה במרחב התמונה, ‫את הדבר הזה,
[01:41:09 - 01:41:13] ‫לא הולכים חזרה לזה, ‫אבל הם נשארים ב-X ועושים את הדבר הזה, ‫אז איך זה היה נראה?
[01:41:13 - 01:41:18] ‫הפיקסלים של התמונה הזאת פשוט ‫היו לאט לאט נעלמים, ‫והפיקסלים של התמונה השנייה היו מופיעים.
[01:41:20 - 01:41:23] ‫זה היה נראה כמו, כאילו, ‫בשלב מסוים היה נראה שיש את שני הפנים,
[01:41:24 - 01:41:29] ‫ואז אחד לאט לאט נעלם דועך ‫והשני לאט לאט נחלץ.
[01:41:30 - 01:41:33] ‫פה זה נראה שתמיד יש פה ‫איזה שהן פנים, ‫אבל הפנים עצמם משתנים.
[01:41:34 - 01:41:38] ‫זה בעצם אומר שזה, ה-latents, ‫הוא בעצם תופס בצורה ‫קצת יותר מעניינת
[01:41:39 - 01:41:43] ‫את הדאטה, את הדרך שהדאטה מוצר. ‫הוא יודע שבעצם
[01:41:44 - 01:41:44] כל
[01:41:47 - 01:41:49] נקודה במרחב אמור להיות פנים,
[01:41:50 - 01:41:52] ‫אין נקודה שבה יש שני פנים ‫אחד על השני.
[01:41:52 - 01:41:53] ‫כל נקודה היא פנים,
[01:41:54 - 01:42:03] ‫אבל אם יש בנקודה אחת עם פנים ‫עם עיניים כחולות ‫ואחד עם עיניים חומות, ‫אז אפשר לעשות איזשהו מעבר חלק יחסית ביניהם, ‫שרק הצבע העיניים משתנים.
[01:42:04 - 01:42:08] ‫האמת שדוגמה של צבע זה יהיה ‫אותו דבר גם בתמונה,
[01:42:09 - 01:42:14] ‫אבל נגיד שאם יש תמונה של פנים ‫שמסתכלים לכאן, ‫או פנים שמסתכלים לכאן,
[01:42:14 - 01:42:19] ‫אז כשעוברים בצורה חלקה על הזדים, ‫לאט לאט הזווית של הסתכלות תשתנה,
[01:42:20 - 01:42:21] ‫ולא שאחד ייעלם ואחד יופיע.
[01:42:22 - 01:42:30] ‫זה סוג הדברים שמחפשים ‫כשעושים את הפיסומים האלה, ‫כדי לראות אם בעצם הלייטן תופס ‫איזה משהו קצת יותר מעניין,
[01:42:31 - 01:42:33] ‫או משהו ש... איזשהו פקטור,
[01:42:33 - 01:42:35] ‫פקטורים שונים ‫שמייצרים לנו את הדף.
[01:42:36 - 01:42:43] ‫אז זה מבחינות בכל מיני מודלים, ‫עושים את הדבר הזה ‫יש פנים עם משקפיים, ‫בלי משקפיים, ‫ועם עם חיוך ובי חיוך.
[01:42:44 - 01:42:46] ‫אני אשאר אתי לכם דוגמה עם חיוך. ‫לא, שלאט לאט החיוך,
[01:42:47 - 01:42:52] ‫אפשר לשלוט בו בעצם. ‫מוכיחים תמונה עם חיוך, תמונה בלי חיוך, ‫וזה תורם להיות חיוך.
[01:42:53 - 01:42:59] ‫אז זו בעצם דרך לפלץ מהתמונה ‫כל מיני תכונות שבאמת
[01:42:59 - 01:43:02] ‫הם גורמים לבריאביליות ‫שיש לנו עבודה כזאת.
[01:43:05 - 01:43:14] ‫אז פה זה דוגמה לדבר הזה ‫בפנים ובאובייקטים אחרים. ‫זה קצת קשה, ‫כי העבודה הזו לא כזה טוב ‫באובייקטים האחרים.
[01:43:17 - 01:43:21] ‫אז בתרגיל, דרך אגב, של ה-VA, ‫גם אתם ביקשתם לכם לעשות, ‫זו השאלה השנייה,
[01:43:22 - 01:43:25] ‫לעשות כזה דבר. ‫שם אין לכם מיפוי יחיד
[01:43:26 - 01:43:27] מה-X ל-Z,
[01:43:28 - 01:43:31] ‫אבל יש לכם מיפוי מ-X, ‫יש לכם את הרשת
[01:43:32 - 01:43:34] ‫שמחשבת לכם את הקוסטיריור ה-Z,
[01:43:34 - 01:43:37] ‫אתם יכולים לדגום ממנה Z1.
[01:43:41 - 01:43:42] ‫אוקיי,
[01:43:43 - 01:43:43] שאלות על זה?
[01:43:47 - 01:43:47] טוב.
[01:43:49 - 01:43:50] ‫אז זה, זה,
[01:43:51 - 01:43:54] יש עוד כל מיני פיתוחים של מודלים של Flows,
[01:43:57 - 01:44:00] ‫ספציפית אני רק רוצה לדבר על,
[01:44:02 - 01:44:10] ‫אפשר לחשוב על זה על עוד סוג של פיתוח, ‫אבל גם על סוג של חיבור ‫בין המודלים של Flows ‫למודלים שראינו קודם, של אוטו-רגרסיב.
[01:44:12 - 01:44:13] ‫אתם זוכרים מה זה?
[01:44:14 - 01:44:16] ‫אוטו-רגרסיב מודל, זה הדבר הזה.
[01:44:17 - 01:44:24] ‫שבעצם כל פיקסל בתמונה תלוי ‫בפיקסלים הקודמים שאני, אוקיי?
[01:44:24 - 01:44:25] ‫אבל לעזוב.
[01:44:26 - 01:44:28] ‫ואנחנו דיברנו, וגם בתרגיל עשיתם,
[01:44:28 - 01:44:31] ‫מימשתם את זה ככה שההתפלגויות האלה ‫הן התפלגויות בדידות.
[01:44:33 - 01:44:41] ‫אבל אפשר לחשוב על זה גם בצורה ‫של התפלגות רציפה, ‫שכל אחד מהמודלים האלה ‫הוא גאוסיין על הפיקסל הבא.
[01:44:42 - 01:44:43] ‫ואם אתם הפיקסלים הקודמים,
[01:44:44 - 01:44:46] ‫יש לי נגיד איזה רשת,
[01:44:46 - 01:44:50] ‫שמחשבת לי את התוחלת של הפיקסל הבא,
[01:44:50 - 01:44:51] ‫לפיקסל ה-i,
[01:44:51 - 01:44:55] ‫ואת הווריאנס של הפיקסל ה-i, ‫בדרך כלל במקום,
[01:44:55 - 01:44:57] ‫זה גם עכשיו בתרגיל, ‫במקום שהווריאנס,
[01:44:58 - 01:45:00] במקום שהרשת ישר להוציא את הווריאנס,
[01:45:01 - 01:45:05] ‫היא מוציאה את הלוג של הווריאנס, ‫את הלוג של ה-STD.
[01:45:07 - 01:45:14] ‫הווריאנס חייב להיות מספר חיובי, כן? והלוג של הווריאנס, ‫הלוג של ה-STD יכולת להיות כל דבר הזה, ‫בזכותו מאלץ ...
[01:45:14 - 01:45:18] זה בדרך כלל לאלץ את הארטפוט של הרשת להיות
[01:45:19 - 01:45:20] להיות וריאנס בלידי
[01:45:22 - 01:45:24] אוקיי אז אפשר לחשוב על הדאטה,
[01:45:24 - 01:45:25] כל פיקסל
[01:45:25 - 01:45:26] הוא גאוסיאני כזה
[01:45:30 - 01:45:35] ואם יש לנו מודל כזה אז בעצם איך אנחנו מייצרים דגימה
[01:45:36 - 01:45:38] שוב בדיוק כמו שעשיתם בתרגיל, מתאים גאוסיאניים
[01:45:39 - 01:45:41] אז המודל הראשון, הפיקסל הראשון
[01:45:43 - 01:45:44] הוא לא תלוי בכלום, נכון?
[01:45:44 - 01:45:46] הוא פשוט יהיה איזשהו גאוסיאן, והדוגמה הזאת הוא...
[01:45:47 - 01:45:49] סליחה, זה תלוי רגע מה זה.
[01:45:51 - 01:45:53] הפיקסל הראשון הוא יהיה פשוט איזשהו גאוסיאן,
[01:45:54 - 01:45:57] אוקיי? שהוא לא תלוי באיזשהו אינפוט אבל תלוי בפרמטרים שלמדנו,
[01:45:58 - 01:46:00] במקרה הזה זה יהיה פשוט Alpha 1 ו-Mu 1.
[01:46:02 - 01:46:08] מu 1 זה בעצם התוחלת של הפיקסל הראשון ו-Alpha 1 זה הלוג של ה-Standard Deviation
[01:46:09 - 01:46:11] ואיך אני דוגם גאוסיאן מהדבר הזה.
[01:46:11 - 01:46:12] אני פשוט דוגם
[01:46:13 - 01:46:20] גאוסיאן 0.1 ומכפיל ב-Standard Deviation שיצא לי, נכון? ומוסיף את התוחלת שיצא לי.
[01:46:22 - 01:46:26] עכשיו בפיקסל השני אני עושה אותו דבר רק שעכשיו אני מחשב
[01:46:26 - 01:46:29] את מu 2 ו-Alpha 2 בהינתן ה-X1 שיצא לי,
[01:46:30 - 01:46:34] נכון? יש לי רשת שמסתכלת על הפיקסל שכבר יצרתי,
[01:46:35 - 01:46:38] מחשבת את התוחלת של הפיקסל השני ואת ה-Varience של,
[01:46:38 - 01:46:39] או את ה-STD של הפיקסל השני,
[01:46:40 - 01:46:45] ואז אני לוקח שוב משתנה מקרי גאוסיאני אחר, התפלגות 0.1,
[01:46:46 - 01:46:56] מכפיל אותו ב-STD שיצא לי ומוסיף את המu שיצא לי, נכון? זה, אתם זוכרים שזה אתם לא עשיתם את זה עם גאוסיאנים אז אולי זה נראה לכם קצת מוזר אבל עשיתם את זה עם,
[01:46:56 - 01:47:00] זה פשוט כל פעם דגמתם מהתפלגות קטגורית
[01:47:00 - 01:47:04] זה דרך כל פעם לדגום מהתפלגות גאוסיאנית זאת הדרך, נכון? אנחנו
[01:47:05 - 01:47:06] דוגמים
[01:47:07 - 01:47:13] משתנה מהתפלגות גאוסיאנית 0.1 ומכפילים אותו ב-Standard Deviation ומוסיפים לו את התוכי הקצי.
[01:47:14 - 01:47:24] אוקיי, אפשר עכשיו בתהליך ככה, שבהתחלה יצרתי את כל ה-Z'ים האלה, את כל הגאוסיאנים ב-0.1, הסטנדרטיים,
[01:47:26 - 01:47:28] יצרתי n כאלה, אחד לכל פיקסל,
[01:47:28 - 01:47:31] ואז כל פעם שאני מגיע לפיקסל הנכון אני מסתכל על ה-Z'
[01:47:32 - 01:47:34] הנכון ומכפיל אותו במה שחישרתי,
[01:47:35 - 01:47:36] אוקיי?
[01:47:36 - 01:47:37] עכשיו, הדבר הזה,
[01:47:38 - 01:47:43] תסתכלו, זה די דומה למה שהיה ב-ReMVP, נכון? אפשר לחשוב על זה, אני מוסיף איזה משהו
[01:47:44 - 01:47:47] שתלוי בחלק מהדאטה, החלק הקודם של הדאטה,
[01:47:48 - 01:47:50] ואני מכפיל במשהו שגם תלוי בחלק מהדאטה,
[01:47:50 - 01:47:53] וזה החלק החדש של הדאטה שלי.
[01:47:53 - 01:47:55] עכשיו כל פעם החלק החדש של הדאטה הוא פיקסל אחד.
[01:47:56 - 01:47:58] הדבר הזה הוא באמת טרנספורמציה שהיא הפיכה.
[01:47:59 - 01:48:01] אם אנחנו עושים את התהליך הזה,
[01:48:02 - 01:48:03] אחד אחד,
[01:48:03 - 01:48:05] אפשר לחשוב על זה בתור תהליך שלוקח לי זה,
[01:48:07 - 01:48:08] ולאט לאט הופך אותו,
[01:48:09 - 01:48:12] זאת אומרת, הוא לוקח התפלגות על Z, ולאט לאט הופך אותה להתפלגות על X,
[01:48:15 - 01:48:17] והתהליך הזה הוא גם הפיך.
[01:48:17 - 01:48:20] אני יכול להפוך אותו חזרה מ-X ל-Z.
[01:48:21 - 01:48:23] בדיוק כמו שהפכנו קודם את...
[01:48:24 - 01:48:25] בואו נכון איך אני הופך לדבר הזה,
[01:48:25 - 01:48:29] יש לי את X1 ואני רוצה לחשב את Z1,
[01:48:30 - 01:48:31] אבל אני מוריד את מי הוא 1,
[01:48:32 - 01:48:37] אם זה הפוך, אני רוצה להגיד את X2 לחשב את...
[01:48:38 - 01:48:39] יש לי X2, אני רוצה לחשב את X1,
[01:48:40 - 01:48:45] אז אני מוריד את U2, עם U2 אני יודע לחשב אותו, כי יש לי כבר את X1,
[01:48:46 - 01:48:55] ואני מחלק באקספוננט של Alpha 2, גם שגם אותו אני יודע לחשב כי יש לי X1. זה בדיוק כמו שראינו קודם, כל תהליך כזה, כל שלב כאן הוא הפיך.
[01:48:56 - 01:49:00] בקיצור, אפשר לחשוב על הגישה הזאת של מודל אוטו-רגרסיב בתור Flow,
[01:49:01 - 01:49:07] שמתרגם לנו התפלגות ה-Z בהרבה שלבים עד שאנחנו מגיעים להתפלגות ה-X.
[01:49:18 - 01:49:27] יש לנו אנשים שמוראים את זה, אוקיי? אז בכל פעם יש לנו מראש דגם לנו את כל ה-Z'ים, ויש לנו דרך עכשיו לתרגם את ה-Z הזה ל-X הזה,
[01:49:27 - 01:49:31] בתנאי שכבר חישבנו את כל ה-X'ים הקודמים.
[01:49:32 - 01:49:33] כי חישבנו את כל ה-X'ים הקודמים,
[01:49:33 - 01:49:39] ואז אנחנו יכולים לחשב את התופלת שאני צריך להוסיף ואת ה-StD שאני צריך להוסיף לזה, כדי לייצר את ההתפלגות שאני חושב לזה.
[01:49:40 - 01:49:41] אוקיי, אז עכשיו
[01:49:42 - 01:49:43] כמה
[01:49:45 - 01:49:47] פונקציות כאלה יש לי פה,
[01:49:48 - 01:49:53] אם המודל אוטו-רגרסיבי, אני רוצה לחשוב עליו בתור Flow, אז כמה פונקציות הרכבתי אחת על השנייה?
[01:49:54 - 01:49:58] X זה הר-פיקסל, כן, כזה כפור לפונקציה של מספר.
[01:49:58 - 01:50:05] שווה למספר הפיקסלים, נכון? המספר כאן יהיה שווה למספר הפיקסלים, כי כל פעם בעצם מה אני אעשה, קונספטואלית, אם אני רוצה לחשוב על זה בתור Flow,
[01:50:05 - 01:50:09] אני מתרגם, אני מעתיק את כל ה-Z'ים, חוץ מפיקסל אחד,
[01:50:10 - 01:50:11] שאני עושה לו את החישוב הזה,
[01:50:12 - 01:50:12] והוא הופך ל-X.
[01:50:13 - 01:50:15] בהתחלה את הפיקסל הראשון אני הופך אותו מ-Z ל-X,
[01:50:16 - 01:50:17] אחר כך את הפיקסל השני אני הופך מ-Z ל-X,
[01:50:18 - 01:50:19] לאט לאט עד שבסוף יש לי
[01:50:20 - 01:50:21] סמפל של X.
[01:50:21 - 01:50:25] זה בעצם אפשר לחשוב על זה בתור Flow עם n צעדים.
[01:50:28 - 01:50:31] יש לזה כמה יתרונות שחושבים על זה ככה,
[01:50:32 - 01:50:33] אז
[01:50:34 - 01:50:37] זה מודל שנקרא מסט אוטורגרסיב flow, שבעצם הוא הגדיר,
[01:50:39 - 01:50:40] בעצם זה מודל אוטורגרסיב
[01:50:41 - 01:50:44] כמו קודם, שוב פעם עם המאסקים האלה, כמו שמומשתם
[01:50:44 - 01:50:45] בקונבולוציות
[01:50:46 - 01:50:50] שיש עליהם פשוט מסק, כל פעם הוא מסתכל רק על חלק מהמודד הזה.
[01:50:51 - 01:51:09] יש פה את המודל ה-forward יש פה את המודל ה-backwards לחשב את הטרמיננטות והכל זה בדיוק כמו ב-rnvp זה אותו חישור,
[01:51:10 - 01:51:12] אפשר לעשות את זה בצורה יעילה,
[01:51:12 - 01:51:19] אפילו יותר יעילה כי זה כמו באוטורגרסיב flow, כל פעם יש לנו את ה-Xים ואנחנו יודעים לחשב את הדברים האלה ואנחנו יכולים לחשב אותם בצורה במקביל.
[01:51:21 - 01:51:26] אז לחשב את המודל הזה, לחשב את ה-forward זה איטי
[01:51:27 - 01:51:27] נדגום
[01:51:28 - 01:51:32] להגיע מזה ל-X זה איטי, אני צריך לעבור N צעדים
[01:51:34 - 01:51:37] אבל ה-backward הוא יעיל ומהיר
[01:51:37 - 01:51:40] כי אני גם צריך לעשות N צעדים אבל אני יכול לעשות אותם במקביל.
[01:51:42 - 01:51:44] זה בדיוק מה שעשינו באוטורגרסיב flow,
[01:51:45 - 01:51:46] חישבנו,
[01:51:46 - 01:51:49] בגלל שהקדמנו את הקונבולוציות האלה עם המאסקים,
[01:51:49 - 01:51:50] יכלנו לחשב
[01:51:51 - 01:51:55] באופן יעיל וחזרה מ-X ל-Z בעצם.
[01:51:56 - 01:51:57] אני חושבת עלייקנו בצורה יעילה
[01:51:58 - 01:52:00] כי פשוט עשינו,
[01:52:01 - 01:52:04] לקחנו את ה-X שלנו, הפעלנו עליו
[01:52:05 - 01:52:10] את הקונבולוציות אבל כל פיקסל רק יסתכל על כל מה שהיה לפניו,
[01:52:10 - 01:52:15] בקונבולוציות האלה ובעצם בצורה מקבילה חישבנו בדיוק את החישוב שיש כאן
[01:52:16 - 01:52:18] זאת אומרת ה-X הזה כדי לחזור חזרה ל-Z
[01:52:19 - 01:52:20] תסתכל על כל מה שהיה לפניו,
[01:52:20 - 01:52:25] זה היה קונבולוציות האלה שראינו ובעצם חישב את הלייקליות של מה שיצא.
[01:52:26 - 01:52:29] אז בעצם אפשר לחשוב על המודל הזה בדיוק בתור
[01:52:30 - 01:52:33] פוטורגרסיב flows שעשינו עליו את החישוב הזה בצורה נוגבילית.
[01:52:34 - 01:52:35] אז הפורוור הוא איטי
[01:52:36 - 01:52:41] אבל הבקוור הוא האינברס נקרא פה, האינברס הוא יעיל.
[01:52:42 - 01:52:43] אז מה זה אומר שהאינברס הוא יעיל?
[01:52:43 - 01:52:48] שלחשב את הלייקליות זה יעיל וגם אם אנחנו רוצים מ-X לחזור חזרה לזה
[01:52:48 - 01:52:49] אנחנו הולכים לעשות את זה בבת אחת.
[01:52:51 - 01:52:54] זאת אומרת זה לא בבת אחת זה בדיוק אותו כמוד חישוב אבל זה מגביל.
[01:52:54 - 01:52:55] עושים את כל החישובים האלה במגביל.
[01:52:59 - 01:53:04] אז ברגע שחשבו על זה ככה אז חשבו אוקיי בואו נסתכל על זה גם הפוך אולי יש יתרון לחשוב על דברים הפוך
[01:53:04 - 01:53:07] יש מה שנקרא inverse auto-regressive flow
[01:53:08 - 01:53:12] inverse auto-regressive flow פשוט הפכו את הדברים אוקיי אז אומרים
[01:53:13 - 01:53:14] אם כאן מ-X
[01:53:14 - 01:53:24] בסדר כאן מ-Z ל-X זה היה איטי אבל מ-X ל-Z זה היה מהיר בואו נבנה מודל הפוך שמ-Z ל-X
[01:53:26 - 01:53:26] יהיה
[01:53:29 - 01:53:29] מהיר
[01:53:30 - 01:53:31] ומ-X ל-Z זה יהיה איטי
[01:53:32 - 01:53:33] זה אוקיי?
[01:53:34 - 01:53:40] אז למה? מורה שבעצם יהיה יותר איטיון. נכון אז להשתמש במודל כזה כדי לאמן
[01:53:41 - 01:53:43] מעצם רייטיות זה לא יהיה איטי
[01:53:44 - 01:53:48] אבל יש דברים אחרים שאפשר לעשות איתו
[01:53:49 - 01:53:53] קודם כל יש כאן בדיוק אותו חישוב וזה בדיוק אותו חישוב פשוט הפכנו את היוצרות בין R ל-X
[01:53:56 - 01:54:07] שוב מה שאני קראתי כאן בחץ אדום בגלל שהכל הפיך אז זה לא כזה משנה מה שאדום אני יכול לקרוא לו החץ ארוך ובגלל שזה הפיך אז אני יכול לחשב את השני אבל השאלה מה אני יכול לחשב בצורה מקבילית ומה לא
[01:54:07 - 01:54:11] עכשיו הפכתי את היוצרות אז אחד מהם עכשיו את הכיוון הזה אני יכול לחשב בצורה
[01:54:11 - 01:54:15] ‫המקבילית, אני יכול לייצר דגימה ‫מאוד מהר,
[01:54:16 - 01:54:16] ‫אבל הפוך,
[01:54:17 - 01:54:22] ‫אני לא יכול לחשב מהר. ‫מה שאומר, מה שנכון, מה שאמרת, ‫שאני לא אוכל לעשות אימון,
[01:54:22 - 01:54:24] ‫ואימון זה הדבר היקר פה,
[01:54:24 - 01:54:26] ‫אני צריך לעשות ‫הרמון איפרציות.
[01:54:27 - 01:54:29] ‫אז למה, איך אני יכול ‫להשתמש בדבר הזה?
[01:54:34 - 01:54:34] ‫אמרתי?
[01:54:35 - 01:54:38] ‫אז זה פאסט טו סאמפל,
[01:54:38 - 01:54:39] ‫ניבט
[01:54:42 - 01:54:47] אוקיי, אז קודם היה כתוב, ‫אני חושב שזה איטי לחשב את ה-likelihood,
[01:54:49 - 01:54:50] ‫פשוט לא היה כתוב, אמרנו את זה,
[01:54:51 - 01:54:54] ‫אבל יש פה נקודה חשובה,
[01:54:55 - 01:54:58] ‫אנחנו יכולים לחשב דגימה בצורה מהירה, ‫אבל גם עוד נקודה,
[01:54:59 - 01:55:01] ‫שאם חישבנו את הדגימה,
[01:55:01 - 01:55:04] ‫אנחנו גם יכולים לחשב את ה-likelihood שלה ‫בצורה מהירה.
[01:55:05 - 01:55:09] ‫אם אנחנו חשבים דגימה, ‫אז בבת אחת אנחנו מקבלים ‫את כל ה-X'ים,
[01:55:10 - 01:55:11] את כל ה-Z'ים,
[01:55:12 - 01:55:16] ‫אז אנחנו יכולים לחשב שוב ‫את ה-likelihood בצורה יעילה.
[01:55:17 - 01:55:20] ‫אז אם אנחנו הגענו מ-Z',
[01:55:21 - 01:55:23] ‫דגמנו את כל ה-Z'ים, ‫שהם גאוסיאנים פשוט,
[01:55:23 - 01:55:26] ‫עכשיו מתוכם בצורה יעילה ‫דגמנו את כל ה-X'ים,
[01:55:27 - 01:55:32] ‫יש לנו את כל ה-X'ים, ‫אז עכשיו אנחנו יכולים לחשב ‫את ה-likelihood של זה בצורה מהירה, ‫כי עבור כל X,
[01:55:33 - 01:55:35] בשביל לחשב את ה-likelihood, ‫אני צריך לחשב את הדבר הזה.
[01:55:36 - 01:55:39] אם רק היה לנו מראש דוגמה ‫מה-data-set שלנו X,
[01:55:40 - 01:55:41] ‫לא הייתי יכול לחשב את זה בצורה יעילה,
[01:55:41 - 01:55:44] ‫כי הייתי צריך עכשיו 1-1 לעבור, ‫עד שהיה לי את כל ה-Z'ים,
[01:55:45 - 01:55:48] ‫כדי לחשב בחזרה את ההסתברות ‫של כל אחד מ-X'ים.
[01:55:49 - 01:55:51] ‫ואם הגעתי מהכיוון הזה, ‫אז כבר יש לי את כל ה-Z'ים,
[01:55:52 - 01:55:54] ‫אז אני יכול לחשב את ה-likelihood בצורה יעילה.
[01:55:55 - 01:55:58] ‫אז אני יכול לחשב את ה-likelihood בצורה יעילה ‫לדגימה שדגמתי בעצמי,
[01:55:59 - 01:56:01] ‫אבל לא לדגימה שקיבלתי מה-training set שלי.
[01:56:02 - 01:56:03] ‫ומה זה שימושי כזה דבר?
[01:56:04 - 01:56:08] בספר סדר ה-Z'. מה? בספר סדר ה-Z', כן.
[01:56:08 - 01:56:15] ‫אפשר לחשוב על כל מיני דוגמאות, ‫אבל אני מתכוון למשהו אחר, ‫ואנחנו לא רוצים לאמן
[01:56:16 - 01:56:24] ‫מהדאטה-סט שלנו, כן? ‫אנחנו לא נרצה עכשיו לקחת מיליוני דוגמאות, ‫על כל אחת מהשטח שלי, ‫זה לא יעילי, ‫אז יש לי משתמש כבר בית,
[01:56:24 - 01:56:25] ‫במדרך שני.
[01:56:27 - 01:56:35] ‫אבל אתם זוכרים, יש לכם בראש ‫דוגמה למצב שבו אנחנו רוצים ‫רק לחשב את ה-likelihood ‫של דוגמאות שיצרנו?
[01:56:38 - 01:56:42] ‫אחת מהדרכים לחשוב על ההבדל הזה, ‫זה הכיוון של ה-KL Divergence. ‫יש לכם שדיברנו על זה?
[01:56:43 - 01:56:46] ‫זה שני דברים שונים, וזה אפשר לעשות את זה.
[01:56:47 - 01:56:49] ‫טוב, כשעושים מקסימום לייט ליווד,
[01:56:54 - 01:56:55] ‫מקסימום לייט ליווד,
[01:56:55 - 01:56:59] ‫אני דוגם דאטה מההתפלגות של הדאטה,
[01:56:59 - 01:57:02] ‫דוגם מה-training set,
[01:57:03 - 01:57:07] ‫ועושה פיט למודל שלי, לפרמטרים של זה.
[01:57:07 - 01:57:09] ‫אז זה כמו KL Divergence
[01:57:10 - 01:57:11] ‫בין
[01:57:14 - 01:57:14] ככה,
[01:57:15 - 01:57:16] ‫זה בעצם תוחלת
[01:57:17 - 01:57:19] ‫על ההתפלגות של הדאטה,
[01:57:20 - 01:57:21] ‫של נוג
[01:57:22 - 01:57:23] ההתפלגות של דאטה.
[01:57:25 - 01:57:26] ‫זה maximum likelihood,
[01:57:27 - 01:57:32] ‫שזה שווה ל-KL בין P data,
[01:57:33 - 01:57:35] ‫הראשון ב-KL זה שגם התוחלת היא איתה.
[01:57:37 - 01:57:38] ‫לבין P תטה,
[01:57:39 - 01:57:44] ‫ועוד איזשהו קבוע, נכון? ‫ניתן לי פה את הלוב של ה-P data, ‫של ה-P data, של האנדרוגיה של הדאטה.
[01:57:46 - 01:57:48] ‫בואו, דיברנו על זה כבר.
[01:57:48 - 01:57:56] ‫מה שאני עושה ללא להקליות, ‫זה שתוחלת פה על P של הדאטה, ‫אני צריך כאילו לדגום מה-P של הדאטה. ‫איך אני דוגם מה-P של דאטה? ‫אני פשוט אוסף דאטה מה-Trainage set שם.
[01:57:57 - 01:58:00] ‫כיוון ההפוך, ה-KL ההפוך, אם אני רוצה שיהיה כאן כתוב KL
[01:58:01 - 01:58:04] ‫בין P תטה לפי דאטה.
[01:58:07 - 01:58:13] ‫מה זה אומר? זה שקול בעצם לדגום מה-P תטה, ‫אני דוגם מהמודל שלי,
[01:58:17 - 01:58:19] ‫ועכשיו אני בודק את הלוג של P,
[01:58:22 - 01:58:23] ‫לוג של P data,
[01:58:25 - 01:58:26] ‫פה אני לא יכול להוריד איבר.
[01:58:27 - 01:58:28] ‫לוג של P data,
[01:58:28 - 01:58:29] פרפורט לוג
[01:58:32 - 01:58:34] פ... ת'סימנים אוניברסיטיין, זה אפשר להגיד.
[01:58:34 - 01:58:35] ‫ויש לי פה את שני הדברים האלה,
[01:58:35 - 01:58:37] ‫אבל אני דוגם מפי של תטה.
[01:58:39 - 01:58:42] ‫אתם זוכרים דוגמה על המצב שאנחנו
[01:58:44 - 01:58:45] ‫עושים את החישוב בצורה הזאת?
[01:58:47 - 01:58:47] ‫גם בצורה הזאת?
[01:58:50 - 01:58:50] ‫כמו ה-L?
[01:58:51 - 01:58:54] ‫כן, בדיוק, ב-L? זה ה-KL Divergence וה-L?
[01:58:54 - 01:59:01] ‫כשאנחנו מחשבים את ה-Posterior, ‫הוא היה בצורה הזאת. ‫אנחנו דוגמים מה-Posterior, ‫מההתפלגות שאנחנו מנסים ללמוד אותה.
[01:59:02 - 01:59:08] ‫אנחנו דוגמים מהתוכן שלנו ‫היא על גימות של ההתפלגות ‫של מה שאנחנו דוגמים ממנו,
[01:59:09 - 01:59:13] ‫מה שאנחנו לומדים, סליחה, ‫ושם אנחנו צריכים לעשות ‫את החישוב הזה, אוקיי?
[01:59:14 - 01:59:19] ‫אז בעצם במקרים כאלה יכול, ‫זה למשל דוגמה, ושם באמת,
[01:59:19 - 01:59:22] ‫בשביל זה המציאו את המודל הזה, ‫אבל אפשר להשתמש בו גם ‫לדברים אחרים,
[01:59:23 - 01:59:25] ‫בשביל לחשב posterior
[01:59:32 - 01:59:44] ‫אוקיי? אתם זוכרים על זה? ‫יש לי פה איזה... ‫אפשר להזכיר לכם קצת מה זה V.A.E.S. ‫אנחנו זוכרים מה זה V.A.E.S. כן, ‫אנחנו צריכים עכשיו ללכת לעשות את זה בתרגיל, ‫כנסת תזכרו.
[01:59:44 - 01:59:53] ‫-V.A.E. זה שאני שמתי פה את שני התרשימים ‫שהיה לנו בצהל V.A.E.S. ‫אז בואו נתחיל מזה רגע, ‫אנחנו לוקחים תמידה, נכון?
[01:59:53 - 02:00:01] ‫אנחנו דוגמים ממנה, ‫הדבר הזה זה ה-Encoder או ה-Inference, ‫הואו מה שנותן לנו את ה-Posterior, אוקיי?
[02:00:01 - 02:00:02] ‫יש לנו את ה-Latence.
[02:00:03 - 02:00:09] ‫אצל המודל שלנו הוא כזה, ‫יש לנו latency שהוא הפרייר שלו, ‫נגיד, הוא גאוסיאן, ‫אנחנו מגדירים מודל של גאוסיאן,
[02:00:09 - 02:00:10] ‫הוא מייצר לנו את ה-Z,
[02:00:11 - 02:00:13] ‫הדבר הזה זה נקודה תוחלת והסיגמי של Z,
[02:00:14 - 02:00:18] ‫ויש לנו איזה דיקודור של בהינתן Z, ‫נותן לנו התפלגות על כל ה-X,
[02:00:19 - 02:00:21] ‫זו ההתפלגות של X בהינתן Z,
[02:00:21 - 02:00:23] ‫הדיקודר או הג'נרטיב מודל.
[02:00:26 - 02:00:30] אבל כדי לאמן את זה, ‫אנחנו צריכים לחשב posterior בהינתן X,
[02:00:30 - 02:00:42] ‫אנחנו צריכים לחשב מה ההתפלגות על Z. ‫אז זו הרשת הזאת, ‫שנותן לנו את הפוסטריאור בהינתן X. ‫היא מחשבת לנו התפלגות על Z, ‫פי שבין Z בהינתן X. ‫והדרך שאנחנו מאמנים את זה, ‫זה האלבור,
[02:00:43 - 02:00:47] ‫כן, שראינו בשיעור שהוא חסם ל-P של X, ‫זה מה שמעניין אותנו.
[02:00:47 - 02:00:51] ‫הוא, יש לו את המרכיב הזה, ‫שפשוט לעשות את המעבר הזה ‫ולראות מה יצא,
[02:00:52 - 02:00:56] ‫זה הלוס, פשוט,
[02:00:57 - 02:00:59] ‫שנראה כמו שלא אותו נקודר, ‫כאילו, זאת הלקחת תמונה,
[02:01:00 - 02:01:13] ‫עובר דרכה, דוגם פה את ה-Z, ‫מעביר את ה-Z דרך ה-Decoder שלי, ‫מצאה לי התפלגות על X. ‫בתרגיל של ההתפלגות הזאת על X, ‫היא תהיה גאוסיאן פשוט. ‫זאת אומרת שה-output של הדבר הזה ‫היהיה התוחלת
[02:01:14 - 02:01:20] של כל פיקסל של הגאוסיאן הזה. ‫אמרתי לכם שאתם יכולים ‫להתייחס לבריאנס שם ‫בתור קבוע פשוט,
[02:01:20 - 02:01:22] ‫אז פשוט השגיאה תהיה פשוט השגיאה הריבועית.
[02:01:23 - 02:01:26] ‫עם ה-output של התוחלת, ‫אבל יש לי פשוט שגיאה ריבועית.
[02:01:26 - 02:01:35] ‫אבל אני צריך להוסיף לזה ‫או להחסיר מזה את ה-KL ‫בין ההתפלגות הזאת שיצאה לי כאן על Z,
[02:01:35 - 02:01:39] ‫לבין ה-prior שהיה לי על Z, ‫שהוא במובן הזה שלנו יהיה גאוסיאנט.
[02:01:41 - 02:01:45] ‫אז הדבר הזה הוא בדיוק נראה ככה. ‫בעצם אני דוגם,
[02:01:46 - 02:01:47] ‫אני מייצר דגימה,
[02:01:48 - 02:01:51] ‫ועד הדגימה הזאת אני מחשב את החישוב הזה.
[02:01:53 - 02:01:55] ‫יש לי את כל מה שצריך לחשב את החישוב הזה.
[02:01:56 - 02:01:59] ‫אם זה גאוסיאנט למשל, ‫אז בתרגיל הדבר הזה יהיה גאוסיאנט.
[02:01:59 - 02:02:01] ‫אוסטריו יהיה גאוסיאנט, שוב פעם,
[02:02:02 - 02:02:09] ‫הרשת תוציא לנו תוחלת וסיגמא, ‫אז כאן כדאי גם שזה יהיה גם סיגמא, ‫לא רק תוחלת,
[02:02:09 - 02:02:11] ‫זה קצת מאולץ מדי,
[02:02:12 - 02:02:15] ‫ואנחנו צריכים להוסיף את האיבר הזה.
[02:02:16 - 02:02:21] ‫דוגמים ומחשבים גם את ה-KL. ‫האמת שבשביל ה-KL לא צריך לדגום, ‫כי יש לנו את הנוסחה,
[02:02:22 - 02:02:27] ‫אם זה גאוסיאנט, ‫יש לנו נוסחה סגורה כדי לנסות את זה. ‫לא חייבים לעשות את זה בגימה, ‫אבל אפשר גם עם גימה.
[02:02:29 - 02:02:29] ‫אוקיי,
[02:02:30 - 02:02:32] ‫עכשיו, זה בדיוק המצב שיש לנו כאן.
[02:02:33 - 02:02:36] ‫אז פה זה היה פשוט עבור גאוסיאנט, ‫אבל גאוסיאנט הוא לא תמיד הכי טוב.
[02:02:37 - 02:02:43] ‫אמרנו שאנחנו זוכרים בהרצאה של DAE ‫שאולי זה לא נורא, ‫כי זה בעצם התפלגות ‫רק על דאטה פוינט אחד.
[02:02:44 - 02:02:45] ‫אבל הפוסטריור של Z עבור נקודה אחת.
[02:02:46 - 02:02:51] ‫אם בפלואו אפילו הגדלנו ‫שזה יהיה ממש מפול אחד לאחד, ‫כאן אנחנו מגדירים ‫שזה יהיה איזושהי התפלגות.
[02:02:51 - 02:02:53] ‫לא כל כך נורא עם ההתפלגות הזו, ‫כי גאוסיאנט.
[02:02:54 - 02:02:56] ‫אבל יכול להיות שאנחנו כן נרצה,
[02:02:57 - 02:03:01] ‫בשלב הבא זה כן לאפשר ‫לפוסטריור להיות קצת יותר מורכב.
[02:03:02 - 02:03:07] ‫ובעצם מה שהם מציעו כאן זה לעשות ‫שהפוסטריור הזה יהיה ‫אינברס אוטורגרסיב פלואו, שהוא יהיה פלואו,
[02:03:08 - 02:03:13] ‫והאינברס אוטורגרסיב פלואו בעצם ‫מאפשר לנו לחשב את הלייקליות
[02:03:14 - 02:03:14] של דגימה.
[02:03:15 - 02:03:21] ‫אנחנו לא צריכים אף פעם לחשב ‫את הלייקליות של דאטה, ‫אין לנו דאטה של Z. ‫יש לנו עוד קצת דגימות ‫שאנחנו דוגמים של Z,
[02:03:22 - 02:03:26] ‫ועל הדגימות האלה אנחנו כן ‫נדאים לחשב את הלייקליות ‫בצורה יעילה.
[02:03:28 - 02:03:30] ‫-אוקיי הרבה, אני לא יודע אם...
[02:03:31 - 02:03:35] ‫זה עבר, אבל הנה יש פה ‫המחשה שאולי קצת מראה למה זה חשוב,
[02:03:35 - 02:03:36] ‫אבל זה למשל
[02:03:37 - 02:03:45] ‫המחשה נחמדה מהמאמר הזה ‫שצריך לעשות פוסטריאו של BA's עם flows.
[02:03:45 - 02:03:50] ‫אז הוא אומר ככה, הפריור שלנו, ‫נגיד שאנחנו מניחים ‫שהפריור הוא גאוסיאן איזוטרופי,
[02:03:52 - 02:03:55] ‫עם 0 ו-1,
[02:03:56 - 02:03:59] ‫אז פה במקרה הזה הלייקליות שלהם ‫הוא פשוט דו-ממדי, כן? יש להם שני ממדים.
[02:04:00 - 02:04:06] ‫עכשיו, מה קורה כשאני מאמן את ה-Va שלי? ‫בעצם אני צריך,
[02:04:06 - 02:04:11] ‫כל X בדאטה-סט שלי צריך לייצר פוסטריאור על ה-Z,
[02:04:13 - 02:04:18] ‫ככה שסך הכול, כל ה-Z'ים סך הכול, ‫יהיו כמה שיותר דומים לפריור.
[02:04:19 - 02:04:21] ‫אז אמרנו כשדיברנו על התרשים הזה,
[02:04:22 - 02:04:25] ‫בעצם כל נקודה בדאטה-סט, ‫יהיה פה איזשהו גאוסיאן,
[02:04:26 - 02:04:28] ‫והמטרה שכל הדאטה-סט, בסך הכול,
[02:04:28 - 02:04:31] ‫להימלא את כל הפריור הזה ‫בצורה שה-KL ביניהם יהיה די
[02:04:33 - 02:04:33] ‫כמה שיותר קרוב לאפס.
[02:04:35 - 02:04:40] ‫אז פה הם עשו דוגמה קצת צעצוע, ‫אבל היא אולי קצת מונחישה למה יש מקרים
[02:04:40 - 02:04:43] ‫שחשוב למדד את הפוסטריאור בצורה יותר טובה.
[02:04:44 - 02:04:45] ‫אז פה יש להם דאטה-סט עם ארבע נקודות.
[02:04:47 - 02:04:49] ‫כל נקודה כאן צבועה תצא באחר,
[02:04:50 - 02:04:52] ‫ובעצם הם צריכים שהפוסטריאור,
[02:04:52 - 02:04:57] ‫ספורם של הפוסטריאור על כל הארבע נקודות האלה, ‫יהיהיה כמה שיותר דומה לגאוסיאן הזה.
[02:04:58 - 02:05:03] ‫אז במקרה, איך שהם אינוי זה, ‫זה מה שיצא להם. זה הביא אי רביעית ‫שמאמנים עם פוסטריאור שהוא גאוסיאן פשוט,
[02:05:04 - 02:05:09] ‫אז הגאוסיאן הראשון יצא, ‫שהנקודה הראשונה, פוסטריאור של זה, ‫בהינתן נקודה ראשונה זה זה,
[02:05:09 - 02:05:11] ‫פוסטריאור של זה, בהינתן נקודה שנייה זה זה,
[02:05:12 - 02:05:13] ‫מקודה שלישית זה...
[02:05:13 - 02:05:17] ‫אז סך הכול זה משהו שהוא איכשהו קרוב לזה, ‫אבל לא ממש, ויש פה כל מיני,
[02:05:18 - 02:05:23] ‫אתם רואים שגם כל מיני חורים ‫שכאן דווקא הם בהסתברות מאוד גבוהה, ‫אבל כאן הם בהסתברות דיינן נבחר.
[02:05:24 - 02:05:28] ‫כן, אז בעצם לא מצליחים לגשר ‫על הפער הזה בין הפער לבין הפוסטריאור.
[02:05:29 - 02:05:33] ‫זאת אומרת, אם פוסטריאור מאפשרים ‫לקוסטריאור להיות קצת יותר מורכב,
[02:05:34 - 02:05:39] ‫עם פגיעות פלואו, כזה שלא רק גאוסיאן, ‫אלא משהו כל קצת יותר מורכב,
[02:05:39 - 02:05:45] ‫אז למשל זה מצליח אפילו ‫הוא הצליח ללמוד ממש איזשהו מיפוי כזה ‫שכל אחד תופס במה של רבע ‫של הפוסטריאור הזה,
[02:05:46 - 02:05:49] ‫וההבדל בין זה לזה הוא יותר קטן.
[02:05:49 - 02:05:52] ‫במקרה יותר קטן, ‫הוא מצליח למצוא פוסטריאור יותר טובים.
[02:05:53 - 02:05:55] ‫חלוקה יותר טובה של הפריור ‫לנקודות השונות.
[02:05:56 - 02:05:58] ‫זה דוגמה קצת קיצונית, כאילו צעצוע,
[02:05:59 - 02:06:01] ‫אבל רק בשביל להמחיש למה
[02:06:02 - 02:06:03] זה יכול לעזור.
[02:06:05 - 02:06:07] אוקיי, אז זו התוצאות, נגיד, של סיגמו,
[02:06:08 - 02:06:12] ‫יש פה כמה השוואות ‫לכמה מודלים שווה להסתגר. ‫חלק מהם כבר פגשנו,
[02:06:13 - 02:06:20] ‫למשל את פיקסל RNN וגייטל פיקסל CNN. ‫זה היה סיפר, אוקיי?
[02:06:23 - 02:06:25] ‫זה מודלים שהם אקזקט.
[02:06:27 - 02:06:30] ‫זה אוטו-ריגרסיד מודלס.
[02:06:31 - 02:06:36] ‫כאן, נייס, זה מה שראינו, זה המודל פלואו.
[02:06:38 - 02:06:43] ‫זה הביטסברג דימנצ'לינג ‫שהוא מושיג.
[02:06:45 - 02:06:48] ‫ריל-NVP, שזה השדרוג שלו, עם ספוננט,
[02:06:50 - 02:06:53] ‫מודלים קצת יותר מתופקמים, ‫כבר מגיע לתוצאות נתמודות.
[02:06:54 - 02:06:59] ‫אז זה כל החצי הזה, זה חלק שאנחנו יכולים ‫לחשב את הלייטליץ בצורה מדויקת,
[02:06:59 - 02:07:02] ‫וכאן זה כל מיני מודלים שהם סוג של VE's,
[02:07:02 - 02:07:05] ‫שאפשר לחלק, אפשר רק לחשב את החסם.
[02:07:05 - 02:07:09] ‫כן, יש כאן מודל של, אנחנו לא נדבר עליו,
[02:07:09 - 02:07:12] ‫בעצם המודל דיפיוז'ן הראשון שהיה.
[02:07:13 - 02:07:18] ‫המודל הזה זה סוג של VE עם הרבה שכבות,
[02:07:20 - 02:07:24] ‫נקרא דרור, גם סוגות לא רעות.
[02:07:25 - 02:07:29] ‫אנחנו, אתם זוכרים, בשיעור על ה-VE ראינו משהו שיצא אחרי המאמר הזה, ‫זה לא מופיע כאן,
[02:07:30 - 02:07:33] ‫שקראו לו NVE, שהיה גם VE עם הרבה שכבות,
[02:07:33 - 02:07:36] ‫שנעשים תוצאות נקשיבים ‫הרבה יותר טובות מהכל כאן,
[02:07:36 - 02:07:41] ‫אבל כאן זו דוגמה של לקחת VE ‫ושה-פוסטריאור שלו יהיה ה-IAF,
[02:07:42 - 02:07:46] ‫ה-אינברסות ל-רגסי פלואו, ‫וזה מאפשר ל-VE, בגלל שהפוסטריאור שלו יותר טוב,
[02:07:46 - 02:07:49] ‫הוא מאפשר ל-VE להשיג תוצאות יותר טובות.
[02:07:52 - 02:07:53] ‫טוב,
[02:07:56 - 02:07:58] זה שאני מסכם? יש לכם שאלות?
[02:07:58 - 02:07:58] ראשון.
[02:07:59 - 02:08:05] ‫זה עבוד יותר פשוט מ-VE.O. ‫אז אפשר להבין.
[02:08:09 - 02:08:10] ‫אוקיי, אז בואו נסכם.
[02:08:11 - 02:08:22] מה ראינו? ‫אז ראינו בעצם את הגישה הזו של normalized flow, ‫שבעצם אנחנו נחשוב על זה ‫בתור איזושהי טרנספורמציה ‫של התפלגות פשוטה, ‫שאתם יודעים לעבוד איתה, ‫כמו גאוסיאנס,
[02:08:22 - 02:08:24] ‫להתפלגות המורכבת,
[02:08:25 - 02:08:27] ‫שאנחנו רוצים לעבוד איתה, כמו שהיא תמונות,
[02:08:28 - 02:08:31] ‫על ידי זה שאנחנו כל פעם ‫משתמשים ב-Changer Variable פורמולה.
[02:08:33 - 02:08:37] ‫היתרונות שבעצם כשאנחנו מחשבים ‫ככה את הכול בצורה כזאת,
[02:08:38 - 02:08:44] ‫אז כל החישוב של ה-Lightlyde הוא טרקטי, ‫אבל אנחנו יכולים לחשב ‫בצורה מדויקת את ה-Lightlyde,
[02:08:45 - 02:08:48] ‫אנחנו יכולים לחשב את ה-Posterיאו ‫בעצם בצורה מדויקת,
[02:08:48 - 02:08:50] ‫אנחנו יכולים לחזור חזרה מ-X ו-Z,
[02:08:51 - 02:08:52] ‫לכן, מה ההתפלגות של Z.
[02:08:57 - 02:08:59] ‫אוקיי, זה היתרונות. חסרונות,
[02:09:00 - 02:09:03] ‫אנחנו רק יכולים לעבוד ‫עם מושכנים רציפים,
[02:09:04 - 02:09:08] ‫אז גם ה-Latence וגם הדאטה שלנו,
[02:09:09 - 02:09:12] ‫לעשות את כל הדברים האלה, ‫זה רק רלוונטי כשהכול רציף.
[02:09:16 - 02:09:18] ‫המימד של Z ל-X הוא את אותו מימד,
[02:09:19 - 02:09:27] ‫שזה לפעמים בעיה חישובית ‫לפעמים גם בעיה קונספטואלית. היינו רוצים ‫ש-Z יתפוס איזה משהו מעניין, אם אנחנו ראינו כבר ‫שאולי הוא כן תופס משהו מעניין,
[02:09:27 - 02:09:36] ‫אבל אלו תתפלגותי שהוא מוריד ‫את המימד של הדאטה לפחות ממדים, ‫בצורה כזאתי שאולי הוא רק תופס ‫את הדברים המעניינים
[02:09:37 - 02:09:38] בצורה נוחה.
[02:09:39 - 02:09:41] ‫אז זה חיסרון.
[02:09:42 - 02:09:45] ‫והחיסרון אולי הכי גדול ‫זה שבעצם אנחנו לא יכולים סתם
[02:09:45 - 02:09:54] ‫להפעיל כל רשת שם, ‫אלא רק כל מיני טריקים כאלה, ‫כמו שראינו ב-NICE ו-RedVNDP,
[02:09:55 - 02:09:56] ‫שמאפשרים את כל החישובים האלה ‫להיות
[02:09:57 - 02:10:06] ‫טרקטיבל, ואנחנו לא יודעים, ‫אולי זה אילוץ שהוא בעצם אומר ‫שאנחנו לא יכולים להגיע לתוצאות טובות.
[02:10:07 - 02:10:09] ‫לא ברור אם כדאי לנו לאלץ את
[02:10:12 - 02:10:13] ‫הסוג המודלים,
[02:10:13 - 02:10:18] ‫או לא לאלץ את המודלים, ‫אבל לעשות קירוב, למשל כמו ב-VA, כן? ‫אגב, אני חושב את האלגרם.
[02:10:19 - 02:10:21] יש כאן איזשהו סוג של trade-off, ‫שלא ברור מה עדיף.
[02:10:22 - 02:10:26] ‫כשזה יצא, ‫אז היתנו תוצאות ‫הרבה יותר טובות מ-VA, ‫אז אבל אחר כך הגיעו ה-VA,
[02:10:26 - 02:10:29] ‫כמו שראינו סוף השיעור של הגיעיז, ‫שהיו כבר יותר טובים,
[02:10:30 - 02:10:32] ‫אבל לא לגמרי ברור מה יותר טוב.
[02:10:33 - 02:10:35] ‫במקום לנצח, זה ברור.
[02:10:36 - 02:10:43] ‫אוקיי, ראינו בעצם האסטרטגיה ‫כדי לבנות כזה דבר, ‫זה שזה בנוי מ-Composition כזה, ‫מהרבה פונקציות פשוטות.
[02:10:44 - 02:10:51] ‫דרך אגב, לא השתמשתי במידע בייג'קשן, ‫אבל כשהפונקציה הפכה, ‫בדרך כלל בקוד ככה קוראים ‫לחלקים האלה שאנחנו מרכיבים.
[02:10:52 - 02:11:00] ‫בייג'קשן פשוט אומר שזה צריך גם לעבור ‫מכל נקודה ב-X וב-Z והפוך.
[02:11:03 - 02:11:07] ‫עוד טריק שבדרך כלל משתמשים, ‫מאלצים את הקודם להיות משומשי,
[02:11:08 - 02:11:12] ‫כדי שבדרך כלל יהיה חשוב, ‫אפשר להילחש שוב בצורה יעילה.
[02:11:13 - 02:11:21] ‫ונקודה האחרונה זה שבעצם אפשר ‫לחשוב על החיבור בין המודלים האלה, ‫חיבור של אוטו-רגרסיב,
[02:11:21 - 02:11:22] ‫וזה נותן קצת
[02:11:26 - 02:11:33] ‫אינסייטס, למשל בכיוון הזה, ‫שאפשר להפוך את הדברים, ‫ואז להשתמש בזה כדי לחשב את הפוסטריאויות ב-DAE,
[02:11:33 - 02:11:38] ‫ויש עוד כל מיני חיבורים בעצם ‫שאפשר לעשות, ‫בעצם בין שלושת המודלים שלנו. ‫עכשיו, VE's,
[02:11:40 - 02:11:44] ‫אוטו-רגרסיב מודלס ופלואו, ‫אפשר לחבר אותם בכל מיני דרכים.
[02:11:46 - 02:11:48] אז זהו להיום.
[02:11:48 - 02:12:01] ‫בשבוע הבא אנחנו נדבר קצת על מודלים ‫גם נדבר קצת על גאנס, ‫שאנחנו לא ניכנס יותר מדי לעומק,
[02:12:02 - 02:12:06] ‫וגם נדבר על איך אפשר,
[02:12:07 - 02:12:16] ‫כיצר זה נובע על ההמשך של הגאנס, ‫מה אנחנו עושים שהמודל שלנו ‫רק יודע לייצר דגימות, ‫ולא להגיד שום דבר ‫על מה ההסתברות של דגימה,
[02:12:16 - 02:12:17] ‫או להפוך
[02:12:18 - 02:12:21] ‫איזשהו היפוך כזה בין מיגלס לזה,
[02:12:23 - 02:12:26] ‫ואולי נתחיל לדבר קצת על ‫אנרג'י-בייסט מודלס,
[02:12:27 - 02:12:28] ‫ואז יש לנו שני שיעורים ‫על
[02:12:31 - 02:12:33] ‫הדיפיוז'ן מודלס, ‫שזה כולל בעצם לדבר על סקור,
[02:12:36 - 02:12:37] ‫זוכרים,
[02:12:38 - 02:12:38] ‫דיברנו על זה קצת,
[02:12:40 - 02:12:45] ‫בשבוע ואני אזכיר לכם ‫מה אתם צריכים להיזכר ‫כקראת הדיפיוז'ן מודלס.
[02:12:45 - 02:12:47] ‫אני שכחתי להגיד, ‫אבל יש לי פה בהתחלה של ה...
[02:12:48 - 02:12:52] ‫הוא מציג את זה. ‫כמה מצגות, שם של מישהו?
[02:12:53 - 02:12:54] ‫ולדימיר?
[02:12:55 - 02:12:56] ‫ולדימיר אונושב?
[02:12:58 - 02:13:00] ‫זה בדיוק איך מבטאים את השם שלו?
[02:13:05 - 02:13:05] ‫שם הזה?
[02:13:06 - 02:13:10] ‫ההרצאות, גם ההרצאה הזאתי ‫וגם ההרצאות הבאות,
[02:13:10 - 02:13:16] ‫דיפיוז'ן אולי פחות, ‫אבל הרבה מההרצאות, ‫יש להרצאות על דיפיוז'ן,
[02:13:17 - 02:13:19] ‫הן מבוססות על קורס שיש לאונליין, ‫מחפשו את השם שלו.
[02:13:20 - 02:13:22] ‫מי שרוצה להסתכל,
[02:13:23 - 02:13:25] ‫כמובן מסתכל לפני השיעור, ‫לפני שיש זמן,
[02:13:26 - 02:13:29] ‫אני חושב שזה מאוד יכול ‫לעזור להבין את השיעור, ‫אם כבר ראיתם איזושהי
[02:13:30 - 02:13:33] ‫בלה של הרצאה, פשוט מאוד דומה. ‫יש לו למשל את ההרצאות,
[02:13:33 - 02:13:37] ‫אני חושב שזה הרצאות של שעה כל פעם, ‫יש לו שתי הרצאות על גנץ
[02:13:38 - 02:13:42] ‫והנך לעשות אבדואיישן ‫של מילים ונרטיבים,
[02:13:42 - 02:13:48] ‫וזה קצת יותר מה שנדבר ‫למשך הבא, ‫יש לו גם הרצאה על אנרגי-בייסט מודלס,
[02:13:49 - 02:13:53] ‫אז אני יכול לספר על זה, ‫ויש לו שתיים-שלוש הרצאות ‫על דיפיוז'ן מודלס,
[02:13:54 - 02:13:55] ‫אז גם, מי שרוצה להסתכל,
[02:13:56 - 02:13:58] ‫זה לא מיץ.
[02:14:12 - 02:14:18] ‫תגידי, מי חושב שזה, ‫בבקשה, בבקשה, בבקשה, ‫תגידי, אולייך.