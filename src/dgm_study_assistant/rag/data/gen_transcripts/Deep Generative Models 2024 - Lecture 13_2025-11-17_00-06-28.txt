[00:00:00 - 00:00:09] ‫טוב, בואו נתחיל.
[00:00:11 - 00:00:14] ‫תודה רבה לכולם, הגענו לשיעור האחרון.
[00:00:15 - 00:00:16] ‫מהר.
[00:00:16 - 00:00:19] ‫עם כל זה שצפיתם בהקלטות ‫של השיעור הקודם,
[00:00:21 - 00:00:27] ‫שיעור היה מידי ההמשך של זה. ‫כן, נעשה בהתחלה איזה סוג של חזרה ‫על החלק החשוב שמסתמך עליו,
[00:00:27 - 00:00:33] ‫אבל זה יהיה קשה, אני חושב, ‫לעקוב, מי שלא ראה,
[00:00:34 - 00:00:35] ‫אני מפרש שהייתם.
[00:00:55 - 00:00:57] ‫אז בעצם הנושא של היום, ‫אנחנו נמשיך לדבר על דיפלוס
[00:00:57 - 00:01:03] ‫אבל בעצם על דרך להשתמש בהם ‫יותר מעל האימון שלהם.
[00:01:06 - 00:01:11] ‫ספציפית אנחנו נדבר על שני דברים ‫שהם יותר דמים, אחד נקרא posterior sampling ‫השני נקרא גיידנס.
[00:01:13 - 00:01:19] ‫אז התוכנית זה שנעשה קודם חזרה ‫קצת על ה-difusion models,
[00:01:20 - 00:01:21] ‫בשבוע שעבר,
[00:01:22 - 00:01:24] ‫ואז נדבר על posterior sampling,
[00:01:25 - 00:01:27] ‫classifier גיידנס ו-classifier free גיידנס.
[00:01:28 - 00:01:31] ‫בסוף נעשה סיכות של כל ה...
[00:01:32 - 00:01:32] ‫כל ה-hoot.
[00:01:36 - 00:01:41] אז ה-classifier free גיידנס, מה שנגיע לבסוף, ‫זה בעצם השיטה שאיתה מגיעים ‫לתוצאות האלה,
[00:01:41 - 00:01:43] ‫שכל כך מכירים מכל
[00:01:44 - 00:01:50] המודלים שאפשר בעזרת איזשהו טקסט חופשי, ‫לייצר,
[00:01:51 - 00:01:53] ‫לשלוט בעצם בייצור של תלונות.
[00:01:55 - 00:02:03] ‫-אוקיי, אז אני מקווה שבסוף היום ‫אתם תבינו את העקרונות של איך מאמנים ‫את המודלים האלה ‫ואיך משתמשים בהם,
[00:02:04 - 00:02:07] ‫וגם בתרגיל שתקבלו,
[00:02:09 - 00:02:13] ‫אז יהיה אספקט של הדברים ‫שנדבר עליהם היום, ‫posterior sampling.
[00:02:14 - 00:02:15] ‫אני מקווה שגם,
[00:02:17 - 00:02:20] תרגיל את זה בשלבי סיום של ההכנה שלו,
[00:02:20 - 00:02:24] ‫שנצליח להגיע למצב שגם אתם מאמנים משהו
[00:02:24 - 00:02:27] ‫שהוא נשלח אל ידי טקסט, ‫פה עד שניתן את זה רק בתור בונוס.
[00:02:29 - 00:02:35] ‫אבל תראו היום לפחות העקרונות ‫שזה לא יהיה מאוד שונה ‫מהדברים שאתם כן תממשו.
[00:02:38 - 00:02:44] ‫אז שבוע שעבר דיברנו על אודי פיוז'ן ‫מכמה אספקטים שונים.
[00:02:45 - 00:02:48] ‫אספקט אחד זה סקור בייסט מודל, ‫שזה גם התחלנו בעצם לפני שבועיים.
[00:02:50 - 00:02:52] ‫דרך זה שאנחנו לומדים את הסקור, ‫אתם לומדים על זה הסקור?
[00:02:53 - 00:02:54] ‫אז נעשה לזה רענון,
[00:02:56 - 00:03:00] ‫ובעצם הכול הגיע דרך זה ‫שאנחנו רוצים לעשות סקור
[00:03:00 - 00:03:04] ‫מצ'ינג ולהשתמש אחר כך ב-Lenget in Dynamics ‫כדי לייצר דגימות.
[00:03:05 - 00:03:08] ‫דרך זה פיתחנו בעצם את המודל של דיפיוז'ן.
[00:03:09 - 00:03:18] ‫אבל שבוע שעבר ראינו שזה גם שקול ‫בעצם לגישה אחרת, ‫שהתחילה אפילו לפני זה, ‫אבל שנה אחרי המאמרים האלה,
[00:03:19 - 00:03:20] ‫הראו שזה בעצם די דומה,
[00:03:21 - 00:03:25] ‫שזה לחשוב על זה בתור משתנים, ‫מודל של משתנים חבועים.
[00:03:26 - 00:03:36] ‫זאת אומרת, כל האיטרציות של ה-Fusion מודלים, ‫אפשר לחשוב על כל האיטרציות האלה ‫בתור סדרה של inference ‫על משתנים חבועים או יצירה, ‫או כיפה אנחנו מסתכלים,
[00:03:38 - 00:03:46] ‫אבל העיקרון הוא שברגע שיש לנו ‫משתנים חבועים, ‫אנחנו משתמשים ב-Mobriational inference ו-Albo, ‫בדיוק כמו ב-VaE,
[00:03:46 - 00:03:49] ‫ומגיעים בדיוק לאותו Objective function,
[00:03:49 - 00:03:52] ‫בעצם לאותו אלגוריתם של אימון ‫ואותו דרך לייצר את הדאטה.
[00:03:55 - 00:04:00] ‫גם בקצרה דיברתי על זה ‫שאפשר לחשוב על המודל הזה ‫בתור
[00:04:01 - 00:04:08] איזשהו פתרון נומרי בזמן בדיד ‫שאיזשהו תהליך שבעצם קורה בזמן רציף,
[00:04:10 - 00:04:15] ‫ומתוך הפיתוח הזה גם עולה השימוש ‫בפסקורט,
[00:04:15 - 00:04:22] ‫והרבה מהמחקר העכשווי, ‫בעיקר על דברים כמו ‫איך להאיץ את הדגימות,
[00:04:22 - 00:04:27] ‫תהליך של יצירת הדגימות, ‫הוא מסתכל על המודלים בתור כאלה.
[00:04:28 - 00:04:31] ‫יש כל מיני שיטות שקשורות ‫למשוואות דיפרנציאליות,
[00:04:32 - 00:04:32] ‫שאפשר,
[00:04:33 - 00:04:37] ברגע שתשובים על המודל הזה ‫בתור משהו שמסתכלים על תהליך בזמן רציף,
[00:04:37 - 00:04:40] ‫אפשר להשתמש בעצם בכל המחקר ‫של משוואות דיפרנציאליות,
[00:04:40 - 00:04:42] ‫ואנחנו נכניס כל מיני שיטות ‫שהן יעצות.
[00:04:42 - 00:04:46] ‫אז זה מאוד חימושי ‫גם לחשוב על זה ככה.
[00:04:47 - 00:04:48] ‫לא נכנסת לזה יותר מדי לעומק,
[00:04:49 - 00:04:55] ‫אבל היום אנחנו כן נמשיך לדבר ‫על המודלים בתור ה-score-base model,
[00:04:56 - 00:05:00] ‫זו הגישה שלדעתי הכי קל ‫להגיע דרכה ‫לאיך בעצם אפשר להשתמש במודל.
[00:05:03 - 00:05:05] ‫אוקיי, אז תזכור את מה זה score.
[00:05:05 - 00:05:06] ‫אפשר לתזכור, זה
[00:05:09 - 00:05:11] ‫הגרדיאנט לפי X של לוק,
[00:05:12 - 00:05:14] ‫ההסתברות שלי זה המדבר שלנו.
[00:05:15 - 00:05:16] ‫פי תטא זה המודל שלנו,
[00:05:17 - 00:05:24] ‫זה בעצם אומר לאיזה כיוון אני צריך, ‫אם אני נותן איזשהו X, ‫לאיזה כיוון אני צריך לזוז ‫כדי להגדיל את הלוב
[00:05:25 - 00:05:27] ‫להסתברות שלי כמה שיותר מהר.
[00:05:28 - 00:05:30] ‫אני נותן שהמודל שלי הוא קבוע, ‫אני נותן שיש לי כבר...
[00:05:32 - 00:05:36] אוקיי, וראינו שזה בעצם, היתרון של זה זה שזה בעצם,
[00:05:37 - 00:05:41] ‫אם אנחנו חושבים על המודל ההסתברותי שלנו, ‫אותור אנרגי-בסט מודל כזה,
[00:05:41 - 00:05:47] ‫זה איזשהו מודל שאנחנו לא יודעים ‫את הנרמול שלו, ‫אנחנו לא יודעים את המקדם הנרמול שלו, ‫אנחנו רק יודעים את הפונקציה,
[00:05:48 - 00:05:50] ‫אנחנו רק לומדים בעצם ‫איזושהי פונקציית אנרגיה,
[00:05:51 - 00:05:53] ‫אז ה-score,
[00:05:54 - 00:05:58] זה בגלל שהמקדם הנרמול ‫לא תלוי ב-X,
[00:05:58 - 00:06:02] ‫המקדם הנרמול Z לא תלוי ב-X,
[00:06:02 - 00:06:03] ‫אז ה-score,
[00:06:03 - 00:06:08] שהוא נגזר לפי X, ‫לא יהיה תלוי במקדם הנרמול הזה, ‫לא יהיה תלוי ב-Z, ‫ובעצם אנחנו יכולים
[00:06:08 - 00:06:13] להסתדר גם בלמידה וגם באימון בלי בכלל לדאוג לנרמון
[00:06:15 - 00:06:17] אוקיי אז הנוסחה שאומרים כאן אם אני גוזר לפי x
[00:06:18 - 00:06:18] אז יש לי פה
[00:06:20 - 00:06:21] שני הדברים האלה
[00:06:22 - 00:06:23] נופיעים בתור שני איברים בגלל הלוג
[00:06:24 - 00:06:29] והאיבר הזה הוא לא תלוי ב-x, זה נקשר רק עם הנגזרת לפי x
[00:06:30 - 00:06:34] אוקיי זה תרשים שמלווה אותנו כבר גם בשבועיים
[00:06:34 - 00:06:35] זה השבוע השלישי
[00:06:36 - 00:06:41] הוא מאוד עוזר אני חושב אבל הוא יכול גם מאוד להטעות אז אני רוצה שוב פעם להגיד כמה דברים עליו, אחד
[00:06:45 - 00:06:47] הדאטה כאן הוא בעצם דו-ממדי
[00:06:47 - 00:06:50] אם זה תמונה זה בעצם תמונה עם שני פיקסלים
[00:06:51 - 00:06:55] וציר ה-x זה בעצם הפיקסל הראשון וציר ה-y זה הפיקסל השני
[00:06:56 - 00:07:01] כל נקודה כאן זה כאילו תמונה אחרת במרחב של כל התמונות האפשריות
[00:07:01 - 00:07:05] כאן יש לי בעצם את המרחב של כל התמונות האפשריות מבין שה...
[00:07:05 - 00:07:07] ערך של פיקסל הוא בין 0 ל-1 נגיד
[00:07:09 - 00:07:17] אז כל התמונות האפשריות נמצאות כאן אבל ההסתברות שלי היא כזאת שיש פה אזור עם תמונות יותר סבירות וכאן אזור עם תמונות יותר סבירות
[00:07:18 - 00:07:25] והסקור אומר בכל נקודה מה הכיוון שמגדיל לי את ההסתברות הזאת או את בלוג ההסתברות הזה
[00:07:26 - 00:07:32] בסדר אז מאוד קל לראות ככה את הדברים שקוראים אבל זה מאוד שונה ממה שקורה במה יותר גבוהה.
[00:07:32 - 00:07:39] בממד יותר גבוה יש כל מיני דברים שבדו-ממד נראים מאוד אינטואיטיביים ופשוטים אבל הם לא קורים שם
[00:07:42 - 00:07:47] אוקיי? אז צריך לקחת את זה ברבל מוגבל, את הוויזואליזציה
[00:07:49 - 00:08:00] אוקיי, אז הרעיון זה כשיהיה לנו score אנחנו נוכל לעשות לנג'אביב דיינאמיקס, נכון? זאת אומרת להתחיל מאיזשהו מקום התחלתי ולהתקדם בכיוון של ה-score
[00:08:01 - 00:08:03] ועם איזשהו גודל צעד,
[00:08:04 - 00:08:05] אבל גם להוסיף קצת רעש,
[00:08:07 - 00:08:07] רעש גאוסיאני,
[00:08:08 - 00:08:12] וזה הראינו עוד לתחילת הקורס שזה מובטח לנו כשזה מתכנס לדגימות
[00:08:14 - 00:08:17] מההסתברות עצמה, מההתפלגות של גיביס.
[00:08:19 - 00:08:22] למה? איך הראינו את זה? הראינו שזה סוג של דגימת גיבס,
[00:08:23 - 00:08:24] שזה סוג של MCMC,
[00:08:26 - 00:08:27] שזה שירות
[00:08:27 - 00:08:33] שמבוססות על בימה בשרשרת מרקורס, זאת אומרת, אנחנו מסתכלים רק על מה שהיה לנו קודם,
[00:08:34 - 00:08:38] ומובטח לנו שזה יתכנס לתלמות האלה.
[00:08:39 - 00:08:39] אוקיי,
[00:08:40 - 00:08:47] אינטואיטיבית אנחנו הולכים לכיוון עם ההזדברות יותר גבוהה, אבל מוסיפים קצת רעש כדי לאפשר לנו לא להיות בצווארק, לא להגיע למקסימום בדיוק,
[00:08:48 - 00:08:52] אפשר להיות בסביבה של המקסימום וגם אולי אפילו לקפוץ ממקסימום אחד לשני.
[00:08:52 - 00:09:05] אוקיי, אז איך אנחנו נאמן מודל כזה? יש לנו התפלגות של הדאטה הלא ידועה, יש לנו Training Data, שזה הרבה נקודות
[00:09:06 - 00:09:08] שמגיעות מההתפלגות הזאת,
[00:09:08 - 00:09:10] זה ה- Training Data שלנו,
[00:09:10 - 00:09:13] שוב זה בדו-מלמד, אבל בפועל כל נקודה כזאת תהיה תמונת.
[00:09:15 - 00:09:20] אנחנו רוצים איכשהו להוציא מהדבר הזה מפה כזאת,
[00:09:20 - 00:09:23] איזושהי פונקציה שתוכל לתת לנו את ה-score בכל נקודה במרחב.
[00:09:24 - 00:09:26] זאת המשימה שלנו באימון.
[00:09:30 - 00:09:34] ספציפית אפשר לכתוב את ה-objective function ככה,
[00:09:34 - 00:09:35] את השגיאה הריבועית
[00:09:36 - 00:09:40] של הנורמה של ההפרש בין
[00:09:42 - 00:09:44] הקשי הערוך שלנו
[00:09:44 - 00:09:46] ל-score האמיתי,
[00:09:47 - 00:09:49] ותוחלת על פני ההסתברות של הדאטה,
[00:09:50 - 00:09:55] זאת אומרת אנחנו רוצים לתת יותר משקל לאזורים שבהם יש הסתברות יותר גבוהה שתהיה לנו דאטה.
[00:09:57 - 00:09:58] וראינו שתי שיטות שאפשר,
[00:09:58 - 00:09:59] זאת אומרת הבעיה פה
[00:10:01 - 00:10:02] שמאזבת בתור
[00:10:03 - 00:10:06] ה-objective function שלנו, הבעיה שאין לנו גישה
[00:10:06 - 00:10:07] לסקורה האמיתית,
[00:10:08 - 00:10:11] אז איך עושים את זה? ראינו שתי שיטות, שיטה אחת
[00:10:12 - 00:10:15] היא מסתמכת על אינטגרציה וחלקים,
[00:10:16 - 00:10:25] שהוא מביא פשוט לנוסחה שאפשר להראות שהיא המקסימום שלה או המינימום שלה שווה למינימום פה,
[00:10:26 - 00:10:30] אבל זה לא סקלאבילי, זאת אומרת אנחנו צריכים לגזור הרבה פעמים,
[00:10:31 - 00:10:35] כבעצם כל צער, אנחנו צריכים לגזור את כל הרשת שלנו כפונקציה של
[00:10:35 - 00:10:37] המימד של הדאטה, של המימד של X,
[00:10:38 - 00:10:42] ושיטה יותר יעילה זה השיטה הזאת שנקראת Dinoising Score Matching,
[00:10:42 - 00:10:46] שהראינו את הפיתוח שלה עם השימוש ב-2Ds פורמולה,
[00:10:47 - 00:10:50] שבעצם אומרת ככה, אם אני לוקח דאטה, מוסיף לדאטה קצת רעש,
[00:10:52 - 00:10:54] ועכשיו אנסה לשערך את הרעש שהוספתי,
[00:10:54 - 00:10:57] זאת אומרת אני נותן לרשת שלי בתור input את הדאטה הרועש,
[00:10:58 - 00:11:02] והoutput צריך להיות הרעש או מינוס הרעש, יש פה אינפורמציות של זה,
[00:11:04 - 00:11:11] אז בעצם הובטחתי שהדבר הזה באופטימום שלו זה ה-score.
[00:11:12 - 00:11:14] אם אני ממזער את זה, מה שאני אקבל זה את ה-score function.
[00:11:18 - 00:11:23] זה יותר יעיל, מה הבעיה עם זה? שזה לא נותן לנו את ה-score function של X,
[00:11:23 - 00:11:25] אלא את ה-score function של X ועוד הרעש.
[00:11:26 - 00:11:29] יש התפלגות אחרת, התפלגות של תמונות רועשות.
[00:11:30 - 00:11:34] באופן עקרוני זה אולי לא כל כך נורא, כי אני יכול להגיד, אוקיי, אני אוסיף רעש קטן,
[00:11:35 - 00:11:37] ואז זה מאוד קרוב להתפלגות של X.
[00:11:38 - 00:11:41] כי ההתפלגות של כל התמונות של הדאטה סט שלי היא עם טיפה
[00:11:42 - 00:11:42] עש.
[00:11:43 - 00:11:45] אם אני יכול לייצר תמונות כאלה, זה מספיק טוב בשבילי.
[00:11:48 - 00:11:54] האינטואיציה של הדבר הזה זה שאם יש לנו את המרחב הזה של התמונות,
[00:11:54 - 00:11:57] אז כאן המרחב לתמונות יוצא בצורה תלת-מימדית,
[00:11:58 - 00:12:00] אם זה מה שהייתם עד עכשיו, שזה ידו-מימדי,
[00:12:00 - 00:12:02] אז אם כל התמונות האפשריות הן
[00:12:03 - 00:12:04] במרחב הזה,
[00:12:05 - 00:12:07] בעצם התמונות האמיתיות,
[00:12:07 - 00:12:10] סוג התמונות שאני רוצה לעבוד איתן,
[00:12:10 - 00:12:12] מה שאנחנו קוראים לתמונות טבעיות,
[00:12:13 - 00:12:14] או זה יכול להיות אפילו כל דומיין אחר,
[00:12:14 - 00:12:17] אבל כנראה שהדומיין, הבעיה מעניינת,
[00:12:18 - 00:12:22] התמונות שמעניינות אותי הן לא סתם נמצאות בתוך כל המרחב,
[00:12:23 - 00:12:25] אלא הן נמצאות בתוך איזשהו,
[00:12:27 - 00:12:28] זה יכול להיות תת-מרחב,
[00:12:28 - 00:12:35] או יש כל מיני דרכים לפרמל את זה, אחת מהדרכים שחושבים על זה זה של איזושהי ירייה, זאת אומרת יש פה איזושהי רציפות בין הדברים,
[00:12:35 - 00:12:36] בין הנקודות,
[00:12:37 - 00:12:41] אבל היריעה הזאת היא בעצם באיזשהו ממד פרק קטן, זאת אומרת
[00:12:41 - 00:12:45] רוב הכיוונים, אם אני מתחיל מאיזושהי נקודה,
[00:12:45 - 00:12:58] רוב הכיוונים שבהם אני אזוז בצורה רנדומלית בנקודה הזאת יוציאו אותי מהיריעה, יובילו אותי למקום שבו ההזדברות מאוד קטנה, זאת אומרת משהו שנראה ממש לא כמו תמונה,
[00:12:59 - 00:13:01] אבל יש כמה כיוונים שאם אני אזוז בהם,
[00:13:02 - 00:13:05] אני כן אשאר בתוך המרחב הזה של התמונות הטובות.
[00:13:07 - 00:13:10] ואם זה באמת המצב, אז מה קורה כשאנחנו מוסיפים
[00:13:11 - 00:13:12] פעם רעש רנדומלי,
[00:13:12 - 00:13:16] רוב הסיכויים יוצאים קצת מהיריעה הזאתי,
[00:13:17 - 00:13:21] ואז בעצם ההתפלגות, ההסתברות שלנו דורכת מאוד מהר לאפס, כמו שאנחנו מודלחקים מהיריעה.
[00:13:22 - 00:13:26] יכול להיות שבתוך העירייה גם יש איזשהו משחק של
[00:13:27 - 00:13:28] מה יותר סביר ממה,
[00:13:29 - 00:13:35] אבל מה שהאינטואיציה הזאת אומרת זה שברגע שאני יוצא מהיריעה, אז ההסתברות מאוד מהר עוברת לאפס.
[00:13:36 - 00:13:38] ולכן אם אני קצת אצא מהיריעה, הכיוון
[00:13:39 - 00:13:48] שבו אני יכול להגדיל את ההסתברות הכי מהר, זה פשוט לחזור כמה שיותר מהר ליריעה מהנקודה שממנה יצאתי.
[00:13:48 - 00:13:55] אז זה בעצם האינטואיציה של למה מינוס הרעש זה הסקופט, זה הכיוון שבו אני אגדיל את ההתפלגות
[00:13:56 - 00:13:58] בצורה הכי מהירה.
[00:14:03 - 00:14:05] שוב זה מדגיש את ה...
[00:14:07 - 00:14:11] לא שוב, לא אמרנו את זה קודם, מה הבעיה עם כל הפומולציה הזאת?
[00:14:12 - 00:14:18] שתזכור שאנחנו רוצים לעשות management dynamics, אנחנו רוצים להתחיל מאיזושהי נקודה עקרית ואז להגיע
[00:14:19 - 00:14:20] לתמונה
[00:14:20 - 00:14:22] סבירה, נכון? לתמונה שנראה טוב.
[00:14:23 - 00:14:28] אם אני מתחיל מנקודה דומה לתמרחב הזה, רוב הסיכויים שאני לא אהיה קרוב לעירייה,
[00:14:29 - 00:14:31] זה באיזשהו מקום מאוד רחוק.
[00:14:32 - 00:14:34] ואם ההנחה שלי זה שברגע שאני יוצא מהעירייה,
[00:14:35 - 00:14:37] אז מאוד מהר ההסתברות יורדת לאפס,
[00:14:39 - 00:14:40] אז כל האזור הזה, זה לא כזה משנה הכיוון
[00:14:41 - 00:14:42] שבו זה גדל, זה מאוד רחוק.
[00:14:43 - 00:14:44] העירייה, אין פה הכיוון, אין לו משמעות,
[00:14:45 - 00:14:47] אולי הוא אפילו לא מוגדר, אולי הכל שם ממש אפס.
[00:14:48 - 00:14:59] אז איך אני אגיע למצב שאני מתחיל מנקודה רנדומלית ומצליח למצוא את הכיוון שיוביל אותי חזרה לעירייה?
[00:15:05 - 00:15:11] ‫-תרונות שהן לא מתחילות, אם רוצים שמותן הסתברות של סרטויות במודלית.
[00:15:11 - 00:15:14] ‫אז אין לנו גישה להסתברות האמיתית של התרונות,
[00:15:15 - 00:15:18] ‫אומפרית במובן הזה זה קצת קשה להגיד, אבל
[00:15:21 - 00:15:25] אפשר אולי להראות שבהינתן איזשהו מודל, ההסתברות שלו
[00:15:26 - 00:15:33] היא כזאתי שיש כיוונים מאוד ספציפיים שמשאירים את ההסתברות גבוהה,
[00:15:33 - 00:15:38] וסתם כיוון רנדומלי מוריד בגדול, מוריד בהרבה את ההסתברות
[00:15:39 - 00:15:42] וסתם אינפוט שתיתן למודל הזה, שאתה גם יקבל
[00:15:46 - 00:15:54] הסתברות גבוהה. למשל אפשר לעשות איזשהו ניסוי שאתה נותן איזושהי תמונה רנדומלית ומסתכל על כל הגרדיאנטים וכל הכיוונים
[00:15:55 - 00:16:02] ולעומת זה שאתה נותן נקודה שהיא קרובה לתמונה אמיתית ומסתכל על הגרדיאנטים ומה שצפוי לראות,
[00:16:02 - 00:16:10] מה שהייתי מצפה לראות זה שאם את בתמונה רנדומלית כל הגרדיאנטים יהיו מאוד קטנים ולא יהיה איזשהו כיוון שהוא יותר טוב מהשני,
[00:16:11 - 00:16:15] לעומת זה אם אתה קרוב ליריעה אז יהיה כיוון אחד שיהיה הרבה יותר טוב מכל הכיוונים
[00:16:17 - 00:16:26] אבל לא עשיתי אף פעם ניסוי פרסום, בוודאי זה התוצאות שיהיו כיוונים, אז איך אפשר לפתור את זה? אז מה שראינו בשבוע שעבר זה בעצם,
[00:16:27 - 00:16:30] זה עוד פעם להראות את הבעיה הזאת עכשיו בדו-ממד,
[00:16:30 - 00:16:36] אם קיבלנו הרבה נקודות מסביב כאן היריעה הזאת זה בעצם שתי הנקודות האלה,
[00:16:36 - 00:16:40] זה לא בדיוק ירייה רדומה הזאת אבל עדיין יש אזורים שבהם
[00:16:40 - 00:16:48] הנקודות הן יותר סבירות, נקודות מאוד ברורות ויש אזורים שבהם ההסתברות היא מאוד קרובה לאפס
[00:16:49 - 00:16:51] היא לא נותנת לנו הרבה מידע,
[00:16:51 - 00:16:57] אז בעצם אם נלמד בצורה שחשבנו אנחנו נדגום נקודות מההתפלגות האמיתית,
[00:16:57 - 00:17:02] נקבל נקודות פה וקצת פה, נוסיף להם טיפה רעש ונפחק רק בקצת
[00:17:03 - 00:17:05] מהאזורים עם ההסתברות הגבוהה
[00:17:05 - 00:17:10] ואז נאמן את המודל שלנו,
[00:17:10 - 00:17:12] נשארך את המינוס של הרעש הזה,
[00:17:12 - 00:17:20] אז הוא יהיה מאוד טוב באזורים האלה כי פה הוא גם קיבל דאטה וגם פה הסיגנל מאוד ברור, זאת אומרת אם אני פה זה בכיוון הזה, אם אני פה זה בכיוון הזה,
[00:17:20 - 00:17:26] אז זה יהיה לו מאוד ברור וקל למדל את זה אבל בכל שער המרחב
[00:17:27 - 00:17:34] הוא גם לא יקבל מספיק נקודות וגם לא בטוח שזה יהיה מוגדר בצורה כזאתי, שיהיה לו קל לעשות את הפרדיקציה
[00:17:36 - 00:17:37] בכיוון הנכון, זאת אומרת זה יהיה
[00:17:38 - 00:17:40] הרבה פחות יציב בסיגנל עכשיו
[00:17:40 - 00:17:44] וגם אם הוא מצליח, יש אזורים שהסיגנל מאוד קרוב לאפס
[00:17:45 - 00:17:50] ואז בעצם לא בטוח שאפשר לעשות משהו עם ג'ווין דיינמיוס בצורה יעילה
[00:17:51 - 00:17:53] אוקיי אז איך פותרים את כל הבעיות האלה?
[00:17:54 - 00:18:05] אז בעצם אם אני, כל הבעיות האלה נפתרות, אם מראש המודל ההסתברותי שלי הוא לא כזה שהוא מרוכז באיזושהי יריעה אלא הוא מפוזר על כל המרחב בצורה טובה
[00:18:05 - 00:18:08] ודרך להשיג את זה זה פשוט אם אני מוסיף הרבה רעש,
[00:18:09 - 00:18:13] אז תזכרו שהמודל שאני ממדל זה בעצם לא את המודל של הדאט האמיתי אלא את המודל פלוס הרעש
[00:18:13 - 00:18:15] אז אם אני אוסיף הרבה רעש
[00:18:15 - 00:18:23] אני אגיע למצב שהמודל שלי הוא טוב מבחינת היציבות והאימון שלו כי הוא יכסה בערך את כל המרחב
[00:18:24 - 00:18:26] ובצורה מספיק טובה
[00:18:29 - 00:18:33] ואם אני אדגום ממנו אני אקבל דוגמאות שמייצגות את כל המרחב
[00:18:34 - 00:18:37] והוא יהיה מוגדר היטב בכל המרחב ויהיה קל גם
[00:18:38 - 00:18:44] ללמוד את ה-score בכל נקודה וגם להשתמש בה כדי לעשות מנג'ימינג בינאנטס
[00:18:46 - 00:18:47] אוקיי אז זה יפתור את
[00:18:48 - 00:18:51] כל הבעיות חוץ מזה שזה לא יהיה מה שיקרה בסוף זה לא יהיה
[00:18:51 - 00:18:54] התפלגות של מה שאני מטפס
[00:18:55 - 00:19:01] זה כבר לא יהיה x פלוס קצת רעש, זה יהיה x פלוס הרבה רעש, זה יהיה מאוד שונה מהתפלגות של תמונות
[00:19:02 - 00:19:07] אז זה בעיה אבל הפתרון של זה זה לעשות את זה בצורה הדרגתית
[00:19:09 - 00:19:10] אז להתחיל מהרבה רעש
[00:19:11 - 00:19:13] אם יש לו מודל שיודע להתמודד עם זה
[00:19:14 - 00:19:16] מודל שיודע להתמודד עם קצת פחות רעש
[00:19:16 - 00:19:22] ופחות ופחות רעש עד שהוא מגיע כמעט למודל של x
[00:19:22 - 00:19:23] של הדאטה
[00:19:24 - 00:19:25] האמיתית
[00:19:25 - 00:19:27] הוא דאטה אמיתית עוד טיפה רעש
[00:19:27 - 00:19:29] אז נוכל להשתמש,
[00:19:29 - 00:19:32] כשנרצה לעשות דגימה נוכל קודם להתחיל מפה
[00:19:33 - 00:19:36] ולעשות קצת כמה צעדי לנדמי דמיין פה,
[00:19:36 - 00:19:38] אחר כך פה וככה עד שאנחנו נגיע
[00:19:40 - 00:19:44] למודל ההתפלגות שהיא הכי קרובה להתפלגות של הדאטה שלי
[00:19:47 - 00:19:54] אוקיי, נדלג על זה הייתי צריך לציין את זה קודם אולי, בעצם
[00:19:55 - 00:19:58] מה שזה, איך זה ייראה, זה ייראה ככה, אני אתחיל
[00:19:58 - 00:20:01] אני אעשה לנדלמי דמי המצפות ובסיום השלב הזה
[00:20:04 - 00:20:09] אני יכול להניח שהדגימות שלי כבר יהיו די קרובות
[00:20:11 - 00:20:12] למקסימומים של האזורים האלה
[00:20:14 - 00:20:15] ולכן אם אני קצת מוריד את הרעש
[00:20:16 - 00:20:20] אני עדיין אהיה במצב שהדגימות שלי יהיו במקום שמוגדר היטב
[00:20:21 - 00:20:24] ואז אני אוכל לעשות פה כמה צעדי אנג'ווינט דייננדיקס
[00:20:24 - 00:20:37] עוד פעם, אני יכול להניח שאני נמצא באזור הזה, שכאן כבר נמצא באזור שאני יכול לעשות את זה, יחסית, מספיק קרוב ליריעייה הזאתי כדי שאני אוכל לעשות אנג'ווינט דייננדיקס יעיל
[00:20:37 - 00:20:39] שיוביל אותי חזרה למבחרתו
[00:20:41 - 00:20:45] אוקיי, אז זו התוכנית והדרך שבדרך כלל מממשים את זה זה שיש לי,
[00:20:45 - 00:20:54] אין לי רשת לכל רעש אחר, אלא יש לי פשוט רשת אחת, זאת אומרת הרשת זה הדבר הזה שאני מכניס לו
[00:20:55 - 00:20:56] את התמונה המורשת
[00:20:58 - 00:21:00] והיא עושה פרדיקציה לרעש
[00:21:01 - 00:21:05] אז לא יהיה לי אחד כזה לכל רמת רעש, אלא יהיה לי מודל אחד
[00:21:06 - 00:21:09] שפשוט בתור אינפוטגם מקבל את רמת הרעש
[00:21:10 - 00:21:13] שיש במודל, שיש בה
[00:21:14 - 00:21:17] ההסתברות הנוכחית, בדאטה הנוכחית שהוא מקבל
[00:21:17 - 00:21:19] בעצם זה אומר לו שהדאטה שהוא קיבל
[00:21:20 - 00:21:24] הוא דאטה של X ועוד איזושהי סיגמה מסוימת
[00:21:28 - 00:21:35] והאימון פשוט נעשה בצורה כזאת שאני באופן רנדומלי, כמו שאני עושה סטוקסטי גרדיאנט דסנט על הדאטה עצמו,
[00:21:36 - 00:21:38] על ה-X שאני לוקח מהדאטה,
[00:21:38 - 00:21:39] מהדאטה אני גם אעשה,
[00:21:40 - 00:21:41] אדגום בצורה אקראית
[00:21:42 - 00:21:45] את הרמת רעש שאני כרגע מסתכל עליה
[00:21:47 - 00:21:48] זה אני רוצה מודל שהוא יהיה טוב לי
[00:21:49 - 00:21:52] לכל ה-Xים, לכל הרעשים ולכל הרמות רעש
[00:21:53 - 00:21:55] אז אני פשוט כל פעם דוגם אקראית
[00:21:56 - 00:21:56] אחד מכל אחד
[00:21:58 - 00:22:01] דוגם X מהדאטה, דוגם את הרעש הספציפי שאני אוסיף
[00:22:01 - 00:22:03] ודוגם את הרמת רעש
[00:22:04 - 00:22:05] ואז אני
[00:22:06 - 00:22:07] את האינפוט, יוצא האינפוט,
[00:22:07 - 00:22:10] דגמתי את זה, דגמתי את זה ודגמתי את זה,
[00:22:10 - 00:22:11] יוצא את החישוב הזה,
[00:22:12 - 00:22:14] זה נותן לי את האינפוט שאני נותן למודל,
[00:22:14 - 00:22:15] פלוס הסיגמא גם
[00:22:16 - 00:22:19] והארטפוט צריך להיות שווה לסיגמא.
[00:22:25 - 00:22:27] עוד דבר אחד זה שבדרך כלל לא,
[00:22:27 - 00:22:32] אני חושב שבמאמרים הראשונים כן היו עושים כמה צעדים בכל אחת מהרמות,
[00:22:33 - 00:22:38] עכשיו הסטנדרט זה שיש ממש איזשהו רצף של סיגמאות בערך אלף
[00:22:39 - 00:22:41] בכל צעד הסיגמא משתנה
[00:22:42 - 00:22:42] וקצת
[00:22:42 - 00:22:46] וכל פעם עושים רק צעד אחד של מנג'מינד אמנט
[00:22:47 - 00:22:54] אבל זה דברים שאפשר לשחק איתם והמוזל הסטנדרטי גם מה שאני מבקש שתממשו יהיה כזה
[00:22:55 - 00:22:59] וכל פעם עושים, כל פעם משנים וקצת את הסיגמא
[00:23:00 - 00:23:02] וקצת את ה... ועושים צעד אחד של מנג'מינד
[00:23:05 - 00:23:07] אין פרנסיפון לא באימון
[00:23:07 - 00:23:15] זה באימון, בסדר, באימון גם ככה זה לא, אתה לא באמת עושה את התהליך של המנג'ס זה כל פעם דוגם רק שחק
[00:23:21 - 00:23:24] אוקיי, זה סוג ההצעות אני יכול לקבל
[00:23:26 - 00:23:28] זה אני חושב מ...
[00:23:29 - 00:23:30] נוצרות מ-2019
[00:23:39 - 00:23:41] וזה אלגוריתמים, זה מאוד מאוד פשוט
[00:23:45 - 00:23:46] בוא נסתכל רגע על שניהם
[00:23:48 - 00:23:50] אז זה אלגוריתם אימון וזה אלגוריתם גימה
[00:23:52 - 00:23:58] יש פה כמה בניות, בטא, אלפא ואלפא בר
[00:23:59 - 00:24:03] אתם רואים משם באף פעם עם קו כזה למעלה
[00:24:04 - 00:24:07] בטא זה פשוט הרע שאנחנו מוסיפים
[00:24:08 - 00:24:11] בין הצעד ה-XT ל-XT
[00:24:12 - 00:24:14] בין ה-XT מיוס אחד ל-XT
[00:24:14 - 00:24:15] זה בטא-P
[00:24:16 - 00:24:17] זאת אומרת
[00:24:20 - 00:24:22] נוספה היא ש-XT שווה
[00:24:29 - 00:24:32] כלומר, אנחנו לוקחים אותו
[00:24:36 - 00:24:51] שורש בטא זה ה-XT, שורש בטא זה סטיית תקן, בטא זה הווריאנס
[00:24:52 - 00:24:53] כפול הרעש
[00:24:54 - 00:25:13] ה-XT זה ה-XT, זה לא הרעש, חשבו ש-T הוא נגיד אחרי הרבה צעדים
[00:25:14 - 00:25:17] זה לא יהיה הרעש הכללי שיתווסף על מקום
[00:25:18 - 00:25:20] X זה X המקורי, זה X0
[00:25:21 - 00:25:24] אלא זה רק כמה רעש הוספנו בין הצעד ה-T מינוס אחד לצעד ה-XT
[00:25:29 - 00:25:31] אוקיי, אז איך ה...
[00:25:32 - 00:25:33] אז זה בינתיים, אוקיי?
[00:25:34 - 00:25:37] אלפא זה פשוט יותר נוחה בכמה פעמים לעבוד במקום עם בטא, הם אחד מינוס בטא
[00:25:38 - 00:25:40] קוראים לזה אלפא
[00:25:40 - 00:25:42] אלפא זה אחד מינוס בטא
[00:25:43 - 00:25:45] ואלפאתי כובע, זה לא כובע,
[00:25:46 - 00:25:50] בר זה המכפלה של כל האלפות עד עכשיו
[00:25:55 - 00:26:02] אז איך האימון מתבצע? אז ככה אנחנו לוקחים X0 מתוך ה-training set שלנו
[00:26:03 - 00:26:08] למה הרבה פעמים קוראים לזה Q? זה לקחתי מהמאמר הזה מ-2020
[00:26:10 - 00:26:11] למה קוראים לזה Q?
[00:26:11 - 00:26:15] כמו שבריאל יש לנו לא טוב עם קוד, נכון, אז כשחושבים על זה כבר יש לנו לא טוב עם קוד
[00:26:15 - 00:26:19] אז בעצם הצעד הזה שמוסיף את הרעש זה בעצם האינפרנס
[00:26:20 - 00:26:25] הצעד שמתוך הדאטה עושה פרדיקציה ל-Latent Variables שלנו
[00:26:26 - 00:26:27] אבל בניגוד ל-BA הוא לא נלמד,
[00:26:28 - 00:26:30] הוא קבוע מראש
[00:26:32 - 00:26:34] אבל עדיין הרבה פעמים קוראים לזה Q,
[00:26:34 - 00:26:39] אוקיי, אז X0 נדגה מתוך ה-training set שלנו,
[00:26:39 - 00:26:41] Q של X0 זה ה-training set,
[00:26:42 - 00:26:45] T נדגה בצורה אוניפורמית זה המיקום שלנו בצעד
[00:26:45 - 00:26:46] זה שקול בעצם
[00:26:49 - 00:26:50] לתוחלת על הסיגמא פה,
[00:26:51 - 00:26:56] אני בעצם דוגם איפה אני, בין 1 ל-L שם במקרה T
[00:27:00 - 00:27:02] איזה צעד אני?
[00:27:03 - 00:27:04] Epsilon זה פשוט הקדימה של הרעש
[00:27:05 - 00:27:09] עכשיו, ברגע שיש לי את שלושת הדברים האלה
[00:27:09 - 00:27:10] אז אני יכול לחשב
[00:27:11 - 00:27:11] את
[00:27:12 - 00:27:23] הסיגנל, את הדאטה הרועש, נכון? אז זה יהיה X0 כפול איזשהו מקדם, אוקיי, זה לא בדיוק כתוב פה, כי פה זה הקפיצה מ-XT מינוס 1 ל-XT,
[00:27:23 - 00:27:26] כאן כתוב הקפיצה מ-X0 ל-XT,
[00:27:27 - 00:27:28] אוקיי, זו טיפה שונה הנוסחה,
[00:27:29 - 00:27:32] אז X0 כפול איזשהו מקדם,
[00:27:32 - 00:27:36] ועוד הרעש כפול איזשהו מקדם, כשהמקדמים האלה הם תלויים ב-T,
[00:27:37 - 00:27:39] אוקיי, ב-T הזה שדגמנו פה
[00:27:39 - 00:27:41] ו-T,
[00:27:42 - 00:27:49] אוקיי, זה בעצם המידע שאני נותן למודל על כמה כמות רעש 800 ספות,
[00:27:51 - 00:27:59] ופשוט מחשבים את הנורמה בריבוע של השגיאה, בין זה ל-Ground Truth של הרעש שבאמת הוספתי,
[00:28:00 - 00:28:02] וגוזרים את כל זה לפי תטא,
[00:28:02 - 00:28:04] ומה שייגזר פה זה הרשת הזאת,
[00:28:05 - 00:28:06] יש הפער מטרים תטא,
[00:28:06 - 00:28:11] ואם נקבל נגזר של תטא, נעשה gradient descent, או אדם,
[00:28:12 - 00:28:13] או כל שאלות.
[00:28:14 - 00:28:17] אז תסביר לנו כמה אנחנו מפעילים בשור שלך ב-T,
[00:28:18 - 00:28:19] או עד פה?
[00:28:19 - 00:28:20] פה?
[00:28:20 - 00:28:21] כן.
[00:28:21 - 00:28:25] אז הסיבה העיקרית זה שרוצים שהכל לדעתה יישאר באותה סקאלה,
[00:28:27 - 00:28:33] וגם שזה לא ישנה כמות הצעדים, שאפשר להשחק עם זה אולי בזמן האימפרנס,
[00:28:34 - 00:28:35] לעומת בזמן הטרנינג,
[00:28:35 - 00:28:38] מה שקורה כשאתה עושה ככה, אז בעצם ה-Variance של הדעתה נשאר אותו דבר
[00:28:40 - 00:28:48] זה תראי איך ביטא-T נקבע, אבל אתה יכול לגרום למצב כזה שהדאטה שלך תמיד מסדר גודל בין 0 ל-1,
[00:28:48 - 00:28:51] או באמת שהדרך הסטנדרטית לבין זה זה בין מינוס 1 ל-1
[00:28:52 - 00:28:59] אבל וריאנס של תפקע מההכפרה ב-F זה נקבע כי בהכפרה שבמונדם אנחנו מפעילים ב-XD זה גם משווי על הווריאנס הזה?
[00:29:00 - 00:29:03] כן, כי אם תיקח תמונה ובמשך אלף פעמים תוסיף לרעש,
[00:29:04 - 00:29:06] אז הערכים ילכו ויגדלו
[00:29:08 - 00:29:15] או יגדלו, זאת אומרת אם אתה תקבל יהיו פיקסלים שכל הזמן קיבלו נגיד ערכים חיוביים,
[00:29:15 - 00:29:15] יקבלו,
[00:29:16 - 00:29:17] יקבלו ערך מאוד סגורם
[00:29:22 - 00:29:24] אתה גם רוצה שבסוף לא יהיה לך שום,
[00:29:24 - 00:29:26] בסוף אתה רוצה להגיע למצב שיש רק רעש
[00:29:28 - 00:29:31] ואם המספר הזה הולך ומתקרב לאחד,
[00:29:32 - 00:29:33] זאת אומרת שהמספר הזה הולך ומתקרב לאפס,
[00:29:34 - 00:29:36] אז סוף לא סוף פגיזוג כמו שאמרתי בינתיים.
[00:29:36 - 00:29:37] כן, בדיוק.
[00:29:40 - 00:29:45] אני חושב שאפשר להגיע לבחינת תאורטית לאותן תוצאות בלי
[00:29:46 - 00:29:47] ההגידה הזאת,
[00:29:47 - 00:29:49] אם אתה נוסיף מספיק רעש
[00:29:50 - 00:29:52] אז בעצם כבר אין סיגנל, גם אם
[00:29:52 - 00:29:56] נגדלת והווריאנסים שלך גדלים בזמן התהליך,
[00:29:57 - 00:29:59] זה לא סותר את כל התיאוריה הזאת,
[00:29:59 - 00:30:03] יותר נוח לעשות את הפורמולציה ככה שהכל נשאר בית גדול בין אוניברסיטת אחד
[00:30:06 - 00:30:08] אוקיי אז זה האימון
[00:30:09 - 00:30:14] ברגע שיש לנו, אימנו את זה, אז יש לנו את הרשת הזאת שיודעת לשערך את הרעש,
[00:30:15 - 00:30:17] אנחנו יכולים לעשות פשוט Langerine Dynamics
[00:30:18 - 00:30:21] Langerine Dynamics אנחנו מתחילים מהדאטה הכי רועש, זאת אומרת מ-P גדול
[00:30:22 - 00:30:26] והחישוב שלנו הוא פשוט החישוב הזה, זאת אומרת אנחנו לוקחים
[00:30:27 - 00:30:30] X-1, אם אתם זוכרים אנחנו מניחים בעצם
[00:30:38 - 00:30:41] שה... איך שזה כתוב כאן זה פשוט לנדלמיד דיינמיקס, נכון? אז זה ה-score,
[00:30:42 - 00:30:52] זה יהיה ה-score וזה יהיה ה-Rעש שמתווסף לנו,
[00:30:53 - 00:30:55] ויש כאן את המקדמים האלה שהם,
[00:30:55 - 00:31:01] בינתיים לדלמיקס אין לנו מקדם כאן, יש לנו רק מקדמים על הרעש ועל ה-score,
[00:31:02 - 00:31:08] אבל זה תלוי, זה קשור באמת לעניין הזה שהדאטה עצמו הולך ודועך בכל איטרציה,
[00:31:08 - 00:31:09] אז פה אנחנו צריכים בעצם להגדיל אותו,
[00:31:10 - 00:31:13] להגדיל קצת מה שאנחנו חושבים שהוא XT,
[00:31:14 - 00:31:16] וזה האחד חלקי קשורש הלפתי הזה.
[00:31:17 - 00:31:23] אוקיי, אז מה קורה פה אבל? אנחנו בגדול מסתכלים על הדגימה,
[00:31:24 - 00:31:26] את הצעד הקודם שלנו,
[00:31:26 - 00:31:29] מוסיפים לה קצת מה-score,
[00:31:30 - 00:31:31] אז כאן אנחנו פורמולציה שזה Epsilon,
[00:31:32 - 00:31:35] זה הרעש, אנחנו צריכים להוריד את זה
[00:31:36 - 00:31:36] מהרעש
[00:31:37 - 00:31:39] ולהוסיף עוד קצת רעש.
[00:31:40 - 00:31:43] יש קצת חופש פה איך בוחרים את הסיגמה, זה האמת שלא לקחתי כאן,
[00:31:45 - 00:31:48] בדרך כלל הסיגמה שבוחרים הוא, אני חושב, פשוט בית דתי.
[00:31:49 - 00:31:52] לא יכול להיות שיש פה מיני גרסאות של זה,
[00:31:52 - 00:31:56] אני כבר רגע לא זוכר, אבל לדעתי הסיגמה היא הכי
[00:31:57 - 00:31:59] סיגמרטי, זה פשוט בית דתי.
[00:32:00 - 00:32:01] מה נושא לזה רעש?
[00:32:03 - 00:32:05] זה ה-Lenjo-Dynamics, ככה עושים ל-Lenjo-Dynamics,
[00:32:05 - 00:32:10] בכל צעד אתה מוסיף לקדם לפי ה-score ואתה מוסיף קצת רעש.
[00:32:12 - 00:32:14] שוב, יש שתי דרכים לחשוב על זה, Lenjo-Dynamics או
[00:32:15 - 00:32:20] מה שהיא הגישה של ה-Varational inference ו-Varational auto-Encoder,
[00:32:21 - 00:32:22] אתה כל פעם עושה
[00:32:22 - 00:32:23] פרדיקציה
[00:32:24 - 00:32:25] של ה-Latent הבאה
[00:32:28 - 00:32:30] ואת ה-Separtment-פילימציה של זה בתור גרסיאן,
[00:32:31 - 00:32:32] ואז אתה דוגם
[00:32:34 - 00:32:37] איך אתה דוגם, נדלך הרבה LATENSE
[00:32:39 - 00:32:40] 1,
[00:32:40 - 00:32:42] 2, מודל גרפיק כזה
[00:32:44 - 00:32:44] עד X
[00:32:46 - 00:32:49] אתה רוצה לדגום את X, אתה דוגם את זה, דוגם את זה ויינתן את זה ויינתן את זה
[00:32:50 - 00:32:55] אם אתה מניח שכל הדבר הזה הוא גרסיאן אז בעצם אתה צריך למצוא את התוחלת של הגרסיאן הזה
[00:32:56 - 00:32:59] ואת ה-Varions, ולהוסיף
[00:33:00 - 00:33:02] רעש מפול השורש של ה-Varions
[00:33:05 - 00:33:09] באמת אתה יכול לחשוב על הדבר הזה בתור התוחלת של הגרסיאן הזה
[00:33:14 - 00:33:15] אוקיי, בואו ניתן ברור?
[00:33:16 - 00:33:16] אוקיי
[00:33:20 - 00:33:22] איך ה-Epsilon תטא הזה מוגרש?
[00:33:25 - 00:33:27] אז קוראים לזה לפעמים Epsilon תטא ולפעמים S תטא
[00:33:28 - 00:33:31] בעיקרון אמור, Epsilon תטא זה אמור להיות השיעור של הרעש
[00:33:31 - 00:33:33] S תטא זה השיעור של ה-score
[00:33:38 - 00:33:39] נגדי של השני,
[00:33:39 - 00:33:41] אבל אני לא בטוח שתמיד מקפידים על זה,
[00:33:41 - 00:33:43] וצריך לשים לב לענייני הסימן
[00:33:45 - 00:33:49] אוקיי, אז זה בעצם מודל שהוא צריך להיות,
[00:33:49 - 00:33:51] בעיקרון יש לנו חופש לפתור כל מודל,
[00:33:51 - 00:33:55] זה מודל שצריך לקבל בתור input תמונה מורשת והoutput
[00:33:56 - 00:33:56] הוא הרעש
[00:33:57 - 00:34:00] אוקיי, אז ה-input הוא תמונה מורשת, וגם עוד איזשהו סקלר
[00:34:01 - 00:34:02] אומר לנו כמה רעש יש שם,
[00:34:03 - 00:34:04] והoutput צריך להיות רעש
[00:34:04 - 00:34:07] אוקיי, אז אפשר לחשוב על הרבה סוגי מודלים
[00:34:07 - 00:34:15] באופן כללי מודל סטנדרטי שמשתמשים בו כשה-input וה-output הם באותו גודל, הם רואים תמונות
[00:34:15 - 00:34:17] זה מודל שנקרא unit
[00:34:18 - 00:34:21] ראיתם את זה ב-deep learning?
[00:34:26 - 00:34:29] אז זה בעיקרון מודל שהוא כמו המודל,
[00:34:30 - 00:34:33] זאת אומרת החלק הראשון שלו הוא כמו רשת קונבולוציה שהיא הולכת וקטנה,
[00:34:34 - 00:34:36] יש כל מיני דרכים להקטין,
[00:34:36 - 00:34:38] למשל אפשר לעשות פולינג או
[00:34:39 - 00:34:43] או סטרייד, זאת אומרת לדלג מעל יותר מפיקסל אחד כל פעם,
[00:34:44 - 00:34:45] ולאט לאט זה הולך וקטן,
[00:34:45 - 00:34:50] אבל אחר כך עושים את התהליך ההפוך, זאת אומרת, במשלב מסוים מגדילים את זה בחזרה,
[00:34:51 - 00:34:55] זאת אומרת ההלכתות של כל קונבולוציה יהיה כמה פיקסלים,
[00:34:56 - 00:34:57] אנחנו נגדיל בחזרה את
[00:35:00 - 00:35:04] התמונה הזאת שיוצאת, את האקטיבציות שיש בין כל שכבה לשכבה,
[00:35:06 - 00:35:14] ועוד משהו שעושים זה משתמשים בטריק של רזנט, אבל שמסתכל על הגודל של כל שכבה,
[00:35:14 - 00:35:15] זאת אומרת,
[00:35:15 - 00:35:19] אנחנו בכל שכבה אנחנו לא, החישוב שאנחנו עושים זה יהיה תוספת
[00:35:20 - 00:35:22] לאיזשהו חישוב שעשינו קודם,
[00:35:22 - 00:35:27] והתוספת הזאת מגיעה מ... זאת אומרת שזה לא החצים האלה, זה לא מצויר פה, זאת אומרת
[00:35:28 - 00:35:32] מה שקורה זה שהאאוטפוט של זה יהיה שווה לתוצאה של החישוב,
[00:35:33 - 00:35:33] ועוד
[00:35:34 - 00:35:35] לתוצאה של החישוב שיצאה פה.
[00:35:36 - 00:35:41] והאאוטפוט של השכבה הזאת תהיה שווה לתוצאה של החישוב,
[00:35:41 - 00:35:44] וגם לפה, פלוס האאוטפוט שהיה כבר פה.
[00:35:45 - 00:35:53] כן, כך כל אחד מקבל בעצם גישה ישירה לאיזושהי שכבה יותר מוקדמת
[00:35:55 - 00:36:02] וזה הוכח כמשהו שעובד מאוד טוב עם מודלים שכאלה שצריך לתת אותו אינקוט תמונה ואאוטקוט תמונה, למשל אנחנו רוצים לעשות סגמנטציה נגיד
[00:36:03 - 00:36:07] זה פותח לראשונה אני חושב בשביל סגמנטציה של דאטה רפואית
[00:36:08 - 00:36:11] אוקיי, מקבלים תמונה ורוצים שאאוטקוט תהיה תמונה שאומרת
[00:36:11 - 00:36:17] איפה נמצאים כל מיני אזורים מעניינים בתמונה מבחינה רפואית
[00:36:18 - 00:36:21] וזה הוכחתי מאוד יעיל כי בעצם יש איזושהי דרך מהירה לעשות
[00:36:23 - 00:36:27] עיבוד של דברים במרחב כבר הגדול של התמונה
[00:36:29 - 00:36:34] ובדרך קצת יותר מורכבת שלוקחת תמונה, מכווצת אותה קצת
[00:36:35 - 00:36:38] ואז עושה שם איזשהו חישוב מעניין,
[00:36:39 - 00:36:41] אז מחזירה אלה שוב למרחב התמונה
[00:36:42 - 00:36:47] וכאילו הדברים הכי מעניינים כשמסתכלים על הדברים הגלובליים של התמונה הם קוראים,
[00:36:47 - 00:36:48] יפגשו באמצע
[00:36:48 - 00:36:50] אבל לא הכל חייב לעבור בערך החוטמית,
[00:36:51 - 00:36:53] יש הרבה דברים שכל אם פשוט תדלג ולעשות חשוב
[00:36:54 - 00:36:58] אבל שוב זה בעצם לא קשור לדיפיוז'ן מודל,
[00:36:59 - 00:36:59] זה פשוט
[00:37:00 - 00:37:02] כמעט תמיד ממומש דיפיוז'ן מודל על ידי יוניט אבל
[00:37:03 - 00:37:08] אפשר לבחור כל דבר אחר ויש מודלים שמשתמשים בטרנספורמרים וכל מיני דברים אחרים
[00:37:09 - 00:37:11] אוקיי אז זה לגבי העיבוד של התמונה אבל יש פה
[00:37:11 - 00:37:16] עוד איזושהי תוספת שאנחנו רוצים גם לקבל את הסקלר ולהכניס אותו, אז גם פה יש
[00:37:18 - 00:37:21] פה אין איזה משהו סדר, אבל יש הרבה טריקים לעשות את זה,
[00:37:21 - 00:37:23] למשל לעשות קודם איזשהו עיבוד
[00:37:26 - 00:37:41] יש כל מיני שיטות של להכניס כל מיני מידע אפילו שהוא לא נלמד דברים שקשורים לפוזיציונל קודינגס למי שמכיר אבל תחשבו על זה בתור איזשהו M&P שיוצא איזשהו עיבוד
[00:37:41 - 00:37:48] למספר הזה לסקלר הזה שמגיע כאן יוצא כאן איזשהו וקטור ומכניסים את הוקטור הזה בכל מקום שאנחנו יכולים
[00:37:49 - 00:37:51] או על ידי קונפנטנציה או על ידי שפה
[00:37:52 - 00:37:52] כל מיני שיטות
[00:37:55 - 00:37:57] אוקיי אז זה המימוש של המודל
[00:38:02 - 00:38:08] ובתרגיל תצטרכו לממש את זה אבל אני חושב שנותן לכם כבר איזה שלט של משהו
[00:38:09 - 00:38:13] שלא תצטרכו להיכנס יותר מדי לפרטים הטכניים פה כי זה באמת
[00:38:14 - 00:38:18] מאוד מעניין אבל זה לא קשור כל כך למידול ההסתברותי של
[00:38:20 - 00:38:21] מה שאנחנו רואים להם בקורס
[00:38:23 - 00:38:30] אוקיי אז זה היה חזרה קצת עם דברים בפיזר ועכשיו אנחנו נדבר על posterior something
[00:38:38 - 00:38:43] אז בעצם מה הכוונה על posterior something ולמה אתם צוחקים את זה
[00:38:44 - 00:38:45] אז מה זה posterior?
[00:38:46 - 00:38:47] posterior זה בהינתן שיש לנו איזשהו
[00:38:51 - 00:38:56] בדרך כלל הכוונה ל-posterior זה השימוש הזה בחוק-base זאת אומרת בהינתן שיש לנו את המודלים בכיוון הזה
[00:38:57 - 00:39:00] אנחנו רוצים לייצר את המודל הזה
[00:39:01 - 00:39:02] זאת אומרת נגיד שיש לנו
[00:39:04 - 00:39:05] איזשהו
[00:39:06 - 00:39:09] משתנה שמייצג משהו x
[00:39:09 - 00:39:11] ואיזשהו משתנה אחר שמייצג משהו y
[00:39:12 - 00:39:13] ואנחנו יודעים את
[00:39:14 - 00:39:15] הפריור עד x
[00:39:16 - 00:39:19] אנחנו יודעים את ההסתברות של y ועם היתן x,
[00:39:20 - 00:39:21] תהליך שמייצר את y בעצם
[00:39:22 - 00:39:24] אנחנו רוצים עכשיו לשאול שאלה על x
[00:39:25 - 00:39:28] אנחנו רואים את y ואנחנו רוצים לשאול שאלה על x, אוקיי? זה חוק-base
[00:39:29 - 00:39:33] ובדרך כלל כשאומרים משהו posterior זאת הכוונה שלי, יש לנו את התהליך
[00:39:34 - 00:39:36] שמייצר את הכיוון ההפוך ואנחנו רוצים להפוך אותו
[00:39:38 - 00:39:39] אוקיי אז למה זה טוב?
[00:39:40 - 00:39:42] למה זה טוב לנו להיות מסוגלים לדגום x?
[00:39:43 - 00:39:48] וזה בעצם יכול לאפשר לנו לעשות הרבה משימות כאלה של פרובליסטיק אינפאנס
[00:39:50 - 00:39:50] הפסקה הסתברותית
[00:39:53 - 00:39:56] שאנחנו יכולים להיות מעוניינים בהן
[00:39:57 - 00:39:57] למשל
[00:39:58 - 00:40:06] נגיד שאנחנו רוצים לעשות ניקוי רעשים מתמונה
[00:40:07 - 00:40:10] אז x במקרה הזה זה יהיה התמונה שלנו
[00:40:11 - 00:40:13] וy זה יהיה התמונה המורעשת
[00:40:18 - 00:40:24] בעצם מה זה אומר? ההסתברות של x בהינתן y זה למצוא את ההסתברות של התמונה בהינתן זה שאנחנו רואים
[00:40:25 - 00:40:25] תמונה מורעשת
[00:40:28 - 00:40:28] אז מה יהיה?
[00:40:30 - 00:40:33] במקרה הזה אני יכול, אם יש לי איזה שהוא, למדתי כבר מודל של תמונות
[00:40:34 - 00:40:38] אני יודע יש לי פיושן מודל על תמונות, ממש טוב
[00:40:39 - 00:40:42] אז הוא יהיה הפרייר שלי, הוא יהיה p של x
[00:40:44 - 00:40:45] הוא יהיה מודל טוב של תמונות
[00:40:46 - 00:40:48] מה יהיה p y של x?
[00:40:51 - 00:40:53] נגיד במקרה של דינוייזינג
[00:40:58 - 00:41:01] כן, אז נגיד איזה, מה זה יכול להיות?
[00:41:05 - 00:41:05] הרעש עצמו
[00:41:06 - 00:41:12] כן ההתפלגות של הרעש, זאת אומרת מה זה ההסתברות של תמונה מורעשת בהינתן תמונה
[00:41:13 - 00:41:17] זה תלוי באיזה סוג רעש שאני הוספתי, אבל נגיד אם זה רעש גאוסיאני זה פשוט יהיה הגאוסיאן הזה
[00:41:18 - 00:41:20] אם זה יהיה פי של y בהינתן x
[00:41:22 - 00:41:27] יהיה שווה לאיזשהו גאוסיאן שהתופלת שלו זה x,
[00:41:27 - 00:41:30] נגיד והרעש והווריאנט זה כמה רעש הוספתי
[00:41:34 - 00:41:37] אוקיי, אז נגיד שיש לי מצב כזה באמת, שיש לי את ה... אני יודע כמה רעש
[00:41:39 - 00:41:41] התווסף לי לתמונה,
[00:41:42 - 00:41:43] יש לי פרייר של התמונה,
[00:41:44 - 00:41:47] אני רוצה להשתמש בדיפיוזן מודל כדי לעשות
[00:41:47 - 00:41:50] תמונות הפוסטריות כפי של x בהינתן y
[00:41:52 - 00:41:53] אז תכף נראה איך אפשר לעשות מזה.
[00:41:53 - 00:41:57] עוד דוגמה זה אינפיינטינג, אינפיינטינג זה ש...
[00:41:57 - 00:41:59] אנחנו רואים רק חלק מהתמונה,
[00:41:59 - 00:42:04] אז היה לכם את זה בתרגיל של הפיצל cml
[00:42:05 - 00:42:11] אז מה יהיה במקרה הזה, פי של x שוב במקרה הזה זה היה מודל שלמדתי על התמונות, זה היה פרייר שלי על התמונות
[00:42:12 - 00:42:16] מה יהיה פי של y בהינתן x במקרה של אינפיינטינג?
[00:42:17 - 00:42:22] אז במקרה של אינפיינטינג באמת אין,
[00:42:23 - 00:42:25] לא ברור מה זה העניין ההסתברותי שיש שם
[00:42:26 - 00:42:30] אבל מה שברור זה ש-y נגיד הוא רק חצי מ-x,
[00:42:30 - 00:42:33] האבסובציה שלי זה למשל החלק מהפיצל it
[00:42:34 - 00:42:36] ואז פי של y בינתיים x
[00:42:38 - 00:42:41] הוא צריך להיות משהו באיזשהו מודל הסתברותי,
[00:42:42 - 00:42:43] אנחנו פורמים במקום בצורה הסתברותית,
[00:42:44 - 00:42:47] שמסתכל רק על חלק מהתמונות,
[00:42:48 - 00:42:50] שהמרחב שלו הוא בכלל יותר קטן,
[00:42:50 - 00:42:51] אז נגיד זה יכול להיות גם גאוסיינג,
[00:42:52 - 00:42:54] רק שהתוחלת שלו זה איזשהו x,
[00:42:56 - 00:42:58] שהוא מ-0 עד
[00:43:00 - 00:43:02] אינפי שתיים,
[00:43:04 - 00:43:10] הוא רק רואה חצי מהפיקסלים ויש לו איזה רעש קטן רק בגלל שאנחנו חייבים לפרמנט דברים בצורה הסתברותית.
[00:43:14 - 00:43:17] בעצם יש לנו בהינתן תמונה אנחנו יודעים איך לייצר
[00:43:17 - 00:43:19] את ה-y, זה מה שזה אומר,
[00:43:20 - 00:43:22] המודל הזה, איך אנחנו מייצרים את ה-y,
[00:43:23 - 00:43:25] אנחנו לוקחים רק חצי מהפיקסלים x,
[00:43:26 - 00:43:27] ומוסיפים להם טיפה רע.
[00:43:29 - 00:43:32] בסדר, אז זה יהיה מודל של p y ו-y יינתן x.
[00:43:33 - 00:43:41] אם אנחנו יודעים לעשות את posterior sampling, זאת אומרת מתוך p של x ו-p של y יינתן x לייצר את ה-p של x יינתן y,
[00:43:42 - 00:43:48] נוכל בהינתן חצי תמונה למצוא את הדגימות של התמונה המלאה.
[00:43:50 - 00:43:52] זה גם מה שניסיתם לעשות בתרגיל.
[00:43:53 - 00:44:00] ראיתם שזה היה מודל שלא היה צריך לעשות את כל הקומולציה הזו בצורה אוטומטית, לכיוון אחד הוא מותן לכם את זה בחינם,
[00:44:00 - 00:44:02] ולכיוון השני זה היה די קשה.
[00:44:02 - 00:44:04] די קשה לעשות את זה.
[00:44:04 - 00:44:05] אז באופן כללי זה היה די קשה.
[00:44:06 - 00:44:08] פה אנחנו נראה שאפשר לעשות את זה יחסית בקלות.
[00:44:11 - 00:44:14] אז שימו לב שאני אפילו לא מדבר על p של y,
[00:44:14 - 00:44:16] שתי סיבות, גם הרבה פעמים
[00:44:17 - 00:44:18] מאוד קשה
[00:44:19 - 00:44:19] ליצור אותו,
[00:44:20 - 00:44:21] וגם
[00:44:23 - 00:44:26] בדרך כלל שאנחנו נשתמש, אנחנו לא נצטרך אותו.
[00:44:27 - 00:44:29] תראי מה זה p של y פה.
[00:44:33 - 00:44:35] תאמין ש-p של y כאן, ושתי הדוגמאות האלה דווקא
[00:44:43 - 00:44:47] פי של y פה זה פי של התמונות הרועשות באופן כללי.
[00:44:47 - 00:44:48] כן, אני לא יודע מה p של y זה.
[00:44:50 - 00:44:51] יכול אולי לחשב אותו מפי של x.
[00:44:51 - 00:44:53] בעיקרון זה אינטגרל של המונה הזה.
[00:44:54 - 00:44:57] זה משהו שהוא באופן כללי של קישור.
[00:44:58 - 00:45:03] אוקיי, עוד דוגמאות שיכולות להיות, אם y למשל יכול להיות איזושהי
[00:45:06 - 00:45:08] מחלקה, איזשהו קלאס לבדוק את האפשרויות.
[00:45:09 - 00:45:10] אם אנחנו מדברים על ספרות,
[00:45:12 - 00:45:13] אז נגיד ש-y הוא
[00:45:16 - 00:45:18] אחת מהספרות,
[00:45:18 - 00:45:21] מה זה אומר לדגום עם מה של p של x בהינתן y,
[00:45:22 - 00:45:24] ולדגום רק תמונות של הספרות האלה.
[00:45:25 - 00:45:27] רק תמונות של הקטגוריה ה-y.
[00:45:28 - 00:45:31] ומה זה יהיה אם p של x, שוב זה ה-prior שלי עם תמונות,
[00:45:31 - 00:45:33] מה זה יהיה p של y בהינתן x?
[00:45:37 - 00:45:39] כן, קוראים לזה בדרך כלל?
[00:45:40 - 00:45:41] סיגמוד.
[00:45:44 - 00:45:48] הרבה פעמים הוא ממש על ידי סיגמוד או איזשהו סופטמאקס,
[00:45:48 - 00:45:49] זה קלאסי פייר שלנו בעצם,
[00:45:49 - 00:45:51] אם הקלאסי פייר זה מה שהוא בעצם ממדד,
[00:45:51 - 00:45:53] ההסתברות של y וילתן x.
[00:45:54 - 00:45:57] אם יש לנו נגיד פריור, ויש לנו קלאסי פייר,
[00:45:58 - 00:46:00] ויש לנו את הדרך לעבור,
[00:46:01 - 00:46:02] להפוך פה את ההסתברויות,
[00:46:03 - 00:46:06] אז נוכל לדגום דגימות ספציפית מי פלאס.
[00:46:09 - 00:46:12] בגופן כללי אפשר להגדיר עוד כל מיני אילוצים, נגיד שיש לי איזשהו אילוץ,
[00:46:14 - 00:46:16] שסכום כל הפיקסלים יהיה שם בעבר אפס,
[00:46:18 - 00:46:21] החצי הימני, יותר באים מהחצי השמאלי,
[00:46:21 - 00:46:27] כל מיני אילוצים כאלה אפשר להגדיר בתור איזשהו y ואיזושהי התפלגות כזאת של y וילתן x,
[00:46:28 - 00:46:31] ואז אנחנו נראה שבתנאים מסוימים אנחנו נוכל לחשב את
[00:46:32 - 00:46:34] ההסתברות ההפוכה הזאת של x וילתן y.
[00:46:39 - 00:46:44] עוד נקודה רק שזה לא, דיברנו קצת בתחילת הקורס על מאק, על מקסימום של הקוסטיריו.
[00:46:45 - 00:46:46] אמ...
[00:46:47 - 00:46:52] הרבה פעמים כשמפרממים דברים ככה, אז בעצם מחפשים את המה.
[00:46:54 - 00:46:55] זאת אומרת,
[00:46:55 - 00:47:03] לא מצליחים להגיע למצב שדוגמים מתוך ההתפלגות הזאת ותופסים את כל ההתפלגות הזאת, אלא רק מוצאים את המקסימום שלה, את המוהות שלה.
[00:47:03 - 00:47:07] זה הרבה פעמים גם פותר כל מיני בעיות, אבל באופן עקרוני, אם יש לנו דרך
[00:47:08 - 00:47:11] לייצר דגימות מהקוסטיריור זה אמור להיות,
[00:47:12 - 00:47:13] נותן לנו יותר אפשרויות.
[00:47:15 - 00:47:20] אני חושב שזאת גם בעיה יותר קשה וגם יותר מאפשרת יותר
[00:47:23 - 00:47:27] כן למשל אפשר להשתמש, כמו שעשיתם בתרגיל, אפשר להשתמש בזה כדי לעשות MCMC, לחשב את ההסתרונות הזאת
[00:47:31 - 00:47:33] אוקיי אז איך אנחנו נעשה את זה?
[00:47:34 - 00:47:37] אז בעצם בואו נזכר שאנחנו עושים,
[00:47:37 - 00:47:41] כבר ראינו שאנחנו יכולים לדגום מהפרייר נכון? עם Lange & Dynandex
[00:47:42 - 00:47:43] בין שיש לנו את ה-score
[00:47:44 - 00:47:46] אנחנו יכולים לחשב את הפרייר
[00:47:47 - 00:47:49] אז אנחנו נרצה להשתמש בדיוק באותה שיטה
[00:47:50 - 00:47:55] רק במקום שיהיה לנו, במקום לחשב כאן את הנגברת של X של הפרייר,
[00:47:55 - 00:47:55] של P של X,
[00:47:56 - 00:47:58] להכניס פה את P של X בנתן Y
[00:48:00 - 00:48:04] לעשות NGEMIN Dynמיקס על התפלגות אחרת
[00:48:04 - 00:48:07] כל ההבטחות של NGEMIN Dynמיקס יעבדו גם על ההתפלגות החדשה
[00:48:08 - 00:48:15] אוקיי אז ננסה לנגד NGEMINININX, נקבל דגימות מ-P של X בהינתן Y
[00:48:15 - 00:48:16] שזה מה שאנחנו רוצים
[00:48:19 - 00:48:22] אוקיי אנחנו צריכים בשביל זה לחשב את ה-score של X בהינתן Y,
[00:48:23 - 00:48:24] את ה-score של הפוסטריום
[00:48:25 - 00:48:28] אז שוב זה ה-score, אנחנו גוזרים לפי X
[00:48:29 - 00:48:29] את
[00:48:32 - 00:48:33] ההתפלגות שלנו
[00:48:34 - 00:48:39] ומשהו נחמד זה שאפשר לפרק את זה לסכום הזה
[00:48:40 - 00:48:42] למה אפשר לפרק את זה לסכום?
[00:48:43 - 00:48:45] כי פשוט תסתכל על מוכב בייס
[00:48:47 - 00:48:50] אם נגזור אותו, אז אם נסתכל עליו בלוג
[00:48:50 - 00:48:54] יש לנו פה את הלוג של זה ועוד הלוג של זה פחות ללוג של זה
[00:48:55 - 00:48:56] אנחנו גוזרים את זה לפי X
[00:48:57 - 00:49:00] זה הנגזרת של הלוג של זה לפי X ועוד הנגזרת של הלוג של זה לפי X
[00:49:01 - 00:49:06] פחות הנגזרת של המכנה fx אבל זה לא תלוי בx אז זה יהיה פשוט 0
[00:49:11 - 00:49:12] אז זה מה שאנחנו נקבל
[00:49:13 - 00:49:16] ואז בעצם מה זה אומר שאנחנו יכולים להשתמש ב...
[00:49:18 - 00:49:21] כתבתי גם את הנוסחה עם ה-T אבל
[00:49:21 - 00:49:23] תכף אנחנו נדבר על העניין הזה של T
[00:49:24 - 00:49:27] אבל מה זה אומר זה אומר שאנחנו יכולים להשתמש
[00:49:28 - 00:49:30] בפריור זה יש לנו כבר נכון
[00:49:30 - 00:49:31] זה הפריור שלמדנו
[00:49:33 - 00:49:36] יוניק הזה ייתן לנו עבור כל x הוא ייתן לנו את
[00:49:38 - 00:49:39] הנגזרת של רש
[00:49:39 - 00:49:42] אחת הנגזרת של הלוג פי
[00:49:43 - 00:49:47] ואנחנו בעצם צריכים איכשהו להגיע למצב שיש לנו את זה
[00:49:48 - 00:49:51] אם נגיע למצב שיש לנו את הסקור הזה של ה-likelihood
[00:49:52 - 00:49:55] נקרא לזה posterior, לזה פריור ולזה likelihood
[00:49:56 - 00:49:57] זה יש לנו את הסקור של הפריור
[00:49:58 - 00:50:01] אנחנו צריכים איכשהו להשיג סקור של likelihood
[00:50:02 - 00:50:02] זה קשור,
[00:50:03 - 00:50:06] אם יהיה לנו את זה אז פשוט בכל צעד נוכל לסכום
[00:50:07 - 00:50:10] את שני הסקורים האלה וזה ייתן לנו סקור של הפוסטריאור
[00:50:10 - 00:50:12] נוכל לעשות
[00:50:20 - 00:50:24] אז מה הבעיה? זה בדיוק העניין הזה של ה-XT אז בדרך כלל
[00:50:24 - 00:50:29] זה לא יהיה לנו בישה למודל כזה
[00:50:31 - 00:50:37] שזה מה שאנחנו צריכים, כי בכל איטרציה אנחנו רק עם מסקר ה-X שאנחנו נמצאים בו אנחנו מניחים שהוא ב-XT, נכון?
[00:50:37 - 00:50:45] ובדרך שעשינו לא עשינו לנג'רי דינמיקס, עשינו עניב לנג'רי דינמיקס, זאת אומרת כל צעד הסתכלנו על רמת רעש הולכת וקטנה
[00:50:46 - 00:50:49] ובעצם כדי לעשות את זה עם הקוסטריאור אנחנו צריכים
[00:50:50 - 00:50:51] לכל צעד שיהיה לנו
[00:50:52 - 00:50:53] את ה...
[00:50:54 - 00:50:55] לייקליות
[00:50:56 - 00:51:02] בהינתן לא ה-X0 אלא ה-XT הנותחי שאנחנו נמצאים בו, כדי שמוכר לעשות את הצעד
[00:51:03 - 00:51:04] בצעד הזה ולאט לאט להקטין את הרעש
[00:51:05 - 00:51:07] ולקבל גימות בפוסטריאוריס
[00:51:08 - 00:51:09] אבל בדרך כלל לא יהיה לנו את הדבר הזה
[00:51:12 - 00:51:14] למשל נסיכו על דוגמה של האינפיינטינג
[00:51:17 - 00:51:22] בעצם בהינתן X רועש אני צריך להגיד מה ההסתברות
[00:51:23 - 00:51:24] של חצי מהתמונה
[00:51:25 - 00:51:26] אבל נקייה
[00:51:27 - 00:51:30] זה כבר לא יהיה כזה פשוט כמו המודל הזה שיש לי פה, אני חושב שתסתכל על החצי
[00:51:31 - 00:51:33] וצריך אפשר להנקות גם את החצי הזה
[00:51:36 - 00:51:39] בסדר, זה בדרך כלל, או למשל אם אני רוצה להכניס איזשהו אילוץ, כמו שאמרתי
[00:51:42 - 00:51:46] שהחצי השמאלי של הדאטה יהיה יותר בהיר מהחצי הימני של הדאטה
[00:51:48 - 00:51:51] אם עכשיו הדאטה שאני רואה זה לא ה-X האמיתי אלא זה X רועש
[00:51:52 - 00:51:53] אז אין לי דרך לחשב את הדבר הזה
[00:51:55 - 00:51:57] אוקיי אז זאת בעיה, איך פותרים את הבעיה הזאת
[00:51:57 - 00:52:03] הדרך המדויקת לפתור את זה זה שוב להגיד אוקיי X0 הוא משתנה חבוי כאן
[00:52:03 - 00:52:09] ובעצם אני צריך לפתור את זה כמו שבדרך כלל אנחנו מתייחסים למשתנים פגועים, דרך כלל אינטגרל המודל
[00:52:09 - 00:52:12] נכון אז זה במשחקת ההסתברות השלמה זה
[00:52:13 - 00:52:14] שווה לזה,
[00:52:14 - 00:52:17] הוספתי כאן עוד משתנה אבל עשיתי אינטגרל עליו
[00:52:18 - 00:52:23] ואת זה אני יכול לפרק למכפלה של שני הדברים האלה
[00:52:23 - 00:52:28] אני מניח שוואי הוא לא פונקציה של XT הוא רק פונקציה של XT
[00:52:29 - 00:52:31] ויש כאן נושאים של מרקוביות כזאת אז
[00:52:32 - 00:52:33] Y בהינתן X0
[00:52:34 - 00:52:36] אני כבר לא צריך להתנות על XT זה המודל שאני יודע
[00:52:37 - 00:52:40] ו-X0 בהינתן XT זה מה שאני צריך להוסיף
[00:52:40 - 00:52:42] לכאן ולעשות עוד את האינטגרל הזה
[00:52:44 - 00:52:47] וזה די מסובך ובדרך כלל לא פותרים את זה בצורה
[00:52:49 - 00:52:49] מדויקת
[00:52:50 - 00:52:52] אף פעם אפשר לפתור את זה בצורה מדויקת
[00:52:53 - 00:52:54] יש כמה דרכים לנסות לפתור את זה
[00:52:57 - 00:53:00] אחת זה פשוט להתעלם מזה ובעצם במקום האינטגרל הזה
[00:53:01 - 00:53:03] להניח שהאינטגרל הזה שווה
[00:53:04 - 00:53:06] אפשר לחשוב על איזשהו Delta function
[00:53:10 - 00:53:12] ולהניח שפשוט ה-X0 שווה ל-XT
[00:53:13 - 00:53:14] פשוט להתעלם מזה
[00:53:16 - 00:53:19] מה זה אומר? זה אומר שיש לנו מודל שהוא מניח שהוא מקבל X
[00:53:20 - 00:53:24] הוא נקי אבל אנחנו ניתן לו X רועש
[00:53:24 - 00:53:28] למשל אם המודל הזה הוא Classifier זה Classifier שיתאמן רק על
[00:53:29 - 00:53:32] תמונות נקיות אבל עכשיו בזמן האינטגרל הזה אנחנו נותנים לו תמונות רועשות
[00:53:33 - 00:53:34] יכול להיות שזה יעבור להיות מספיק טוב
[00:53:35 - 00:53:38] אבל לא בטוח זה דרך אחת להתמודד עם זה
[00:53:38 - 00:53:41] דרך שנייה להתמודד עם זה זה גם להפוך את האינטגרל הזה
[00:53:43 - 00:53:45] לנקודה אחת זאת אומרת לא לפתור את האינטגרל
[00:53:45 - 00:53:49] אבל שהנקודה הזאת לא תהיה XT אלא שהיא תהיה התוכן
[00:53:49 - 00:53:59] של X0 בהינתן XT. אז בעצם במקום להחליף את ההסתברות הזאת בדלתא פונקשן מסביב לתוחלת של ההסתברות הזאת
[00:54:01 - 00:54:11] מה זה אומר? זה אומר שוב הדוגמה של ה-classifier זה שאם אימנתי קלאסיפיירים על תמונות נקיות
[00:54:13 - 00:54:14] אז אני עכשיו מניח
[00:54:14 - 00:54:19] זה הייתי אמור להפוך פה, כאן זה T וכאן
[00:54:21 - 00:54:24] זה לא משנה אבל כאן זה אמור להיות XT
[00:54:33 - 00:54:34] אז כאן זה אמור להיות T
[00:54:35 - 00:54:42] אז ההסתברות של Y ביותר N XT שאני לא יודע אותו זה בעצם ה-classifier שיודע להסתכל על תמונות רועשות
[00:54:43 - 00:54:44] איך אני אחשב אותו?
[00:54:44 - 00:54:48] אני קודם אחשב מה התוחלת של התמונה הנקייה שיש לי בצעד ה-T
[00:54:49 - 00:54:51] ואתן את ה... זו תמונה
[00:54:52 - 00:54:53] ואתן את התמונה הזאתי לקלאסיפיירים
[00:54:56 - 00:54:58] בסדר? יש לי פה עוד צעד באמצע.
[00:54:58 - 00:55:00] אני לוקח את ה... בכל איטרציה שאני עושה
[00:55:01 - 00:55:02] מנג'רמין דינמיקס
[00:55:03 - 00:55:06] אני רוצה להוסיף את ה-likely יותר מזה
[00:55:06 - 00:55:10] בשביל לחשב אותו אני קודם אומר מה אני חושב
[00:55:10 - 00:55:13] מה התוחלת של X0 של התמונה הנקייה
[00:55:15 - 00:55:17] ומכניס את הדבר הזה לתוך ה-classifier שלי
[00:55:18 - 00:55:19] גוזר אותו,
[00:55:20 - 00:55:20] מקבל את ה-score
[00:55:21 - 00:55:24] וזה מה שאני מוסיף, אני תכף אראה את הסיכום של זה
[00:55:25 - 00:55:29] אוקיי, תכף נתמקד קצת יותר על השידה הזאת
[00:55:29 - 00:55:38] ונקודה שלישית זה דרך קצת יותר מורכבת שכן מתייחסת לדבר הזה מונטגרל ועושה שם איזה דבר ראשון אני מפרץ פה לדברים יותר יותר טובים
[00:55:39 - 00:55:42] אנחנו נדבר עכשיו על נקודה שתיים, אוקיי?
[00:55:44 - 00:55:44] נסביר את זה, כן
[00:55:50 - 00:55:51] אתה יכול לחשב
[00:55:53 - 00:55:54] את האינטגרל קשה לעשות
[00:55:57 - 00:55:59] כן, אז זה אפשר לעשות מונטגרל אבל יש לי אינפרנס
[00:56:02 - 00:56:04] כתבתי פה רק מונטגרל אבל בעיקרון גם מונטגרלו זה
[00:56:09 - 00:56:13] אוקיי, אז באמת מה התמונה שלא נראה שחסר לי
[00:56:14 - 00:56:20] כיף, רגע בואו נחזור לכאן, איך ייראה התהליך בסופו של דבר
[00:56:21 - 00:56:24] אנחנו רוצים לעשות לנג'אמין דיינאמיקס וזה
[00:56:27 - 00:56:29] רק שבמקום שפה כל פעם אני אוסיף
[00:56:30 - 00:56:35] את הסקור של הפריור אני רוצה להוסיף את הסקור של הפוסטריאור
[00:56:35 - 00:56:39] אני אשים את הסקור של הפריור ועוד הסקור של הלייטליות
[00:56:40 - 00:56:44] כן, שזה היה... כל השאר אותו דבר, זאת אומרת בכל איטרצייתי
[00:56:45 - 00:56:49] תהיה לי פה דוגמה קצת יותר פחות ופחות רועשת
[00:56:49 - 00:56:54] ואני אחשב לנגזרת הזאת ואני אחשב ואני אוסיף לזה עוד קצת רעש
[00:56:55 - 00:56:57] ככה אני אתקדם עד שאני אגיד לדגימה אחת
[00:56:57 - 00:57:05] כן, אבל הדבר היחיד שאני צריך לשנות זה את זה, אני צריך להפוך אותו לדגימה של הפוסטריאור על ידי זה שזה יהיה סקור של הפריור ועוד הלייטליות
[00:57:06 - 00:57:06] כן, הנוסחה הזאת
[00:57:07 - 00:57:16] זה כל איטרצייתי של הלנג'אמין דיינמיקס, אני צריך לחשב את זה ואת זה, זה כבר יש לי
[00:57:16 - 00:57:17] לדיפיוז'ל מודל,
[00:57:18 - 00:57:20] מה שחסר לי זה הדבר הזה,
[00:57:21 - 00:57:23] אוקיי? עכשיו אם הדבר הזה, למשל זה קלאסיפייר,
[00:57:25 - 00:57:29] אז איך אני יכול לחשב את זה? זה שאני אעשה gradient descent דרך הקלאסיפייר
[00:57:30 - 00:57:33] לאינפוט שלו, ל-X, אוקיי? זה נותן לי את הנגזרת לפי X
[00:57:34 - 00:57:35] זה הקלאסיפייר.
[00:57:36 - 00:57:37] אינטואיטיבית מה זה אומר?
[00:57:38 - 00:57:39] בהינתן ה-X שיש לי כרגע
[00:57:40 - 00:57:45] איך אני צריך לשנות את התמונה הזאת כדי שהיא תיראה יותר כמו כאלה
[00:57:47 - 00:57:48] אוקיי?
[00:57:49 - 00:57:52] אז אוקיי, אז זה מה שהיה חסר, השקף שהיה חסר לי
[00:57:53 - 00:57:59] אבל אז שוב הבעיה כאן זה שהדגימה הזאת שאני נותן לקלאסיפייר זה דגימה רועשת
[00:58:01 - 00:58:03] אוקיי? זה לא בטוח שהקלאסיפייר הזה יהיה טוב,
[00:58:03 - 00:58:05] זה לא בדיוק המודל הזה נכון
[00:58:05 - 00:58:08] אז זה מה שהדגתי כאן שיש לנו
[00:58:09 - 00:58:15] שלוש פרחים להתייחס לזה, אחד זה להתעלם לגמרי, שניים זה במקום לתת לקלאסיפייר את התמונה הרועשת
[00:58:15 - 00:58:18] לתת את השיערוך שלנו לתמונה הנקייה
[00:58:19 - 00:58:21] שזה פשוט אותו חלק של X0 בינתן XT
[00:58:22 - 00:58:28] וגישה אחרת זה לעשות איזשהו משהו יותר טוב תוכן שמתייחס לאינטגרליזם
[00:58:30 - 00:58:32] איך נראית ההתפלגות הזאת של X0 בינתן XT?
[00:58:35 - 00:58:40] אנחנו לא יודעים איך לחשב אותה, אנחנו יודעים עליה הרשום?
[00:58:44 - 00:58:49] אנחנו לא יודעים עליה הרבה, אוקיי? זו הסתברות שאנחנו לא יודעים לחשב, אם יודעים לחשב X0
[00:58:50 - 00:58:52] מה שאנחנו, המודל שלנו
[00:58:58 - 00:58:59] המודל שאנחנו
[00:59:05 - 00:59:26] עכשיו אנחנו מניחים פה בכל שלב זה שאנחנו יודעים לחשב את ההסתברות של XT מינוס 1 בהינתן XT
[00:59:28 - 00:59:34] אוקיי? בהינתן שזה צעד אחד קטן אחורה זה שאנחנו ממדלים את זה בצורה כזאת של גאוסיאן
[00:59:34 - 00:59:37] זה לא כל כך נורא
[00:59:38 - 00:59:41] אוקיי? יש לנו את התוכלת ואת הווריאנס
[00:59:41 - 00:59:45] אבל לקפוץ ישר מאיזשהו T גדול לאפס
[00:59:46 - 00:59:50] אנחנו יכולים למדל את זה בתור גאוסיאן
[00:59:50 - 00:59:52] אבל זה יהיה מידול מאוד גבוה
[00:59:54 - 00:59:55] אוקיי? אבל עדיין יכול להיות שזה יהיה יותר טוב
[00:59:56 - 01:00:00] למשל להכניס את התוכלת של הדבר הזה במקום את XT עצמו
[01:00:02 - 01:00:03] ל-classifier או ל-lightlיות שלנו
[01:00:04 - 01:00:06] אז זה הרעיון שיש לנו פה
[01:00:10 - 01:00:11] סליחה על הקפיצות האלה
[01:00:12 - 01:00:17] ופה יש פשוט את הנוסחה של החישוב של התוכלת של X0 בינתי XT
[01:00:19 - 01:00:21] לפי המידול הזה שיש לנו
[01:00:25 - 01:00:29] בדיוק אותו מידול של XT מינוס 1 בינתי XT רק שאנחנו הולכים עד 0 כל פעם
[01:00:30 - 01:00:32] זה מודל שהוא לא כל כך טוב
[01:00:32 - 01:00:33] כש-T הוא רחוק מ-0
[01:00:34 - 01:00:36] אבל עדיין אפשר להתחשב את התוכלת הזאת,
[01:00:37 - 01:00:43] אוקיי? אז זה התוכלת שוב, זה איזשהו הדבר הזה זה ה-score, אוקיי? זה הרעש
[01:00:44 - 01:00:46] שאנחנו חושבים שיתווסף לנו
[01:00:47 - 01:00:48] וזה איזשהו משקול בין
[01:00:52 - 01:00:54] הדאטה שאנחנו נמצאים בו כרגע, הדגימה שלנו כרגע
[01:00:55 - 01:00:58] והרעש, כן? אנחנו מנסים להוריד בעצם את כל הרעש
[01:00:59 - 01:01:00] מהדגימה המורשת שיש לנו
[01:01:01 - 01:01:05] בסדר? זה מה שאנחנו... הרי המודל שלנו הוא עושה...
[01:01:06 - 01:01:08] זה כאילו מה שבאמת נכון?
[01:01:08 - 01:01:10] וזה מה שאנחנו נעשה בפועל
[01:01:11 - 01:01:15] אוקיי? שה-score הזה הוא איזשהו שיערוך לדבר הזה בכל צד, נכון?
[01:01:15 - 01:01:16] ה-score האמיתי
[01:01:17 - 01:01:20] זה מה שאנחנו נעשה בכל צד בינתי XT, אנחנו נחשב
[01:01:20 - 01:01:22] במודל שלנו נעשה שיערוך בכמה רעש יש פה
[01:01:23 - 01:01:28] ואנחנו פשוט נוריד את כל הרעש הזה והדברים האלה זה כדי לעשות סקיילינג נכון של הדאטה
[01:01:31 - 01:01:34] בסדר? אז זה יהיה השיערוך שלנו למה התמונה הנקייה
[01:01:36 - 01:01:40] זה שונה למה שאנחנו עושים כשאנחנו עושים ל�ג'ווין דינאמיקס, שאנחנו כל פעם הולכים רק צעד אחד אחורה
[01:01:41 - 01:01:43] אנחנו צריכים לעשות את כל השיערוך הזה עד הסוף
[01:01:43 - 01:01:44] אנחנו עושים...
[01:01:45 - 01:01:51] בעצם זה אותו משקול, זה אותו שני איברים רק עם משקולים שונים שאנחנו חוזרים צעד אחד אחורה
[01:01:54 - 01:01:57] זה ברור מה ההבדל בין X0 ל-XT מינוס 1?
[01:02:01 - 01:02:07] באלגוריתם לנג'ווין דיינאמיקס אנחנו כל פעם הולכים צעד אחד אחורה, משערכים את הרעש להתווסק והולכים קצת אחורה
[01:02:08 - 01:02:09] וגם מוסיפים רעש
[01:02:10 - 01:02:18] כאן אנחנו בשביל שיהיה לנו משהו אינפוט טוב להכניס ללייקליות שלנו אנחנו גם רוצים על הדרך לשערך את ה-X0 שלנו
[01:02:19 - 01:02:26] אז בכל צעד מה שאנחנו נעשה זה לשערך את X0 על ידי זה שאנחנו נוריד את כל הרעש, ברגע שנשאר את הרעש נוריד את כל הרעש
[01:02:26 - 01:02:35] אוקיי, אז האלגוריתם נראה ככה בסופו של דבר,
[01:02:35 - 01:02:37] שוב זה אלגוריתם של מה? של דגימה
[01:02:38 - 01:02:43] מתוך הפוסטריאור, בהינתן שיש לנו
[01:02:43 - 01:02:45] יש כל מיני דברים חסרים וזה גם כתוב קצת,
[01:02:46 - 01:02:48] זה ממאמר שעושה פוסטריאור אינטברנטס
[01:02:49 - 01:02:52] זה כתוב קצת אימונטציה פיפה שונה
[01:02:56 - 01:02:59] אבל אוקיי, בוא נראה
[01:03:04 - 01:03:05] הדבר היחיד של
[01:03:06 - 01:03:10] בלי השורה האדומה הזאת זה בדיוק שקול למה שהיה לנו קודם, אוקיי? אז זה כתוב טיפה שונה.
[01:03:11 - 01:03:12] מה קורה כאן?
[01:03:12 - 01:03:17] זה כל איטרציה, סליחה, זה דגימה, אוקיי? אז אני רץ מ...
[01:03:19 - 01:03:25] נקרא n עד 0 זה ה-T שלנו, מה שנקרא נקרא i
[01:03:26 - 01:03:30] ומחשב את הרעש
[01:03:31 - 01:03:33] שאני חושב שזה יתאסף לי,
[01:03:34 - 01:03:34] אוקיי?
[01:03:35 - 01:03:36] ויש כאן צעד
[01:03:41 - 01:03:47] שאם אנחנו מתעלמים מהשורה האדומה אז כאילו לא צריך לעשות את הדבר הזה אבל אפשר, תכף אנחנו נראה שיש לזה גם יתרון
[01:03:48 - 01:03:48] מה קורה כאן?
[01:03:49 - 01:03:50] בהינתן שיש לי את הרעש
[01:03:50 - 01:03:55] אני קודם כל מוריד את כל הרעש ועושה סקיילינג נכון ככה שיהיה לי את השערוך ל-X0
[01:03:57 - 01:04:00] ואחר כך אני עושה סקיילינג מחדש
[01:04:01 - 01:04:05] של XI ו-X0 ככה שיהיה לי XI מינוס 1
[01:04:07 - 01:04:09] טיפה ננסה הפוך זאת אומרת אני
[01:04:26 - 01:04:29] אז מה שאנחנו ראינו בנוסחות שראינו קודם
[01:04:37 - 01:04:39] את זה, מה שאנחנו ראינו קודם פה
[01:04:39 - 01:04:43] שכל פעם לקחנו איזשהו משקול של ה-XT ושל הרעש
[01:04:44 - 01:04:45] ככה שכל פעם ראינו
[01:04:46 - 01:04:47] אחד אחורה
[01:04:48 - 01:04:48] אוקיי?
[01:04:49 - 01:04:54] זאת אומרת לא הורדנו את כל הרעש, הורדנו כמה שאנחנו חושבים שיביאו אותנו ל-X2
[01:04:55 - 01:04:57] אוקיי? וגם הוספנו עוד קצת רעש
[01:04:59 - 01:05:00] פה
[01:05:02 - 01:05:04] בעצם אנחנו עושים את אותו חישוב
[01:05:05 - 01:05:07] אבל בדרך קצת עקיפה
[01:05:13 - 01:05:15] מ-XT עד ל-X0
[01:05:17 - 01:05:18] ואז חוזרים מ-X0
[01:05:25 - 01:05:29] אם תפתחו את כל המקדמים האלה, את כל החישוב, זה יוצא אותו דבר הזה
[01:05:31 - 01:05:33] אבל בדרך כלל אנחנו עוברים באיזשהו שיערוך של X0
[01:05:34 - 01:05:35] שזה יכול לעזור לנו בהחלט פנימי
[01:05:36 - 01:05:37] אני מראה לכם למה
[01:05:38 - 01:05:38] אוקיי?
[01:05:39 - 01:05:41] אז זה אפילו אם אנחנו לא עושים posterior samples
[01:05:42 - 01:05:43] תלמדו רגע מהשורה האדומה
[01:05:44 - 01:05:48] אז זה בדיוק אותו דבר כמו קודם, אוקיי? אז כל פעם אני מחשב
[01:05:49 - 01:05:52] זה נותן לי את התוחלת של XT מינוס 1
[01:05:53 - 01:05:55] ועל זה אני מוסיף גם קצת רעש כי זה היה לאנג' אמין דינאנטוס
[01:05:58 - 01:05:59] של לעבור דרך X0?
[01:05:59 - 01:06:03] אז תכף נראה, יש לזה שתי יתרונות, אחד שמאפשר לנו לעשות את זה
[01:06:06 - 01:06:08] את הפתרון הזה כי יש לנו עכשיו את התוחלת של X0
[01:06:09 - 01:06:11] ושתיים, תכף אני אסביר
[01:06:13 - 01:06:18] אוקיי, אז איך בעצם, אז זו הדגימה הרגילה
[01:06:19 - 01:06:20] עכשיו אני רוצה להוסיף גם
[01:06:21 - 01:06:22] עוד דגימה מה-score של ה-Lighter
[01:06:23 - 01:06:24] כדי שהיא נסקור מה-Posterיאור
[01:06:25 - 01:06:28] אז זה מה שכתוב כאן, אוקיי? במנתן שיש לי את הדגימה הזאתי
[01:06:29 - 01:06:32] אני מוסיף לה עוד איזושהי נגזרת של משהו
[01:06:32 - 01:06:35] של מה? אז זה המקרה עבור, נגיד שיש לי רעש גרסיאן
[01:06:36 - 01:06:38] ועוד איזשהו
[01:06:40 - 01:06:43] מיפוי, למשל, אם פיינטינג, אני פחדתי את זה
[01:06:44 - 01:06:47] ה-A של X0 זה שהוא לוקח רק חצי מהפיקסל
[01:06:48 - 01:06:51] אוקיי? זה המיפוי שלוקח רק חצי מהפיקסלים
[01:06:51 - 01:06:52] ומשווה את זה
[01:06:54 - 01:06:56] לחצי של הפיקסלים שאני מקבל
[01:06:58 - 01:06:59] וכבר כתוב כאן
[01:07:00 - 01:07:06] אני עושה שלב אחד של האיטרציה, אז אני יודע איך להתקדם מ-X3 ל-X2
[01:07:07 - 01:07:09] אבל אני לא הולך בדיוק ככה, אני מוסיף
[01:07:12 - 01:07:14] לוקטור הזה שאני מתקדם בו עוד איזשהו איבר
[01:07:14 - 01:07:18] שהוא הנגזרת של הלוס הזה
[01:07:19 - 01:07:23] זאת אומרת אני רוצה גם למזער את הלוס הזה, אני רוצה קצת להקטין את המרחק בין
[01:07:23 - 01:07:27] חצי מהפיקסלים שקיבלתי למה שאני רואה
[01:07:32 - 01:07:33] וואלג' אמינאי נאמיקס
[01:07:34 - 01:07:35] נמצא בנקודה הזאת
[01:07:36 - 01:07:38] הפריור שלי אומר לי לך לכאן
[01:07:45 - 01:07:48] בסוף, אוקיי?
[01:07:49 - 01:07:52] בעיקרון הייתי יכול להמשיך ככה, לפי הפריור
[01:07:53 - 01:07:58] אני לא רוצה סתם דגימה לפי הפריור, אני רוצה דגימה לפי הפוסטריור, אז אני צריך להוסיף בכל דגימה כזאת
[01:07:59 - 01:08:00] עוד איזשהו רכיב
[01:08:02 - 01:08:02] אוקיי?
[01:08:02 - 01:08:03] רכיב שהוא הפוסטריור
[01:08:05 - 01:08:07] שאומר לי איך אני צריך להתקדם
[01:08:07 - 01:08:10] כדי לשפר את הלייטלב
[01:08:11 - 01:08:13] עוד במקרה הזה של אימפייטינג,
[01:08:13 - 01:08:16] כדי שהפיקסלים בחצי התחתון של התמונה יהיו דומים
[01:08:16 - 01:08:19] לאובזרבציה של החצי התחתון של התמונה שקיבלתי.
[01:08:21 - 01:08:23] אז זה קצת נותן לו עוד איזשהו כיוון,
[01:08:23 - 01:08:23] ואז
[01:08:24 - 01:08:27] גם הכיוון יהיה איזשהו סכום שלהם,
[01:08:28 - 01:08:32] ככה אני אגיע בסופו של דבר למקום אחר.
[01:08:33 - 01:08:35] זה הנגזרת של איי כאילו
[01:08:36 - 01:08:38] ועוד איזשהו ייטלב
[01:08:38 - 01:08:44] ‫כן, ולשמר את התמונה שקיבלנו בעצם.
[01:08:47 - 01:08:55] ‫כן, להגדיל את הלייטלב בעצם, ‫שזה אומר, אם זה אימפייטינג, כן, ‫שהחצי התחתון של התמונה יהיה דומה לחצי שקיבלנו.
[01:08:56 - 01:08:57] ‫כלומר שכאילו יישאר,
[01:08:57 - 01:09:00] כאילו אנחנו לא רוצים... ‫-לא יישאר, אנחנו לא מתחילים ממנו.
[01:09:00 - 01:09:04] אה, מתחילים דווקא. נכון, אבל אנחנו מתחילים ‫מאיזשהו שורה אשר דומה לי.
[01:09:05 - 01:09:10] ‫אבל בכל איטרציה אנחנו נותנים ‫לכל התמונה שלנו עוד נגזרת
[01:09:11 - 01:09:14] ‫שהיא תלויה באובזרבציה שקיבלנו, ‫בלייטלב יותר.
[01:09:16 - 01:09:19] ‫אז נגיד במקרה של אימפייטינג,
[01:09:20 - 01:09:22] ‫אז הדבר הזה הוא בגודל של חצי תמונה,
[01:09:22 - 01:09:25] ‫אני חושב נורמה בריבוע של זה זה סקלר,
[01:09:27 - 01:09:30] ‫אבל אז אני גוזר את זה לפי איקס איי,
[01:09:31 - 01:09:33] ‫ואני מתקבל נגזרת של כל התמונה.
[01:09:34 - 01:09:38] ‫זה יהיה משהו שהוא בגודל של תמונה שלמה.
[01:09:39 - 01:09:40] ‫זה מה שאני מוסיף.
[01:09:40 - 01:09:42] ‫זה אומר כמה אני צריך להגדיל,
[01:09:42 - 01:09:44] ‫לשנות את כל התמונה,
[01:09:44 - 01:09:46] ‫ככה שחצי ממנה
[01:09:47 - 01:09:50] ‫יהיה דומה יותר לחצי שקיבלתי.
[01:09:55 - 01:09:58] ‫ולמה כל התמונה משפיעה כאן?
[01:09:59 - 01:10:02] שוב, אני אגש את הדבר הזה, ‫גוזר את זה לפי איקס איי.
[01:10:03 - 01:10:04] ‫אז איפה איקס איי נכנס כאן?
[01:10:05 - 01:10:08] ‫אני לא רואה אותו כאן, ‫אבל הוא בתוך החישוב של איקס אפס.
[01:10:09 - 01:10:11] ‫איקס אפס שווה איקס איי,
[01:10:11 - 01:10:16] ‫ועוד הדבר הזה. ‫נגיד, אני צריך לשנות את זה,
[01:10:16 - 01:10:18] ‫ועוד יותר מזה, ‫זה גם תלוי באיקס איי.
[01:10:20 - 01:10:23] ‫אז אם אני אגזור את זה, ‫בעצם אני צריך לגזור ‫דרך כל הרשת שלי שם.
[01:10:25 - 01:10:29] ‫בכל איטרציה כזאתי אני בעצם גוזר ‫דרך ה-likely term שלי,
[01:10:29 - 01:10:32] ‫שהוא יכול להיות, נגיד, ‫בסקל וחצי תמונה, או ה-classifier,
[01:10:32 - 01:10:36] ‫וגם דרך הרשת,
[01:10:37 - 01:10:37] ‫ה-unit,
[01:10:38 - 01:10:39] ‫שאני חושב שזה יותר ס.
[01:10:49 - 01:11:00] ‫אז ספקטואלית זה די הגיוני. ‫זאת אומרת, בכל איטרציה אני לא רק מסתכל ‫בכיוון שמשפר לי את הפרייר,
[01:11:01 - 01:11:03] ‫אלא בכיוון שמשפר לי משהו
[01:11:04 - 01:11:05] ‫שמעניין אותי, משהו אחר שמעניין אותי.
[01:11:06 - 01:11:09] ‫אם מעניין אותי שהתמונה תהיה ‫תנועה של כלב,
[01:11:10 - 01:11:14] ‫אז מה היא נשפרת? ‫תשפר את התוצאה של ה-classifier כלב.
[01:11:15 - 01:11:17] ‫זאת אומרת, אם זה היה שגיאה של ה-classifier,
[01:11:18 - 01:11:21] ‫אני יכול לגזור את ה... ‫אני גוזר את זה, ‫ככה שאני רוצה לשפר את זה קצת.
[01:11:22 - 01:11:22] אוקיי?
[01:11:23 - 01:11:28] אז אני בעצם אוסיף ‫לאיטרציה הזאתי שמוסיפה לי,
[01:11:29 - 01:11:32] ‫שמתקדמת אותי בכיוון של הפרייר, ‫אני מוסיף משהו ש...
[01:11:33 - 01:11:36] ‫בכל איטרציה צריך לשפר ‫איזשהו תנאי שמעניין אותי.
[01:11:39 - 01:11:46] ‫התנאי הזה, אפשר לכתוב אותו ‫בצורה ממש הסתברותית, ‫ואז הם ממש מבינים ‫למה זה מסתדר, אוקיי? זה פשוט,
[01:11:47 - 01:11:48] ‫אם הדבר הזה זה לייקלי יותר,
[01:11:49 - 01:11:52] ‫אז אנחנו מקבלים בסופו של דבר ‫Lenge of in Dynamics על ה-Costerיאו.
[01:11:53 - 01:12:02] ‫אוקיי, כמה נקודות של אה... ‫שקשורות למימוש של הדבר הזה, ‫אז
[01:12:03 - 01:12:14] ‫שקשור בעצם ליתרון הזה ‫שלהסתכל על X0. ‫אז קודם כול, משהו שפעם אמרתי קודם, ‫בדרך כלל עובדים ‫בתוך איזשהו פונק כזה של ערכים, ‫בדרך כלל מפעים את הערכים ‫למיוס אחד ואת חד,
[01:12:14 - 01:12:15] ‫בין נגיד קיוזי מוזלס.
[01:12:15 - 01:12:19] זאת אומרת, אני חושב שבדוגמאות ‫שעשינו עד עכשיו, ‫נפינו ל-0,
[01:12:20 - 01:12:22] 1.אז עשינו תודה.
[01:12:22 - 01:12:25] ‫אנחנו דבר רק מכפילים בין 2 ומורידים 1.
[01:12:26 - 01:12:31] ‫אז הצבעים שלנו, או חיי אפור, ‫בין 0 ל-255,
[01:12:32 - 01:12:34] ‫הם ממופים בצורה ליניארית ‫בין מינוס 1 לאחד.
[01:12:35 - 01:12:37] ‫זה בדרך כלל בדרך שהם ממופים, ‫אתה יודע, אתה?
[01:12:38 - 01:12:44] ‫עכשיו, מה שקורה זה שבגלל ‫שבכל איתרציה ‫אנחנו מוסיפים רעש, ‫יש לנו דאטה רועש,
[01:12:45 - 01:12:48] ‫אז הרבה פעמים אנחנו יכולים לקבל,
[01:12:50 - 01:12:52] ‫לא יודע אם הרבה פעמים, אבל מדי פעם, ‫חלק מהטיקסלים,
[01:12:52 - 01:12:59] ‫תמיד יהיו עם ערכים מאוד שונים, ‫מאוד רחוקים, ‫מתוך הטווח הזה של מינוס 1 לאחד.
[01:13:00 - 01:13:02] ‫וזה יכול לשגע כל מיני דברים.
[01:13:03 - 01:13:09] ‫רשתות ניורונים בדרך כלל אוהבות ‫שהסיגנל מוגדר ‫בתוך איזשהו תחום מסוים.
[01:13:10 - 01:13:21] ‫ואם אנחנו עוברים דרך X0, ‫אז יש לנו דרך יחסית פשוטה ‫לדאוג שזה לא משפיע עלינו כל כך הרבה, ‫אלא פשוט בתוך ה-X0 ‫אנחנו עושים קליפים כל מיני דברים.
[01:13:21 - 01:13:29] ‫אנחנו פשוט כותמים את כל מה ‫שחורג ממינוס 1 ל-X0. ‫לעשות את זה ב-XT, ‫זה לא כל כך טוב,
[01:13:30 - 01:13:31] ‫כי שם אנחנו כן רוצים.
[01:13:33 - 01:13:39] ‫שם הדאטה הוא אמור להיות רועש, ‫אז הוא אמור קצת לחרוב ממינוס 1 ל-X. ‫אפשר לעשות את זה גם שם, ‫אבל זה קצת פחות...
[01:13:41 - 01:13:43] ‫זה בעצם טעות קצת יותר
[01:13:48 - 01:13:50] משמעותית במודל.
[01:13:52 - 01:13:55] ‫עוד דרך לחשוב על זה זה שאנחנו מתקנים פה ‫איזה משהו שהוא לא לגמרי נכון,
[01:13:56 - 01:13:58] ‫הקירוב הגאוסיאני הזה,
[01:13:58 - 01:14:08] ‫הוא בעצם לא קח את זה בחשבון ‫ש-X0 חייב להיות במינוס 1 ב-1. ‫זה נותן לו בסופו של דבר גם הסתברות ‫להיות בערכים מאוד רחוקים ב-X1.
[01:14:13 - 01:14:17] ‫עוד איזשהו פשוט שעושים לפעמים,
[01:14:18 - 01:14:21] ‫ואני חושב שעדיף לכם לעשות את זה ‫כשאתם ממשים את זה,
[01:14:21 - 01:14:26] ‫לעשות את זה ככה, זה שלא גוזרים ‫דרך כל הרשת כל פעם. זה קצת איטי,
[01:14:27 - 01:14:28] ‫אלא בעצם,
[01:14:28 - 01:14:31] אם תחשבו על מה שעשינו פה,
[01:14:31 - 01:14:36] ‫אז עושים במקום בכל נקודה ‫להתחשב את הנגזרת לפי שניהם,
[01:14:37 - 01:14:40] ‫גם לפי ה-Prior וגם ה-Lightlyhood,
[01:14:40 - 01:14:41] ‫ולספון אותם,
[01:14:42 - 01:14:43] ‫לעשות את זה אחד-אחד.
[01:14:44 - 01:14:47] ‫אז עושים אינטרציה אחת ‫שמדגימה לפי ה-Prior,
[01:14:48 - 01:14:49] ‫מגיעים לאיזושהי נקודה,
[01:14:50 - 01:14:51] ‫והנקודה הזאת, ‫אם מחשבים את ה-Lightlyhood,
[01:14:52 - 01:14:54] ‫ועושים צעד גרדיאנט לפי ה-Lightly.
[01:14:55 - 01:14:56] ‫זה יוצא,
[01:14:57 - 01:14:59] יש לי כאן, זה יוצא משהו כזה,
[01:15:05 - 01:15:09] ‫במקום לספום את הגרדיאנטים כל פעם, ‫אני עושה גדיאנט וסנט כל פעם פונקציה אחרת.
[01:15:10 - 01:15:14] ‫בעצם, איך זה מתרגם פה לאלגוריתם הזה?
[01:15:16 - 01:15:16] ‫שפה,
[01:15:17 - 01:15:19] כשאני גוזר את זה לפי XT,
[01:15:20 - 01:15:25] ‫אז אני לא נכנס לתוך ה-Unit הזה,
[01:15:26 - 01:15:27] ‫אני רק גוזר את זה לפי ה-XT שלו.
[01:15:28 - 01:15:30] ‫זה יהיה חישוב הרבה יותר מהיר,
[01:15:31 - 01:15:33] ‫כבר אני אצטרך לגזור את זה ‫דרך רשת הלוקה.
[01:15:39 - 01:15:40] ‫אתם יכולים לממש את זה כבר עכשיו?
[01:15:41 - 01:15:42] ‫הסבר כזה, או ש...
[01:15:49 - 01:15:53] ‫אוקיי, האלגוריתם הזה הוא די עדפור, ‫די קל למימוש.
[01:15:54 - 01:15:54] ‫מה אתם צריכים לעשות?
[01:15:55 - 01:15:57] ‫אתם צריכים להוסיף את השורה הזאתי,
[01:15:58 - 01:16:00] ‫שהיא תלויה באיזשהו Lightfoot function,
[01:16:01 - 01:16:01] ‫למשל
[01:16:03 - 01:16:04] חצי תמונה, אוקיי?
[01:16:04 - 01:16:10] ‫לוקחת חצי מהתמונה, ‫לבדוק עד כמה היא קרובה ‫לתמונה שקיבלתם,
[01:16:11 - 01:16:15] ‫ולחשב את השגיאה הריבועית, אוקיי? ‫הממוצעת של כל זה,
[01:16:17 - 01:16:18] ‫ואז לגזור את זה,
[01:16:19 - 01:16:20] ‫אוקיי? אבל לפי מה לגזור את זה?
[01:16:21 - 01:16:21] ‫לפי
[01:16:27 - 01:16:31] ‫הפונקציה של X0 בהינתן XT, אוקיי? ‫אז אתם צריכים להגדיר את הדבר הזה בתור,
[01:16:32 - 01:16:35] ‫בתוך הגרף שלה, כן? ‫פיית'ון שעושה לכם את זה בצורה אוטומטית, אוקיי?
[01:16:36 - 01:16:39] ‫אם יש לכם כאן משהו שמחשב את X0 מתוך XT,
[01:16:40 - 01:16:42] ‫כן, אני בדרך כלל קורא לזה T, ‫כאן זה קורא לזה I,
[01:16:42 - 01:16:44] ‫אז כשאחר כך תבקשו,
[01:16:45 - 01:16:46] ‫זה יהיה הלוס שלכם,
[01:16:46 - 01:16:48] ‫תבקשו את הנגזרת של הלוס לפי XT,
[01:16:50 - 01:16:51] ‫אוקיי? הוא ייתן לכם את זה.
[01:16:51 - 01:16:53] ‫עכשיו, אם S יהיה בתוך הגרף שלכם,
[01:16:55 - 01:16:55] ‫של החישוב,
[01:16:56 - 01:16:59] ‫אז הוא יעשה את הנגזרת הזאת גם, ‫כולל ה-XT שנכנס כאן לגרף.
[01:17:00 - 01:17:01] ‫אם הוא יהיה מחוץ לגרף,
[01:17:02 - 01:17:06] ‫אז הוא לא יעשה את הנגזרת כמו לזה.
[01:17:07 - 01:17:08] ‫אבל המימוש הזה זה די קל.
[01:17:11 - 01:17:15] ‫להבין קצת את ההבדלים ובדיוק מה קרה, ‫אולי זה בסוף קשה.
[01:17:16 - 01:17:22] ‫בסדר, אני מקווה שהתרגיל יהיה מספיק מודרף ‫שנופץ לי כמו למימוש הזה.
[01:17:26 - 01:17:26] ‫טוב,
[01:17:27 - 01:17:30] בואו נעשה הפסקה ‫ואז נדבר על גיידנס,
[01:17:31 - 01:17:32] ‫שזה בעצם אותו עיקרון,
[01:17:34 - 01:17:34] ‫אבל
[01:17:37 - 01:17:38] שם אחר ועוד איזה טריקים.
[01:17:46 - 01:17:49] ‫לא היה גיידנס, לא היה גיידנס, נכון?
[01:17:51 - 01:17:51] ‫כן,
[01:17:51 - 01:17:52] הבנתי,
[01:17:52 - 01:17:53] ‫כי אפשר להשתמש בקלאס,
[01:17:55 - 01:17:55] ‫כן.
[01:17:59 - 01:18:03] ‫גיידנס זה בדרך כלל פריק פרוסיאנס.
[01:18:16 - 01:18:16] ‫שלום.
[01:18:17 - 01:18:19] ‫אבל יש את זה כבר אירוע קטן, ‫הוא היה שישי זמן.
[01:18:20 - 01:18:21] ‫אצלנו גם.
[01:18:22 - 01:18:23] ‫זה נראה לי קצת יותר.
[01:18:23 - 01:18:24] ‫קצת יותר.
[01:18:24 - 01:18:25] ‫נראה לי בסדר.
[01:18:26 - 01:18:27] ‫מה העיקרון שאיתנו עכשיו?
[01:18:28 - 01:18:30] ‫אני אענה לי המדד העיקרי יקצר את זה.
[01:18:31 - 01:18:33] ‫בימי שיש שש בימי רביעי עכשיו הרצאה.
[01:18:34 - 01:18:34] ‫כן.
[01:18:35 - 01:18:36] ‫לא לא מתכוונתי למהר.
[01:18:38 - 01:18:39] ‫אמת שלא הסתכלתי, אבל נפלא את זה בזה,
[01:18:40 - 01:18:42] ‫אבל אה...
[01:18:42 - 01:18:44] ‫לא ראיתי על...
[01:18:46 - 01:18:55] ‫אני חושב שאנחנו חושבים את הסמסטר. ‫אני חושב שאנחנו חושבים את הסמסטר, ‫נראה לי בפרק שצריך להגיע ‫קצת זמן, קצת.
[01:18:56 - 01:19:04] ‫אני חושב שאין לך איזה שבועיים של ראש. ‫-אז נראה לי מישהו? לא, לא, לא, ‫אם עוד מישהו, כן, אבל הוא...
[01:19:05 - 01:19:06] ‫לא כן, היה לו איזשהו רואה עכשיו.
[01:19:06 - 01:19:09] ‫זה נראה לי משהו כזה.
[01:19:11 - 01:19:12] ‫אז הוא גם נהנה מזה שקצת היה אפשרי.
[01:19:16 - 01:19:17] ‫סבבה?
[01:19:18 - 01:19:22] ‫אז אני קורא לתת אותו ללטר לאבק ‫לגבל מרנק חילקסי וגם מזליגה.
[01:19:28 - 01:19:31] ‫אני חושב שאתה מקייס מזוכר עשיתי בעצם ‫בסמסטר קיץ שנ הבא.
[01:19:32 - 01:19:34] ‫הוא דיבר שסיפי את הקורס בלי להירשם.
[01:19:35 - 01:19:38] ‫הוא נרשם אבל כמובן לא עושה עניין ‫בסמסטר, אז תוכל לדבר.
[01:19:39 - 01:19:42] ‫אז אני רוצה להתאומן בעצם לבין אדם,
[01:19:43 - 01:19:43] לבין
[01:19:43 - 01:19:45] ‫תגיד מה,
[01:19:46 - 01:19:48] ‫הדברים פשוט מפגנות.
[01:19:51 - 01:19:52] ‫טוב, נבין.
[01:19:52 - 01:19:53] ‫הדיברסים זה ה...
[01:19:54 - 01:19:55] ‫הדיברסים מפגיעים אצל ה...
[01:19:56 - 01:19:59] ‫אפשר לומר, כן.
[01:20:02 - 01:20:06] ‫זה לא עובר המוצץ ‫כל המוצץ משתמש בו.
[01:20:07 - 01:20:09] ‫הוא באמת גם הכי...
[01:20:10 - 01:20:11] ‫כאילו, הכי יודע איך להשתמש בו.
[01:20:12 - 01:20:17] ‫שוב, גם המסקנה של הממודדים, ‫כל פעם הם פתוחים ‫שרק מודל אחד לגבי זה של...
[01:20:18 - 01:20:20] ‫תגיד, זה קצת הסיכורת.
[01:20:21 - 01:20:26] ‫לא המודד שייתן בתשובה לכל, ‫אבל זה מגיע עם המסקנה ‫שהכול די במאה.
[01:20:27 - 01:20:30] ‫כאילו, גם אפשר לא למכור עכשיו ‫לעביר את הצוג של BAE,
[01:20:30 - 01:20:31] ‫כמו הסוג של קלות.
[01:20:34 - 01:20:36] ‫פעם חשבו שגם זה יראה, ‫שאתה
[01:20:38 - 01:20:38] ‫יוצא תמונות.
[01:20:39 - 01:20:45] ‫בכל החיטות מצליחים לייצר דברים טובים. ‫-אני מתכוון.
[01:20:47 - 01:20:48] ‫כי גם צריך להתערב בזה.
[01:20:53 - 01:20:58] ‫זה גם הייתה ביקורת כזאת, ‫אני חושב שזה קשה מאוד גדולה.
[01:20:59 - 01:21:01] ‫זה משתמש כמו הקרבות שמוסרות בה?
[01:21:01 - 01:21:03] ‫מה? יש דברים מאוד קרבים ‫שאני חושב שאתה רוצה ללכת רגמן?
[01:21:05 - 01:21:06] ‫מבחינת זה?
[01:21:06 - 01:21:07] ‫-לייצר בגיל מאוד טובים.
[01:21:08 - 01:21:12] ‫בגלל זה דבר רעים מאוד גדולים, ‫מבחינת הברירה.
[01:21:13 - 01:21:16] ‫המציאות שלהם לאימונית ‫היא חופשית קשה.
[01:21:16 - 01:21:21] ‫כן, כאילו זה די טובים יותר טובים, ‫זה די מדהים וכו'.
[01:21:22 - 01:21:23] ‫-אבל בגלל אם הוספו לזה את כל העניין ‫לעבודה,
[01:21:24 - 01:21:27] ‫יש הקריטיק שעושים לו, של הוואן ליבש יוצאון.
[01:21:28 - 01:21:37] ‫כן, האמת שאני לא כל כך מכיר בעד איתו, ‫אבל יש הרבה דברים על הגם, ‫יש כבר לא מדיבן, ‫כאילו, יש לזה גם פחות חשובים טוב,
[01:21:37 - 01:21:40] ‫ב-DAE כזה, שיש לו, ‫משהו שהופך אותו, אני חושב, ככה,
[01:21:41 - 01:21:41] ‫סריאור,
[01:21:43 - 01:21:44] ‫יש כל מיני פריטיקאים.
[01:21:45 - 01:21:48] ‫אז בגלל, כאילו, ‫מצפים עם כל השיטות האלה ‫שדיברנו בקורס,
[01:21:48 - 01:21:53] ‫אצל גימות מנהימות ולעשות כל מיני ‫טוב מלבד.
[01:21:55 - 01:21:58] ‫היום נראה לי ש... ‫בשביל ללמוד, ‫כאילו זה הכי חדש, ‫אז אתם מתלהבים ממנו.
[01:21:59 - 01:22:04] ‫לא יודע אם שיהיה חודר ב... ‫נראה שהוא הכי קל לאימון הזה, ‫גם סרטיב באוזן.
[01:22:04 - 01:22:05] ‫יש לי גם מגבלת השאלה.
[01:22:06 - 01:22:09] ‫אז גימה הוא לפצצות גיאורל.
[01:22:11 - 01:22:13] ‫זה הביא את המילדון ב-2011?
[01:22:15 - 01:22:24] ‫כן, בטוחה שהם מייצרים, ‫מצליחים לעשות משהו טוב, כן. ‫ב-2015 כבר זה הגדירו את אותו מודל, ‫וכל מיני תוצאות,
[01:22:25 - 01:22:26] ‫אבל שם זה לא היה נראה כל כך מבטיח.
[01:22:28 - 01:22:28] ‫כן,
[01:22:29 - 01:22:31] ‫מודל משוגע שלו זה משהו שלו.
[01:22:32 - 01:22:33] ‫אני,
[01:22:33 - 01:22:42] ‫בפיזי וספר, אני לא יודע אם ‫מה הוא רוצה, ואז לאצלנו אחרי כמה שנים, ‫הבינו שזה די דומה למודלים ‫שטילפו את זה.
[01:22:44 - 01:22:44] ‫תודה רבה לך.
[01:22:59 - 01:23:01] ‫זה יהיה חצי הרצאה, ‫ואנחנו תהיה איזו הרצאה.
[01:23:02 - 01:23:05] ‫זה ההצעות האחרונות, ‫אנחנו לא בעצם של הדברים, זה בסדר.
[01:23:08 - 01:23:09] ‫בקורסים של איף משהו,
[01:23:09 - 01:23:09] ‫הם מלא
[01:23:12 - 01:23:14] ‫מלא תחומים שהם נהגים לרעה.
[01:23:18 - 01:23:21] ‫ארבע שנים הייתי נהג ‫לדבר על גני עם שלושה שבעים.
[01:23:25 - 01:23:28] ‫אני רוצה לך קצת להעמיק, ‫אבל כשיש לך את דבר המוזיאון.
[01:23:28 - 01:23:37] ‫זה מאוד נורגש בקורס ‫של למידה עמוקה, ‫שאתה כזה עובדים בכל דבר, ‫ואפשר להתעסק עם הראש הקדוש.
[01:23:58 - 01:24:09] ‫-כן, תודה רבה. ‫-כן, תודה רבה.
[01:24:28 - 01:24:38] ‫-כן, תודה רבה, אדוני היושב-ראש. ‫-כן, תודה רבה, אדוני היושב-ראש, חברותיי חברי הכנסת, ‫תודה רבה.
[01:24:58 - 01:25:28] ‫-כן, תודה רבה.
[01:25:28 - 01:25:29] ‫-כן, תודה רבה.
[01:25:58 - 01:25:59] ‫-כן, תודה רבה.
[01:26:28 - 01:26:31] ‫-כן, תודה רבה.
[01:26:58 - 01:27:02] ‫תודה רבה, אדוני היושב-ראש.
[01:27:28 - 01:27:29] ‫-כן, תודה רבה.
[01:27:58 - 01:27:59] ‫-כן, תודה רבה.
[01:28:28 - 01:28:32] ‫-כן, תודה רבה, אדוני היושב-ראש. ‫-כן, תודה רבה.
[01:28:58 - 01:29:00] ‫-כן, אדוני היושב-ראש.
[01:29:28 - 01:29:29] ‫-כן, אדוני היושב-ראש.
[01:29:58 - 01:30:00] ‫-כן, תודה רבה, אדוני היושב-ראש.
[01:30:28 - 01:30:29] ‫-כן, תודה רבה.
[01:30:58 - 01:31:02] ‫-כן, תודה רבה, אדוני היושב-ראש.
[01:31:16 - 01:31:17] ‫אוקיי, המשך.
[01:31:23 - 01:31:26] ‫אוקיי, אז יש לנו דבר ‫על מה שנקרא גיידנס, יש לנו, בדרך כלל,
[01:31:27 - 01:31:29] ‫שני גרסאות, Classifier Guidance,
[01:31:30 - 01:31:33] ‫שזה איך שהם הגיעו לזה,
[01:31:33 - 01:31:36] ו- Classified Free Guidance, ‫זה מה שהם משתמשים בדרך כלל,
[01:31:37 - 01:31:43] ‫וזה גם מה שהם משתמשים בו ‫בשביל לייצר תמונות ‫מתוך פרונטים של טקסטים.
[01:31:44 - 01:31:49] אוקיי, אז בעצם כבר ראינו ‫שאנחנו יכולים לעשות פוסטריו עם סמפרינג,
[01:31:50 - 01:31:54] ‫עם הסכום הזה של הפרייר והלייקליות.
[01:31:55 - 01:31:57] ‫זאת אומרת, y יכול להיות איזשהו קלאס,
[01:31:58 - 01:32:00] ‫יכול להיות פצי של תמונה או כל דבר אחר,
[01:32:01 - 01:32:03] ‫אבל בין השאר יכול להיות איזשהו קלאס,
[01:32:04 - 01:32:06] ‫ואז פי של y בהינתן x
[01:32:07 - 01:32:08] זה Classifier.
[01:32:12 - 01:32:16] ‫בעצם בשביל זה, אנחנו צריכים לעשות את זה, ‫להגיע לזה, אנחנו צריכים לאמן Classifier.
[01:32:16 - 01:32:17] y בהינתן x,
[01:32:19 - 01:32:21] ‫אמרנו שובה, יש לנו את הבעיה הזאת עם רעש,
[01:32:22 - 01:32:27] ‫ש-x בעצם פה איטרציה הוא רעש, ‫אז יש לנו כמה דרכים לפתור את זה, נכון?
[01:32:27 - 01:32:29] ‫אחת מהדרכים זה לתת את הדוגמה
[01:32:32 - 01:32:32] הנקייה,
[01:32:33 - 01:32:36] ‫זאת אומרת, שיערוך של הדאטה הנקי כל פעם,
[01:32:37 - 01:32:44] ‫אבל במקרה של Classifier, ‫אולי זה דווקא די קל, ‫אנחנו יכולים פשוט לאמן את ה-Plassifier,
[01:32:44 - 01:32:48] ‫לא רק על תמונות הלקיות, ‫אלא גם על תמונות רועשות, ‫ואז אם ה-Plassifier שעובד טוב,
[01:32:49 - 01:32:51] ‫או עד כמה שהוא יכול,
[01:32:51 - 01:32:53] ‫גם כשהתמונה היא רועשת.
[01:32:54 - 01:32:57] ‫אז זה למשל מה שאפשר לעשות,
[01:32:57 - 01:32:59] ‫אז זה רק כשעושים אותו.
[01:33:00 - 01:33:06] ‫זאת אומרת, מאמנים הדבר הזה, ‫זה בעצם ה-Output של ה-Plassifier שלנו,
[01:33:07 - 01:33:11] ‫שהוא אומן מראש על איקסים ברעשים שונים, ‫אז לכן הוא יכול לעבוד
[01:33:14 - 01:33:14] ‫בכל מיני
[01:33:15 - 01:33:21] ‫בכל מיני שלבים בדגימה, ‫תהליך הדגימה.
[01:33:23 - 01:33:26] ‫ומה ששמו לב זה שאם נותנים יותר משקל
[01:33:30 - 01:33:32] ‫לבר הזה שמגיע מה-Plassifier,
[01:33:33 - 01:33:35] ‫אז זה מוביל לתוצאות יותר טובות.
[01:33:38 - 01:33:40] ‫אז דוגמים מה-Prior, ‫אבל נותנים גם משקל יותר גדול
[01:33:42 - 01:33:43] ל-Plassifier,
[01:33:44 - 01:33:45] ‫נגזרת של כוספי.
[01:33:46 - 01:33:47] ‫זה בדרך כלל קוראים גיידנס.
[01:33:49 - 01:34:01] ‫אז לפעמים באופן טלי, ‫לכל העניין הזה של Posterior Sampling, ‫זה שמוסיפים את זה לקוראים גיידנס, ‫אבל בדרך כלל המקור של הגיידנס ‫זה זה שנותנים פה משקל קצת יותר גדול.
[01:34:06 - 01:34:11] ‫אז פה זה מסומן בתור גמא, לפעמים ב-S,
[01:34:12 - 01:34:17] ‫קוראים לזה גיידנס-סקל. ‫ובעצם מה שמקבלים,
[01:34:18 - 01:34:20] ‫אם ה-S פה הוא 0,
[01:34:22 - 01:34:23] זאת אומרת, אנחנו לא נותנים פה שום משקל,
[01:34:25 - 01:34:27] ‫אז אנחנו בעצם רק דוגמים מה-Prior,
[01:34:28 - 01:34:28] נכון?
[01:34:28 - 01:34:32] ‫שזה אחד, אפשר לחשוב על זה ‫בתור פוסטריאו רגיל,
[01:34:33 - 01:34:40] ‫אבל ככל שאנחנו מגדילים את הגמא הזה, ‫זה בעצם סוג של אינטרטורה ‫של המשקל הזה, כי זה מחוץ ללוג.
[01:34:41 - 01:34:43] ‫זה כמו שזה יהיה פה באקספוננט,
[01:34:44 - 01:34:48] ‫זה בעצם מחדד את ההתפלגות הזאת ‫של ה-Classic-Fair,
[01:34:48 - 01:34:53] ‫זאת אומרת שכשאתה יוצא מהטווס הנכון, ‫הההסתברות מאוד מהר דועכת.
[01:34:55 - 01:34:55] ‫וזה
[01:34:58 - 01:34:59] אמפירית פשוט,
[01:35:00 - 01:35:04] ‫הגיעו למצב שהם שמים פה ‫מספרים יחסית גדולים, כמו 10, ‫מגיעים לתוצאות הרבה יותר טובות.
[01:35:05 - 01:35:09] זו דוגמה מהמאמר המקורי שראה את זה, ‫אני לא יודע אם אתם רואים אותו כך,
[01:35:09 - 01:35:17] ‫אבל זה כאן פגימה של תמונות של Y זה כלב, ‫זה classic fire של תמונות,
[01:35:18 - 01:35:20] ‫והם פשוט מייצרים על ידי ‫לנג'מין דיינאמיקס,
[01:35:21 - 01:35:25] ‫בדיוק כמו שאמרנו קודם, ‫רק שמוסיפים לפריו את הנגזרת הזאת ‫מתוך ה-Classic-Fair,
[01:35:26 - 01:35:31] ‫שוב, איך מחשבים את זה? ‫לוקחים קלאסי-Fair, ‫שאומן על תמונות, ‫גם רועשות וגם עמוקיות,
[01:35:32 - 01:35:35] ‫נותנים לו את ה-X הנוכחי,
[01:35:36 - 01:35:41] ‫ואם מסתכלים מה ההסתברות ‫שהוא נותן ל-Y שמעניין אותנו,
[01:35:42 - 01:35:43] ‫כלב,
[01:35:44 - 01:35:45] ‫והם חושבים את הנגזרת של זה,
[01:35:46 - 01:35:51] ‫אוקיי? זאת אומרת, ‫אנחנו גוזרים דרך ה-classifier ל-input ל-X, ‫זה מה שמוסיפים,
[01:35:51 - 01:35:53] ‫ואם הם מכפילים את הדבר הזה ‫שמוסיפים,
[01:35:54 - 01:35:58] אני רק אומר עוד פעם אני מבהיר, ‫שתי הדברים האלה ‫הם בגודל של התמונה, אוקיי? ‫בגודל של X.
[01:36:00 - 01:36:03] יש שתי תמונות בעצם שאני מוסיף לתמונה,
[01:36:04 - 01:36:05] ‫הדגימה הנוכחית שלי שיש פה תמונה.
[01:36:06 - 01:36:10] ‫אז מה שראו, שאם הדבר הזה ‫הוא יותר ויותר גדול,
[01:36:11 - 01:36:13] ‫אז זה נותן תמונות ‫שהן יותר ויותר טובות, ‫נראות יותר טוב,
[01:36:14 - 01:36:16] ‫אבל זה על חשבון קצת ה-diversity.
[01:36:18 - 01:36:21] ‫אז פה התמונות, ‫יש פה כל מיני דברים מוזרים,
[01:36:23 - 01:36:25] ‫אבל יש פה גם כלבים ‫בכל מיני פנוחות שונות,
[01:36:25 - 01:36:26] ‫במקומות שונים.
[01:36:28 - 01:36:30] ‫לא יודע אם הדבר הזה זה כלב, אבל משהו...
[01:36:31 - 01:36:32] ‫מה רבי? זה רק חילומי, טיפח. כן.
[01:36:34 - 01:36:38] ‫וכאן הכלבים נראים הרבה יותר טוב, ‫הכול כאילו יותר נכון,
[01:36:39 - 01:36:39] ‫אבל
[01:36:44 - 01:36:45] הגיוון קצת יותר נמוך,
[01:36:47 - 01:36:48] ‫יש פחות גיוון.
[01:36:49 - 01:36:51] ‫כל הכלבים האלה הם בערך באותה פוזיציה,
[01:36:52 - 01:36:57] ‫לא יודע, יש הרבה דמיון בצורה של הרבה מהם,
[01:36:57 - 01:37:02] ‫ואני רואה שאם הדבר הזה, ככל שהדבר הזה גדל,
[01:37:02 - 01:37:04] ‫בעצם אנחנו כאילו פחות מתייחסים לפריור,
[01:37:04 - 01:37:14] ‫ויותר לקלאסיפייר עצמו, ‫שעושים backpropagation דרך הקלאסיפייר ‫עד שמוצאים את התמונה שנותנת ‫הההסתברות הכי גבוהה לקלו.
[01:37:16 - 01:37:18] ‫אז יש פה איזשהו טריידוף בין הדברים,
[01:37:19 - 01:37:21] ‫אבל עדיין הגיעו, ‫בדרך כלל הטריידוף הוא לא באחד.
[01:37:22 - 01:37:26] ‫תמונות שאנשים אהבו ותוצאות שאנשים אהבו,
[01:37:27 - 01:37:31] ‫בדרך כלל היו איזשהו סקל שהוא גדול, ‫5, 10, כל מיני דברים כאלה.
[01:37:32 - 01:37:33] ‫בדבר הזה קוראים לי דיידנס.
[01:37:35 - 01:37:39] ‫עכשיו, יש דרך אחרת לעשות, ‫לייצר תמונה שהיא conditional.
[01:37:42 - 01:37:44] ‫במקום לקחת, גם לאמן פריור על כל הדאטה,
[01:37:45 - 01:37:49] ‫וגם לאמן קלאסיפייר על כל הדאטה, ‫ואחר כך לשלב ביניהם,
[01:37:49 - 01:37:54] ‫אפשר מראש לאמן את הפריור שלנו ‫ככה שהוא יהיה conditional.
[01:37:55 - 01:38:01] אם אתם זוכרים נכון, ‫אם אתם זוכרים, אימנו unit ככה שהוא לא רק מקבל ‫את התמונה הרועשת.
[01:38:02 - 01:38:07] ‫לנותן את הרעש, את הפרדיקה של הרעש, ‫אלא גם מקבל את הזמן, נכון? ‫זה conditional על הזמן.
[01:38:08 - 01:38:12] ‫אז באותו אופן אנחנו יכולים לעשות ‫אולי conditional על דברים אחרים, ‫למשל על הקלאס.
[01:38:13 - 01:38:18] ‫-אז במצב כזה זה לא הופך להיות כמו פחות גנרטיבי, ‫כאילו זה סופרוויזד.
[01:38:19 - 01:38:24] ‫זה סופרוויזד, זה הופך להיות טיפה, לא? ‫-זה פחות סופרוויזד, ‫אבל זה עדיין מודל גנרטיבי.
[01:38:25 - 01:38:27] ‫זה עדיין מודל גנרטיבי, אבל כן.
[01:38:28 - 01:38:29] ‫אז אתה צודק.
[01:38:29 - 01:38:32] ‫אז זו נקודה שהיא חשובה.
[01:38:35 - 01:38:38] ‫תכף נראה בעצם איך אפשר ‫לדבר על זה.
[01:38:41 - 01:38:53] ‫אבל זו שיטה שגם אפשר לעשות, ‫ולאמן ככה משהו שהוא condition, ‫ואז כל פעם שאנחנו רוצים ‫לייצר תמונה של כלב, ‫נכניס כאן בכל האיטרציות ‫של הדגימה את ה-y של כלב,
[01:38:54 - 01:38:55] ‫ואם זה משהו אחר, ‫אז נכניס משהו אחר.
[01:38:56 - 01:38:57] ‫אז זה גם אפשרי,
[01:38:58 - 01:39:02] ‫אבל בפועל בעצם, ‫זה מה שנקרא, קלאסיפי פרי גיידנס,
[01:39:04 - 01:39:08] ‫התוצרות הכי טובות הצליחו להגיע ‫למגדל שמשתמשים בטריק הזה של
[01:39:11 - 01:39:15] ‫להגדיל בעצם את הסקייט של הגיידנס, ‫אבל דרך מודלים שהם קונדישיונל.
[01:39:16 - 01:39:16] ‫איך עושים את זה?
[01:39:17 - 01:39:19] ‫אז מאמנים קודם כול מודל שהוא קונדישיונל,
[01:39:20 - 01:39:22] ‫אבל הוא לא רק קונדישיונל, ‫הוא גם יכול להיות un-condition.
[01:39:24 - 01:39:24] הוא שניהם.
[01:39:25 - 01:39:25] איך עושים את זה?
[01:39:26 - 01:39:27] פשוט עושים דרופ-אאוט.
[01:39:27 - 01:39:28] ‫מי זה דרופ-אאוט?
[01:39:29 - 01:39:31] ‫דיפ-נרנינג?
[01:39:32 - 01:39:38] ‫זה פשוט בצורה רנזורמלית, ‫לפעמים מאפסים חלק מה...
[01:39:39 - 01:39:41] ‫דרופ-אאוט רגיל, ‫ומאפסים חלק מהמשקולות.
[01:39:41 - 01:39:43] ‫פה הדרופ-אאוט הוא על ה-conditioning,
[01:39:44 - 01:39:44] ‫זאת אומרת,
[01:39:45 - 01:39:46] ‫מתקנים על המודל הזה.
[01:39:48 - 01:39:56] ‫אז מדי פעם אנחנו ניתן פה פשוט מספר שהוא לא קשור, ‫אבל 0 או מינוס 1 או משהו כזה,
[01:39:57 - 01:40:00] ‫שהוא יהיה פשוט... ‫ואז אנחנו ניתן לו תמונה שהיא
[01:40:02 - 01:40:04] ‫יכולה להיות כלב, ‫אבל יכול להיות גם משהו אחר,
[01:40:04 - 01:40:11] ‫וזה יאפשר לנו, בהקשר למה ששאלת, ‫זה יאפשר לנו לעשות גם Unsupervised Data,
[01:40:11 - 01:40:17] ‫אנחנו יכולים להכניס כל התמונות שיש לנו, ‫לחלק מהם יש לנו מידע על ה-class ולחלק אין,
[01:40:18 - 01:40:21] ‫ובאופן רדולמי מדי פעם להשתמש ‫בתמונות עם מידע על ה-class,
[01:40:21 - 01:40:23] ‫שנכניס להם את המידע על ה-class,
[01:40:24 - 01:40:28] ‫ובאופן, מדי פעם נותן תמונה בלי ה-class, ‫ואז נכניס כאן מינוס 1,
[01:40:30 - 01:40:35] ‫ולפעמים את התמונה שיש לנו מידע על ה-class, ‫אבל להסתיר את ה-class,
[01:40:35 - 01:40:39] ‫נשים שם בכל פעם מינוס 1. ‫אז המודל הזה אפשר להשתמש בו ‫בעצם בשתי דרכים,
[01:40:40 - 01:40:44] ‫גם בדרך שהיא Unconditional ‫וגם בדרך שהיא conditional.
[01:40:45 - 01:40:46] ‫ואז ברגע שיש לנו את המודל הזה,
[01:40:47 - 01:40:49] ‫הדריק הזה של Classify free guidance אומר,
[01:40:50 - 01:40:53] ‫אנחנו נעשה את החישוב הזה, ‫נחשב,
[01:40:55 - 01:41:02] ‫נייצר שתי דגימות בכל איטרציה, ‫נחשב שני סקור, ‫אחד שהוא Unconditional ואחד שהוא Conditional.
[01:41:12 - 01:41:14] ‫אוקיי, אז אנחנו מייצרים את הסקור ‫שהוא Unconditional,
[01:41:16 - 01:41:18] ‫על ידי זה שאנחנו שמים מיוס 1 שם ב-Y,
[01:41:19 - 01:41:22] ‫ופעם אחת אנחנו מייצרים את הסקור ‫שהוא Conditional.
[01:41:23 - 01:41:28] ‫עכשיו, במקום להתקדם פשוט בצד ‫שהוא ה-conditional,
[01:41:28 - 01:41:33] ‫אנחנו מסתכלים מה ההבדל בין הכיוון ‫של ה-conditional וה-unconditional,
[01:41:34 - 01:41:38] ‫ואנחנו מגבירים את ההבדל ביניהם.
[01:41:39 - 01:41:40] ‫אז אנחנו מסתכלים על ה...
[01:41:42 - 01:41:44] ‫אנחנו מתקדמים בכיוון ה-unconditional, ‫שזה הפרעיון רגיל,
[01:41:45 - 01:41:49] ‫מסתכלים על ההבדל בין ה-conditional ‫ל-unconditional,
[01:41:50 - 01:41:53] ‫ומוסיפים את הדבר הזה, ‫אבל לא באחד, אלא באחד ועוד איזשהו...
[01:41:56 - 01:41:57] ‫כלומר, זה נקרדה בלימוד.
[01:41:58 - 01:41:59] אוקיי?
[01:42:01 - 01:42:05] זה ברור, אנחנו מקבלים את אותו אפקט ‫שהיה לנו קודם עם ה-guidance,
[01:42:06 - 01:42:08] ‫עם ה-classifier guidance,
[01:42:09 - 01:42:12] ‫שיכול לעשות סקייל יותר גדול ‫ושדהיה יותר חד,
[01:42:13 - 01:42:14] תמונות יותר טובות,
[01:42:14 - 01:42:19] ‫אז זה טריק שעושים פה, אוקיי? ‫אז בעצם מאמנים מודל ‫שהוא גם קונדישיונל, גם unconditional,
[01:42:20 - 01:42:21] ‫ומגבירים את
[01:42:24 - 01:42:26] ‫הכיוון של ההפרש בינינו.
[01:42:28 - 01:42:29] ‫ואפשר להראות שזה בעצם, כאילו זה,
[01:42:30 - 01:42:33] ‫החישוב הזה שווה לזה.
[01:42:34 - 01:42:37] ‫בעצם אפשר להראות שוב, ‫שזה סוג של posterior sampling,
[01:42:38 - 01:42:41] ‫שאני נותן ל-posterior משקל יותר גדול מאחד.
[01:42:43 - 01:42:43] ‫זה אותו דבר.
[01:42:44 - 01:42:52] ‫כי הנגזרת של הפוסטריאוזוס, ‫שווה הנגזרת של ה-lightlיות ‫ושל ה...
[01:42:54 - 01:42:55] ‫נופיע כאן הפוך.
[01:42:58 - 01:43:02] ‫אני רוצה לעבוד את הכיוון ההפוך, אוקיי? ‫שאם אני מחשב את ה...
[01:43:14 - 01:43:20] ‫אוקיי, אז פשוט מה שזה מראה כאן ‫זה הקשר בין posterior sampling, ‫שזה סליחה, זה הצד הזה,
[01:43:21 - 01:43:30] ‫לאיך שראינו קודם שזה ‫קלאסיפייר גיידנס, אוקיי? ‫בקלאסיפייר גיידנס נתנו משקל יותר גדול מאחד ‫ל-likelihood שלנו,
[01:43:31 - 01:43:34] ‫אוקיי? וזה שקול לזה שאנחנו יכולים ‫מראש לחשב
[01:43:35 - 01:43:36] ‫קונדישיונל,
[01:43:37 - 01:43:39] ‫מודל קונדישיונל, שהוא בעצם סוג של פוסטריאור,
[01:43:40 - 01:43:41] ‫ולתת לו את המשקל היותר גדול.
[01:43:44 - 01:43:46] ‫שוב, אם אני מחשב,
[01:43:47 - 01:43:50] ‫אני רוצה לעשות את הדבר הזה ‫שעשיתי קודם,
[01:43:51 - 01:43:54] ‫למצוא את הוקטור הזה ולהגביר אותו קצת,
[01:43:55 - 01:43:56] ‫אוקיי? אז אני אלך לכאן.
[01:43:58 - 01:44:01] ‫והדרך שאני עושה את זה, ‫זה במקום לאמן קלאסיפייר ‫בכיוון הזה,
[01:44:02 - 01:44:03] ‫זה קלאסיפייר גיידנס,
[01:44:04 - 01:44:06] ‫אני מאמן את ה-likelihood,
[01:44:07 - 01:44:09] ‫שזה פי של x בין את n y.
[01:44:10 - 01:44:17] ‫שוב, בגלל שהפוסטריאור שווה ‫ללייקליות
[01:44:18 - 01:44:19] ‫פחות ה-prior,
[01:44:21 - 01:44:21] ‫אז זה שקול.
[01:44:26 - 01:44:29] ‫הוא נפק תופס גם את ה-likelihood וגם את ה-prior.
[01:44:29 - 01:44:29] ‫בדיוק.
[01:44:30 - 01:44:35] ‫תוהים לתופס את ה-likelihood ואת ה-prior, ‫ואז אין לכל ספקט, אם זה מה שהוא רוצה לעשות, ‫לחשב את זה את ה-posterior,
[01:44:36 - 01:44:37] ‫וגן לחשב את ה-guidance הזה,
[01:44:37 - 01:44:41] ‫כן ללכת אפילו עוד יותר ‫בכיוון של הלייקליות.
[01:44:44 - 01:44:46] ‫זה גם את ה-prior,
[01:44:47 - 01:44:48] ‫ואלך יותר לכיוון של ה-posterior.
[01:44:53 - 01:44:57] ‫זה, כל התוצאות הטובות שאתם רואים, ‫זה בדיוק כבר עם הטריק הזה.
[01:45:03 - 01:45:06] ‫אוקיי, ושוב, זה נותן לנו תוצאות שהן יותר
[01:45:08 - 01:45:08] ‫יותר טובות.
[01:45:09 - 01:45:11] ‫זאת אומרת, המודל ה-conditioned יותר אוהב אותן,
[01:45:12 - 01:45:13] ‫אבל הן פחות דייברס.
[01:45:15 - 01:45:18] ‫פחות מגוונות ממה שה-prior היה ייצר.
[01:45:19 - 01:45:22] ‫אולי, כאילו, בשלב מסוים זה אפילו יותר ‫ממה שהיינו רוצים
[01:45:24 - 01:45:26] ‫שה-conditional sampling הזה ייצר לנו.
[01:45:26 - 01:45:27] ‫כן, בסדר, אני אראה דוגמאות.
[01:45:28 - 01:45:29] ‫אבל שוב, מה שזה מאפשר לנו,
[01:45:29 - 01:45:31] ‫המידול הזה,
[01:45:32 - 01:45:34] ה-conditional,
[01:45:35 - 01:45:38] ‫זה שזה מאפשר לנו בעצם איזשהו ‫שארינג של ה...
[01:45:41 - 01:45:44] ‫אנחנו משתפים את המודל בין לדעת, ‫אנחנו יכולים לקחת דעת ה-unsupervised,
[01:45:45 - 01:45:47] ‫וחלק ממנו כן-supervised,
[01:45:48 - 01:45:59] ‫ופשוט לא יודע אם יש הסבר ‫ממש תיאורטי למה זה עובד, ‫אבל זה בצורה אמפירית, ‫נראה שזה תתפס בצורה מאוד טובה את ה...
[01:46:00 - 01:46:04] ‫או, בוא נאמר, מאפשר בצורה מאוד גמישה ‫לעבוד עם סוגי דאטה שונים.
[01:46:05 - 01:46:09] ‫זאת אומרת, עם כמויות דאטה ‫מאוד גדולות on-supervised ‫וכמות קטנה שהיא כן סופרוויזת,
[01:46:10 - 01:46:13] ‫מאוד מאפשר לעשות את ה-Trade-off הזה בצורה איתה.
[01:46:16 - 01:46:17] ‫אוקיי? ובעצם ה...
[01:46:18 - 01:46:20] כן, אז זה למשל דוגמה של
[01:46:23 - 01:46:24] ‫תמונה בהינתן כלב.
[01:46:25 - 01:46:30] ‫אז מה שנקרא כאן נוג איידנס, ‫זה אומר שפשוט ה-W שווה,
[01:46:31 - 01:46:33] ‫האומגה הזה שווה 0, ‫זאת אומרת שהמשקל,
[01:46:35 - 01:46:37] ‫של ה-Lineer הוא אחד פשוט רגיל,
[01:46:37 - 01:46:39] ‫אוקיי? סגמנו את זה אחד ועוד...
[01:46:43 - 01:46:44] ‫תקנו את זה אחר כך בפתרונות,
[01:46:45 - 01:46:48] ‫אבל נראה פה זה הרבה יותר מגוון,
[01:46:49 - 01:46:53] ‫וזה הולך ונהיה פחות מגוון, ‫אבל תמונות יותר טובות.
[01:46:55 - 01:46:57] ‫כנראה שפשוט איזשהו פריידות באמצע.
[01:46:57 - 01:47:05] ‫אוקיי, איך מגיעים מפה לתמונה בהינתן פרומפט?
[01:47:07 - 01:47:16] ‫אז זה פשוט אותו דבר, ‫אנחנו מסתכלים במודל הזה ככה, ‫רק שבמקום שה-Y יהיה קלאס, ‫ה-Y יהיה איזשהו פרומפט
[01:47:16 - 01:47:17] ואיזשהו טקסט,
[01:47:18 - 01:47:24] ‫אבל בדרך כלל אנחנו לא נכניס טקסט ישר, ‫אנחנו נכניס
[01:47:25 - 01:47:26] איזשהו אמבדינג של טקסט,
[01:47:27 - 01:47:35] ‫שהוא הגיע כבר מאיזשהו language model ‫שאומן על טקסט.
[01:47:36 - 01:47:45] ‫יש פה כמה גרסאות, ‫נגיד ב-Jat, ב-Dallay 2 משתמשים ב-Clip. ‫אתם מבינים איזה קליפ?
[01:47:46 - 01:47:51] ‫אז קליפ זה כבר מודל שמחבר בעצם ‫בין אמבדינג של language models
[01:47:52 - 01:47:54] ‫למודלים של תמונות.
[01:47:55 - 01:47:57] ‫אז יש פה בעצם, בואו נסתכל על הדוגמה הזאתי,
[01:47:58 - 01:48:00] ‫אז יש פה בעצם שלושה שלבים.
[01:48:01 - 01:48:03] ‫הסיבה שזה כל כך הרבה שלבים ‫זה כי באמת רוצים,
[01:48:04 - 01:48:06] ‫מה שגורם לדברים האלה לעבוד, ‫זה
[01:48:07 - 01:48:08] זה שמעניין אותם על המון דאטה.
[01:48:09 - 01:48:12] ‫ויש לנו הרבה דאטה של טקסט,
[01:48:15 - 01:48:18] ‫הרבה דאטה של תמונות, ‫אבל פחות,
[01:48:18 - 01:48:20] ‫יותר קשה קצת להגיע אליהן,
[01:48:21 - 01:48:29] ‫והרבה דאטה של תמונות וטקסט ביחד, ‫מדוברים, ‫אבל עוד פחות משל שניהם.
[01:48:30 - 01:48:37] ‫אז בעצם כל הסיבה שהדבר הזה עובד, ‫זה גם חילה לשלב בין המקורות ‫השונים האלה של הדאטה שלנו.
[01:48:37 - 01:48:38] ‫אז עם טקסט,
[01:48:39 - 01:48:42] עם כל הטקסט שיש, ‫מאמנים language model,
[01:48:42 - 01:48:43] ‫שזה משהו שלא כל כך דיברנו עליו,
[01:48:44 - 01:48:49] זה חבל, אבל זה בגדול תחשבו, ‫זה כמו פיקסל CNN על משפטים.
[01:48:50 - 01:48:51] ‫במקום שזה יהיה על פיקסלנט בתמונות,
[01:48:52 - 01:48:55] ‫זה על משפטים של מילים,
[01:48:59 - 01:49:03] ‫זה גם ממש בדרך כלל ‫עם טרנספורמה או עם קונבולוציות.
[01:49:05 - 01:49:07] ‫אבל זה מודל הסתברותי של טקסט.
[01:49:09 - 01:49:14] ‫אז זה השלב הראשון, ‫ועושים אותו על הכי הרבה דאטה, ‫כל הטקסט שיש באינטרנט.
[01:49:16 - 01:49:17] ‫השלב השני זה קליפ.
[01:49:18 - 01:49:20] ‫לוקחים זוגות של...
[01:49:21 - 01:49:26] ‫דאטה שיש לנו בזוגות, ‫של טקסט ותמונות ביחד,
[01:49:27 - 01:49:35] ‫וזו שיטה שנקראת contrastive learning, ‫שבעצם מאמנים איזשהו ייצוג של הטקסט.
[01:49:36 - 01:49:38] ‫זאת אומרת, לוקחים את הייצוג ‫מתוך המודל שפה,
[01:49:39 - 01:49:42] ‫פלוס הייצוג, לומדים איזשהו ייצוג של התמונה,
[01:49:42 - 01:49:45] ‫ככה שהייצוגים שלהם יהיו די קרובים אחד לשני.
[01:49:47 - 01:49:54] ‫אז מה זה אומר? ‫זה תופס איזשהו ייצוג שהוא טוב, ‫שהוא מחבר יחסית טוב בין תמונות לטקסט.
[01:49:55 - 01:49:59] ‫הייצוג של הטקסט כבר הוא די טוב, ‫כי הוא יודע כבר, הוא למד על כל הטקסט
[01:49:59 - 01:50:08] ‫באינטרנט, אז הוא כבר יודע ‫מה זה מודל טוב של טקסט, ‫ויש לו דרך טובה לייצג טקסט. ‫זה בעצם סוג של ‫רפוזנטיישן learning שעושים על הטקסט.
[01:50:09 - 01:50:12] ‫אז משתמשים בו כדי לקרב ייצוג ‫של תמונות לייצוג הזה.
[01:50:13 - 01:50:16] ‫עכשיו יש לנו איזשהו ייצוג טוב ‫שהוא תופס טוב טקסט,
[01:50:16 - 01:50:19] ‫אבל בעיקר דברים שקשורים ‫לאיך דברים נראים.
[01:50:21 - 01:50:22] ‫אז הוא רלוונטי לתמונות.
[01:50:23 - 01:50:27] ‫אבל זה כשלעצמו לא מספיק ‫כדי לייצר מודל טוב של תמונות.
[01:50:27 - 01:50:34] ‫אנחנו צריכים גם רק להסתכל, ‫בעצם הוא לא ראה כל כך הרבה תמונות, ‫הוא רק ראה את כל התמונות ‫שהן מגיעות ביחד עם טקסט.
[01:50:35 - 01:50:37] ‫זה גם לא בדיוק מודל גנרטיבי של תמונות.
[01:50:38 - 01:50:41] ‫אז השלב השלישי זה השלב של ה-difusion model,
[01:50:42 - 01:50:45] ‫זה שמאמנים מודל של תמונה ‫שהוא יכול להיות
[01:50:45 - 01:50:54] ‫קונדישן על האמבדינג הזה, ‫ולפעמים הוא גם לא. ‫זאת אומרת, הוא יכול להסתכל ‫על כל התמונות באינטרנט,
[01:50:55 - 01:50:58] ‫ואת האמבדינג הזה אפשר לחשב אותו ‫גם מהתמונה וגם מהטקסט.
[01:50:59 - 01:51:03] ‫אני לא זוכר בדיוק, יש כמה גרסאות גם, ‫אני לא זוכר בדיוק איפה הם עשו את זה,
[01:51:03 - 01:51:09] ‫אבל בסופו של דבר ‫אנחנו מקבלים חשיפה להמון תמונות,
[01:51:10 - 01:51:12] ‫אנחנו יכולים לאמן פרייור טוב על התמונות,
[01:51:13 - 01:51:14] ‫וגם למספיק
[01:51:14 - 01:51:17] תמונות שהן קונדישנד על ה-Latent.
[01:51:18 - 01:51:19] ‫בגלל שה-Latent הזה הוא כבר,
[01:51:19 - 01:51:20] ‫Latent די,
[01:51:21 - 01:51:27] תופס די טוב, ‫כל מה שקשור לשפה בקשר לתמונות, ‫אז לא צריך יותר מדי דוגמאות של זוגות.
[01:51:29 - 01:51:35] אוקיי? אני אומר לא צריך יותר מדי, ‫זה עדיין כנראה כל הדאטה ‫שגוגל או אופן-איי הצליחו להשיג, שזה המון.
[01:51:36 - 01:51:40] אוקיי, ואז משתמשים בדיוק ‫בקלאסיפייר פרי גיידאס.
[01:51:41 - 01:51:47] ‫אנחנו נותנים בתור Y את האמבדינג ‫שמגיע מטקסט,
[01:51:50 - 01:51:53] ‫ועושים חישוב אחד לפי הפרייור,
[01:51:53 - 01:51:56] ‫נוסחה הזאת שאתה פה, חישוב אחד לפי הפרייור,
[01:51:57 - 01:51:58] ‫שמטעמם מהטקסט,
[01:51:59 - 01:52:03] ‫חישוב אחד שכן מסתכל על הטקסט, ‫ומגבירים את ההפרש ביניהם.
[01:52:05 - 01:52:09] ‫וזה הצעד שעושים צעד אחד ‫באיטרציה של Langerine Dynamics.
[01:52:10 - 01:52:11] ‫נגיד, אלף צעדים,
[01:52:12 - 01:52:14] ‫ומקבלים תמונה שהיא
[01:52:15 - 01:52:17] ‫קשורה לטקסט שקיבלנו.
[01:52:20 - 01:52:20] ‫ברור?
[01:52:21 - 01:52:25] ‫אז זה די חשוד ודי מרשים שזה עובר.
[01:52:26 - 01:52:30] ‫אוקיי, אז הדוגמאות עוד ישרנו ‫לפני איזה שנה ומשהו,
[01:52:31 - 01:52:37] ‫אוקיי, דלי שטיין ואימג'ן, ‫אז המימוש ביניהם הוא טיפה שונה, ‫אז דלי שטיין משתמש בקליפ אימג'ן,
[01:52:38 - 01:52:43] ‫הוא משתמש ישירות באיזשהו אמבדינג ‫של המודל שפה שהיה אז, ‫אני לא זוכר איך קראו לו,
[01:52:44 - 01:52:51] ‫אבל העיקרון, מבחינת ‫הדיפיוז'ן מודל, ‫העיקרון הוא די דומה, ‫איך בדיוק האמבדינג הזה ‫נחושב בפני ושונה.
[01:52:51 - 01:53:06] ‫אז כל אינטרציה כאילו התמונה נכנסת לקליפ, ‫ואז נמצא אמבדינג ‫ואז אתה משמיד על האמבדינג שבצית? ‫-אני לא חושב שהתמונה נכנסת לקליפ, ‫דווקא היא כי אתה, בהינתן הקליפ, ‫אתה יכול להגיע לאמבדינג ‫מהטקסט או מהתמונה.
[01:53:07 - 01:53:11] ‫כל שיש גם איזה משהו, ‫איזשהו פידבק כזה מהתמונה, ‫אבל באופן, נגיד,
[01:53:12 - 01:53:14] ‫התירוב הראשון של המודל זה ‫שאתה לוקח את הטקסט,
[01:53:15 - 01:53:17] ‫משתמש בו כדי לייצר את האמבדינג,
[01:53:17 - 01:53:23] ‫אז אתה מכניס את זה ל-unit ‫של ה... של הדיפיוז'ן מודל,
[01:53:24 - 01:53:24] ‫שאתה משתמש בו בכל צעדה.
[01:53:26 - 01:53:29] פעם אחת אתה מכניס אותו, ‫ופעם אחת אתה לא מכניס אותו, ‫וזה מה שמייצר לך את הפריור,
[01:53:31 - 01:53:33] ‫את הפריור ואת ה-likelihood,
[01:53:34 - 01:53:35] ‫או את הפוסטריאור, פריור ואת הפוסטריאור,
[01:53:36 - 01:53:39] ‫ואז אתה מחשב את החישוב הזה ‫עם איזשהו דע בדיוק.
[01:53:40 - 01:53:41] אתה מדון מאפס בדרך כלל,
[01:53:42 - 01:53:46] ‫סוג של פוסטריאור סמפלינג, ‫אבל עם קצת יותר דגש על ה...
[01:53:47 - 01:53:48] ‫אבל אני אוהב את זה.
[01:53:55 - 01:53:57] ‫אוקיי, יש פה גם דוגמה לאיך ה...
[01:54:00 - 01:54:01] ‫איך ה...
[01:54:01 - 01:54:02] ‫הסקייל הזה משפיע.
[01:54:03 - 01:54:04] ‫אז פה נגיד הטקסט הוא...
[01:54:05 - 01:54:10] ‫הפונקט שנתנו זה, ‫-Astain Glass Window of a Panda Eating Bumboי.
[01:54:11 - 01:54:15] ‫אז זה עם הסקייל אחד, זאת אומרת,
[01:54:17 - 01:54:20] ‫שוב, יש פה כל מיני שמות, ‫דרכים אחריות לקרוא לסקייל הזה.
[01:54:21 - 01:54:25] ‫כאן כתוב אחד, no guidance, ‫זאת אומרת שה-S הוא אחד, ‫זאת אומרת שה-1 ועוד
[01:54:26 - 01:54:31] ‫אומגה זה אומגה שווה 0. ‫אז הם מכוונים משהו שהוא יחסית,
[01:54:32 - 01:54:36] ‫חלק מההטמעות הן נחמדות, ‫אבל יש כל מיני דברים ודברים,
[01:54:37 - 01:54:39] ‫שהם לא לגמרי ברורים,
[01:54:39 - 01:54:45] ‫אבל זה קצת יותר מגוון מפה, אוקיי? ‫פה הכול נראה ממש טוב מבחינת המבנה,
[01:54:46 - 01:54:49] ‫אבל הרבה פחות מגוון.
[01:54:49 - 01:54:53] ‫זה סוג אחד של תמונות, ‫ויש פה עוד דוגמה ל...
[01:54:54 - 01:55:00] ‫אני משפט יותר מורכב, ‫-A cozy living room with a painting of a corgi, ‫זה סוג של כלב, on the wall,
[01:55:01 - 01:55:06] ‫עבוב הקו של around copy table ‫בפרונט of a car ‫של a vase of flower on a copy table.
[01:55:07 - 01:55:14] כן, יש את החלק שלוקח את המשפט המפרצתי הזה ‫והופך את זה ‫לא לאיזשהו וקטור שהוא משהו סביר,
[01:55:15 - 01:55:22] ‫אז זה לא ה-Defusium, כן? ‫זה המודל שפה קודם כול, ‫ואז ה-Clip שיודע לקשר את זה ‫לאיזשהו אמבדינג של תמונות.
[01:55:23 - 01:55:37] ‫אז אחר כך ה-Defusium, ‫יודע להשתמש בזה כדי לייצר תמונות, ‫והם פחות סקל מקבלים יותר גיוון, ‫אבל חלק מהדברים ‫הם לא לגמרי או לא אביוניים או לא נכונים, ‫פהוא למשל יש כלב על השפה, ‫במקום בציור על הקיר.
[01:55:38 - 01:55:41] ‫גם פה יש כלב על השפה. ‫אהאמור להיות כלב על השפה?
[01:55:45 - 01:55:49] ‫משפטים כאלה הם מורכבים, ‫זה אולי מודלים יותר חדשים אם הם מצליחים,
[01:55:50 - 01:55:58] ‫אבל כאן בכל מקרה מה שרואים ‫זה שה-diversity הרבה יותר קטן. ‫תמונות כאילו נראות יותר טוב, ‫אבל דגלנו כאן 16 תמונות ‫שהן נראות כמעט אותו דבר.
[01:56:00 - 01:56:02] ‫-הקליפ זה רק לאימון או גם בתמונות?
[01:56:03 - 01:56:05] ‫משתמשים בו בטסט-טיים,
[01:56:06 - 01:56:11] ‫אתהצורך לתרגם את המשפט ‫ל-Embedding ‫שאתה נותן ל-Defusiumium.
[01:56:12 - 01:56:15] ‫רגע, הקליפ גם משנה את האמבדינג ‫של המילים או את של התמונות.
[01:56:18 - 01:56:19] ‫קליפ הוא לוקח,
[01:56:20 - 01:56:26] ‫הדרך שאתה מאמן קליפ זה לגרום ‫ל-Embedding של טקסט ‫ול-Embedding של תמונות להיות קרוב אחד לשני,
[01:56:26 - 01:56:32] ‫ואז אתה יכול להגיע ‫ל-Embedding הזה משני הכיוונים. ‫בהינתן תמונה אתה יכול להגיע ‫ל-Embedding, ‫בהינתן טקסט אתה יכול להגיע לזה.
[01:56:33 - 01:56:33] ‫אבל כאילו,
[01:56:34 - 01:56:38] הוא משנה גם שהוא רק את האמבדינג ‫של התמונות או שגם אם תרגישי המילים?
[01:56:39 - 01:56:42] ‫-כאילו, בהינתן משפט... ‫-באימון שלו,
[01:56:43 - 01:56:46] ‫או כשאתה מאמן קליפ, ‫אז אני חושב שהוא משנה גם של הטקסט.
[01:56:48 - 01:56:55] ‫הוא לומד אמבדינג של הטקסט ‫שמתחיל, אני חושב, ‫מאיזשהו אמבדינג של איזשהו language model ‫ב-GPT, זה כנראה.
[01:56:59 - 01:57:04] ‫אבל זה נגיד שונה ב-Image-Z, ‫ב-Image-Z זה בדיבובר על הקליפ, ‫אני חושב שזה ישירות מה language model.
[01:57:04 - 01:57:07] ‫האמבדינג שנכנס ל-Defuric model הוא ישירות מ language model.
[01:57:09 - 01:57:15] ‫כל המשחקים האלה זה בעצם אתה רוצה ‫איכשהו להיות מסוגל גם לעבוד ‫עם כל הטקסטים באינטרנט,
[01:57:17 - 01:57:20] ‫וכל התמונות באינטרנט וכל הזוגות ‫בצורה שהיא יחסית פעילה.
[01:57:22 - 01:57:28] ‫אם היה לך בצורה עילה דרך לעבוד ‫עם הכול ישר ב-Defusion model, ‫אז היית יכול ישר ללמוד ‫הכול ביחד כנראה.
[01:57:29 - 01:57:37] ‫אתה פתח טקסט, ‫בדיוק כמו classic fire guidance. ‫פתח טקסט, לומד איך להפוך אותו למשהו ‫שאחר כך הוא משתמש בו ב-Defusion model.
[01:57:39 - 01:57:45] ‫טוב, אז רק סיכום של השיעור היום,
[01:57:47 - 01:57:54] ‫סיכום קצר, כי האמת שזה היה טיפה, ‫חשבתי שזה יזרום קצת יותר את ההתחלה.
[01:57:55 - 01:57:59] ‫אני מקווה שתצליחו בתרגיל להעביר את זה, ‫אני מקווה שזה יהיה קצת יותר ברור.
[01:58:00 - 01:58:02] אבל העיקרון הוא שבעצם,
[01:58:03 - 01:58:06] ‫אחד מהיתרונות של דefusion model, ‫שיחסית קל לנו להשתמש בזה,
[01:58:06 - 01:58:17] ‫בתור פרייר שהוא כללי כזה, ‫ואז להשתמש בו, לעשות ממש פלאג-אין ‫לכל מיני סוגים של העסקה הסתובבותית.
[01:58:18 - 01:58:26] ‫אז אנחנו ראינו דוגמה של להשתמש בזה ‫כדי לייצר דגימות מהפוסטריאור, ‫שזה כשלעצמו מאפשר לנו ‫לעשות כל מיני דברים אחר כך.
[01:58:27 - 01:58:28] ‫אנחנו רוצים לעשות MCMC,
[01:58:29 - 01:58:32] ‫לפשט כל מיני חישובים עם MCMC, ‫מה שאנחנו צריכים זה לדעת
[01:58:33 - 01:58:35] ‫לייצר דגימות מהפוסטריאור.
[01:58:37 - 01:58:39] ‫הדרך,
[01:58:39 - 01:58:44] החלק האחרון בעצם, ‫הראה שבעצם השיטות האלה,
[01:58:44 - 01:58:46] ‫ביחד עם זה שאנחנו רואים לנו דרך
[01:58:48 - 01:58:51] חכמה להשתמש בדאטה שונה, בטקסט,
[01:58:52 - 01:58:54] ‫עשות conditional training, לחלק,
[01:58:58 - 01:59:03] ‫לעשות את זה בצורה גמישה כזאת, זאת אומרת, ‫כשיש לך מודל שאתה יכול ‫שהוא יהיה conditional או לא conditional,
[01:59:03 - 01:59:05] ‫ולשחק עם כל הסקייל שם,
[01:59:05 - 01:59:08] ‫זה מגיע, מאפשר לנו להגיע ‫לתוצאות מדהימות.
[01:59:11 - 01:59:17] ‫שבאמת, כאילו אף אחד לא... ‫עוד לפני כמה שנים אף אחד ‫לא דמיין שזה בכלל אפשרי, ‫אז הרבה מזה זה המודל שפה עצמו,
[01:59:19 - 01:59:24] ‫שהוא מצליח להבין משהו יותר חכם ‫על ה...
[01:59:25 - 01:59:28] בעצם איזשהו אמבדינג טוב של משפטים ‫מאוד מאוד מורכבים,
[01:59:29 - 01:59:34] ‫אבל גם בעצם משאירים שהדפיושן מודל, ‫די קל לעשות לו קונדישיינינג, ‫על כל מיני גברים יחסית מורכבים כאלה.
[01:59:36 - 01:59:45] ‫הסיקרט אינבדיאנט, המרכיב הסודי, ‫זה כמובן הדאטה אחרת, ‫כל הדבר הזה, מה שגורם לזה ‫להיות ממודל שעובד טוב,
[01:59:46 - 01:59:56] ‫אבל בעצם הסיבה שאנחנו יכולים ‫להשתמש בכל כך הרבה דאטה ‫זה שהמודל הזה הוא די יעיל, ‫הוא לומד יחסית מהר, ‫אתם תראו בתרגיל ‫שדי מהר אפשר להגיע ‫לתוצאות טובות.
[02:00:00 - 02:00:05] ‫כן, אז זה כבר בדברנו, זאת אומרת, ‫היכולת לאמן על המון טקסט בנפרד, ‫ואז לעשות קונדישיינג,
[02:00:06 - 02:00:08] ‫יעיל על ה-Refusion Model, ‫זה חלק מה...
[02:00:12 - 02:00:17] ‫טוב, אז הגענו לסיכום של הקורס, ‫אנחנו נעשה רק סיכום קצר.
[02:00:21 - 02:00:24] ‫זה היה השיקף שהיה ‫קודם יותר בהתחלה של כל שיעור.
[02:00:25 - 02:00:29] ‫הצלחתי אותו במלחמתם בהיגיון בהתחלה, ‫שנגיע אליו עכשיו.
[02:00:30 - 02:00:35] ‫בעצם המטרה שלנו כאן הייתה תמיד ‫שאנחנו רוצים ללמוד ‫הזדברות של הדאטה, נכון?
[02:00:35 - 02:00:38] ‫אנחנו פשוט מחליטים על איזשהו גידול,
[02:00:39 - 02:00:41] ‫מודל הסתברותי, ‫איזה שהם פרמטרים גדולים,
[02:00:42 - 02:00:45] ‫ומחפשים את הפרמטרים ‫שיהיו כמה שיותר קרובים לדאטה.
[02:00:46 - 02:00:52] ‫אנחנו רוצים לעשות את זה בצורה כזאת, ‫שתאפשר לנו לעשות ‫כל מיני דברים אחר כך. ‫קודם כול, שזה יהיה יעיל למצוא,
[02:00:52 - 02:00:57] ‫כמה דברים, ‫שזה יהיה עשיר יחסית, נכון? ‫שנכנס הרבה הסתברויות,
[02:00:58 - 02:01:01] ‫שיהיהיה יעיל לעשות את האימון, חושב, את ה... ‫לצמצם את המרחק הזה,
[02:01:02 - 02:01:10] ‫ושאחר כך גם אוכל להשתמש לכל מיני דברים. ‫נכון? אז ראינו הרבה דוגמאות ‫של להשתמש בזה ‫בשביל לייצר פשוט דאטה, לדגום לזה.
[02:01:11 - 02:01:21] ‫ראיתם קצת בתרגולים להשתמש בזה ‫כדי ללמוד ייצוג ‫שאולי תופס דברים בצורה יותר יעילה ‫מאשר התמונות
[02:01:21 - 02:01:23] של הפיקסלים עצמם באופן ישיר.
[02:01:23 - 02:01:25] ‫זה לא כל כך התעסקנו,
[02:01:26 - 02:01:27] ‫לא ראינו דוגמאות של זה.
[02:01:29 - 02:01:31] חבל קצת, אבל אולי...
[02:01:32 - 02:01:43] ‫אצל יותר זמן הייתי יכול להכניס ‫קצת דוגמאות של איך גילה ‫שזה מודל הסתברותי, שהוא ממודל ‫לא רק את הנקודה הכי טובה, ‫אלא איזושהי התפלגות.
[02:01:43 - 02:01:44] ‫אפשר להשתמש בזה כדי לאחר כך,
[02:01:45 - 02:01:50] ‫בתור משהו מודולרי, ‫שהם משתמשים בזה ‫כדי לקבל החלטות אחר כך
[02:01:51 - 02:01:54] ‫על קומפוננטות אחרות.
[02:01:55 - 02:02:00] ‫וראינו כבר, היום גם דיברנו, ‫וגם בתרגיל ראיתם, ‫כן תראו גם בתרגיל האחרון,
[02:02:00 - 02:02:12] ‫איך לעשות כל מיני פורליסטיק אינסונס, ‫והיא נותנת איזשהו משהו לדעת עדיין, ‫לדעת לייצר את ההתפלגות של פוסטיריות.
[02:02:16 - 02:02:19] ‫דיברנו על זה בעצם, ‫שיש כמה מרכיבים,
[02:02:20 - 02:02:22] ‫אז שוב, הדאטה זה כנראה דבר חשוב.
[02:02:23 - 02:02:25] ‫שאל אותי קודם מה המודל
[02:02:26 - 02:02:30] ‫הכי טוב בכל המודלים, ‫דיפיישן מודלים זה המודל
[02:02:30 - 02:02:31] ‫שבאמת מי טוב.
[02:02:31 - 02:02:35] ‫בעצם הדבר הכי חשוב זה הדאטה. ‫אם יש לנו דאטה, מספיק דאטה
[02:02:37 - 02:02:40] ‫שמייצג את מה שאנחנו רוצים לבדל,
[02:02:41 - 02:02:46] ‫והמודל שלנו הוא כזה ‫שאנחנו יכולים להשתמש בו
[02:02:47 - 02:02:50] ‫כדי לעשות פרוססינג ‫לכל הדאטה הזה, ‫לאמן על כל הדאטה הזה,
[02:02:51 - 02:02:54] ‫אז כנראה שנצליח, ‫לא משנה איזה מודל להשתמש.
[02:02:55 - 02:02:58] ‫יש מודלים שיותר קל איתם ‫לעשות כל מיני דברים,
[02:02:58 - 02:03:02] ‫כיום דיפיישן מודל באמת הוא המודל ‫שהכי קל לעשות איתו את רוב הדברים,
[02:03:03 - 02:03:03] ‫אבל
[02:03:06 - 02:03:07] ‫לא בטוח שזה יישאר ככה.
[02:03:09 - 02:03:12] ‫אז הדאטה זה כנראה הדבר ‫הכי חשוב בכל
[02:03:13 - 02:03:14] ‫המרכיבים האלה,
[02:03:15 - 02:03:17] ‫אבל יש עוד כמה דברים חשובים. ‫אז המודל,
[02:03:18 - 02:03:21] ‫אז יש כל מיני דרכים למדל את ה...
[02:03:24 - 02:03:26] ‫זה בעצם פרמטריזציה, ‫להניח כל מיני הנחות,
[02:03:27 - 02:03:29] ‫ראינו כל מיני דוגמאות, ‫אז ראינו גרסיונים,
[02:03:29 - 02:03:30] ‫ראינו תרומטריזציונים,
[02:03:31 - 02:03:34] ‫ראינו מודלים ממשתנים חבויים,
[02:03:35 - 02:03:36] ‫מודלים אוטו-רגרסיב,
[02:03:37 - 02:03:40] ‫כל אחד בעצם ההנחות ‫שאנחנו עושים שם קצת שונות.
[02:03:42 - 02:03:43] ‫עכשיו יש את הobjectives,
[02:03:45 - 02:03:46] ‫זה מה אנחנו,
[02:03:46 - 02:03:51] איך אנחנו מאמנים את המודל שלנו, ‫בעצם ראינו שתי דוגמאות, ‫ראינו מקסימום לייקליות,
[02:03:52 - 02:03:55] ‫ובארצות האחרונות, ‫עכשיו ראינו סקור ומצ'ים.
[02:03:56 - 02:04:11] ‫גם ראינו שזה קשור, נכון? ‫אז דיברנו על Diffusion Model ‫בתור Variational Out-Encoder, ‫ראינו שבעצם אפשר להגיע ‫בדיוק לאותו Objective function ‫כשאנחנו מתחילים מכאן ‫עם כל מיני קירובים, ‫או כשאנחנו מתחילים מכאן ‫עם כל מיני קירובים.
[02:04:13 - 02:04:18] ‫אז הרבה פעמים לא ממש ברור ‫במה אנחנו משתמשים,
[02:04:19 - 02:04:24] ‫אבל בדרך טובה כדי לחשוב ‫כדי לפתח את המודל, ‫כדי להגיע אל ה-objective שאנחנו רוצים.
[02:04:25 - 02:04:32] ‫ודבר רביעי זה איך אנחנו עושים ‫בדיוק את האופטימיזציה, ‫אז לאורנו עשינו פשוט ‫גדיאנט-דסנט פשוט, ‫אבל לפעמים היינו צריכים ‫דברים קצת יותר מורכבים,
[02:04:33 - 02:04:35] ‫כמו הבעיה של ה-Inference,
[02:04:35 - 02:04:40] ‫שראינו גם ב-GMM וגם ב-BIAE.
[02:04:41 - 02:04:44] ‫לא כל כך ראינו על להשתמש ב-NCLC ‫לאימון עצמו,
[02:04:46 - 02:04:51] ‫אבל כן ראינו כל מיני דוגמאות ‫שאנחנו צריכים להשתמש ב-NCLC, ‫ואנחנו לא יכולים לעשות פשוט ‫מקסימום לייקליות באופן ישיר.
[02:04:54 - 02:05:02] ‫כן, זה היה שקט מההרצאה הראשונה, ‫שבעצם, מה שאתם זוכרים, ‫המטרה שלו הייתה להגיד
[02:05:03 - 02:05:04] ‫שסתם למדל את הכול,
[02:05:05 - 02:05:11] ‫פשוט יש יותר מדי פרמטרים, נכון? ‫אם אנחנו רוצים לתפוס את ההסתברות ‫של כל האפשרויות שייקרו,
[02:05:12 - 02:05:18] ‫אנחנו חייבים לעשות איזושהי הנחות ‫כדי לברוח מקללת המימד.
[02:05:19 - 02:05:24] ‫ובגדול אפשר לחשוב על שני כיוונים ‫שהלכנו בהם,
[02:05:25 - 02:05:29] ‫שאפשר לחשוב עליהם ‫מתוך שני חוקים של הסתברות, ‫אחד זה
[02:05:30 - 02:05:32] של מקהל השרשרת, והשני זה חוק בייס.
[02:05:33 - 02:05:36] ‫מקהל השרשרת הגענו למודלים ‫האוטו-רגרסיב,
[02:05:37 - 02:05:43] ‫ומחוק בייס הגענו למודלים ‫שמבוססים על מודל משתמש חבוני.
[02:05:44 - 02:05:49] ‫נכון, זה היה מושלך שלנו ‫את שני השקפים האלה,
[02:05:50 - 02:05:51] ‫את שני הגישות האלה.
[02:05:52 - 02:05:54] ‫מודל אוטו-רגרסיב,
[02:05:54 - 02:05:57] ‫או כזה שאנחנו בעצם ‫מפרקים את הדאטה לחלקים,
[02:05:58 - 02:06:00] ‫כל חלק תלוי בחלק הקודם,
[02:06:01 - 02:06:06] ‫ואז אפשר או להניח ‫כל מיני הנחות אי-תלות, למשל שכל חלק תלוי ‫רק בחלק מהקודמים,
[02:06:06 - 02:06:08] ‫כדי לחסוך בפרמטרים,
[02:06:08 - 02:06:13] ‫או להניח איזושהי פרמטריזציה ‫לא מלאה, זאת אומרת,
[02:06:13 - 02:06:17] ‫של לפשט, למשל ראינו דוגמה ‫של ליניארית כזאת, נכון? ‫או איזושהי רשת
[02:06:18 - 02:06:20] ‫שלוקחת את כל האינפוט,
[02:06:21 - 02:06:25] ‫מספר סופי של פרמטרים, ‫צריכה לעשות פרדיקציה ‫על הפיקסל הבא.
[02:06:27 - 02:06:29] ‫אז השתמשנו בזה ‫כדי להוריד לנו את מספר הפרמטרים,
[02:06:30 - 02:06:37] ‫וגישה אחרת זה להשתמש, ‫להסתכל על אייטן פריבל, נכון? ‫אז בעצם בהינתן המשתנה החבוי שלנו,
[02:06:37 - 02:06:40] ‫כל הפרמטרים הם בלתי תלויים,
[02:06:40 - 02:06:42] ‫או שוב, שיש פה איזושהי הנחה של
[02:06:43 - 02:06:49] ‫איזשהו פישוט של הפרמטריזציה. ‫למשל ב-VAE,
[02:06:50 - 02:06:53] ‫אז X בהינתן Z היה...
[02:06:57 - 02:07:01] כן, אז X בהינתן Z גם שם היה בעצם, ‫כל פיקסל היה בלתי תלוי איך שנימשתם את זה,
[02:07:01 - 02:07:12] ‫אבל התוכלת של זה הושבה על ידי ‫איזושהי קונבולוציה, זאת אומרת שבעצם היה ‫איזשהו שיתוף של כל Z, ‫השפיע על כמה Xים
[02:07:14 - 02:07:14] בסביבה שלו.
[02:07:16 - 02:07:18] ‫אז דוגמאות של המודלים פה שראינו,
[02:07:19 - 02:07:21] ‫אז דיברנו על זה, ‫הכול היה בהרצאה אחת,
[02:07:22 - 02:07:24] ‫זה מודלים די פשוטים בסופו של דבר,
[02:07:24 - 02:07:27] ‫NATE, פיקסל CNN ופיקסל CNN,
[02:07:28 - 02:07:32] ‫וכאן זה בעצם היה, רוב הקורס היה ‫במודלים מהסוג הזה,
[02:07:34 - 02:07:38] ‫כי זה גם מצריך אוטימיזציה יותר קשה,
[02:07:39 - 02:07:42] ‫כל מיני שיטות להתמודד עם זה, כמו E&,
[02:07:42 - 02:07:44] ‫דגימת NCC, ובריישון אינפרנס,
[02:07:45 - 02:07:55] ‫וגם יש הרבה מודלים שונים ‫שדיברנו על זה. אז GMM זה המודל הראשון ‫שדיברנו עליו, שהוא לא מודל מרשתות,
[02:07:56 - 02:08:00] ‫ואחר כך דיברנו על BAE, ‫Normalizing Flows ו-Defusion Models.
[02:08:03 - 02:08:06] ‫גם דיברנו על גאנדס, חשבתי להוסיף פעם, ‫אבל נכנסית מעט.
[02:08:07 - 02:08:16] ‫אוקיי, זה באופן כללי, אני רק רציתי להזכיר ‫איך הן נשכחות
[02:08:18 - 02:08:20] ‫מההרצאות הראשונות,
[02:08:23 - 02:08:25] ‫שבעצם דיברנו קצת שם על
[02:08:29 - 02:08:36] ‫גישות בייזיאניות, אבל למה בכלל נכנסנו ‫לעניין הזה של BAE, כי אפשר לחשוב על משתנים כאלה,
[02:08:36 - 02:08:37] ‫Latent Variables,
[02:08:38 - 02:08:39] ‫בתור איזשהו משהו שהוא בין
[02:08:40 - 02:08:44] למידה רגילה של ‫מקסימום לייטליות לבין למידה בייזיאנית.
[02:08:45 - 02:08:46] אתם זוכרים מה זה למידה בייזיאנית?
[02:08:47 - 02:08:49] ‫או שירוך בייזיאני של פרמטרים?
[02:08:51 - 02:08:52] אם יש לי איזשהו מודל,
[02:08:52 - 02:08:55] ‫אם אני פשוט מחפש את התטא ‫שממקסן את ה-Lightelיות,
[02:08:56 - 02:08:58] ‫זה דרך אחת, לרוב הדברים שעשינו,
[02:08:59 - 02:09:04] אבל דרך אחרת זה לחשוב על תטא ‫בתור משתנה חבוי בפני עצמו,
[02:09:04 - 02:09:08] ‫הפרמטרים שהם לא יודעים של המודל, ‫הם משתנים חבויים,
[02:09:08 - 02:09:11] ‫ואני מחפש את ההתפלגות ‫על המשתנים החבויים האלה.
[02:09:12 - 02:09:16] ‫זה שיש לי מודל עם משתנה חבוי, ‫זה אפשר לחשוב על זה ‫בתור איזשהו משהו באמצע.
[02:09:17 - 02:09:19] בעצם יש לי, הדברים שאני קורא להם תטא,
[02:09:19 - 02:09:23] ‫זה הדברים שאני לא עושה עליהם ‫כפי שהוא בייזיאנית, ‫אני עושה עליהם מקסימום לייטליות.
[02:09:23 - 02:09:24] ‫מה שאני קורא לו Z,
[02:09:25 - 02:09:30] זה בעצם פרמטר שאני כן ממדל אותו ‫בתור משתנה הסתברותי,
[02:09:31 - 02:09:32] ‫סטוחטי, לא ידוע,
[02:09:33 - 02:09:37] ‫שאני צריך למדל עליו את כל ההתפלגות, ‫ולא רק ליצור את ה Z דרך כלל סביר פה.
[02:09:39 - 02:09:40] ‫אפשר לחשוב על זה בתור משהו באמצע,
[02:09:41 - 02:09:45] ‫והשיטות הבייזיאניות בעצם זה ‫שתי הגישות שדיברנו עליהן,
[02:09:46 - 02:09:46] ‫גישה אחת
[02:09:47 - 02:09:49] דגימת,
[02:09:49 - 02:09:53] ‫וגישה אחרת שבה ישנה על ה... ‫זה בעצם שתי דרכים להתמודד
[02:09:53 - 02:09:54] עם latent variables.
[02:09:55 - 02:09:58] ‫באופן כללי זה שתי דרכים ‫להתמודד עם בייז'ן inference, זאת אומרת,
[02:09:58 - 02:10:02] ‫בהקשר שלנו רק דיברנו על זה ‫בהקשר של זה שיש לנו בתוך
[02:10:02 - 02:10:03] המודל איזשהו משתנה חבוי.
[02:10:04 - 02:10:08] ‫אז אפשר לפתור כל מיני דברים ‫על ידי דגימת MCLC,
[02:10:09 - 02:10:15] ‫גיבס זה סוג של דגימת MCLC שדיברנו עליה, ‫ול�ג'לין דיינאמיק זה גם סוג של גיבס MCLC.
[02:10:17 - 02:10:23] ‫ו-Varational inference זה בעצם להפוך ‫את הבעיה הזאת של האינפרנס
[02:10:23 - 02:10:26] ‫על משתנה חבוי לבעיית אופטימיזציה בפני עצמה.
[02:10:27 - 02:10:31] ‫אתם זוכרים, היה לנו שלב שהיה לנו, אם רצינו ‫לאמן עם Variational inference,
[02:10:31 - 02:10:32] ‫צריך לעשות שתי לולאות.
[02:10:34 - 02:10:39] ‫אחת בשביל לעשות שיערה פנימית, ‫בשביל לעשות שיערוך למה קורה ‫עם המשתנה החבוי הזה,
[02:10:39 - 02:10:42] ‫והחיצוני זה כדי לעשות ‫את המקסימום להקריאות לתטא.
[02:10:43 - 02:10:49] ‫ב-VaE ראינו שאפשר קצת לאחד ‫בין הדברים האלה, ‫ובעצם במקום לעשות לולאה פנימית,
[02:10:50 - 02:10:52] ‫ללמוד איזושהי רשת שבעצם כבר
[02:10:52 - 02:10:56] ‫היא לומדת איך לעשות ‫את האופטימיזציה הזאת ‫בצורה יותר יעילה.
[02:11:01 - 02:11:08] ‫אוקיי, זה היה לגבי כזה מין overview על החומר. ‫היה לנו בעצם שני שלבים. ‫היה את השלב הראשון שהיה מאוד תיאורטי,
[02:11:08 - 02:11:12] ‫אני חושב שכולם היו לגמרי, לא לגמרי, הבינו ‫איך הדברים מתחברים.
[02:11:12 - 02:11:18] ‫היה את השלב היותר פרקטי, ‫שאני מקווה שצרמכם לחבר ‫את הדברים התיאורטיים, ‫אבל
[02:11:19 - 02:11:24] במקרה של השלושת השקפים הקודמים, ‫זה היה קצת להזכיר לכם ‫מה היה שם בשלב התיאורטי יותר.
[02:11:25 - 02:11:30] ‫הדברים שבעצם התעסקנו איתם, ‫ואתם בתרגיל,
[02:11:30 - 02:11:37] ‫בצורה יותר פרקטית, ‫ואני מקווה שזה יורד לכם להבין ‫יותר את כל הדברים האלה.
[02:11:37 - 02:11:38] ‫יש פה רשימה של...
[02:11:39 - 02:11:44] ‫בעצם זה מבחינתי הכלים שיצאתם איתם.
[02:11:44 - 02:11:46] ‫אחד זה להבין מה זה גאוסיאנים רב-למדיים,
[02:11:47 - 02:11:48] ‫איך לעבוד איתם,
[02:11:49 - 02:11:55] ‫גם מבחינת יותר לא לממש את זה, ‫אלא גם ממש להבין מה זה,
[02:11:56 - 02:11:59] מה המשמעות של גידול גאוסיאני של דברים.
[02:12:01 - 02:12:04] ‫היו שיטות MCMC,
[02:12:04 - 02:12:10] ‫אז היה לכם... היה בדיוק מימוש, ‫אבל קצת היה לכם כמה שאלות
[02:12:11 - 02:12:12] ‫קצת יותר מעשיות על גיפס,
[02:12:13 - 02:12:17] ‫ולנג'ר מתנתן עכשיו, פוטו, ‫במה שאתם מבחינתם בעצם למונש את זה עכשיו.
[02:12:18 - 02:12:21] ‫אם גם לא ממש מימשתם, אני חושב, ‫אבל עשיתם איזשהו,
[02:12:22 - 02:12:26] ‫היו לכם כל מיני תרגילים ‫קצת יותר טכניים על זה.
[02:12:27 - 02:12:30] ‫אותו דבר שבהקשר, ‫גם יהיה וגם גאוס של מיקשר מודלס.
[02:12:33 - 02:12:37] ‫יש לכם קצת ניסיון לפרמנט דברים ‫בתור
[02:12:38 - 02:12:43] בעיה של פורמליסטיק אינפרנס ‫ולמצוא דרכים
[02:12:44 - 02:12:50] ‫להבין מתי אפשר לעשות את זה בקלות אולי, ‫ומתי צריך לעשות איזושהי שיטה שהיא יותר מקורבת.
[02:12:51 - 02:12:59] ‫ואימון של רשתות ניורונים שממדלים, ‫כל מיני מודלים הצטרפותיים.
[02:12:59 - 02:13:02] ‫נכון, אז שימשתם את פיקסל-CNN, ‫ואי-איי,
[02:13:03 - 02:13:04] ‫נורמל אייל פלוס, ועכשיו,
[02:13:05 - 02:13:09] ‫אה, אני מקווה את פרווקט התרגיל ‫אל דיפיוז'ון מודלס.
[02:13:13 - 02:13:16] ‫זה לגבי המילה המעשי.
[02:13:17 - 02:13:19] משהו שלא לגמרי דיברנו,
[02:13:19 - 02:13:28] ‫אבל בעצם דיברנו על כל מודל, ‫כאילו, בפני עצמו, ‫אבל יש הרבה פיתוח של, ‫שילוב של מודלים. ‫נתתי פה דוגמה אחת, ‫אבל יש עוד כל מיני דוגמאות.
[02:13:29 - 02:13:30] ‫אני חושב שדיברנו גם פעם על
[02:13:33 - 02:13:36] ‫שימוש ב-VaE עם פוסטריור, ‫שהוא אוטו-רגרסיב.
[02:13:37 - 02:13:38] ‫לא ראוי, דיברנו עם זה, אני חושב. ‫-כן.
[02:13:39 - 02:13:43] ‫אז פה יש עוד דוגמה שגם של אבת VE ‫עם דיפיוז'ון מודלס. ‫זה בעצם
[02:13:43 - 02:13:45] זה נקרא לייטן דיפיוז'ן,
[02:13:46 - 02:13:52] ‫שזה בעצם המודל שמשתמשים בו ‫בסטייבל דיפיוז'ן, אם אתם מכירים, ‫ונראה לי שגם במידג'ורנית.
[02:13:53 - 02:13:54] ‫אני חושב שזה יותר טוב מודל.
[02:13:56 - 02:13:59] ‫אז היחשובה זה שיש לנו בעצם קודם ‫VaE,
[02:14:00 - 02:14:06] ‫מאמינים קודם כול VE. ‫בעצם יש פה את הלקודר,
[02:14:06 - 02:14:08] ‫זה ה-posterior network,
[02:14:08 - 02:14:10] ‫והלקודר זה המודל הגנרטיבי,
[02:14:11 - 02:14:20] ‫ואנחנו מניחים שיש פה איזשהו פריור. ‫בדרך כלל ב-VE, ‫גם כשאתם מימשתם, ‫מניחים שהפריור הזה הוא גרסיין פשוט.
[02:14:21 - 02:14:30] ‫לפעמים לומדים את הפרמטרים של הגרסיין, ‫לפעמים לומדים לפריור, ‫זה נהיה קצת יותר מורכב מגרסיין, ‫יש לזה יתרונות.
[02:14:31 - 02:14:36] ‫ופה מה שעושים זה פשוט אחרי, ‫עושים את זה בנפרד, ‫קודם כול לומדים את ה-VE, ‫ואז
[02:14:37 - 02:14:38] לומדים את הפריור
[02:14:39 - 02:14:41] של ה-VE בנפרד,
[02:14:42 - 02:14:45] ‫כשאת הפריור לומדים עם דיפיוז'ן מרווי.
[02:14:46 - 02:14:49] ‫בעצם אנחנו יכולים לייצר הרבה,
[02:14:50 - 02:14:56] ‫קוראים לזה Z0, כי זה כבר לא X, ‫זו לא תמונה, זה ה-Latence של ה-VE,
[02:14:57 - 02:14:58] ‫אז זה יינתן הרבה,
[02:14:59 - 02:15:03] ‫אחרי שאימנו כבר את ה-VE, ‫אנחנו יכולים לקחת כל דוגמה, ‫להפוך אותה ל-Z.
[02:15:04 - 02:15:08] בעצם אנחנו משתמשים ב-VE בתור משהו ‫שנותן לסוציה מאוד פרזנטיישן לרנינג,
[02:15:09 - 02:15:11] ‫הוא מביא אותנו לאיזשהו מרחב ‫שהוא יותר טוב,
[02:15:14 - 02:15:16] ‫שהוא נקרא Z. ‫וה-Z הזה, עכשיו אנחנו קוראים לו Z0,
[02:15:17 - 02:15:19] ‫וממדלים אותו עם דיפיוז'ן מודל, ‫זאת אומרת, חוספים לו רעש.
[02:15:21 - 02:15:24] ‫הפורמולציה הזאת, הרציפה, ‫הנקודה האחרונה נקלט 1,
[02:15:25 - 02:15:26] ‫כאילו הוא רציל בין 0 ל-1.
[02:15:27 - 02:15:29] ‫בדיוק זה הפורמולציה הרגילה שראינו,
[02:15:31 - 02:15:34] ‫הציורים האלה זה כאילו הציורים של ה...
[02:15:35 - 02:15:38] ‫מקודת מבט הרציפה.
[02:15:38 - 02:15:42] ‫אז הדבר הזה זה דיפיוז'ן מודל, ‫שכל מיאמן אותו,
[02:15:42 - 02:15:49] ‫זה כמו לאמן את הקייל דיורג'נס הזה ‫בין הפריור האמיתי שלנו, ‫בין ה-VaE,
[02:15:50 - 02:15:53] סליחה, בין הפוסטריאור שמגיע מה-VaE, ‫לפריור שעכשיו אנחנו לומדים.
[02:15:55 - 02:15:57] ‫כן, אתם מחלקים את זה לשניים. ‫קודם לומדים את ה-VaE,
[02:15:58 - 02:16:02] ‫אנחנו כל כך לומדים את ה-Defיוז'ן מודל ‫על הלאטנט.
[02:16:03 - 02:16:07] ‫עכשיו, אם רוצים לייצר דגימה ‫מהמודל המשולב הזה, אז
[02:16:08 - 02:16:09] ‫מגיעים מהגאוסייה מזל,
[02:16:09 - 02:16:17] ‫מייצרים ממנו גבעיים ‫לג'אבין דיינאמיקס, ‫עד שמגיעים ל-P של Z0, ‫שזה הפריור של ה-VaE,
[02:16:18 - 02:16:19] ‫דוגמים ממנו,
[02:16:19 - 02:16:21] ‫ואז מריצים את הדיקודר והמוצבים.
[02:16:24 - 02:16:27] ‫ויש עוד כל מיני דרכים ‫לשלב כל מיני דברים.
[02:16:28 - 02:16:32] ‫אז אל תחשבו שכל המודלים האלה ‫הם ממש משהו נפרד בפני עצמו, ‫אפשר לעשות כל מיני שילובים.
[02:16:32 - 02:16:41] ‫עוד משהו שלא דיברנו עליו וקצת חבל, ‫אם היה לי...
[02:16:41 - 02:16:46] ‫אני חושב שבשנה הבאה ‫כשיהיה עוד שיעור, ‫אז אולי כן נכניס את זה,
[02:16:47 - 02:16:50] ‫זה מודלי שפה.
[02:16:56 - 02:17:01] שוב, אפשר לחשוב על זה ‫בתור פשוט מודל הסתברותי ‫של טקסט במקום של תמונות.
[02:17:02 - 02:17:05] ‫מבחינתכם, בעצם הדבר הכי קרוב ‫זה פיקסל CNN,
[02:17:05 - 02:17:07] ‫מימשתם אותו, אתם יודעים איך הוא עובד,
[02:17:07 - 02:17:10] ‫מימשתם אותו על פיקסלים מסקלטים,
[02:17:11 - 02:17:14] ‫נכון? בעצם היה כל פעם ‫הרד חוץ להתפלגות
[02:17:14 - 02:17:16] של 250 שנה האפשרויות.
[02:17:17 - 02:17:21] ‫אז תחשבו על פיקסל CNN ‫שבמקום על פיקסלים, ‫או ממש על מילים,
[02:17:22 - 02:17:25] ‫במקום 256 יש שם את גודל המיליון,
[02:17:25 - 02:17:26] ‫כל המילים האפשרויות,
[02:17:27 - 02:17:30] ‫וגם במקום להשתמש בקונבולוציות,
[02:17:32 - 02:17:33] ‫זאת אומרת שכל,
[02:17:34 - 02:17:42] ‫זה לא בדיוק קונבולוציות, ‫זה מסק קונבולוציות זה היה נכון? ‫כל פיקסל היה תלוי רק בפיקסלים ‫שהיו קצת לידו.
[02:17:44 - 02:17:50] ‫אפשר להשתמש גם בקונבולוציות בשפה, ‫זאת אומרת שכל מילה תהיה תלויה ‫רק במילים שקורידות בידיים,
[02:17:51 - 02:17:53] ‫אבל בשפה יותר חשוב,
[02:17:53 - 02:17:56] ‫עכשיו אפילו עוד יותר חשוב ‫בתמונות להסתכל לגמרי בחוק,
[02:17:57 - 02:18:02] ‫ולכן השיפור הגדול הגיע ‫כשהשתמשו בטרנספורמר.
[02:18:03 - 02:18:04] ‫טרנספורמר זה מאוד דרג שלאי.
[02:18:05 - 02:18:06] ‫למדתם את זה לדבר בלתי?
[02:18:07 - 02:18:12] ‫אז בגדול אפשר לחשוב על זה ‫שבמקום להסתכל תמיד ‫רק על הפיקסלים השכנים,
[02:18:12 - 02:18:17] ‫בכל רגע, כל פיקסל יכול לבחור ‫על איזה פיקסלים זה מסתכל.
[02:18:19 - 02:18:21] ‫אז יש כמה פורמולציות של זה,
[02:18:21 - 02:18:26] ‫אחת זה שזה גם אוטו-רגרסיב, ‫בדיוק כמו פיקסל CNN, זאת אומרת שכל מילה יכולה להסתכל ‫רק על כל המילים,
[02:18:26 - 02:18:28] ‫שהיו קודם,
[02:18:29 - 02:18:31] ‫ולעשות אטנשן ‫רק על המילים שהיו קודם.
[02:18:36 - 02:18:42] ‫כן, זה בעצם מה שיש פה אולי שפה, אוקיי? ‫אז מבחינה קונספטואלית,
[02:18:42 - 02:18:45] ‫אתם אמורים להבין ‫את המודל ההסתברותי שיש פה
[02:18:46 - 02:18:47] בלנדוויג' מודל.
[02:18:48 - 02:18:51] ‫רק רציתי להגיד שהטרנספורמר ‫גם משתמשים בו לפעמים במודלי תמונה.
[02:18:52 - 02:18:59] ‫גם אפשר לעשות סוג של מודל ‫אוטו-רגרסיב כזה על תמונות,
[02:19:00 - 02:19:02] ‫כשיש להסתכל על טרנספורמר ‫כמו קונבולוציות.
[02:19:03 - 02:19:08] ‫גם למשל בדיפיוז'ן, מודל ביונט, ‫הרבה פעמים מה שעושים ‫זה בשכבות הפנימיות,
[02:19:09 - 02:19:11] ‫ששם הדברים כבר קצת יותר קטנים,
[02:19:11 - 02:19:19] ‫עושים אטנשן. ‫בעצם יש שם איזשהו טרנספורמר בפנים, ‫שמחלק את המרחב הזה ‫לכמה אזורים, כמה פייצ'ינג כאלה,
[02:19:19 - 02:19:23] ‫ועושה אטנשן ביניהם ‫במקום לעשות סוגונבולוציות.
[02:19:23 - 02:19:32] ‫יש הזדמנות לאיזשהו עיבוד מידע יותר מורכב ‫שקורה כזה ברמה הגלובלית של התמונה,
[02:19:33 - 02:19:36] ‫ואז זה חוזר להיות החלק השני של היונט.
[02:19:37 - 02:19:41] ‫אז משתמשים בהחלט בטרנספורמר ‫גם בתמונות.
[02:19:41 - 02:19:43] ‫יש גם מודל דיפיריון ‫שהוא רק טרנספורמר,
[02:19:45 - 02:19:45] ‫והיא יונט.
[02:19:46 - 02:19:54] ‫לפקח את הפיקסלים או פאצ'ים בתמונה ‫וצער עם איזשהו עיבוד, ‫והאאוטפוט יוצא, אמור להיות הרעש של התמונות.
[02:20:02 - 02:20:09] ‫אז זהו, רק רציתי לפני שאנחנו מסיימים, ‫להגיד שבעצם אני עוסק במחקר ‫במקום הזה, ויש
[02:20:10 - 02:20:13] הרבה מחקר שמתעסק בתחום הזה ‫של מודלני דינטיביים.
[02:20:14 - 02:20:17] ‫הדפתי פה על הדברים שאני מתעסק בהם יותר, ‫אם זה מעניין מישהו.
[02:20:18 - 02:20:24] ‫אז אחד זה שיפור המודלים הקיימים, ‫כדי שהם יהיו יותר יעילים,
[02:20:26 - 02:20:33] ‫וגם נתעסק קצת בדברים ‫שקשורים למידול יותר נכון ‫של האי-ודאות,
[02:20:33 - 02:20:35] ‫לא כל כך דיברנו עליו,
[02:20:37 - 02:20:40] ‫אבל יש מחקר בעצם לשפר קצת את המודלים.
[02:20:40 - 02:20:42] ‫גם המודלים האלה שכבר עובדים די מדהים,
[02:20:43 - 02:20:49] ‫בעצם הם צריכים הרבה דאטה, ‫והם הרבה פעמים מאוד רגישים ‫לכל מיני שינויים קטנים שקורים בהתפלגויות,
[02:20:50 - 02:20:53] ‫ויש מקום לעבוד על שיפור ‫של המודלים האלה
[02:20:54 - 02:21:00] בצורה כזאת שבבנת דאטה, ‫או רק להציג איזשהו שינוי קטן ‫שקרה בדאטה,
[02:21:00 - 02:21:03] ‫יאפשר לאמן את המודלים בצורה יעילה.
[02:21:05 - 02:21:09] ‫יש מחקר בתחום של איך להשתמש ‫בכל השיטות האלה,
[02:21:10 - 02:21:15] ‫בכל מיני דומיינס חדשים ‫שלא השתמשו בהם לפני.
[02:21:16 - 02:21:18] ‫יש שם תלת מימד זה משהו ‫שאני מתעסק בו הרבה,
[02:21:19 - 02:21:25] ‫גם בהקשר של נדמה של מולקולות, ‫זאת דברים כאלה, ‫זה גם חשוב לשלת מימד.
[02:21:26 - 02:21:33] ‫ויש פשוט להשתמש בזה ‫לכל מיני אפליקציות אמיתיות, ‫ששם זה גם לפעמים דומיין חדש,
[02:21:33 - 02:21:38] ‫גם לפעמים מתעוררות בעיות ‫שהן מאוד ספציפיות, ‫שקשורות על מגבלות על דאטה, ‫אלהם כאלה ש...
[02:21:39 - 02:21:42] ‫ומעניין לראות איך הדברים האלה עובדים, ‫ספציפית,
[02:21:42 - 02:21:49] ‫היא מתעסקת בבנים שקשורים ‫לתמונות מתחת למים, ‫שצריך לנקות שם פעם להקשיב,
[02:21:49 - 02:21:53] ‫אולי למצוא את המבנה התלת-מרמדית שם ‫של דברים מתחת למים,
[02:21:53 - 02:21:55] ‫וגם דברים שקשורים
[02:21:59 - 02:22:00] ‫לדיזיין של חלבונית.
[02:22:02 - 02:22:04] שוב, זה קשור למבנה התלת-מרמדית ‫של המולקולות,
[02:22:04 - 02:22:07] ‫לנסות להבין את זה ‫לתוך אבזורבציות מאוד מאוד רועשות.
[02:22:08 - 02:22:11] ‫אז בעצם אין דאטה נקי שאתה רוצה,
[02:22:11 - 02:22:13] ‫שאתה יכול לאמן עליהם, ‫שאתה יכול לאמן דאטה רועש.
[02:22:14 - 02:22:17] ‫יש כל מיני שאלות כאלה, ‫מי שמעניין אותו מחקר ‫מוזמן לבוא ולמדבר.
[02:22:27 - 02:22:28] אז זהו, אני רק רוצה להגיד ‫לפני סיום שני דברים.
[02:22:29 - 02:22:29] ‫אחד,
[02:22:29 - 02:22:30] אז יש תרגיל
[02:22:32 - 02:22:34] ‫הרגיל על דיפיוז'ן מודל,
[02:22:35 - 02:22:38] ‫תצטרכו לעשות בו גם פוסטריו סאמפלינג.
[02:22:39 - 02:22:41] ‫אני מקווה שזה יהיה ברור.
[02:22:42 - 02:22:46] ‫אני מקווה ש... כן, בימים הקרובים ‫נפרסם גם מחר או בינתיים,
[02:22:48 - 02:22:49] ‫תסתכלו על ההוראות.
[02:22:51 - 02:22:54] ‫ודבר שני, זה קורס שניתן אותו פעם ראשונה,
[02:22:54 - 02:22:58] ‫אז אני אשמח לקבל פידבק. ‫אני חושב שקיבלתם איזשהו...
[02:23:00 - 02:23:01] את הראשון,
[02:23:01 - 02:23:06] ‫אז מי שיכול, אני מאוד מרצה ‫למלא את זה. ‫במיוחד את הטקסט.
[02:23:07 - 02:23:10] ‫תכתבו מה עבד טוב, מה עבד פחות טוב,
[02:23:10 - 02:23:13] ‫יש כאלה הצעות לדברים שאפשר לשפר.
[02:23:14 - 02:23:14] ‫נשמח לשמוע.
[02:23:15 - 02:23:16] ‫זהו, תודה רבה.
[02:23:31 - 02:23:32] ‫-תודה רבה.